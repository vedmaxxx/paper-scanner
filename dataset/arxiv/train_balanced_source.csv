text,label
"Schedule Earth Observation satellites with Deep Reinforcement Learning. Optical Earth observation satellites acquire images worldwide , covering up
to several million square kilometers every day. The complexity of scheduling
acquisitions for such systems increases exponentially when considering the
interoperabil-ity of several satellite constellations together with the
uncertainties from weather forecasts. In order to deliver valid images to
customers as fast as possible, it is crucial to acquire cloud-free images.
Depending on weather forecasts, up to 50% of images acquired by operational
satellites can be trashed due to excessive cloud covers, showing there is room
for improvement. We propose an acquisition scheduling approach based on Deep
Reinforcement Learning and experiment on a simplified environment. We find that
it challenges classical methods relying on human-expert heuristic.","cs.AI,cs.LG"
"Learning Implicit Credit Assignment for Cooperative Multi-Agent Reinforcement Learning. We present a multi-agent actor-critic method that aims to implicitly address
the credit assignment problem under fully cooperative settings. Our key
motivation is that credit assignment among agents may not require an explicit
formulation as long as (1) the policy gradients derived from a centralized
critic carry sufficient information for the decentralized agents to maximize
their joint action value through optimal cooperation and (2) a sustained level
of exploration is enforced throughout training. Under the centralized training
with decentralized execution (CTDE) paradigm, we achieve the former by
formulating the centralized critic as a hypernetwork such that a latent state
representation is integrated into the policy gradients through its
multiplicative association with the stochastic policies; to achieve the latter,
we derive a simple technique called adaptive entropy regularization where
magnitudes of the entropy gradients are dynamically rescaled based on the
current policy stochasticity to encourage consistent levels of exploration. Our
algorithm, referred to as LICA, is evaluated on several benchmarks including
the multi-agent particle environments and a set of challenging StarCraft II
micromanagement tasks, and we show that LICA significantly outperforms previous
methods.","cs.LG,cs.MA,stat.ML"
"Spatio-spectral networks for color-texture analysis. Texture is one of the most-studied visual attribute for image
characterization since the 1960s. However, most hand-crafted descriptors are
monochromatic, focusing on the gray scale images and discarding the color
information. In this context, this work focus on a new method for color texture
analysis considering all color channels in a more intrinsic approach. Our
proposal consists of modeling color images as directed complex networks that we
named Spatio-Spectral Network (SSN). Its topology includes within-channel edges
that cover spatial patterns throughout individual image color channels, while
between-channel edges tackle spectral properties of channel pairs in an
opponent fashion. Image descriptors are obtained through a concise topological
characterization of the modeled network in a multiscale approach with radially
symmetric neighborhoods. Experiments with four datasets cover several aspects
of color-texture analysis, and results demonstrate that SSN overcomes all the
compared literature methods, including known deep convolutional networks, and
also has the most stable performance between datasets, achieving $98.5(\pm1.1)$
of average accuracy against $97.1(\pm1.3)$ of MCND and $96.8(\pm3.2)$ of
AlexNet. Additionally, an experiment verifies the performance of the methods
under different color spaces, where results show that SSN also has higher
performance and robustness.","cs.CV,cs.LG,physics.data-an"
"Symmetry Detection of Occluded Point Cloud Using Deep Learning. Symmetry detection has been a classical problem in computer graphics, many of
which using traditional geometric methods. In recent years, however, we have
witnessed the arising deep learning changed the landscape of computer graphics.
In this paper, we aim to solve the symmetry detection of the occluded point
cloud in a deep-learning fashion. To the best of our knowledge, we are the
first to utilize deep learning to tackle such a problem. In such a deep
learning framework, double supervisions: points on the symmetry plane and
normal vectors are employed to help us pinpoint the symmetry plane. We
conducted experiments on the YCB- video dataset and demonstrate the efficacy of
our method.","cs.CV,cs.GR"
"Missing Slice Recovery for Tensors Using a Low-rank Model in Embedded Space. Let us consider a case where all of the elements in some continuous slices
are missing in tensor data.
  In this case, the nuclear-norm and total variation regularization methods
usually fail to recover the missing elements.
  The key problem is capturing some delay/shift-invariant structure.
  In this study, we consider a low-rank model in an embedded space of a tensor.
  For this purpose, we extend a delay embedding for a time series to a
""multi-way delay-embedding transform"" for a tensor, which takes a given
incomplete tensor as the input and outputs a higher-order incomplete Hankel
tensor.
  The higher-order tensor is then recovered by Tucker-based low-rank tensor
factorization.
  Finally, an estimated tensor can be obtained by using the inverse multi-way
delay embedding transform of the recovered higher-order tensor.
  Our experiments showed that the proposed method successfully recovered
missing slices for some color images and functional magnetic resonance images.","cs.CV,cs.DS"
"Identifying individual facial expressions by deconstructing a neural network. This paper focuses on the problem of explaining predictions of psychological
attributes such as attractiveness, happiness, confidence and intelligence from
face photographs using deep neural networks. Since psychological attribute
datasets typically suffer from small sample sizes, we apply transfer learning
with two base models to avoid overfitting. These models were trained on an age
and gender prediction task, respectively. Using a novel explanation method we
extract heatmaps that highlight the parts of the image most responsible for the
prediction. We further observe that the explanation method provides important
insights into the nature of features of the base model, which allow one to
assess the aptitude of the base model for a given transfer learning task.
Finally, we observe that the multiclass model is more feature rich than its
binary counterpart. The experimental evaluation is performed on the 2222 images
from the 10k US faces dataset containing psychological attribute labels as well
as on a subset of KDEF images.","cs.CV,cs.NE,stat.ML"
"Learning deep features for source color laser printer identification based on cascaded learning. Color laser printers have fast printing speed and high resolution, and
forgeries using color laser printers can cause significant harm to society. A
source printer identification technique can be employed as a countermeasure to
those forgeries. This paper presents a color laser printer identification
method based on cascaded learning of deep neural networks. The refiner network
is trained by adversarial training to refine the synthetic dataset for halftone
color decomposition. The halftone color decomposing ConvNet is trained with the
refined dataset, and the trained knowledge is transferred to the printer
identifying ConvNet to enhance the accuracy. The robustness about rotation and
scaling is considered in training process, which is not considered in existing
methods. Experiments are performed on eight color laser printers, and the
performance is compared with several existing methods. The experimental results
clearly show that the proposed method outperforms existing source color laser
printer identification methods.","cs.CR,cs.CV"
"Distributed Value Function Approximation for Collaborative Multi-Agent Reinforcement Learning. In this paper we propose several novel distributed gradient-based temporal
difference algorithms for multi-agent off-policy learning of linear
approximation of the value function in Markov decision processes with strict
information structure constraints, limiting inter-agent communications to small
neighborhoods. The algorithms are composed of: 1) local parameter updates based
on single-agent off-policy gradient temporal difference learning algorithms,
including eligibility traces with state dependent parameters, and 2) linear
stochastic time varying consensus schemes, represented by directed graphs. The
proposed algorithms differ by their form, definition of eligibility traces,
selection of time scales and the way of incorporating consensus iterations. The
main contribution of the paper is a convergence analysis based on the general
properties of the underlying Feller-Markov processes and the stochastic time
varying consensus model. We prove, under general assumptions, that the
parameter estimates generated by all the proposed algorithms weakly converge to
the corresponding ordinary differential equations (ODE) with precisely defined
invariant sets. It is demonstrated how the adopted methodology can be applied
to temporal-difference algorithms under weaker information structure
constraints. The variance reduction effect of the proposed algorithms is
demonstrated by formulating and analyzing an asymptotic stochastic differential
equation. Specific guidelines for communication network design are provided.
The algorithms' superior properties are illustrated by characteristic
simulation results.","cs.DC,cs.LG,cs.SY,eess.SY,stat.ML"
"Loss-analysis via Attention-scale for Physiologic Time Series. Physiologic signals have properties across multiple spatial and temporal
scales, which can be shown by the complexity-analysis of the coarse-grained
physiologic signals by scaling techniques such as the multiscale.
Unfortunately, the results obtained from the coarse-grained signals by the
multiscale may not fully reflect the properties of the original signals because
there is a loss caused by scaling techniques and the same scaling technique may
bring different losses to different signals. Another problem is that multiscale
does not consider the key observations inherent in the signal. Here, we show a
new analysis method for time series called the loss-analysis via
attention-scale. We show that multiscale is a special case of attention-scale.
The loss-analysis can complement to the complexity-analysis to capture aspects
of the signals that are not captured using previously developed measures. This
can be used to study ageing, diseases, and other physiologic phenomenon.","cs.LG,eess.SP,physics.data-an"
"Cautious Reinforcement Learning with Logical Constraints. This paper presents the concept of an adaptive safe padding that forces
Reinforcement Learning (RL) to synthesise optimal control policies while
ensuring safety during the learning process. Policies are synthesised to
satisfy a goal, expressed as a temporal logic formula, with maximal
probability. Enforcing the RL agent to stay safe during learning might limit
the exploration, however we show that the proposed architecture is able to
automatically handle the trade-off between efficient progress in exploration
(towards goal satisfaction) and ensuring safety. Theoretical guarantees are
available on the optimality of the synthesised policies and on the convergence
of the learning algorithm. Experimental results are provided to showcase the
performance of the proposed method.","cs.AI,cs.LG,cs.LO,cs.SY,eess.SY,stat.ML"
"E-RNN: Design Optimization for Efficient Recurrent Neural Networks in FPGAs. Recurrent Neural Networks (RNNs) are becoming increasingly important for time
series-related applications which require efficient and real-time
implementations. The two major types are Long Short-Term Memory (LSTM) and
Gated Recurrent Unit (GRU) networks. It is a challenging task to have
real-time, efficient, and accurate hardware RNN implementations because of the
high sensitivity to imprecision accumulation and the requirement of special
activation function implementations.
  A key limitation of the prior works is the lack of a systematic design
optimization framework of RNN model and hardware implementations, especially
when the block size (or compression ratio) should be jointly optimized with RNN
type, layer size, etc. In this paper, we adopt the block-circulant matrix-based
framework, and present the Efficient RNN (E-RNN) framework for FPGA
implementations of the Automatic Speech Recognition (ASR) application. The
overall goal is to improve performance/energy efficiency under accuracy
requirement. We use the alternating direction method of multipliers (ADMM)
technique for more accurate block-circulant training, and present two design
explorations providing guidance on block size and reducing RNN training trials.
Based on the two observations, we decompose E-RNN in two phases: Phase I on
determining RNN model to reduce computation and storage subject to accuracy
requirement, and Phase II on hardware implementations given RNN model,
including processing element design/optimization, quantization, activation
implementation, etc. Experimental results on actual FPGA deployments show that
E-RNN achieves a maximum energy efficiency improvement of 37.4$\times$ compared
with ESE, and more than 2$\times$ compared with C-LSTM, under the same
accuracy.","cs.CV,cs.LG,eess.SP"
"Quick Learner Automated Vehicle Adapting its Roadmanship to Varying Traffic Cultures with Meta Reinforcement Learning. It is essential for an automated vehicle in the field to perform
discretionary lane changes with appropriate roadmanship - driving safely and
efficiently without annoying or endangering other road users - under a wide
range of traffic cultures and driving conditions. While deep reinforcement
learning methods have excelled in recent years and been applied to automated
vehicle driving policy, there are concerns about their capability to quickly
adapt to unseen traffic with new environment dynamics. We formulate this
challenge as a multi-Markov Decision Processes (MDPs) adaptation problem and
developed Meta Reinforcement Learning (MRL) driving policies to showcase their
quick learning capability. Two types of distribution variation in environments
were designed and simulated to validate the fast adaptation capability of
resulting MRL driving policies which significantly outperform a baseline RL.","cs.LG,cs.SY,eess.SY"
"Content-based Analysis of the Cultural Differences between TikTok and Douyin. Short-form video social media shifts away from the traditional media paradigm
by telling the audience a dynamic story to attract their attention. In
particular, different combinations of everyday objects can be employed to
represent a unique scene that is both interesting and understandable. Offered
by the same company, TikTok and Douyin are popular examples of such new media
that has become popular in recent years, while being tailored for different
markets (e.g. the United States and China). The hypothesis that they express
cultural differences together with media fashion and social idiosyncrasy is the
primary target of our research. To that end, we first employ the Faster
Regional Convolutional Neural Network (Faster R-CNN) pre-trained with the
Microsoft Common Objects in COntext (MS-COCO) dataset to perform object
detection. Based on a suite of objects detected from videos, we perform
statistical analysis including label statistics, label similarity, and
label-person distribution. We further use the Two-Stream Inflated 3D ConvNet
(I3D) pre-trained with the Kinetics dataset to categorize and analyze human
actions. By comparing the distributional results of TikTok and Douyin, we
uncover a wealth of similarity and contrast between the two closely related
video social media platforms along the content dimensions of object quantity,
object categories, and human action categories.","cs.CV,cs.MM,cs.SI"
"Learning Multiple Belief Propagation Fixed Points for Real Time Inference. In the context of inference with expectation constraints, we propose an
approach based on the ""loopy belief propagation"" algorithm LBP, as a surrogate
to an exact Markov Random Field MRF modelling. A prior information composed of
correlations among a large set of N variables, is encoded into a graphical
model; this encoding is optimized with respect to an approximate decoding
procedure LBP, which is used to infer hidden variables from an observed subset.
We focus on the situation where the underlying data have many different
statistical components, representing a variety of independent patterns.
Considering a single parameter family of models we show how LBP may be used to
encode and decode efficiently such information, without solving the NP hard
inverse problem yielding the optimal MRF. Contrary to usual practice, we work
in the non-convex Bethe free energy minimization framework, and manage to
associate a belief propagation fixed point to each component of the underlying
probabilistic mixture. The mean field limit is considered and yields an exact
connection with the Hopfield model at finite temperature and steady state, when
the number of mixture components is proportional to the number of variables. In
addition, we provide an enhanced learning procedure, based on a straightforward
multi-parameter extension of the model in conjunction with an effective
continuous optimization procedure. This is performed using the stochastic
search heuristic CMAES and yields a significant improvement with respect to the
single parameter basic model.","cond-mat.dis-nn,cs.LG,physics.data-an"
"Federated Deep Reinforcement Learning. In deep reinforcement learning, building policies of high-quality is
challenging when the feature space of states is small and the training data is
limited. Despite the success of previous transfer learning approaches in deep
reinforcement learning, directly transferring data or models from an agent to
another agent is often not allowed due to the privacy of data and/or models in
many privacy-aware applications. In this paper, we propose a novel deep
reinforcement learning framework to federatively build models of high-quality
for agents with consideration of their privacies, namely Federated deep
Reinforcement Learning (FedRL). To protect the privacy of data and models, we
exploit Gausian differentials on the information shared with each other when
updating their local models. In the experiment, we evaluate our FedRL framework
in two diverse domains, Grid-world and Text2Action domains, by comparing to
various baselines.","cs.AI,cs.LG"
"On the Stability of Random Matrix Product with Markovian Noise: Application to Linear Stochastic Approximation and TD Learning. This paper studies the exponential stability of random matrix products driven
by a general (possibly unbounded) state space Markov chain. It is a cornerstone
in the analysis of stochastic algorithms in machine learning (e.g. for
parameter tracking in online learning or reinforcement learning). The existing
results impose strong conditions such as uniform boundedness of the
matrix-valued functions and uniform ergodicity of the Markov chains. Our main
contribution is an exponential stability result for the $p$-th moment of random
matrix product, provided that (i) the underlying Markov chain satisfies a
super-Lyapunov drift condition, (ii) the growth of the matrix-valued functions
is controlled by an appropriately defined function (related to the drift
condition). Using this result, we give finite-time $p$-th moment bounds for
constant and decreasing stepsize linear stochastic approximation schemes with
Markovian noise on general state space. We illustrate these findings for linear
value-function estimation in reinforcement learning. We provide finite-time
$p$-th moment bound for various members of temporal difference (TD) family of
algorithms.","cs.LG,math.PR,math.ST,stat.ML,stat.TH"
"Multi-scale Attributed Node Embedding. We present network embedding algorithms that capture information about a node
from the local distribution over node attributes around it, as observed over
random walks following an approach similar to Skip-gram. Observations from
neighborhoods of different sizes are either pooled (AE) or encoded distinctly
in a multi-scale approach (MUSAE). Capturing attribute-neighborhood
relationships over multiple scales is useful for a diverse range of
applications, including latent feature identification across disconnected
networks with similar attributes. We prove theoretically that matrices of
node-feature pointwise mutual information are implicitly factorized by the
embeddings. Experiments show that our algorithms are robust, computationally
efficient and outperform comparable models on social networks and web graphs.","cs.LG,cs.NI,cs.SI,stat.ML"
"Towards Expressive Graph Representation. Graph Neural Network (GNN) aggregates the neighborhood of each node into the
node embedding and shows its powerful capability for graph representation
learning. However, most existing GNN variants aggregate the neighborhood
information in a fixed non-injective fashion, which may map different graphs or
nodes to the same embedding, reducing the model expressiveness. We present a
theoretical framework to design a continuous injective set function for
neighborhood aggregation in GNN. Using the framework, we propose expressive GNN
that aggregates the neighborhood of each node with a continuous injective set
function, so that a GNN layer maps similar nodes with similar neighborhoods to
similar embeddings, different nodes to different embeddings and the equivalent
nodes or isomorphic graphs to the same embeddings. Moreover, the proposed
expressive GNN can naturally learn expressive representations for graphs with
continuous node attributes. We validate the proposed expressive GNN (ExpGNN)
for graph classification on multiple benchmark datasets including simple graphs
and attributed graphs. The experimental results demonstrate that our model
achieves state-of-the-art performances on most of the benchmarks.","cs.AI,cs.IT,cs.LG,math.IT"
"Attention Aware Cost Volume Pyramid Based Multi-view Stereo Network for 3D Reconstruction. We present an efficient multi-view stereo (MVS) network for 3D reconstruction
from multiview images. While previous learning based reconstruction approaches
performed quite well, most of them estimate depth maps at a fixed resolution
using plane sweep volumes with a fixed depth hypothesis at each plane, which
requires densely sampled planes for desired accuracy and therefore is difficult
to achieve high resolution depth maps. In this paper we introduce a
coarseto-fine depth inference strategy to achieve high resolution depth. This
strategy estimates the depth map at coarsest level, while the depth maps at
finer levels are considered as the upsampled depth map from previous level with
pixel-wise depth residual. Thus, we narrow the depth searching range with
priori information from previous level and construct new cost volumes from the
pixel-wise depth residual to perform depth map refinement. Then the final depth
map could be achieved iteratively since all the parameters are shared between
different levels. At each level, the self-attention layer is introduced to the
feature extraction block for capturing the long range dependencies for depth
inference task, and the cost volume is generated using similarity measurement
instead of the variance based methods used in previous work. Experiments were
conducted on both the DTU benchmark dataset and recently released BlendedMVS
dataset. The results demonstrated that our model could outperform most
state-of-the-arts (SOTA) methods. The codebase of this project is at
https://github.com/ArthasMil/AACVP-MVSNet.","cs.CV,cs.LG,eess.IV"
"On the emergence of simplex symmetry in the final and penultimate layers of neural network classifiers. A recent numerical study observed that neural network classifiers enjoy a
large degree of symmetry in the penultimate layer. Namely, if $h(x) = Af(x) +b$
where $A$ is a linear map and $f$ is the output of the penultimate layer of the
network (after activation), then all data points $x_{i, 1}, \dots, x_{i, N_i}$
in a class $C_i$ are mapped to a single point $y_i$ by $f$ and the points $y_i$
are located at the vertices of a regular $k-1$-dimensional standard simplex in
a high-dimensional Euclidean space.
  We explain this observation analytically in toy models for highly expressive
deep neural networks. In complementary examples, we demonstrate rigorously that
even the final output of the classifier $h$ is not uniform over data samples
from a class $C_i$ if $h$ is a shallow network (or if the deeper layers do not
bring the data samples into a convenient geometric configuration).","62H30,68T07,cs.LG,stat.ML"
"Learning Set Functions that are Sparse in Non-Orthogonal Fourier Bases. Many applications of machine learning on discrete domains, such as learning
preference functions in recommender systems or auctions, can be reduced to
estimating a set function that is sparse in the Fourier domain. In this work,
we present a new family of algorithms for learning Fourier-sparse set
functions. They require at most $nk - k \log_2 k + k$ queries (set function
evaluations), under mild conditions on the Fourier coefficients, where $n$ is
the size of the ground set and $k$ the number of non-zero Fourier coefficients.
In contrast to other work that focused on the orthogonal Walsh-Hadamard
transform, our novel algorithms operate with recently introduced non-orthogonal
Fourier transforms that offer different notions of Fourier-sparsity. These
naturally arise when modeling, e.g., sets of items forming substitutes and
complements. We demonstrate effectiveness on several real-world applications.","cs.AI,cs.DM,cs.LG,eess.SP,stat.ML"
"Transfer Learning Using Ensemble Neural Networks for Organic Solar Cell Screening. Organic Solar Cells are a promising technology for solving the clean energy
crisis in the world. However, generating candidate chemical compounds for solar
cells is a time-consuming process requiring thousands of hours of laboratory
analysis. For a solar cell, the most important property is the power conversion
efficiency which is dependent on the highest occupied molecular orbitals (HOMO)
values of the donor molecules. Recently, machine learning techniques have
proved to be very useful in building predictive models for HOMO values of donor
structures of Organic Photovoltaic Cells (OPVs). Since experimental datasets
are limited in size, current machine learning models are trained on data
derived from calculations based on density functional theory (DFT). Molecular
line notations such as SMILES or InChI are popular input representations for
describing the molecular structure of donor molecules. The two types of line
representations encode different information, such as SMILES defines the bond
types while InChi defines protonation. In this work, we present an ensemble
deep neural network architecture, called SINet, which harnesses both the SMILES
and InChI molecular representations to predict HOMO values and leverage the
potential of transfer learning from a sizeable DFT-computed dataset- Harvard
CEP to build more robust predictive models for relatively smaller HOPV
datasets. Harvard CEP dataset contains molecular structures and properties for
2.3 million candidate donor structures for OPV while HOPV contains DFT-computed
and experimental values of 350 and 243 molecules respectively. Our results
demonstrate significant performance improvement from the use of transfer
learning and leveraging both molecular representations.","cs.LG,physics.chem-ph,stat.ML"
"Plugin procedure in segmentation and application to hyperspectral image segmentation. In this article we give our contribution to the problem of segmentation with
plug-in procedures. We give general sufficient conditions under which plug in
procedure are efficient. We also give an algorithm that satisfy these
conditions. We give an application of the used algorithm to hyperspectral
images segmentation. Hyperspectral images are images that have both spatial and
spectral coherence with thousands of spectral bands on each pixel. In the
proposed procedure we combine a reduction dimension technique and a spatial
regularisation technique. This regularisation is based on the mixlet
modelisation of Kolaczyck and Al.","math.ST,stat.ML,stat.TH"
"Meta-Learning for Time Series Forecasting Ensemble. Amounts of historical data collected increase together with business
intelligence applicability and demands for automatic forecasting of time
series. While no single time series modeling method is universal to all types
of dynamics, forecasting using ensemble of several methods is often seen as a
compromise. Instead of fixing ensemble diversity and size we propose to
adaptively predict these aspects using meta-learning. Meta-learning here
considers two separate random forest regression models, built on 390 time
series features, to rank 22 univariate forecasting methods and to recommend
ensemble size. Forecasting ensemble is consequently formed from methods ranked
as the best and forecasts are pooled using either simple or weighted average
(with weight corresponding to reciprocal rank). Proposed approach was tested on
12561 micro-economic time series (expanded to 38633 for various forecasting
horizons) of M4 competition where meta-learning outperformed Theta and Comb
benchmarks by relative forecasting errors for all data types and horizons. Best
overall results were achieved by weighted pooling with symmetric mean absolute
percentage error of 9.21% versus 11.05% obtained using Theta method.","91B84,G.3; I.5,cs.LG,stat.ME,stat.ML"
"Spatial Attention as an Interface for Image Captioning Models. The internal workings of modern deep learning models stay often unclear to an
external observer, although spatial attention mechanisms are involved. The idea
of this work is to translate these spatial attentions into natural language to
provide a simpler access to the model's function. Thus, I took a neural image
captioning model and measured the reactions to external modification in its
spatial attention for three different interface methods: a fixation over the
whole generation process, a fixation for the first time-steps and an addition
to the generator's attention. The experimental results for bounding box based
spatial attention vectors have shown that the captioning model reacts to method
dependent changes in up to 52.65% and includes in 9.00% of the cases object
categories, which were otherwise unmentioned. Afterwards, I established such a
link to a hierarchical co-attention network for visual question answering by
extraction of its word, phrase and question level spatial attentions. Here,
generated captions for the word level included details of the question-answer
pairs in up to 55.20% of the cases. This work indicates that spatial attention
seen as an external interface for image caption generators is an useful method
to access visual functions in natural language.","cs.CL,cs.CV"
"Temporal Poisson Square Root Graphical Models. We propose temporal Poisson square root graphical models (TPSQRs), a
generalization of Poisson square root graphical models (PSQRs) specifically
designed for modeling longitudinal event data. By estimating the temporal
relationships for all possible pairs of event types, TPSQRs can offer a
holistic perspective about whether the occurrences of any given event type
could excite or inhibit any other type. A TPSQR is learned by estimating a
collection of interrelated PSQRs that share the same template parameterization.
These PSQRs are estimated jointly in a pseudo-likelihood fashion, where Poisson
pseudo-likelihood is used to approximate the original more
computationally-intensive pseudo-likelihood problem stemming from PSQRs.
Theoretically, we demonstrate that under mild assumptions, the Poisson
pseudo-likelihood approximation is sparsistent for recovering the underlying
PSQR. Empirically, we learn TPSQRs from Marshfield Clinic electronic health
records (EHRs) with millions of drug prescription and condition diagnosis
events, for adverse drug reaction (ADR) detection. Experimental results
demonstrate that the learned TPSQRs can recover ADR signals from the EHR
effectively and efficiently.","cs.LG,stat.ML"
"Hybrid Reasoning Network for Video-based Commonsense Captioning. The task of video-based commonsense captioning aims to generate event-wise
captions and meanwhile provide multiple commonsense descriptions (e.g.,
attribute, effect and intention) about the underlying event in the video. Prior
works explore the commonsense captions by using separate networks for different
commonsense types, which is time-consuming and lacks mining the interaction of
different commonsense. In this paper, we propose a Hybrid Reasoning Network
(HybridNet) to endow the neural networks with the capability of semantic-level
reasoning and word-level reasoning. Firstly, we develop multi-commonsense
learning for semantic-level reasoning by jointly training different commonsense
types in a unified network, which encourages the interaction between the clues
of multiple commonsense descriptions, event-wise captions and videos. Then,
there are two steps to achieve the word-level reasoning: (1) a memory module
records the history predicted sequence from the previous generation processes;
(2) a memory-routed multi-head attention (MMHA) module updates the word-level
attention maps by incorporating the history information from the memory module
into the transformer decoder for word-level reasoning. Moreover, the multimodal
features are used to make full use of diverse knowledge for commonsense
reasoning. Experiments and abundant analysis on the large-scale
Video-to-Commonsense benchmark show that our HybridNet achieves
state-of-the-art performance compared with other methods.","68T07,cs.CL,cs.CV"
"SAR: Learning Cross-Language API Mappings with Little Knowledge. To save manual effort, developers often translate programs from one
programming language to another, instead of implementing it from scratch.
Translating application program interfaces (APIs) used in one language to
functionally equivalent ones available in another language is an important
aspect of program translation. Existing approaches facilitate the translation
by automatically identifying the API mappings across programming languages.
However, all these approaches still require large amount of manual effort in
preparing parallel program corpora, ranging from pairs of APIs, to manually
identified code in different languages that are considered as functionally
equivalent. To minimize the manual effort in identifying parallel program
corpora and API mappings, this paper aims at an automated approach to map APIs
across languages with much less knowledge a priori needed than other existing
approaches. The approach is based on an realization of the notion of domain
adaption combined with code embedding, which can better align two vector
spaces: taking as input large sets of programs, our approach first generates
numeric vector representations of the programs, especially the APIs used in
each language, and it adapts generative adversarial networks (GAN) to align the
vectors from the spaces of two languages. For a better alignment, we initialize
the GAN with parameters derived from optional API mapping seeds that can be
identified accurately with a simple automatic signature-based matching
heuristic. Then the cross-language API mappings can be identified via
nearest-neighbors queries in the aligned vector spaces.","cs.LG,cs.SE,stat.ML"
"Performing Deep Recurrent Double Q-Learning for Atari Games. Currently, many applications in Machine Learning are based on define new
models to extract more information about data, In this case Deep Reinforcement
Learning with the most common application in video games like Atari, Mario, and
others causes an impact in how to computers can learning by himself with only
information called rewards obtained from any action. There is a lot of
algorithms modeled and implemented based on Deep Recurrent Q-Learning proposed
by DeepMind used in AlphaZero and Go. In this document, We proposed Deep
Recurrent Double Q-Learning that is an implementation of Deep Reinforcement
Learning using Double Q-Learning algorithms and Recurrent Networks like LSTM
and DRQN.","cs.AI,cs.LG,cs.NE,stat.ML"
"Deep Microlocal Reconstruction for Limited-Angle Tomography. We present a deep learning-based algorithm to jointly solve a reconstruction
problem and a wavefront set extraction problem in tomographic imaging. The
algorithm is based on a recently developed digital wavefront set extractor as
well as the well-known microlocal canonical relation for the Radon transform.
We use the wavefront set information about x-ray data to improve the
reconstruction by requiring that the underlying neural networks simultaneously
extract the correct ground truth wavefront set and ground truth image. As a
necessary theoretical step, we identify the digital microlocal canonical
relations for deep convolutional residual neural networks. We find strong
numerical evidence for the effectiveness of this approach.","35A18,65T60,68T10,cs.CV,cs.LG,cs.NA,math.FA,math.NA"
"Dealing with Non-Stationarity in Multi-Agent Deep Reinforcement Learning. Recent developments in deep reinforcement learning are concerned with
creating decision-making agents which can perform well in various complex
domains. A particular approach which has received increasing attention is
multi-agent reinforcement learning, in which multiple agents learn concurrently
to coordinate their actions. In such multi-agent environments, additional
learning problems arise due to the continually changing decision-making
policies of agents. This paper surveys recent works that address the
non-stationarity problem in multi-agent deep reinforcement learning. The
surveyed methods range from modifications in the training procedure, such as
centralized training, to learning representations of the opponent's policy,
meta-learning, communication, and decentralized learning. The survey concludes
with a list of open problems and possible lines of future research.","cs.AI,cs.LG,cs.MA,stat.ML"
"High-dimensional Graphical Model Search with gRapHD R Package. This paper presents the R package gRapHD for efficient selection of
high-dimensional undirected graphical models. The package provides tools for
selecting trees, forests and decomposable models minimizing information
criteria such as AIC or BIC, and for displaying the independence graphs of the
models. It has also some useful tools for analysing graphical structures. It
supports the use of discrete, continuous, or both types of variables
simultaneously.","stat.CO,stat.ML"
"And/or trade-off in artificial neurons: impact on adversarial robustness. Since its discovery in 2013, the phenomenon of adversarial examples has
attracted a growing amount of attention from the machine learning community. A
deeper understanding of the problem could lead to a better comprehension of how
information is processed and encoded in neural networks and, more in general,
could help to solve the issue of interpretability in machine learning. Our idea
to increase adversarial resilience starts with the observation that artificial
neurons can be divided in two broad categories: AND-like neurons and OR-like
neurons. Intuitively, the former are characterised by a relatively low number
of combinations of input values which trigger neuron activation, while for the
latter the opposite is true. Our hypothesis is that the presence in a network
of a sufficiently high number of OR-like neurons could lead to classification
""brittleness"" and increase the network's susceptibility to adversarial attacks.
After constructing an operational definition of a neuron AND-like behaviour, we
proceed to introduce several measures to increase the proportion of AND-like
neurons in the network: L1 norm weight normalisation; application of an input
filter; comparison between the neuron output's distribution obtained when the
network is fed with the actual data set and the distribution obtained when the
network is fed with a randomised version of the former called ""scrambled data
set"". Tests performed on the MNIST data set hint that the proposed measures
could represent an interesting direction to explore.","cs.AI,cs.CR,cs.LG"
"Manipulating Reinforcement Learning: Poisoning Attacks on Cost Signals. This chapter studies emerging cyber-attacks on reinforcement learning (RL)
and introduces a quantitative approach to analyze the vulnerabilities of RL.
Focusing on adversarial manipulation on the cost signals, we analyze the
performance degradation of TD($\lambda$) and $Q$-learning algorithms under the
manipulation. For TD($\lambda$), the approximation learned from the manipulated
costs has an approximation error bound proportional to the magnitude of the
attack. The effect of the adversarial attacks on the bound does not depend on
the choice of $\lambda$. In $Q$-learning, we show that $Q$-learning algorithms
converge under stealthy attacks and bounded falsifications on cost signals. We
characterize the relation between the falsified cost and the $Q$-factors as
well as the policy learned by the learning agent which provides fundamental
limits for feasible offensive and defensive moves. We propose a robust region
in terms of the cost within which the adversary can never achieve the targeted
policy. We provide conditions on the falsified cost which can mislead the agent
to learn an adversary's favored policy. A case study of TD($\lambda$) learning
is provided to corroborate the results.","cs.CR,cs.LG,cs.SY,eess.SY,math.OC,stat.ML"
"Working Memory Graphs. Transformers have increasingly outperformed gated RNNs in obtaining new
state-of-the-art results on supervised tasks involving text sequences. Inspired
by this trend, we study the question of how Transformer-based models can
improve the performance of sequential decision-making agents. We present the
Working Memory Graph (WMG), an agent that employs multi-head self-attention to
reason over a dynamic set of vectors representing observed and recurrent state.
We evaluate WMG in three environments featuring factored observation spaces: a
Pathfinding environment that requires complex reasoning over past observations,
BabyAI gridworld levels that involve variable goals, and Sokoban which
emphasizes future planning. We find that the combination of WMG's
Transformer-based architecture with factored observation spaces leads to
significant gains in learning efficiency compared to baseline architectures
across all tasks. WMG demonstrates how Transformer-based models can
dramatically boost sample efficiency in RL environments for which observations
can be factored.","cs.AI,cs.CL,cs.LG"
"Efficient Statistics for Sparse Graphical Models from Truncated Samples. In this paper, we study high-dimensional estimation from truncated samples.
We focus on two fundamental and classical problems: (i) inference of sparse
Gaussian graphical models and (ii) support recovery of sparse linear models.
  (i) For Gaussian graphical models, suppose $d$-dimensional samples ${\bf x}$
are generated from a Gaussian $N(\mu,\Sigma)$ and observed only if they belong
to a subset $S \subseteq \mathbb{R}^d$. We show that ${\mu}$ and ${\Sigma}$ can
be estimated with error $\epsilon$ in the Frobenius norm, using
$\tilde{O}\left(\frac{\textrm{nz}({\Sigma}^{-1})}{\epsilon^2}\right)$ samples
from a truncated $\mathcal{N}({\mu},{\Sigma})$ and having access to a
membership oracle for $S$. The set $S$ is assumed to have non-trivial measure
under the unknown distribution but is otherwise arbitrary.
  (ii) For sparse linear regression, suppose samples $({\bf x},y)$ are
generated where $y = {\bf x}^\top{{\Omega}^*} + \mathcal{N}(0,1)$ and $({\bf
x}, y)$ is seen only if $y$ belongs to a truncation set $S \subseteq
\mathbb{R}$. We consider the case that ${\Omega}^*$ is sparse with a support
set of size $k$. Our main result is to establish precise conditions on the
problem dimension $d$, the support size $k$, the number of observations $n$,
and properties of the samples and the truncation that are sufficient to recover
the support of ${\Omega}^*$. Specifically, we show that under some mild
assumptions, only $O(k^2 \log d)$ samples are needed to estimate ${\Omega}^*$
in the $\ell_\infty$-norm up to a bounded error.
  For both problems, our estimator minimizes the sum of the finite population
negative log-likelihood function and an $\ell_1$-regularization term.","cs.DS,cs.LG,math.ST,stat.CO,stat.ML,stat.TH"
"Scalable and Flexible Deep Bayesian Optimization with Auxiliary Information for Scientific Problems. Bayesian optimization (BO) is a popular paradigm for global optimization of
expensive black-box functions, but there are many domains where the function is
not completely black-box. The data may have some known structure, e.g.
symmetries, and the data generation process can yield useful intermediate or
auxiliary information in addition to the value of the optimization objective.
However, surrogate models traditionally employed in BO, such as Gaussian
Processes (GPs), scale poorly with dataset size and struggle to incorporate
known structure or auxiliary information. Instead, we propose performing BO on
complex, structured problems by using Bayesian Neural Networks (BNNs), a class
of scalable surrogate models that have the representation power and flexibility
to handle structured data and exploit auxiliary information. We demonstrate BO
on a number of realistic problems in physics and chemistry, including topology
optimization of photonic crystal materials using convolutional neural networks,
and chemical property optimization of molecules using graph neural networks. On
these complex tasks, we show that BNNs often outperform GPs as surrogate models
for BO in terms of both sampling efficiency and computational cost.","cs.LG,physics.app-ph,physics.chem-ph,physics.optics"
"Learning Unknown from Correlations: Graph Neural Network for Inter-novel-protein Interaction Prediction. The study of multi-type Protein-Protein Interaction (PPI) is fundamental for
understanding biological processes from a systematic perspective and revealing
disease mechanisms. Existing methods suffer from significant performance
degradation when tested in unseen dataset. In this paper, we investigate the
problem and find that it is mainly attributed to the poor performance for
inter-novel-protein interaction prediction. However, current evaluations
overlook the inter-novel-protein interactions, and thus fail to give an
instructive assessment. As a result, we propose to address the problem from
both the evaluation and the methodology. Firstly, we design a new evaluation
framework that fully respects the inter-novel-protein interactions and gives
consistent assessment across datasets. Secondly, we argue that correlations
between proteins must provide useful information for analysis of novel
proteins, and based on this, we propose a graph neural network based method
(GNN-PPI) for better inter-novel-protein interaction prediction. Experimental
results on real-world datasets of different scales demonstrate that GNN-PPI
significantly outperforms state-of-the-art PPI prediction methods, especially
for the inter-novel-protein interaction prediction.","cs.CE,cs.LG"
"Approximability of Discriminators Implies Diversity in GANs. While Generative Adversarial Networks (GANs) have empirically produced
impressive results on learning complex real-world distributions, recent works
have shown that they suffer from lack of diversity or mode collapse. The
theoretical work of Arora et al. suggests a dilemma about GANs' statistical
properties: powerful discriminators cause overfitting, whereas weak
discriminators cannot detect mode collapse.
  By contrast, we show in this paper that GANs can in principle learn
distributions in Wasserstein distance (or KL-divergence in many cases) with
polynomial sample complexity, if the discriminator class has strong
distinguishing power against the particular generator class (instead of against
all possible generators). For various generator classes such as mixture of
Gaussians, exponential families, and invertible and injective neural networks
generators, we design corresponding discriminators (which are often neural nets
of specific architectures) such that the Integral Probability Metric (IPM)
induced by the discriminators can provably approximate the Wasserstein distance
and/or KL-divergence. This implies that if the training is successful, then the
learned distribution is close to the true distribution in Wasserstein distance
or KL divergence, and thus cannot drop modes. Our preliminary experiments show
that on synthetic datasets the test IPM is well correlated with KL divergence
or the Wasserstein distance, indicating that the lack of diversity in GANs may
be caused by the sub-optimality in optimization instead of statistical
inefficiency.","cs.DS,cs.LG,stat.ML"
"How to Certify Machine Learning Based Safety-critical Systems? A Systematic Literature Review. Context: Machine Learning (ML) has been at the heart of many innovations over
the past years. However, including it in so-called 'safety-critical' systems
such as automotive or aeronautic has proven to be very challenging, since the
shift in paradigm that ML brings completely changes traditional certification
approaches.
  Objective: This paper aims to elucidate challenges related to the
certification of ML-based safety-critical systems, as well as the solutions
that are proposed in the literature to tackle them, answering the question 'How
to Certify Machine Learning Based Safety-critical Systems?'.
  Method: We conduct a Systematic Literature Review (SLR) of research papers
published between 2015 to 2020, covering topics related to the certification of
ML systems. In total, we identified 217 papers covering topics considered to be
the main pillars of ML certification: Robustness, Uncertainty, Explainability,
Verification, Safe Reinforcement Learning, and Direct Certification. We
analyzed the main trends and problems of each sub-field and provided summaries
of the papers extracted.
  Results: The SLR results highlighted the enthusiasm of the community for this
subject, as well as the lack of diversity in terms of datasets and type of
models. It also emphasized the need to further develop connections between
academia and industries to deepen the domain study. Finally, it also
illustrated the necessity to build connections between the above mention main
pillars that are for now mainly studied separately.
  Conclusion: We highlighted current efforts deployed to enable the
certification of ML based software systems, and discuss some future research
directions.","cs.LG,cs.SE"
"Auto Deep Compression by Reinforcement Learning Based Actor-Critic Structure. Model-based compression is an effective, facilitating, and expanded model of
neural network models with limited computing and low power. However,
conventional models of compression techniques utilize crafted features [2,3,12]
and explore specialized areas for exploration and design of large spaces in
terms of size, speed, and accuracy, which usually have returns Less and time is
up. This paper will effectively analyze deep auto compression (ADC) and
reinforcement learning strength in an effective sample and space design, and
improve the compression quality of the model. The results of compression of the
advanced model are obtained without any human effort and in a completely
automated way. With a 4- fold reduction in FLOP, the accuracy of 2.8% is higher
than the manual compression model for VGG-16 in ImageNet.","cs.AI,cs.LG,stat.ML"
"Backdoor Attacks against Transfer Learning with Pre-trained Deep Learning Models. Transfer learning provides an effective solution for feasibly and fast
customize accurate \textit{Student} models, by transferring the learned
knowledge of pre-trained \textit{Teacher} models over large datasets via
fine-tuning. Many pre-trained Teacher models used in transfer learning are
publicly available and maintained by public platforms, increasing their
vulnerability to backdoor attacks. In this paper, we demonstrate a backdoor
threat to transfer learning tasks on both image and time-series data leveraging
the knowledge of publicly accessible Teacher models, aimed at defeating three
commonly-adopted defenses: \textit{pruning-based}, \textit{retraining-based}
and \textit{input pre-processing-based defenses}. Specifically, (A)
ranking-based selection mechanism to speed up the backdoor trigger generation
and perturbation process while defeating \textit{pruning-based} and/or
\textit{retraining-based defenses}. (B) autoencoder-powered trigger generation
is proposed to produce a robust trigger that can defeat the \textit{input
pre-processing-based defense}, while guaranteeing that selected neuron(s) can
be significantly activated. (C) defense-aware retraining to generate the
manipulated model using reverse-engineered model inputs.
  We launch effective misclassification attacks on Student models over
real-world images, brain Magnetic Resonance Imaging (MRI) data and
Electrocardiography (ECG) learning systems. The experiments reveal that our
enhanced attack can maintain the $98.4\%$ and $97.2\%$ classification accuracy
as the genuine model on clean image and time series inputs respectively while
improving $27.9\%-100\%$ and $27.1\%-56.1\%$ attack success rate on trojaned
image and time series inputs respectively in the presence of pruning-based
and/or retraining-based defenses.","cs.CR,cs.LG"
"Discriminative Multi-modality Speech Recognition. Vision is often used as a complementary modality for audio speech recognition
(ASR), especially in the noisy environment where performance of solo audio
modality significantly deteriorates. After combining visual modality, ASR is
upgraded to the multi-modality speech recognition (MSR). In this paper, we
propose a two-stage speech recognition model. In the first stage, the target
voice is separated from background noises with help from the corresponding
visual information of lip movements, making the model 'listen' clearly. At the
second stage, the audio modality combines visual modality again to better
understand the speech by a MSR sub-network, further improving the recognition
rate. There are some other key contributions: we introduce a pseudo-3D residual
convolution (P3D)-based visual front-end to extract more discriminative
features; we upgrade the temporal convolution block from 1D ResNet with the
temporal convolutional network (TCN), which is more suitable for the temporal
tasks; the MSR sub-network is built on the top of Element-wise-Attention Gated
Recurrent Unit (EleAtt-GRU), which is more effective than Transformer in long
sequences. We conducted extensive experiments on the LRS3-TED and the LRW
datasets. Our two-stage model (audio enhanced multi-modality speech
recognition, AE-MSR) consistently achieves the state-of-the-art performance by
a significant margin, which demonstrates the necessity and effectiveness of
AE-MSR.","cs.CL,cs.CV,cs.SD,eess.AS,eess.IV"
"Improving Text to Image Generation using Mode-seeking Function. Generative Adversarial Networks (GANs) have long been used to understand the
semantic relationship between the text and image. However, there are problems
with mode collapsing in the image generation that causes some preferred output
modes. Our aim is to improve the training of the network by using a specialized
mode-seeking loss function to avoid this issue. In the text to image synthesis,
our loss function differentiates two points in latent space for the generation
of distinct images. We validate our model on the Caltech Birds (CUB) dataset
and the Microsoft COCO dataset by changing the intensity of the loss function
during the training. Experimental results demonstrate that our model works very
well compared to some state-of-the-art approaches.","cs.CV,eess.IV"
"The Geometry of Over-parameterized Regression and Adversarial Perturbations. Classical regression has a simple geometric description in terms of a
projection of the training labels onto the column space of the design matrix.
However, for over-parameterized models -- where the number of fit parameters is
large enough to perfectly fit the training data -- this picture becomes
uninformative. Here, we present an alternative geometric interpretation of
regression that applies to both under- and over-parameterized models. Unlike
the classical picture which takes place in the space of training labels, our
new picture resides in the space of input features. This new feature-based
perspective provides a natural geometric interpretation of the double-descent
phenomenon in the context of bias and variance, explaining why it can occur
even in the absence of label noise. Furthermore, we show that adversarial
perturbations -- small perturbations to the input features that result in large
changes in label values -- are a generic feature of biased models, arising from
the underlying geometry. We demonstrate these ideas by analyzing three minimal
models for over-parameterized linear least squares regression: without basis
functions (input features equal model features) and with linear or nonlinear
basis functions (two-layer neural networks with linear or nonlinear activation
functions, respectively).","cond-mat.dis-nn,cs.LG,stat.ML"
"Method and Dataset Mining in Scientific Papers. Literature analysis facilitates researchers better understanding the
development of science and technology. The conventional literature analysis
focuses on the topics, authors, abstracts, keywords, references, etc., and
rarely pays attention to the content of papers. In the field of machine
learning, the involved methods (M) and datasets (D) are key information in
papers. The extraction and mining of M and D are useful for discipline analysis
and algorithm recommendation. In this paper, we propose a novel entity
recognition model, called MDER, and constructe datasets from the papers of the
PAKDD conferences (2009-2019). Some preliminary experiments are conducted to
assess the extraction performance and the mining results are visualized.","cs.CL,cs.IR,cs.LG,stat.ML"
"Neural Enhanced Belief Propagation for Cooperative Localization. Location-aware networks will introduce innovative services and applications
for modern convenience, applied ocean sciences, and public safety. In this
paper, we establish a hybrid method for model-based and data-driven inference.
We consider a cooperative localization (CL) scenario where the mobile agents in
a wireless network aim to localize themselves by performing pairwise
observations with other agents and by exchanging location information. A
traditional method for distributed CL in large agent networks is belief
propagation (BP) which is completely model-based and is known to suffer from
providing inconsistent (overconfident) estimates. The proposed approach
addresses these limitations by complementing BP with learned information
provided by a graph neural network (GNN). We demonstrate numerically that our
method can improve estimation accuracy and avoid overconfident beliefs, while
its computational complexity remains comparable to BP. Notably, more consistent
beliefs are obtained by not explicitly addressing overconfidence in the loss
function used for training of the GNN.","cs.LG,cs.MA,cs.RO,eess.SP"
"A flexible outlier detector based on a topology given by graph communities. Outlier, or anomaly, detection is essential for optimal performance of
machine learning methods and statistical predictive models. It is not just a
technical step in a data cleaning process but a key topic in many fields such
as fraudulent document detection, in medical applications and assisted
diagnosis systems or detecting security threats. In contrast to
population-based methods, neighborhood based local approaches are simple
flexible methods that have the potential to perform well in small sample size
unbalanced problems. However, a main concern of local approaches is the impact
that the computation of each sample neighborhood has on the method performance.
Most approaches use a distance in the feature space to define a single
neighborhood that requires careful selection of several parameters. This work
presents a local approach based on a local measure of the heterogeneity of
sample labels in the feature space considered as a topological manifold.
Topology is computed using the communities of a weighted graph codifying mutual
nearest neighbors in the feature space. This way, we provide with a set of
multiple neighborhoods able to describe the structure of complex spaces without
parameter fine tuning. The extensive experiments on real-world data sets show
that our approach overall outperforms, both, local and global strategies in
multi and single view settings.","cs.LG,cs.SI,stat.ML"
"Deep Moment Matching Kernel for Multi-source Gaussian Processes. Human learners have the ability to solve new tasks efficiently if previous
knowledge is relevant, which has motivated research into few-shot learning and
transfer learning. We formalize the integration of relevant knowledge as
multi-source regression in which the target function is inferred using Gaussian
Process (GP) with the deep moment matching (DMM) kernel. We obtain a
non-stationary DMM kernel from prior relevant data by analytically calculating
the covariance of the target function. We interpret the data-informed DMM
kernel, which serves as prior for target function, as: (1) a refined similarity
determined by squared distance in the latent space and (2) as propagating
uncertainty measured in RKHS defined by the posterior covariance from the prior
learning. In comparison with the autoregressive models, variational DGP models
and others, results show GP regression with the DMM kernels is effective when
applying to the standard synthetic and real-world multi-fidelity data sets.","cond-mat.dis-nn,cs.LG,stat.ML"
"CNNPred: CNN-based stock market prediction using several data sources. Feature extraction from financial data is one of the most important problems
in market prediction domain for which many approaches have been suggested.
Among other modern tools, convolutional neural networks (CNN) have recently
been applied for automatic feature selection and market prediction. However, in
experiments reported so far, less attention has been paid to the correlation
among different markets as a possible source of information for extracting
features. In this paper, we suggest a CNN-based framework with specially
designed CNNs, that can be applied on a collection of data from a variety of
sources, including different markets, in order to extract features for
predicting the future of those markets. The suggested framework has been
applied for predicting the next day's direction of movement for the indices of
S&P 500, NASDAQ, DJI, NYSE, and RUSSELL markets based on various sets of
initial features. The evaluations show a significant improvement in
prediction's performance compared to the state of the art baseline algorithms.","cs.CE,cs.LG,cs.NE,q-fin.CP,stat.ML"
"KGAT: Knowledge Graph Attention Network for Recommendation. To provide more accurate, diverse, and explainable recommendation, it is
compulsory to go beyond modeling user-item interactions and take side
information into account. Traditional methods like factorization machine (FM)
cast it as a supervised learning problem, which assumes each interaction as an
independent instance with side information encoded. Due to the overlook of the
relations among instances or items (e.g., the director of a movie is also an
actor of another movie), these methods are insufficient to distill the
collaborative signal from the collective behaviors of users. In this work, we
investigate the utility of knowledge graph (KG), which breaks down the
independent interaction assumption by linking items with their attributes. We
argue that in such a hybrid structure of KG and user-item graph, high-order
relations --- which connect two items with one or multiple linked attributes
--- are an essential factor for successful recommendation. We propose a new
method named Knowledge Graph Attention Network (KGAT) which explicitly models
the high-order connectivities in KG in an end-to-end fashion. It recursively
propagates the embeddings from a node's neighbors (which can be users, items,
or attributes) to refine the node's embedding, and employs an attention
mechanism to discriminate the importance of the neighbors. Our KGAT is
conceptually advantageous to existing KG-based recommendation methods, which
either exploit high-order relations by extracting paths or implicitly modeling
them with regularization. Empirical results on three public benchmarks show
that KGAT significantly outperforms state-of-the-art methods like Neural FM and
RippleNet. Further studies verify the efficacy of embedding propagation for
high-order relation modeling and the interpretability benefits brought by the
attention mechanism.","cs.IR,cs.LG,stat.ML"
"Verifying Individual Fairness in Machine Learning Models. We consider the problem of whether a given decision model, working with
structured data, has individual fairness. Following the work of Dwork, a model
is individually biased (or unfair) if there is a pair of valid inputs which are
close to each other (according to an appropriate metric) but are treated
differently by the model (different class label, or large difference in
output), and it is unbiased (or fair) if no such pair exists. Our objective is
to construct verifiers for proving individual fairness of a given model, and we
do so by considering appropriate relaxations of the problem. We construct
verifiers which are sound but not complete for linear classifiers, and
kernelized polynomial/radial basis function classifiers. We also report the
experimental results of evaluating our proposed algorithms on publicly
available datasets.","cs.AI,cs.LG,stat.ML"
"Non-Adversarial Imitation Learning and its Connections to Adversarial Methods. Many modern methods for imitation learning and inverse reinforcement
learning, such as GAIL or AIRL, are based on an adversarial formulation. These
methods apply GANs to match the expert's distribution over states and actions
with the implicit state-action distribution induced by the agent's policy.
However, by framing imitation learning as a saddle point problem, adversarial
methods can suffer from unstable optimization, and convergence can only be
shown for small policy updates. We address these problems by proposing a
framework for non-adversarial imitation learning. The resulting algorithms are
similar to their adversarial counterparts and, thus, provide insights for
adversarial imitation learning methods. Most notably, we show that AIRL is an
instance of our non-adversarial formulation, which enables us to greatly
simplify its derivations and obtain stronger convergence guarantees. We also
show that our non-adversarial formulation can be used to derive novel
algorithms by presenting a method for offline imitation learning that is
inspired by the recent ValueDice algorithm, but does not rely on small policy
updates for convergence. In our simulated robot experiments, our offline method
for non-adversarial imitation learning seems to perform best when using many
updates for policy and discriminator at each iteration and outperforms
behavioral cloning and ValueDice.","cs.IT,cs.LG,cs.RO,math.IT,stat.ML"
"Probabilistic sequential matrix factorization. We introduce the probabilistic sequential matrix factorization (PSMF) method
for factorizing time-varying and non-stationary datasets consisting of
high-dimensional time-series. In particular, we consider nonlinear Gaussian
state-space models where sequential approximate inference results in the
factorization of a data matrix into a dictionary and time-varying coefficients
with potentially nonlinear Markovian dependencies. The assumed Markovian
structure on the coefficients enables us to encode temporal dependencies into a
low-dimensional feature space. The proposed inference method is solely based on
an approximate extended Kalman filtering scheme, which makes the resulting
method particularly efficient. PSMF can account for temporal nonlinearities
and, more importantly, can be used to calibrate and estimate generic
differentiable nonlinear subspace models. We also introduce a robust version of
PSMF, called rPSMF, which uses Student-t filters to handle model
misspecification. We show that PSMF can be used in multiple contexts: modeling
time series with a periodic subspace, robustifying changepoint detection
methods, and imputing missing data in several high-dimensional time-series,
such as measurements of pollutants across London.","cs.LG,stat.CO,stat.ML"
"Spanning Tree Constrained Determinantal Point Processes are Hard to (Approximately) Evaluate. We consider determinantal point processes (DPPs) constrained by spanning
trees. Given a graph $G=(V,E)$ and a positive semi-definite matrix $\mathbf{A}$
indexed by $E$, a spanning-tree DPP defines a distribution such that we draw
$S\subseteq E$ with probability proportional to $\det(\mathbf{A}_S)$ only if
$S$ induces a spanning tree. We prove $\sharp\textsf{P}$-hardness of computing
the normalizing constant for spanning-tree DPPs and provide an
approximation-preserving reduction from the mixed discriminant, for which FPRAS
is not known. We show similar results for DPPs constrained by forests.","cs.DS,cs.LG"
"Behavioral Research and Practical Models of Drivers' Attention. Driving is a routine activity for many, but it is far from simple. Drivers
deal with multiple concurrent tasks, such as keeping the vehicle in the lane,
observing and anticipating the actions of other road users, reacting to
hazards, and dealing with distractions inside and outside the vehicle. Failure
to notice and respond to the surrounding objects and events can cause
accidents.
  The ongoing improvements of the road infrastructure and vehicle mechanical
design have made driving safer overall. Nevertheless, the problem of driver
inattention has remained one of the primary causes of accidents. Therefore,
understanding where the drivers look and why they do so can help eliminate
sources of distractions and identify unsafe attention patterns. Research on
driver attention has implications for many practical applications such as
policy-making, improving driver education, enhancing road infrastructure and
in-vehicle infotainment systems, as well as designing systems for driver
monitoring, driver assistance, and automated driving.
  This report covers the literature on changes in drivers' visual attention
distribution due to factors, internal and external to the driver. Aspects of
attention during driving have been explored across multiple disciplines,
including psychology, human factors, human-computer interaction, intelligent
transportation, and computer vision, each offering different perspectives,
goals, and explanations for the observed phenomena. We link cross-disciplinary
theoretical and behavioral research on driver's attention to practical
solutions. Furthermore, limitations and directions for future research are
discussed. This report is based on over 175 behavioral studies, nearly 100
practical papers, 20 datasets, and over 70 surveys published since 2010. A
curated list of papers used for this report is available at
https://github.com/ykotseruba/attention_and_driving.","cs.CV,cs.RO"
"Unsupervised Learning for Cuboid Shape Abstraction via Joint Segmentation from Point Clouds. Representing complex 3D objects as simple geometric primitives, known as
shape abstraction, is important for geometric modeling, structural analysis,
and shape synthesis. In this paper, we propose an unsupervised shape
abstraction method to map a point cloud into a compact cuboid representation.
We jointly predict cuboid allocation as part segmentation and cuboid shapes and
enforce the consistency between the segmentation and shape abstraction for
self-learning. For the cuboid abstraction task, we transform the input point
cloud into a set of parametric cuboids using a variational auto-encoder
network. The segmentation network allocates each point into a cuboid
considering the point-cuboid affinity. Without manual annotations of parts in
point clouds, we design four novel losses to jointly supervise the two branches
in terms of geometric similarity and cuboid compactness. We evaluate our method
on multiple shape collections and demonstrate its superiority over existing
shape abstraction methods. Moreover, based on our network architecture and
learned representations, our approach supports various applications including
structured shape generation, shape interpolation, and structural shape
clustering.","cs.CV,cs.GR"
"Large-scale representation learning from visually grounded untranscribed speech. Systems that can associate images with their spoken audio captions are an
important step towards visually grounded language learning. We describe a
scalable method to automatically generate diverse audio for image captioning
datasets. This supports pretraining deep networks for encoding both audio and
images, which we do via a dual encoder that learns to align latent
representations from both modalities. We show that a masked margin softmax loss
for such models is superior to the standard triplet loss. We fine-tune these
models on the Flickr8k Audio Captions Corpus and obtain state-of-the-art
results---improving recall in the top 10 from 29.6% to 49.5%. We also obtain
human ratings on retrieval outputs to better assess the impact of incidentally
matching image-caption pairs that were not associated in the data, finding that
automatic evaluation substantially underestimates the quality of the retrieved
results.","cs.CL,cs.CV,cs.SD,eess.AS"
"Description-based Label Attention Classifier for Explainable ICD-9 Classification. ICD-9 coding is a relevant clinical billing task, where unstructured texts
with information about a patient's diagnosis and treatments are annotated with
multiple ICD-9 codes. Automated ICD-9 coding is an active research field, where
CNN- and RNN-based model architectures represent the state-of-the-art
approaches. In this work, we propose a description-based label attention
classifier to improve the model explainability when dealing with noisy texts
like clinical notes. We evaluate our proposed method with different
transformer-based encoders on the MIMIC-III-50 dataset. Our method achieves
strong results together with augmented explainablilty.","cs.IR,cs.LG"
"A Cross-Modal Image Fusion Method Guided by Human Visual Characteristics. The characteristics of feature selection, nonlinear combination and
multi-task auxiliary learning mechanism of the human visual perception system
play an important role in real-world scenarios, but the research of image
fusion theory based on the characteristics of human visual perception is less.
Inspired by the characteristics of human visual perception, we propose a robust
multi-task auxiliary learning optimization image fusion theory. Firstly, we
combine channel attention model with nonlinear convolutional neural network to
select features and fuse nonlinear features. Then, we analyze the impact of the
existing image fusion loss on the image fusion quality, and establish the
multi-loss function model of unsupervised learning network. Secondly, aiming at
the multi-task auxiliary learning mechanism of human visual perception system,
we study the influence of multi-task auxiliary learning mechanism on image
fusion task on the basis of single task multi-loss network model. By simulating
the three characteristics of human visual perception system, the fused image is
more consistent with the mechanism of human brain image fusion. Finally, in
order to verify the superiority of our algorithm, we carried out experiments on
the combined vision system image data set, and extended our algorithm to the
infrared and visible image and the multi-focus image public data set for
experimental verification. The experimental results demonstrate the superiority
of our fusion theory over state-of-arts in generality and robustness.","cs.CV,cs.IT,cs.LG,math.IT"
"Marginal Densities, Factor Graph Duality, and High-Temperature Series Expansions. We prove that the marginal densities of a global probability mass function in
a primal normal factor graph and the corresponding marginal densities in the
dual normal factor graph are related via local mappings. The mapping depends on
the Fourier transform of the local factors of the models. Details of the
mapping, including its fixed points, are derived for the Ising model, and then
extended to the Potts model. By employing the mapping, we can transform
simultaneously all the estimated marginal densities from one domain to the
other, which is advantageous if estimating the marginals can be carried out
more efficiently in the dual domain. An example of particular significance is
the ferromagnetic Ising model in a positive external field, for which there is
a rapidly mixing Markov chain (called the subgraphs-world process) to generate
configurations in the dual normal factor graph of the model. Our numerical
experiments illustrate that the proposed procedure can provide more accurate
estimates of marginal densities in various settings.","cs.IT,cs.LG,math.IT,stat.CO,stat.ML"
"JuICe: A Large Scale Distantly Supervised Dataset for Open Domain Context-based Code Generation. Interactive programming with interleaved code snippet cells and natural
language markdown is recently gaining popularity in the form of Jupyter
notebooks, which accelerate prototyping and collaboration. To study code
generation conditioned on a long context history, we present JuICe, a corpus of
1.5 million examples with a curated test set of 3.7K instances based on online
programming assignments. Compared with existing contextual code generation
datasets, JuICe provides refined human-curated data, open-domain code, and an
order of magnitude more training data. Using JuICe, we train models for two
tasks: (1) generation of the API call sequence in a code cell, and (2) full
code cell generation, both conditioned on the NL-Code history up to a
particular code cell. Experiments using current baseline code generation models
show that both context and distant supervision aid in generation, and that the
dataset is challenging for current systems.","cs.CL,cs.LG,cs.SE"
"MWQ: Multiscale Wavelet Quantized Neural Networks. Model quantization can reduce the model size and computational latency, it
has become an essential technique for the deployment of deep neural networks on
resourceconstrained hardware (e.g., mobile phones and embedded devices). The
existing quantization methods mainly consider the numerical elements of the
weights and activation values, ignoring the relationship between elements. The
decline of representation ability and information loss usually lead to the
performance degradation. Inspired by the characteristics of images in the
frequency domain, we propose a novel multiscale wavelet quantization (MWQ)
method. This method decomposes original data into multiscale frequency
components by wavelet transform, and then quantizes the components of different
scales, respectively. It exploits the multiscale frequency and spatial
information to alleviate the information loss caused by quantization in the
spatial domain. Because of the flexibility of MWQ, we demonstrate three
applications (e.g., model compression, quantized network optimization, and
information enhancement) on the ImageNet and COCO datasets. Experimental
results show that our method has stronger representation ability and can play
an effective role in quantized neural networks.","cs.AR,cs.CV"
"Toward Transformer-Based Object Detection. Transformers have become the dominant model in natural language processing,
owing to their ability to pretrain on massive amounts of data, then transfer to
smaller, more specific tasks via fine-tuning. The Vision Transformer was the
first major attempt to apply a pure transformer model directly to images as
input, demonstrating that as compared to convolutional networks,
transformer-based architectures can achieve competitive results on benchmark
classification tasks. However, the computational complexity of the attention
operator means that we are limited to low-resolution inputs. For more complex
tasks such as detection or segmentation, maintaining a high input resolution is
crucial to ensure that models can properly identify and reflect fine details in
their output. This naturally raises the question of whether or not
transformer-based architectures such as the Vision Transformer are capable of
performing tasks other than classification. In this paper, we determine that
Vision Transformers can be used as a backbone by a common detection task head
to produce competitive COCO results. The model that we propose, ViT-FRCNN,
demonstrates several known properties associated with transformers, including
large pretraining capacity and fast fine-tuning performance. We also
investigate improvements over a standard detection backbone, including superior
performance on out-of-domain images, better performance on large objects, and a
lessened reliance on non-maximum suppression. We view ViT-FRCNN as an important
stepping stone toward a pure-transformer solution of complex vision tasks such
as object detection.","cs.AI,cs.CV,cs.LG"
"On the Estimation of Network Complexity: Dimension of Graphons. Network complexity has been studied for over half a century and has found a
wide range of applications. Many methods have been developed to characterize
and estimate the complexity of networks. However, there has been little
research with statistical guarantees. In this paper, we develop a statistical
theory of graph complexity in a general model of random graphs, the so-called
graphon model.
  Given a graphon, we endow the latent space of the nodes with the neighborhood
distance that measures the propensity of two nodes to be connected with similar
nodes. Our complexity index is then based on the covering number and the
Minkowski dimension of (a purified version of) this metric space. Although the
latent space is not identifiable, these indices turn out to be identifiable.
This notion of complexity has simple interpretations on popular examples of
random graphs: it matches the number of communities in stochastic block models;
the dimension of the Euclidean space in random geometric graphs; the regularity
of the link function in H\""older graphon models.
  From a single observation of the graph, we construct an estimator of the
neighborhood-distance and show universal non-asymptotic bounds for its risk,
matching minimax lower bounds. Based on this estimated distance, we compute the
corresponding covering number and Minkowski dimension and we provide optimal
non-asymptotic error bounds for these two plug-in estimators.","cs.LG,cs.SI,math.ST,stat.ML,stat.TH"
"Density of States Graph Kernels. A fundamental problem on graph-structured data is that of quantifying
similarity between graphs. Graph kernels are an established technique for such
tasks; in particular, those based on random walks and return probabilities have
proven to be effective in wide-ranging applications, from bioinformatics to
social networks to computer vision. However, random walk kernels generally
suffer from slowness and tottering, an effect which causes walks to
overemphasize local graph topology, undercutting the importance of global
structure. To correct for these issues, we recast return probability graph
kernels under the more general framework of density of states -- a framework
which uses the lens of spectral analysis to uncover graph motifs and properties
hidden within the interior of the spectrum -- and use our interpretation to
construct scalable, composite density of states based graph kernels which
balance local and global information, leading to higher classification
accuracies on a host of benchmark datasets.","cs.LG,cs.NA,cs.SI,math.NA"
"Temporal graph-based approach for behavioural entity classification. Graph-based analyses have gained a lot of relevance in the past years due to
their high potential in describing complex systems by detailing the actors
involved, their relations and their behaviours. Nevertheless, in scenarios
where these aspects are evolving over time, it is not easy to extract valuable
information or to characterize correctly all the actors. In this study, a two
phased approach for exploiting the potential of graph structures in the
cybersecurity domain is presented. The main idea is to convert a network
classification problem into a graph-based behavioural one. We extract these
graph structures that can represent the evolution of both normal and attack
entities and apply a temporal dissection approach in order to highlight their
micro-dynamics. Further, three clustering techniques are applied to the normal
entities in order to aggregate similar behaviours, mitigate the imbalance
problem and reduce noisy data. Our approach suggests the implementation of two
promising deep learning paradigms for entity classification based on Graph
Convolutional Networks.","cs.IT,cs.LG,math.IT"
"On Lyapunov Exponents for RNNs: Understanding Information Propagation Using Dynamical Systems Tools. Recurrent neural networks (RNNs) have been successfully applied to a variety
of problems involving sequential data, but their optimization is sensitive to
parameter initialization, architecture, and optimizer hyperparameters.
Considering RNNs as dynamical systems, a natural way to capture stability,
i.e., the growth and decay over long iterates, are the Lyapunov Exponents
(LEs), which form the Lyapunov spectrum. The LEs have a bearing on stability of
RNN training dynamics because forward propagation of information is related to
the backward propagation of error gradients. LEs measure the asymptotic rates
of expansion and contraction of nonlinear system trajectories, and generalize
stability analysis to the time-varying attractors structuring the
non-autonomous dynamics of data-driven RNNs. As a tool to understand and
exploit stability of training dynamics, the Lyapunov spectrum fills an existing
gap between prescriptive mathematical approaches of limited scope and
computationally-expensive empirical approaches. To leverage this tool, we
implement an efficient way to compute LEs for RNNs during training, discuss the
aspects specific to standard RNN architectures driven by typical sequential
datasets, and show that the Lyapunov spectrum can serve as a robust readout of
training stability across hyperparameters. With this exposition-oriented
contribution, we hope to draw attention to this understudied, but theoretically
grounded tool for understanding training stability in RNNs.","cs.LG,math.DS,nlin.CD,stat.ML"
"Integration of Neural Network-Based Symbolic Regression in Deep Learning for Scientific Discovery. Symbolic regression is a powerful technique that can discover analytical
equations that describe data, which can lead to explainable models and
generalizability outside of the training data set. In contrast, neural networks
have achieved amazing levels of accuracy on image recognition and natural
language processing tasks, but are often seen as black-box models that are
difficult to interpret and typically extrapolate poorly. Here we use a neural
network-based architecture for symbolic regression called the Equation Learner
(EQL) network and integrate it with other deep learning architectures such that
the whole system can be trained end-to-end through backpropagation. To
demonstrate the power of such systems, we study their performance on several
substantially different tasks. First, we show that the neural network can
perform symbolic regression and learn the form of several functions. Next, we
present an MNIST arithmetic task where a separate part of the neural network
extracts the digits. Finally, we demonstrate prediction of dynamical systems
where an unknown parameter is extracted through an encoder. We find that the
EQL-based architecture can extrapolate quite well outside of the training data
set compared to a standard neural network-based architecture, paving the way
for deep learning to be applied in scientific exploration and discovery.","cs.LG,cs.NE,physics.data-an,stat.ML"
"Transparency, Auditability and eXplainability of Machine Learning Models in Credit Scoring. A major requirement for credit scoring models is to provide a maximally
accurate risk prediction. Additionally, regulators demand these models to be
transparent and auditable. Thus, in credit scoring, very simple predictive
models such as logistic regression or decision trees are still widely used and
the superior predictive power of modern machine learning algorithms cannot be
fully leveraged. Significant potential is therefore missed, leading to higher
reserves or more credit defaults. This paper works out different dimensions
that have to be considered for making credit scoring models understandable and
presents a framework for making ``black box'' machine learning models
transparent, auditable and explainable. Following this framework, we present an
overview of techniques, demonstrate how they can be applied in credit scoring
and how results compare to the interpretability of score cards. A real world
case study shows that a comparable degree of interpretability can be achieved
while machine learning techniques keep their ability to improve predictive
power.","cs.LG,econ.GN,q-fin.EC,stat.AP,stat.ME,stat.ML"
"Free energy-based reinforcement learning using a quantum processor. Recent theoretical and experimental results suggest the possibility of using
current and near-future quantum hardware in challenging sampling tasks. In this
paper, we introduce free energy-based reinforcement learning (FERL) as an
application of quantum hardware. We propose a method for processing a quantum
annealer's measured qubit spin configurations in approximating the free energy
of a quantum Boltzmann machine (QBM). We then apply this method to perform
reinforcement learning on the grid-world problem using the D-Wave 2000Q quantum
annealer. The experimental results show that our technique is a promising
method for harnessing the power of quantum sampling in reinforcement learning
tasks.","cs.AI,cs.LG,cs.NE,math.OC,quant-ph"
"Adversarial Robustness: What fools you makes you stronger. We prove an exponential separation for the sample complexity between the
standard PAC-learning model and a version of the Equivalence-Query-learning
model. We then show that this separation has interesting implications for
adversarial robustness. We explore a vision of designing an adaptive defense
that in the presence of an attacker computes a model that is provably robust.
In particular, we show how to realize this vision in a simplified setting.
  In order to do so, we introduce a notion of a strong adversary: he is not
limited by the type of perturbations he can apply but when presented with a
classifier can repetitively generate different adversarial examples. We explain
why this notion is interesting to study and use it to prove the following.
There exists an efficient adversarial-learning-like scheme such that for every
strong adversary $\mathbf{A}$ it outputs a classifier that (a) cannot be
strongly attacked by $\mathbf{A}$, or (b) has error at most $\epsilon$. In both
cases our scheme uses exponentially (in $\epsilon$) fewer samples than what the
PAC bound requires.","cs.CR,cs.LG"
"Adversarial Attacks on Brain-Inspired Hyperdimensional Computing-Based Classifiers. Being an emerging class of in-memory computing architecture, brain-inspired
hyperdimensional computing (HDC) mimics brain cognition and leverages random
hypervectors (i.e., vectors with a dimensionality of thousands or even more) to
represent features and to perform classification tasks. The unique hypervector
representation enables HDC classifiers to exhibit high energy efficiency, low
inference latency and strong robustness against hardware-induced bit errors.
Consequently, they have been increasingly recognized as an appealing
alternative to or even replacement of traditional deep neural networks (DNNs)
for local on device classification, especially on low-power Internet of Things
devices. Nonetheless, unlike their DNN counterparts, state-of-the-art designs
for HDC classifiers are mostly security-oblivious, casting doubt on their
safety and immunity to adversarial inputs. In this paper, we study for the
first time adversarial attacks on HDC classifiers and highlight that HDC
classifiers can be vulnerable to even minimally-perturbed adversarial samples.
Concretely, using handwritten digit classification as an example, we construct
a HDC classifier and formulate a grey-box attack problem, where an attacker's
goal is to mislead the target HDC classifier to produce erroneous prediction
labels while keeping the amount of added perturbation noise as little as
possible. Then, we propose a modified genetic algorithm to generate adversarial
samples within a reasonably small number of queries. Our results show that
adversarial images generated by our algorithm can successfully mislead the HDC
classifier to produce wrong prediction labels with a high probability (i.e.,
78% when the HDC classifier uses a fixed majority rule for decision). Finally,
we also present two defense strategies -- adversarial training and retraining--
to strengthen the security of HDC classifiers.","68M25,68T05,cs.CR,cs.LG"
"Low Dose Helical CBCT denoising by using domain filtering with deep reinforcement learning. Cone Beam Computed Tomography(CBCT) is a now known method to conduct CT
imaging. Especially, The Low Dose CT imaging is one of possible options to
protect organs of patients when conducting CT imaging. Therefore Low Dose CT
imaging can be an alternative instead of Standard dose CT imaging. However Low
Dose CT imaging has a fundamental issue with noises within results compared to
Standard Dose CT imaging. Currently, there are lots of attempts to erase the
noises. Most of methods with artificial intelligence have many parameters and
unexplained layers or a kind of black-box methods. Therefore, our research has
purposes related to these issues. Our approach has less parameters than usual
methods by having Iterative learn-able bilateral filtering approach with Deep
reinforcement learning. And we applied The Iterative learn-able filtering
approach with deep reinforcement learning to sinograms and reconstructed volume
domains. The method and the results of the method can be much more explainable
than The other black box AI approaches. And we applied the method to Helical
Cone Beam Computed Tomography(CBCT), which is the recent CBCT trend. We tested
this method with on 2 abdominal scans(L004, L014) from Mayo Clinic TCIA
dataset. The results and the performances of our approach overtake the results
of the other previous methods.","cs.AI,cs.CV"
"Safe Learning and Optimization Techniques: Towards a Survey of the State of the Art. Safe learning and optimization deals with learning and optimization problems
that avoid, as much as possible, the evaluation of non-safe input points, which
are solutions, policies, or strategies that cause an irrecoverable loss (e.g.,
breakage of a machine or equipment, or life threat). Although a comprehensive
survey of safe reinforcement learning algorithms was published in 2015, a
number of new algorithms have been proposed thereafter, and related works in
active learning and in optimization were not considered. This paper reviews
those algorithms from a number of domains including reinforcement learning,
Gaussian process regression and classification, evolutionary algorithms, and
active learning. We provide the fundamental concepts on which the reviewed
algorithms are based and a characterization of the individual algorithms. We
conclude by explaining how the algorithms are connected and suggestions for
future research.","I.2.8,cs.LG,cs.NE,math.OC"
"Fast Federated Learning by Balancing Communication Trade-Offs. Federated Learning (FL) has recently received a lot of attention for
large-scale privacy-preserving machine learning. However, high communication
overheads due to frequent gradient transmissions decelerate FL. To mitigate the
communication overheads, two main techniques have been studied: (i) local
update of weights characterizing the trade-off between communication and
computation and (ii) gradient compression characterizing the trade-off between
communication and precision. To the best of our knowledge, studying and
balancing those two trade-offs jointly and dynamically while considering their
impacts on convergence has remained unresolved even though it promises
significantly faster FL. In this paper, we first formulate our problem to
minimize learning error with respect to two variables: local update
coefficients and sparsity budgets of gradient compression who characterize
trade-offs between communication and computation/precision, respectively. We
then derive an upper bound of the learning error in a given wall-clock time
considering the interdependency between the two variables. Based on this
theoretical analysis, we propose an enhanced FL scheme, namely Fast FL (FFL),
that jointly and dynamically adjusts the two variables to minimize the learning
error. We demonstrate that FFL consistently achieves higher accuracies faster
than similar schemes existing in the literature.","cs.AI,cs.DC,cs.LG,cs.NE"
"Towards Graph Representation Learning in Emergent Communication. Recent findings in neuroscience suggest that the human brain represents
information in a geometric structure (for instance, through conceptual spaces).
In order to communicate, we flatten the complex representation of entities and
their attributes into a single word or a sentence. In this paper we use graph
convolutional networks to support the evolution of language and cooperation in
multi-agent systems. Motivated by an image-based referential game, we propose a
graph referential game with varying degrees of complexity, and we provide
strong baseline models that exhibit desirable properties in terms of language
emergence and cooperation. We show that the emerged communication protocol is
robust, that the agents uncover the true factors of variation in the game, and
that they learn to generalize beyond the samples encountered during training.","cs.AI,cs.CL,cs.LG,cs.MA,stat.ML"
"CLIP-It! Language-Guided Video Summarization. A generic video summary is an abridged version of a video that conveys the
whole story and features the most important scenes. Yet the importance of
scenes in a video is often subjective, and users should have the option of
customizing the summary by using natural language to specify what is important
to them. Further, existing models for fully automatic generic summarization
have not exploited available language models, which can serve as an effective
prior for saliency. This work introduces CLIP-It, a single framework for
addressing both generic and query-focused video summarization, typically
approached separately in the literature. We propose a language-guided
multimodal transformer that learns to score frames in a video based on their
importance relative to one another and their correlation with a user-defined
query (for query-focused summarization) or an automatically generated dense
video caption (for generic video summarization). Our model can be extended to
the unsupervised setting by training without ground-truth supervision. We
outperform baselines and prior work by a significant margin on both standard
video summarization datasets (TVSum and SumMe) and a query-focused video
summarization dataset (QFVS). Particularly, we achieve large improvements in
the transfer setting, attesting to our method's strong generalization
capabilities.","cs.AI,cs.CV,cs.MM"
"Comparing Natural Language Processing Techniques for Alzheimer's Dementia Prediction in Spontaneous Speech. Alzheimer's Dementia (AD) is an incurable, debilitating, and progressive
neurodegenerative condition that affects cognitive function. Early diagnosis is
important as therapeutics can delay progression and give those diagnosed vital
time. Developing models that analyse spontaneous speech could eventually
provide an efficient diagnostic modality for earlier diagnosis of AD. The
Alzheimer's Dementia Recognition through Spontaneous Speech task offers
acoustically pre-processed and balanced datasets for the classification and
prediction of AD and associated phenotypes through the modelling of spontaneous
speech. We exclusively analyse the supplied textual transcripts of the
spontaneous speech dataset, building and comparing performance across numerous
models for the classification of AD vs controls and the prediction of Mental
Mini State Exam scores. We rigorously train and evaluate Support Vector
Machines (SVMs), Gradient Boosting Decision Trees (GBDT), and Conditional
Random Fields (CRFs) alongside deep learning Transformer based models. We find
our top performing models to be a simple Term Frequency-Inverse Document
Frequency (TF-IDF) vectoriser as input into a SVM model and a pre-trained
Transformer based model `DistilBERT' when used as an embedding layer into
simple linear models. We demonstrate test set scores of 0.81-0.82 across
classification metrics and a RMSE of 4.58.","cs.CL,cs.LG,cs.SD,eess.AS,stat.ML"
"New Perspectives on the Use of Online Learning for Congestion Level Prediction over Traffic Data. This work focuses on classification over time series data. When a time series
is generated by non-stationary phenomena, the pattern relating the series with
the class to be predicted may evolve over time (concept drift). Consequently,
predictive models aimed to learn this pattern may become eventually obsolete,
hence failing to sustain performance levels of practical use. To overcome this
model degradation, online learning methods incrementally learn from new data
samples arriving over time, and accommodate eventual changes along the data
stream by implementing assorted concept drift strategies. In this manuscript we
elaborate on the suitability of online learning methods to predict the road
congestion level based on traffic speed time series data. We draw interesting
insights on the performance degradation when the forecasting horizon is
increased. As opposed to what is done in most literature, we provide evidence
of the importance of assessing the distribution of classes over time before
designing and tuning the learning model. This previous exercise may give a hint
of the predictability of the different congestion levels under target.
Experimental results are discussed over real traffic speed data captured by
inductive loops deployed over Seattle (USA). Several online learning methods
are analyzed, from traditional incremental learning algorithms to more
elaborated deep learning models. As shown by the reported results, when
increasing the prediction horizon, the performance of all models degrade
severely due to the distribution of classes along time, which supports our
claim about the importance of analyzing this distribution prior to the design
of the model.","cs.LG,eess.SP,stat.ML"
"Reinforcement Learning for Adaptive Routing. Reinforcement learning means learning a policy--a mapping of observations
into actions--based on feedback from the environment. The learning can be
viewed as browsing a set of policies while evaluating them by trial through
interaction with the environment. We present an application of gradient ascent
algorithm for reinforcement learning to a complex domain of packet routing in
network communication and compare the performance of this algorithm to other
routing methods on a benchmark problem.","C.2.1; C.2.2; C.2.4; C.2.6; F.1.1; I.2.6; I.2.8; I.2.9,cs.AI,cs.LG,cs.NI"
"A Unified Off-Policy Evaluation Approach for General Value Function. General Value Function (GVF) is a powerful tool to represent both the {\em
predictive} and {\em retrospective} knowledge in reinforcement learning (RL).
In practice, often multiple interrelated GVFs need to be evaluated jointly with
pre-collected off-policy samples. In the literature, the gradient temporal
difference (GTD) learning method has been adopted to evaluate GVFs in the
off-policy setting, but such an approach may suffer from a large estimation
error even if the function approximation class is sufficiently expressive.
Moreover, none of the previous work have formally established the convergence
guarantee to the ground truth GVFs under the function approximation settings.
In this paper, we address both issues through the lens of a class of GVFs with
causal filtering, which cover a wide range of RL applications such as reward
variance, value gradient, cost in anomaly detection, stationary distribution
gradient, etc. We propose a new algorithm called GenTD for off-policy GVFs
evaluation and show that GenTD learns multiple interrelated multi-dimensional
GVFs as efficiently as a single canonical scalar value function. We further
show that unlike GTD, the learned GVFs by GenTD are guaranteed to converge to
the ground truth GVFs as long as the function approximation power is
sufficiently large. To our best knowledge, GenTD is the first off-policy GVF
evaluation algorithm that has global optimality guarantee.","cs.AI,cs.LG,math.ST,stat.TH"
"Record fusion: A learning approach. Record fusion is the task of aggregating multiple records that correspond to
the same real-world entity in a database. We can view record fusion as a
machine learning problem where the goal is to predict the ""correct"" value for
each attribute for each entity. Given a database, we use a combination of
attribute-level, recordlevel, and database-level signals to construct a feature
vector for each cell (or (row, col)) of that database. We use this feature
vector alongwith the ground-truth information to learn a classifier for each of
the attributes of the database.
  Our learning algorithm uses a novel stagewise additive model. At each stage,
we construct a new feature vector by combining a part of the original feature
vector with features computed by the predictions from the previous stage. We
then learn a softmax classifier over the new feature space. This greedy
stagewise approach can be viewed as a deep model where at each stage, we are
adding more complicated non-linear transformations of the original feature
vector. We show that our approach fuses records with an average precision of
~98% when source information of records is available, and ~94% without source
information across a diverse array of real-world datasets. We compare our
approach to a comprehensive collection of data fusion and entity consolidation
methods considered in the literature. We show that our approach can achieve an
average precision improvement of ~20%/~45% with/without source information
respectively.","cs.DB,cs.IR,cs.LG,stat.ML"
"GP-ALPS: Automatic Latent Process Selection for Multi-Output Gaussian Process Models. A simple and widely adopted approach to extend Gaussian processes (GPs) to
multiple outputs is to model each output as a linear combination of a
collection of shared, unobserved latent GPs. An issue with this approach is
choosing the number of latent processes and their kernels. These choices are
typically done manually, which can be time consuming and prone to human biases.
We propose Gaussian Process Automatic Latent Process Selection (GP-ALPS), which
automatically chooses the latent processes by turning off those that do not
meaningfully contribute to explaining the data. We develop a variational
inference scheme, assess the quality of the variational posterior by comparing
it against the gold standard MCMC, and demonstrate the suitability of GP-ALPS
in a set of preliminary experiments.","cs.LG,stat.ME,stat.ML"
"Prophet: Proactive Candidate-Selection for Federated Learning by Predicting the Qualities of Training and Reporting Phases. Although the challenge of the device connection is much relieved in 5G
networks, the training latency is still an obstacle preventing Federated
Learning (FL) from being largely adopted. One of the most fundamental problems
that lead to large latency is the bad candidate-selection for FL. In the
dynamic environment, the mobile devices selected by the existing reactive
candidate-selection algorithms very possibly fail to complete the training and
reporting phases of FL, because the FL parameter server only knows the
currently-observed resources of all candidates. To this end, we study the
proactive candidate-selection for FL in this paper. We first let each candidate
device predict the qualities of both its training and reporting phases locally
using LSTM. Then, the proposed candidateselection algorithm is implemented by
the Deep Reinforcement Learning (DRL) framework. Finally, the real-world
trace-driven experiments prove that the proposed approach outperforms the
existing reactive algorithms","cs.DC,cs.LG,stat.ML"
"Pathfinder Discovery Networks for Neural Message Passing. In this work we propose Pathfinder Discovery Networks (PDNs), a method for
jointly learning a message passing graph over a multiplex network with a
downstream semi-supervised model. PDNs inductively learn an aggregated weight
for each edge, optimized to produce the best outcome for the downstream
learning task. PDNs are a generalization of attention mechanisms on graphs
which allow flexible construction of similarity functions between nodes, edge
convolutions, and cheap multiscale mixing layers. We show that PDNs overcome
weaknesses of existing methods for graph attention (e.g. Graph Attention
Networks), such as the diminishing weight problem. Our experimental results
demonstrate competitive predictive performance on academic node classification
tasks. Additional results from a challenging suite of node classification
experiments show how PDNs can learn a wider class of functions than existing
baselines. We analyze the relative computational complexity of PDNs, and show
that PDN runtime is not considerably higher than static-graph models. Finally,
we discuss how PDNs can be used to construct an easily interpretable attention
mechanism that allows users to understand information propagation in the graph.","cs.AI,cs.LG,cs.SI"
"Asynchronous Advantage Actor Critic: Non-asymptotic Analysis and Linear Speedup. Asynchronous and parallel implementation of standard reinforcement learning
(RL) algorithms is a key enabler of the tremendous success of modern RL. Among
many asynchronous RL algorithms, arguably the most popular and effective one is
the asynchronous advantage actor-critic (A3C) algorithm. Although A3C is
becoming the workhorse of RL, its theoretical properties are still not
well-understood, including the non-asymptotic analysis and the performance gain
of parallelism (a.k.a. speedup). This paper revisits the A3C algorithm with
TD(0) for the critic update, termed A3C-TD(0), with provable convergence
guarantees. With linear value function approximation for the TD update, the
convergence of A3C-TD(0) is established under both i.i.d. and Markovian
sampling. Under i.i.d. sampling, A3C-TD(0) obtains sample complexity of
$\mathcal{O}(\epsilon^{-2.5}/N)$ per worker to achieve $\epsilon$ accuracy,
where $N$ is the number of workers. Compared to the best-known sample
complexity of $\mathcal{O}(\epsilon^{-2.5})$ for two-timescale AC, A3C-TD(0)
achieves \emph{linear speedup}, which justifies the advantage of parallelism
and asynchrony in AC algorithms theoretically for the first time. Numerical
tests on synthetically generated instances and OpenAI Gym environments have
been provided to verify our theoretical analysis.","cs.LG,math.OC"
"Interactive Machine Learning of Musical Gesture. This chapter presents an overview of Interactive Machine Learning (IML)
techniques applied to the analysis and design of musical gestures. We go
through the main challenges and needs related to capturing, analysing, and
applying IML techniques to human bodily gestures with the purpose of performing
with sound synthesis systems. We discuss how different algorithms may be used
to accomplish different tasks, including interacting with complex synthesis
techniques and exploring interaction possibilities by means of Reinforcement
Learning (RL) in an interaction paradigm we developed called Assisted
Interactive Machine Learning (AIML). We conclude the chapter with a description
of how some of these techniques were employed by the authors for the
development of four musical pieces, thus outlining the implications that IML
have for musical practice.","cs.HC,cs.LG"
"Learning Edge Representations via Low-Rank Asymmetric Projections. We propose a new method for embedding graphs while preserving directed edge
information. Learning such continuous-space vector representations (or
embeddings) of nodes in a graph is an important first step for using network
information (from social networks, user-item graphs, knowledge bases, etc.) in
many machine learning tasks.
  Unlike previous work, we (1) explicitly model an edge as a function of node
embeddings, and we (2) propose a novel objective, the ""graph likelihood"", which
contrasts information from sampled random walks with non-existent edges.
Individually, both of these contributions improve the learned representations,
especially when there are memory constraints on the total size of the
embeddings. When combined, our contributions enable us to significantly improve
the state-of-the-art by learning more concise representations that better
preserve the graph structure.
  We evaluate our method on a variety of link-prediction task including social
networks, collaboration networks, and protein interactions, showing that our
proposed method learn representations with error reductions of up to 76% and
55%, on directed and undirected graphs. In addition, we show that the
representations learned by our method are quite space efficient, producing
embeddings which have higher structure-preserving accuracy but are 10 times
smaller.","cs.LG,cs.SI,stat.ML"
"Unsupervised Learning of Graph Hierarchical Abstractions with Differentiable Coarsening and Optimal Transport. Hierarchical abstractions are a methodology for solving large-scale graph
problems in various disciplines. Coarsening is one such approach: it generates
a pyramid of graphs whereby the one in the next level is a structural summary
of the prior one. With a long history in scientific computing, many coarsening
strategies were developed based on mathematically driven heuristics. Recently,
resurgent interests exist in deep learning to design hierarchical methods
learnable through differentiable parameterization. These approaches are paired
with downstream tasks for supervised learning. In practice, however, supervised
signals (e.g., labels) are scarce and are often laborious to obtain. In this
work, we propose an unsupervised approach, coined OTCoarsening, with the use of
optimal transport. Both the coarsening matrix and the transport cost matrix are
parameterized, so that an optimal coarsening strategy can be learned and
tailored for a given set of graphs. We demonstrate that the proposed approach
produces meaningful coarse graphs and yields competitive performance compared
with supervised methods for graph classification and regression.","cs.LG,stat.ML"
"Nonlinear Higher-Order Label Spreading. Label spreading is a general technique for semi-supervised learning with
point cloud or network data, which can be interpreted as a diffusion of labels
on a graph. While there are many variants of label spreading, nearly all of
them are linear models, where the incoming information to a node is a weighted
sum of information from neighboring nodes. Here, we add nonlinearity to label
spreading through nonlinear functions of higher-order structure in the graph,
namely triangles in the graph. For a broad class of nonlinear functions, we
prove convergence of our nonlinear higher-order label spreading algorithm to
the global solution of a constrained semi-supervised loss function. We
demonstrate the efficiency and efficacy of our approach on a variety of point
cloud and network datasets, where the nonlinear higher-order model compares
favorably to classical label spreading, as well as hypergraph models and graph
neural networks.","cs.LG,cs.SI,math.SP,physics.data-an,stat.ML"
"Deep Multi-attribute Graph Representation Learning on Protein Structures. Graphs as a type of data structure have recently attracted significant
attention. Representation learning of geometric graphs has achieved great
success in many fields including molecular, social, and financial networks. It
is natural to present proteins as graphs in which nodes represent the residues
and edges represent the pairwise interactions between residues. However, 3D
protein structures have rarely been studied as graphs directly. The challenges
include: 1) Proteins are complex macromolecules composed of thousands of atoms
making them much harder to model than micro-molecules. 2) Capturing the
long-range pairwise relations for protein structure modeling remains
under-explored. 3) Few studies have focused on learning the different
attributes of proteins together. To address the above challenges, we propose a
new graph neural network architecture to represent the proteins as 3D graphs
and predict both distance geometric graph representation and dihedral geometric
graph representation together. This gives a significant advantage because this
network opens a new path from the sequence to structure. We conducted extensive
experiments on four different datasets and demonstrated the effectiveness of
the proposed method.","cs.LG,q-bio.BM,q-bio.QM"
"Neural Closure Models for Dynamical Systems. Complex dynamical systems are used for predictions in many domains. Because
of computational costs, models are truncated, coarsened, or aggregated. As the
neglected and unresolved terms become important, the utility of model
predictions diminishes. We develop a novel, versatile, and rigorous methodology
to learn non-Markovian closure parameterizations for known-physics/low-fidelity
models using data from high-fidelity simulations. The new ""neural closure
models"" augment low-fidelity models with neural delay differential equations
(nDDEs), motivated by the Mori-Zwanzig formulation and the inherent delays in
complex dynamical systems. We demonstrate that neural closures efficiently
account for truncated modes in reduced-order-models, capture the effects of
subgrid-scale processes in coarse models, and augment the simplification of
complex biological and physical-biogeochemical models. We find that using
non-Markovian over Markovian closures improves long-term prediction accuracy
and requires smaller networks. We derive adjoint equations and network
architectures needed to efficiently implement the new discrete and distributed
nDDEs, for any time-integration schemes and allowing nonuniformly-spaced
temporal training data. The performance of discrete over distributed delays in
closure models is explained using information theory, and we find an optimal
amount of past information for a specified architecture. Finally, we analyze
computational complexity and explain the limited additional cost due to neural
closure models.","34A99,68T01 (Primary) 37M05,86-08 (Secondary),J.2; I.2.m,cs.LG,math.DS,physics.flu-dyn"
"Decentralized Knowledge Graph Representation Learning. Knowledge graph (KG) representation learning methods have achieved
competitive performance in many KG-oriented tasks, among which the best ones
are usually based on graph neural networks (GNNs), a powerful family of
networks that learns the representation of an entity by aggregating the
features of its neighbors and itself. However, many KG representation learning
scenarios only provide the structure information that describes the
relationships among entities, causing that entities have no input features. In
this case, existing aggregation mechanisms are incapable of inducing embeddings
of unseen entities as these entities have no pre-defined features for
aggregation. In this paper, we present a decentralized KG representation
learning approach, decentRL, which encodes each entity from and only from the
embeddings of its neighbors. For optimization, we design an algorithm to
distill knowledge from the model itself such that the output embeddings can
continuously gain knowledge from the corresponding original embeddings.
Extensive experiments show that the proposed approach performed better than
many cutting-edge models on the entity alignment task, and achieved competitive
performance on the entity prediction task. Furthermore, under the inductive
setting, it significantly outperformed all baselines on both tasks.","cs.AI,cs.CL,cs.LG"
"More Real than Real: A Study on Human Visual Perception of Synthetic Faces. Deep fakes became extremely popular in the last years, also thanks to their
increasing realism. Therefore, there is the need to measures human's ability to
distinguish between real and synthetic face images when confronted with
cutting-edge creation technologies. We describe the design and results of a
perceptual experiment we have conducted, where a wide and diverse group of
volunteers has been exposed to synthetic face images produced by
state-of-the-art Generative Adversarial Networks (namely, PG-GAN, StyleGAN,
StyleGAN2). The experiment outcomes reveal how strongly we should call into
question our human ability to discriminate real faces from synthetic ones
generated through modern AI.","cs.CV,cs.CY"
"The Sylvester Graphical Lasso (SyGlasso). This paper introduces the Sylvester graphical lasso (SyGlasso) that captures
multiway dependencies present in tensor-valued data. The model is based on the
Sylvester equation that defines a generative model. The proposed model
complements the tensor graphical lasso (Greenewald et al., 2019) that imposes a
Kronecker sum model for the inverse covariance matrix by providing an
alternative Kronecker sum model that is generative and interpretable. A
nodewise regression approach is adopted for estimating the conditional
independence relationships among variables. The statistical convergence of the
method is established, and empirical studies are provided to demonstrate the
recovery of meaningful conditional dependency graphs. We apply the SyGlasso to
an electroencephalography (EEG) study to compare the brain connectivity of
alcoholic and nonalcoholic subjects. We demonstrate that our model can
simultaneously estimate both the brain connectivity and its temporal
dependencies.","cs.LG,stat.ME,stat.ML"
"Feature-Attention Graph Convolutional Networks for Noise Resilient Learning. Noise and inconsistency commonly exist in real-world information networks,
due to inherent error-prone nature of human or user privacy concerns. To date,
tremendous efforts have been made to advance feature learning from networks,
including the most recent Graph Convolutional Networks (GCN) or attention GCN,
by integrating node content and topology structures. However, all existing
methods consider networks as error-free sources and treat feature content in
each node as independent and equally important to model node relations. The
erroneous node content, combined with sparse features, provide essential
challenges for existing methods to be used on real-world noisy networks. In
this paper, we propose FA-GCN, a feature-attention graph convolution learning
framework, to handle networks with noisy and sparse node content. To tackle
noise and sparse content in each node, FA-GCN first employs a long short-term
memory (LSTM) network to learn dense representation for each feature. To model
interactions between neighboring nodes, a feature-attention mechanism is
introduced to allow neighboring nodes learn and vary feature importance, with
respect to their connections. By using spectral-based graph convolution
aggregation process, each node is allowed to concentrate more on the most
determining neighborhood features aligned with the corresponding learning task.
Experiments and validations, w.r.t. different noise levels, demonstrate that
FA-GCN achieves better performance than state-of-the-art methods on both
noise-free and noisy networks.","cs.AI,cs.LG,cs.SI"
"Adversarial Attacks on Multivariate Time Series. Classification models for the multivariate time series have gained
significant importance in the research community, but not much research has
been done on generating adversarial samples for these models. Such samples of
adversaries could become a security concern. In this paper, we propose
transforming the existing adversarial transformation network (ATN) on a
distilled model to attack various multivariate time series classification
models. The proposed attack on the classification model utilizes a distilled
model as a surrogate that mimics the behavior of the attacked classical
multivariate time series classification models. The proposed methodology is
tested onto 1-Nearest Neighbor Dynamic Time Warping (1-NN DTW) and a Fully
Convolutional Network (FCN), all of which are trained on 18 University of East
Anglia (UEA) and University of California Riverside (UCR) datasets. We show
both models were susceptible to attacks on all 18 datasets. To the best of our
knowledge, adversarial attacks have only been conducted in the domain of
univariate time series and have not been conducted on multivariate time series.
such an attack on time series classification models has never been done before.
Additionally, we recommend future researchers that develop time series
classification models to incorporating adversarial data samples into their
training data sets to improve resilience on adversarial samples and to consider
model robustness as an evaluative metric.","cs.CR,cs.LG,stat.ML"
"learn2learn: A Library for Meta-Learning Research. Meta-learning researchers face two fundamental issues in their empirical
work: prototyping and reproducibility. Researchers are prone to make mistakes
when prototyping new algorithms and tasks because modern meta-learning methods
rely on unconventional functionalities of machine learning frameworks. In turn,
reproducing existing results becomes a tedious endeavour -- a situation
exacerbated by the lack of standardized implementations and benchmarks. As a
result, researchers spend inordinate amounts of time on implementing software
rather than understanding and developing new ideas.
  This manuscript introduces learn2learn, a library for meta-learning research
focused on solving those prototyping and reproducibility issues. learn2learn
provides low-level routines common across a wide-range of meta-learning
techniques (e.g. meta-descent, meta-reinforcement learning, few-shot learning),
and builds standardized interfaces to algorithms and benchmarks on top of them.
In releasing learn2learn under a free and open source license, we hope to
foster a community around standardized software for meta-learning research.","cs.CV,cs.LG,cs.RO,stat.ML"
"Deep neural networks can be improved using human-derived contextual expectations. Real-world objects occur in specific contexts. Such context has been shown to
facilitate detection by constraining the locations to search. But can context
directly benefit object detection? To do so, context needs to be learned
independently from target features. This is impossible in traditional object
detection where classifiers are trained on images containing both target
features and surrounding context. In contrast, humans can learn context and
target features separately, such as when we see highways without cars. Here we
show for the first time that human-derived scene expectations can be used to
improve object detection performance in machines. To measure contextual
expectations, we asked human subjects to indicate the scale, location and
likelihood at which cars or people might occur in scenes without these objects.
Humans showed highly systematic expectations that we could accurately predict
using scene features. This allowed us to predict human expectations on novel
scenes without requiring manual annotation. On augmenting deep neural networks
with predicted human expectations, we obtained substantial gains in accuracy
for detecting cars and people (1-3%) as well as on detecting associated objects
(3-20%). In contrast, augmenting deep networks with other conventional features
yielded far smaller gains. This improvement was due to relatively poor matches
at highly likely locations being correctly labelled as target and conversely
strong matches at unlikely locations being correctly rejected as false alarms.
Taken together, our results show that augmenting deep neural networks with
human-derived context features improves their performance, suggesting that
humans learn scene context separately unlike deep networks.","cs.CV,q-bio.NC"
"Overlapping Spaces for Compact Graph Representations. Various non-trivial spaces are becoming popular for embedding structured data
such as graphs, texts, or images. Following spherical and hyperbolic spaces,
more general product spaces have been proposed. However, searching for the best
configuration of product space is a resource-intensive procedure, which reduces
the practical applicability of the idea. We generalize the concept of product
space and introduce an overlapping space that does not have the configuration
search problem. The main idea is to allow subsets of coordinates to be shared
between spaces of different types (Euclidean, hyperbolic, spherical). As a
result, parameter optimization automatically learns the optimal configuration.
Additionally, overlapping spaces allow for more compact representations since
their geometry is more complex. Our experiments confirm that overlapping spaces
outperform the competitors in graph embedding tasks. Here, we consider both
distortion setup, where the aim is to preserve distances, and ranking setup,
where the relative order should be preserved. The proposed method effectively
solves the problem and outperforms the competitors in both settings. We also
perform an empirical analysis in a realistic information retrieval task, where
we compare all spaces by incorporating them into DSSM. In this case, the
proposed overlapping space consistently achieves nearly optimal results without
any configuration tuning. This allows for reducing training time, which can be
significant in large-scale applications.","I.2.6,cs.IR,cs.LG,stat.ML"
"Policy Mirror Descent for Reinforcement Learning: Linear Convergence, New Sampling Complexity, and Generalized Problem Classes. We present new policy mirror descent (PMD) methods for solving reinforcement
learning (RL) problems with either strongly convex or general convex
regularizers. By exploring the structural properties of these overall highly
nonconvex problems we show that the PMD methods exhibit fast linear rate of
convergence to the global optimality. We develop stochastic counterparts of
these methods, and establish an ${\cal O}(1/\epsilon)$ (resp., ${\cal
O}(1/\epsilon^2)$) sampling complexity for solving these RL problems with
strongly (resp., general) convex regularizers using different sampling schemes,
where $\epsilon$ denote the target accuracy. We further show that the
complexity for computing the gradients of these regularizers, if necessary, can
be bounded by ${\cal O}\{(\log_\gamma \epsilon) [(1-\gamma)L/\mu]^{1/2}\log
(1/\epsilon)\}$ (resp., ${\cal O} \{(\log_\gamma \epsilon )
(L/\epsilon)^{1/2}\}$)for problems with strongly (resp., general) convex
regularizers. Here $\gamma$ denotes the discounting factor. To the best of our
knowledge, these complexity bounds, along with our algorithmic developments,
appear to be new in both optimization and RL literature. The introduction of
these convex regularizers also greatly expands the flexibility and
applicability of RL models.","cs.AI,cs.LG,math.OC"
"Liver Segmentation in Abdominal CT Images via Auto-Context Neural Network and Self-Supervised Contour Attention. Accurate image segmentation of the liver is a challenging problem owing to
its large shape variability and unclear boundaries. Although the applications
of fully convolutional neural networks (CNNs) have shown groundbreaking
results, limited studies have focused on the performance of generalization. In
this study, we introduce a CNN for liver segmentation on abdominal computed
tomography (CT) images that shows high generalization performance and accuracy.
To improve the generalization performance, we initially propose an auto-context
algorithm in a single CNN. The proposed auto-context neural network exploits an
effective high-level residual estimation to obtain the shape prior. Identical
dual paths are effectively trained to represent mutual complementary features
for an accurate posterior analysis of a liver. Further, we extend our network
by employing a self-supervised contour scheme. We trained sparse contour
features by penalizing the ground-truth contour to focus more contour
attentions on the failures. The experimental results show that the proposed
network results in better accuracy when compared to the state-of-the-art
networks by reducing 10.31% of the Hausdorff distance. We used 180 abdominal CT
images for training and validation. Two-fold cross-validation is presented for
a comparison with the state-of-the-art neural networks. Novel multiple N-fold
cross-validations are conducted to verify the performance of generalization.
The proposed network showed the best generalization performance among the
networks. Additionally, we present a series of ablation experiments that
comprehensively support the importance of the underlying concepts.","68U10,cs.CV"
"SPINN: Sparse, Physics-based, and partially Interpretable Neural Networks for PDEs. We introduce a class of Sparse, Physics-based, and partially Interpretable
Neural Networks (SPINN) for solving ordinary and partial differential equations
(PDEs). By reinterpreting a traditional meshless representation of solutions of
PDEs we develop a class of sparse neural network architectures that are
partially interpretable. The SPINN model we propose here serves as a seamless
bridge between two extreme modeling tools for PDEs, namely dense neural network
based methods like Physics Informed Neural Networks (PINNs) and traditional
mesh-free numerical methods, thereby providing a novel means to develop a new
class of hybrid algorithms that build on the best of both these viewpoints. A
unique feature of the SPINN model that distinguishes it from other neural
network based approximations proposed earlier is that it is (i) interpretable,
in a particular sense made precise in the work, and (ii) sparse in the sense
that it has much fewer connections than typical dense neural networks used for
PDEs. Further, the SPINN algorithm implicitly encodes mesh adaptivity and is
able to handle discontinuities in the solutions. In addition, we demonstrate
that Fourier series representations can also be expressed as a special class of
SPINN and propose generalized neural network analogues of Fourier
representations. We illustrate the utility of the proposed method with a
variety of examples involving ordinary differential equations, elliptic,
parabolic, hyperbolic and nonlinear partial differential equations, and an
example in fluid dynamics.","cs.LG,physics.comp-ph"
"Visual Scene Graphs for Audio Source Separation. State-of-the-art approaches for visually-guided audio source separation
typically assume sources that have characteristic sounds, such as musical
instruments. These approaches often ignore the visual context of these sound
sources or avoid modeling object interactions that may be useful to better
characterize the sources, especially when the same object class may produce
varied sounds from distinct interactions. To address this challenging problem,
we propose Audio Visual Scene Graph Segmenter (AVSGS), a novel deep learning
model that embeds the visual structure of the scene as a graph and segments
this graph into subgraphs, each subgraph being associated with a unique sound
obtained by co-segmenting the audio spectrogram. At its core, AVSGS uses a
recursive neural network that emits mutually-orthogonal sub-graph embeddings of
the visual graph using multi-head attention. These embeddings are used for
conditioning an audio encoder-decoder towards source separation. Our pipeline
is trained end-to-end via a self-supervised task consisting of separating audio
sources using the visual graph from artificially mixed sounds. In this paper,
we also introduce an ""in the wild'' video dataset for sound source separation
that contains multiple non-musical sources, which we call Audio Separation in
the Wild (ASIW). This dataset is adapted from the AudioCaps dataset, and
provides a challenging, natural, and daily-life setting for source separation.
Thorough experiments on the proposed ASIW and the standard MUSIC datasets
demonstrate state-of-the-art sound separation performance of our method against
recent prior approaches.","cs.AI,cs.CV,cs.LG,cs.SD,eess.AS"
"Minimax Estimation of Neural Net Distance. An important class of distance metrics proposed for training generative
adversarial networks (GANs) is the integral probability metric (IPM), in which
the neural net distance captures the practical GAN training via two neural
networks. This paper investigates the minimax estimation problem of the neural
net distance based on samples drawn from the distributions. We develop the
first known minimax lower bound on the estimation error of the neural net
distance, and an upper bound tighter than an existing bound on the estimator
error for the empirical neural net distance. Our lower and upper bounds match
not only in the order of the sample size but also in terms of the norm of the
parameter matrices of neural networks, which justifies the empirical neural net
distance as a good approximation of the true neural net distance for training
GANs in practice.","cs.IT,cs.LG,math.IT,stat.ML"
"Machine learning with quantum field theories. The precise equivalence between discretized Euclidean field theories and a
certain class of probabilistic graphical models, namely the mathematical
framework of Markov random fields, opens up the opportunity to investigate
machine learning from the perspective of quantum field theory. In this
contribution we will demonstrate, through the Hammersley-Clifford theorem, that
the $\phi^{4}$ scalar field theory on a square lattice satisfies the local
Markov property and can therefore be recast as a Markov random field. We will
then derive from the $\phi^{4}$ theory machine learning algorithms and neural
networks which can be viewed as generalizations of conventional neural network
architectures. Finally, we will conclude by presenting applications based on
the minimization of an asymmetric distance between the probability distribution
of the $\phi^{4}$ machine learning algorithms and target probability
distributions.","cs.LG,hep-lat,hep-th,math-ph,math.MP,math.PR"
"Input Dropout for Spatially Aligned Modalities. Computer vision datasets containing multiple modalities such as color, depth,
and thermal properties are now commonly accessible and useful for solving a
wide array of challenging tasks. However, deploying multi-sensor heads is not
possible in many scenarios. As such many practical solutions tend to be based
on simpler sensors, mostly for cost, simplicity and robustness considerations.
In this work, we propose a training methodology to take advantage of these
additional modalities available in datasets, even if they are not available at
test time. By assuming that the modalities have a strong spatial correlation,
we propose Input Dropout, a simple technique that consists in stochastic hiding
of one or many input modalities at training time, while using only the
canonical (e.g. RGB) modalities at test time. We demonstrate that Input Dropout
trivially combines with existing deep convolutional architectures, and improves
their performance on a wide range of computer vision tasks such as dehazing,
6-DOF object tracking, pedestrian detection and object classification.","cs.CV,cs.LG,eess.IV"
"Efficient and Scalable Structure Learning for Bayesian Networks: Algorithms and Applications. Structure Learning for Bayesian network (BN) is an important problem with
extensive research. It plays central roles in a wide variety of applications in
Alibaba Group. However, existing structure learning algorithms suffer from
considerable limitations in real world applications due to their low efficiency
and poor scalability. To resolve this, we propose a new structure learning
algorithm LEAST, which comprehensively fulfills our business requirements as it
attains high accuracy, efficiency and scalability at the same time. The core
idea of LEAST is to formulate the structure learning into a continuous
constrained optimization problem, with a novel differentiable constraint
function measuring the acyclicity of the resulting graph. Unlike with existing
work, our constraint function is built on the spectral radius of the graph and
could be evaluated in near linear time w.r.t. the graph node size. Based on it,
LEAST can be efficiently implemented with low storage overhead. According to
our benchmark evaluation, LEAST runs 1 to 2 orders of magnitude faster than
state of the art method with comparable accuracy, and it is able to scale on
BNs with up to hundreds of thousands of variables. In our production
environment, LEAST is deployed and serves for more than 20 applications with
thousands of executions per day. We describe a concrete scenario in a ticket
booking service in Alibaba, where LEAST is applied to build a near real-time
automatic anomaly detection and root error cause analysis system. We also show
that LEAST unlocks the possibility of applying BN structure learning in new
areas, such as large-scale gene expression data analysis and explainable
recommendation system.","cs.AI,cs.DB,cs.LG"
"Binary Classification as a Phase Separation Process. We propose a new binary classification model called Phase Separation Binary
Classifier (PSBC). It consists of a discretization of a nonlinear
reaction-diffusion equation coupled with an Ordinary Differential Equation, and
is inspired by fluids behavior, namely, on how binary fluids phase separate.
Thus, parameters and hyperparameters have physical meaning, whose effects are
studied in several different scenarios.
  PSBC's equations can be seen as a dynamical system whose coefficients are
trainable weights, with a similar architecture to that of a Recurrent Neural
Network. As such, forward propagation amounts to an initial value problem.
Boundary conditions are also present, bearing similarity with figure padding
techniques in Computer Vision. Model compression is exploited in several ways,
with weight sharing taking place both across and within layers.
  The model is tested on pairs of digits of the classical MNIST database. An
associated multiclass classifier is also constructed using a combination of
Ensemble Learning and one versus one techniques. It is also shown how the PSBC
can be combined with other methods - like aggregation and PCA - in order to
construct better binary classifiers. The role of boundary conditions and
viscosity is thoroughly studied in the case of digits ``0'' and ``1''.","35B50,35K57,37N30,65M06,I.5,cs.LG,cs.NA,math.DS,math.NA,math.OC,stat.ML"
"Estimating permeability of 3D micro-CT images by physics-informed CNNs based on DNS. In recent years, convolutional neural networks (CNNs) have experienced an
increasing interest for their ability to perform fast approximation of
effective hydrodynamic parameters in porous media research and applications.
This paper presents a novel methodology for permeability prediction from
micro-CT scans of geological rock samples. The training data set for CNNs
dedicated to permeability prediction consists of permeability labels that are
typically generated by classical lattice Boltzmann methods (LBM) that simulate
the flow through the pore space of the segmented image data. We instead perform
direct numerical simulation (DNS) by solving the stationary Stokes equation in
an efficient and distributed-parallel manner. As such, we circumvent the
convergence issues of LBM that frequently are observed on complex pore
geometries, and therefore, improve on the generality and accuracy of our
training data set. Using the DNS-computed permeabilities, a physics-informed
CNN PhyCNN) is trained by additionally providing a tailored characteristic
quantity of the pore space. More precisely, by exploiting the connection to
flow problems on a graph representation of the pore space, additional
information about confined structures is provided to the network in terms of
the maximum flow value, which is the key innovative component of our workflow.
As a result, unprecedented prediction accuracy and robustness are observed for
a variety of sandstone samples from archetypal rock formations.","05C21,68T07,76D07,76M10,76S05,cs.LG,cs.NA,math.NA"
"Fourier-based and Rational Graph Filters for Spectral Processing. Data are represented as graphs in a wide range of applications, such as
Computer Vision (e.g., images) and Graphics (e.g., 3D meshes), network analysis
(e.g., social networks), and bio-informatics (e.g., molecules). In this
context, our overall goal is the definition of novel Fourier-based and graph
filters induced by rational polynomials for graph processing, which generalise
polynomial filters and the Fourier transform to non-Euclidean domains. For the
efficient evaluation of discrete spectral Fourier-based and wavelet operators,
we introduce a spectrum-free approach, which requires the solution of a small
set of sparse, symmetric, well-conditioned linear systems and is oblivious of
the evaluation of the Laplacian or kernel spectrum. Approximating arbitrary
graph filters with rational polynomials provides a more accurate and
numerically stable alternative with respect to polynomials. To achieve these
goals, we also study the link between spectral operators, wavelets, and
filtered convolution with integral operators induced by spectral kernels.
According to our tests, main advantages of the proposed approach are (i) its
generality with respect to the input data (e.g., graphs, 3D shapes),
applications (e.g., signal reconstruction and smoothing, shape correspondence),
and filters (e.g., polynomial, rational polynomial), and (ii) a spectrum-free
computation with a generally low computational cost and storage overhead.","cs.GR,cs.LG"
"MobiSR: Efficient On-Device Super-Resolution through Heterogeneous Mobile Processors. In recent years, convolutional networks have demonstrated unprecedented
performance in the image restoration task of super-resolution (SR). SR entails
the upscaling of a single low-resolution image in order to meet
application-specific image quality demands and plays a key role in mobile
devices. To comply with privacy regulations and reduce the overhead of cloud
computing, executing SR models locally on-device constitutes a key alternative
approach. Nevertheless, the excessive compute and memory requirements of SR
workloads pose a challenge in mapping SR networks on resource-constrained
mobile platforms. This work presents MobiSR, a novel framework for performing
efficient super-resolution on-device. Given a target mobile platform, the
proposed framework considers popular model compression techniques and traverses
the design space to reach the highest performing trade-off between image
quality and processing speed. At run time, a novel scheduler dispatches
incoming image patches to the appropriate model-engine pair based on the
patch's estimated upscaling difficulty in order to meet the required image
quality with minimum processing latency. Quantitative evaluation shows that the
proposed framework yields on-device SR designs that achieve an average speedup
of 2.13x over highly-optimized parallel difficulty-unaware mappings and 4.79x
over highly-optimized single compute engine implementations.","cs.CV,cs.DC"
"Inductive biases and Self Supervised Learning in modelling a physical heating system. Model Predictive Controllers (MPC) require a good model for the controlled
process. In this paper I infer inductive biases about a physical system. I use
these biases to derive a new neural network architecture that can model this
real system that has noise and inertia. The main inductive biases exploited
here are: the delayed impact of some inputs on the system and the separability
between the temporal component and how the inputs interact to produce the
output of a system. The inputs are independently delayed using shifted
convolutional kernels. Feature interactions are modelled using a fully
connected network that does not have access to temporal information. The
available data and the problem setup allow the usage of Self Supervised
Learning in order to train the models. The baseline architecture is an
Attention based Reccurent network adapted to work with MPC like inputs. The
proposed networks are faster, better at exploiting larger data volumes and are
almost as good as baseline networks in terms of prediction performance. The
proposed architecture family called Delay can be used in a real scenario to
control systems with delayed responses with respect to its controls or inputs.
Ablation studies show that the presence of delay kernels are vital to obtain
any learning in proposed architecture. Code and some experimental data are
available online.","I.2.8,cs.LG,cs.SY,eess.SY"
"Mutual Information-driven Subject-invariant and Class-relevant Deep Representation Learning in BCI. In recent years, deep learning-based feature representation methods have
shown a promising impact in electroencephalography (EEG)-based brain-computer
interface (BCI). Nonetheless, owing to high intra- and inter-subject
variabilities, many studies on decoding EEG were designed in a subject-specific
manner by using calibration samples, with no concern of its practical use,
hampered by time-consuming steps and a large data requirement. To this end,
recent studies adopted a transfer learning strategy, especially domain
adaptation techniques. Among those, to our knowledge, an adversarial learning
has shown its potential in BCIs. In the meantime, it is known that adversarial
learning-based domain adaptation methods are prone to negative transfer that
disrupts learning generalized feature representations, applicable to diverse
domains, e.g., subjects or sessions in BCIs. In this paper, we propose a novel
framework that learns class-relevant and subject-invariant feature
representations in an information-theoretic manner, without using adversarial
learning. To be specific, we devise two operational components in a deep
network that explicitly estimate mutual information between feature
representations; (1) to decompose features in an intermediate layer into
class-relevant and class-irrelevant ones, (2) to enrich class-discriminative
feature representation. On two large EEG datasets, we validated the
effectiveness of our proposed framework by comparing with several comparative
methods in performance. Further, we conducted rigorous analyses by performing
an ablation study in regard to the components in our network, explaining our
model's decision on input EEG signals via layer-wise relevance propagation, and
visualizing the distribution of learned features via t-SNE.","cs.HC,cs.LG,stat.ML"
"Estimation of Driver's Gaze Region from Head Position and Orientation using Probabilistic Confidence Regions. A smart vehicle should be able to understand human behavior and predict their
actions to avoid hazardous situations. Specific traits in human behavior can be
automatically predicted, which can help the vehicle make decisions, increasing
safety. One of the most important aspects pertaining to the driving task is the
driver's visual attention. Predicting the driver's visual attention can help a
vehicle understand the awareness state of the driver, providing important
contextual information. While estimating the exact gaze direction is difficult
in the car environment, a coarse estimation of the visual attention can be
obtained by tracking the position and orientation of the head. Since the
relation between head pose and gaze direction is not one-to-one, this paper
proposes a formulation based on probabilistic models to create salient regions
describing the visual attention of the driver. The area of the predicted region
is small when the model has high confidence on the prediction, which is
directly learned from the data. We use Gaussian process regression (GPR) to
implement the framework, comparing the performance with different regression
formulations such as linear regression and neural network based methods. We
evaluate these frameworks by studying the tradeoff between spatial resolution
and accuracy of the probability map using naturalistic recordings collected
with the UTDrive platform. We observe that the GPR method produces the best
result creating accurate predictions with localized salient regions. For
example, the 95% confidence region is defined by an area that covers 3.77%
region of a sphere surrounding the driver.","68T45,cs.CV"
"On the Convergence of Approximate and Regularized Policy Iteration Schemes. Entropy regularized algorithms such as Soft Q-learning and Soft Actor-Critic,
recently showed state-of-the-art performance on a number of challenging
reinforcement learning (RL) tasks. The regularized formulation modifies the
standard RL objective and thus generally converges to a policy different from
the optimal greedy policy of the original RL problem. Practically, it is
important to control the sub-optimality of the regularized optimal policy. In
this paper, we establish sufficient conditions for convergence of a large class
of regularized dynamic programming algorithms, unified under regularized
modified policy iteration (MPI) and conservative value iteration (VI) schemes.
We provide explicit convergence rates to the optimality depending on the
decrease rate of the regularization parameter. Our experiments show that the
empirical error closely follows the established theoretical convergence rates.
In addition to optimality, we demonstrate two desirable behaviours of the
regularized algorithms even in the absence of approximations: robustness to
stochasticity of environment and safety of trajectories induced by the policy
iterates.","cs.LG,stat.ML"
"A Unified Framework of Online Learning Algorithms for Training Recurrent Neural Networks. We present a framework for compactly summarizing many recent results in
efficient and/or biologically plausible online training of recurrent neural
networks (RNN). The framework organizes algorithms according to several
criteria: (a) past vs. future facing, (b) tensor structure, (c) stochastic vs.
deterministic, and (d) closed form vs. numerical. These axes reveal latent
conceptual connections among several recent advances in online learning.
Furthermore, we provide novel mathematical intuitions for their degree of
success. Testing various algorithms on two synthetic tasks shows that
performances cluster according to our criteria. Although a similar clustering
is also observed for gradient alignment, alignment with exact methods does not
alone explain ultimate performance, especially for stochastic algorithms. This
suggests the need for better comparison metrics.","cs.LG,cs.NE,q-bio.NC,stat.ML"
"A Survey of Pansharpening Methods with A New Band-Decoupled Variational Model. Most satellites decouple the acquisition of a panchromatic image at high
spatial resolution from the acquisition of a multispectral image at lower
spatial resolution. Pansharpening is a fusion technique used to increase the
spatial resolution of the multispectral data while simultaneously preserving
its spectral information. In this paper, we consider pansharpening as an
optimization problem minimizing a cost function with a nonlocal regularization
term. The energy functional which is to be minimized decouples for each band,
thus permitting the application to misregistered spectral components. This
requirement is achieved by dropping the, commonly used, assumption that relates
the spectral and panchromatic modalities by a linear transformation. Instead, a
new constraint that preserves the radiometric ratio between the panchromatic
and each spectral component is introduced. An exhaustive performance comparison
of the proposed fusion method with several classical and state-of-the-art
pansharpening techniques illustrates its superiority in preserving spatial
details, reducing color distortions, and avoiding the creation of aliasing
artifacts.","15A29,65F22,65K10,68U10,90C25,90C46,cs.CV,math.OC"
"Fast and Secure Distributed Nonnegative Matrix Factorization. Nonnegative matrix factorization (NMF) has been successfully applied in
several data mining tasks. Recently, there is an increasing interest in the
acceleration of NMF, due to its high cost on large matrices. On the other hand,
the privacy issue of NMF over federated data is worthy of attention, since NMF
is prevalently applied in image and text analysis which may involve leveraging
privacy data (e.g, medical image and record) across several parties (e.g.,
hospitals). In this paper, we study the acceleration and security problems of
distributed NMF. Firstly, we propose a distributed sketched alternating
nonnegative least squares (DSANLS) framework for NMF, which utilizes a matrix
sketching technique to reduce the size of nonnegative least squares subproblems
with a convergence guarantee. For the second problem, we show that DSANLS with
modification can be adapted to the security setting, but only for one or
limited iterations. Consequently, we propose four efficient distributed NMF
methods in both synchronous and asynchronous settings with a security
guarantee. We conduct extensive experiments on several real datasets to show
the superiority of our proposed methods. The implementation of our methods is
available at https://github.com/qianyuqiu79/DSANLS.","cs.CR,cs.DC,cs.LG,stat.ML"
"Deep Reinforcement Learning for Autonomous Driving: A Survey. With the development of deep representation learning, the domain of
reinforcement learning (RL) has become a powerful learning framework now
capable of learning complex policies in high dimensional environments. This
review summarises deep reinforcement learning (DRL) algorithms and provides a
taxonomy of automated driving tasks where (D)RL methods have been employed,
while addressing key computational challenges in real world deployment of
autonomous driving agents. It also delineates adjacent domains such as behavior
cloning, imitation learning, inverse reinforcement learning that are related
but are not classical RL algorithms. The role of simulators in training agents,
methods to validate, test and robustify existing solutions in RL are discussed.","cs.AI,cs.LG,cs.RO"
"Learning Restricted Boltzmann Machines via Influence Maximization. Graphical models are a rich language for describing high-dimensional
distributions in terms of their dependence structure. While there are
algorithms with provable guarantees for learning undirected graphical models in
a variety of settings, there has been much less progress in the important
scenario when there are latent variables. Here we study Restricted Boltzmann
Machines (or RBMs), which are a popular model with wide-ranging applications in
dimensionality reduction, collaborative filtering, topic modeling, feature
extraction and deep learning.
  The main message of our paper is a strong dichotomy in the feasibility of
learning RBMs, depending on the nature of the interactions between variables:
ferromagnetic models can be learned efficiently, while general models cannot.
In particular, we give a simple greedy algorithm based on influence
maximization to learn ferromagnetic RBMs with bounded degree. In fact, we learn
a description of the distribution on the observed variables as a Markov Random
Field. Our analysis is based on tools from mathematical physics that were
developed to show the concavity of magnetization. Our algorithm extends
straighforwardly to general ferromagnetic Ising models with latent variables.
  Conversely, we show that even for a contant number of latent variables with
constant degree, without ferromagneticity the problem is as hard as sparse
parity with noise. This hardness result is based on a sharp and surprising
characterization of the representational power of bounded degree RBMs: the
distribution on their observed variables can simulate any bounded order MRF.
This result is of independent interest since RBMs are the building blocks of
deep belief networks.","cs.DS,cs.LG,math.PR,stat.ML"
"Liver Segmentation in Abdominal CT Images via Auto-Context Neural Network and Self-Supervised Contour Attention. Accurate image segmentation of the liver is a challenging problem owing to
its large shape variability and unclear boundaries. Although the applications
of fully convolutional neural networks (CNNs) have shown groundbreaking
results, limited studies have focused on the performance of generalization. In
this study, we introduce a CNN for liver segmentation on abdominal computed
tomography (CT) images that shows high generalization performance and accuracy.
To improve the generalization performance, we initially propose an auto-context
algorithm in a single CNN. The proposed auto-context neural network exploits an
effective high-level residual estimation to obtain the shape prior. Identical
dual paths are effectively trained to represent mutual complementary features
for an accurate posterior analysis of a liver. Further, we extend our network
by employing a self-supervised contour scheme. We trained sparse contour
features by penalizing the ground-truth contour to focus more contour
attentions on the failures. The experimental results show that the proposed
network results in better accuracy when compared to the state-of-the-art
networks by reducing 10.31% of the Hausdorff distance. We used 180 abdominal CT
images for training and validation. Two-fold cross-validation is presented for
a comparison with the state-of-the-art neural networks. Novel multiple N-fold
cross-validations are conducted to verify the performance of generalization.
The proposed network showed the best generalization performance among the
networks. Additionally, we present a series of ablation experiments that
comprehensively support the importance of the underlying concepts.","68U10,cs.CV"
"Zero-Shot Reinforcement Learning with Deep Attention Convolutional Neural Networks. Simulation-to-simulation and simulation-to-real world transfer of neural
network models have been a difficult problem. To close the reality gap, prior
methods to simulation-to-real world transfer focused on domain adaptation,
decoupling perception and dynamics and solving each problem separately, and
randomization of agent parameters and environment conditions to expose the
learning agent to a variety of conditions. While these methods provide
acceptable performance, the computational complexity required to capture a
large variation of parameters for comprehensive scenarios on a given task such
as autonomous driving or robotic manipulation is high. Our key contribution is
to theoretically prove and empirically demonstrate that a deep attention
convolutional neural network (DACNN) with specific visual sensor configuration
performs as well as training on a dataset with high domain and parameter
variation at lower computational complexity. Specifically, the attention
network weights are learned through policy optimization to focus on local
dependencies that lead to optimal actions, and does not require tuning in
real-world for generalization. Our new architecture adapts perception with
respect to the control objective, resulting in zero-shot learning without
pre-training a perception network. To measure the impact of our new deep
network architecture on domain adaptation, we consider autonomous driving as a
use case. We perform an extensive set of experiments in
simulation-to-simulation and simulation-to-real scenarios to compare our
approach to several baselines including the current state-of-art models.","cs.LG,cs.RO,cs.SY,eess.SY"
"Curriculum learning for multilevel budgeted combinatorial problems. Learning heuristics for combinatorial optimization problems through graph
neural networks have recently shown promising results on some classic NP-hard
problems. These are single-level optimization problems with only one player.
Multilevel combinatorial optimization problems are their generalization,
encompassing situations with multiple players taking decisions sequentially. By
framing them in a multi-agent reinforcement learning setting, we devise a
value-based method to learn to solve multilevel budgeted combinatorial problems
involving two players in a zero-sum game over a graph. Our framework is based
on a simple curriculum: if an agent knows how to estimate the value of
instances with budgets up to $B$, then solving instances with budget $B+1$ can
be done in polynomial time regardless of the direction of the optimization by
checking the value of every possible afterstate. Thus, in a bottom-up approach,
we generate datasets of heuristically solved instances with increasingly larger
budgets to train our agent. We report results close to optimality on graphs up
to $100$ nodes and a $185 \times$ speedup on average compared to the quickest
exact solver known for the Multilevel Critical Node problem, a max-min-max
trilevel problem that has been shown to be at least $\Sigma_2^p$-hard.","cs.GT,cs.LG,stat.ML"
"Graph Representations for Higher-Order Logic and Theorem Proving. This paper presents the first use of graph neural networks (GNNs) for
higher-order proof search and demonstrates that GNNs can improve upon
state-of-the-art results in this domain. Interactive, higher-order theorem
provers allow for the formalization of most mathematical theories and have been
shown to pose a significant challenge for deep learning. Higher-order logic is
highly expressive and, even though it is well-structured with a clearly defined
grammar and semantics, there still remains no well-established method to
convert formulas into graph-based representations. In this paper, we consider
several graphical representations of higher-order logic and evaluate them
against the HOList benchmark for higher-order theorem proving.","cs.AI,cs.LG,cs.LO,stat.ML"
"Novel Meta-Heuristic Model for Discrimination between Iron Deficiency Anemia and B-Thalassemia with CBC Indices Based on Dynamic Harmony Search. In recent decades, attention has been directed at anemia classification for
various medical purposes, such as thalassemia screening and predicting iron
deficiency anemia (IDA). In this study, a new method has been successfully
tested for discrimination between IDA and \b{eta}-thalassemia trait
(\b{eta}-TT). The method is based on a Dynamic Harmony Search (DHS). Complete
blood count (CBC), a fast and inexpensive laboratory test, is used as the input
of the system. Other models, such as a genetic programming method called
structured representation on genetic algorithm in non-linear function fitting
(STROGANOFF), an artificial neural network (ANN), an adaptive neuro-fuzzy
inference system (ANFIS), a support vector machine (SVM), k-nearest neighbor
(KNN), and certain traditional methods, are compared with the proposed method.","68T05,cs.LG,stat.ML"
"Non-parametric Estimation of Stochastic Differential Equations with Sparse Gaussian Processes. The application of Stochastic Differential Equations (SDEs) to the analysis
of temporal data has attracted increasing attention, due to their ability to
describe complex dynamics with physically interpretable equations. In this
paper, we introduce a non-parametric method for estimating the drift and
diffusion terms of SDEs from a densely observed discrete time series. The use
of Gaussian processes as priors permits working directly in a function-space
view and thus the inference takes place directly in this space. To cope with
the computational complexity that requires the use of Gaussian processes, a
sparse Gaussian process approximation is provided. This approximation permits
the efficient computation of predictions for the drift and diffusion terms by
using a distribution over a small subset of pseudo-samples. The proposed method
has been validated using both simulated data and real data from economy and
paleoclimatology. The application of the method to real data demonstrates its
ability to capture the behaviour of complex systems.","physics.data-an,stat.ML"
"Submodular Combinatorial Information Measures with Applications in Machine Learning. Information-theoretic quantities like entropy and mutual information have
found numerous uses in machine learning. It is well known that there is a
strong connection between these entropic quantities and submodularity since
entropy over a set of random variables is submodular. In this paper, we study
combinatorial information measures that generalize independence, (conditional)
entropy, (conditional) mutual information, and total correlation defined over
sets of (not necessarily random) variables. These measures strictly generalize
the corresponding entropic measures since they are all parameterized via
submodular functions that themselves strictly generalize entropy. Critically,
we show that, unlike entropic mutual information in general, the submodular
mutual information is actually submodular in one argument, holding the other
fixed, for a large class of submodular functions whose third-order partial
derivatives satisfy a non-negativity property. This turns out to include a
number of practically useful cases such as the facility location and set-cover
functions. We study specific instantiations of the submodular information
measures on these, as well as the probabilistic coverage, graph-cut, and
saturated coverage functions, and see that they all have mathematically
intuitive and practically useful expressions. Regarding applications, we
connect the maximization of submodular (conditional) mutual information to
problems such as mutual-information-based, query-based, and privacy-preserving
summarization -- and we connect optimizing the multi-set submodular mutual
information to clustering and robust partitioning.","cs.DS,cs.IT,cs.LG,math.IT,math.OC,stat.ML"
"Explainable Enterprise Credit Rating via Deep Feature Crossing Network. Due to the powerful learning ability on high-rank and non-linear features,
deep neural networks (DNNs) are being applied to data mining and machine
learning in various fields, and exhibit higher discrimination performance than
conventional methods. However, the applications based on DNNs are rare in
enterprise credit rating tasks because most of DNNs employ the ""end-to-end""
learning paradigm, which outputs the high-rank representations of objects and
predictive results without any explanations. Thus, users in the financial
industry cannot understand how these high-rank representations are generated,
what do they mean and what relations exist with the raw inputs. Then users
cannot determine whether the predictions provided by DNNs are reliable, and not
trust the predictions providing by such ""black box"" models. Therefore, in this
paper, we propose a novel network to explicitly model the enterprise credit
rating problem using DNNs and attention mechanisms. The proposed model realizes
explainable enterprise credit ratings. Experimental results obtained on
real-world enterprise datasets verify that the proposed approach achieves
higher performance than conventional methods, and provides insights into
individual rating results and the reliability of model training.","cs.CE,cs.LG"
"Mitigating Multi-Stage Cascading Failure by Reinforcement Learning. This paper proposes a cascading failure mitigation strategy based on
Reinforcement Learning (RL) method. Firstly, the principles of RL are
introduced. Then, the Multi-Stage Cascading Failure (MSCF) problem is presented
and its challenges are investigated. The problem is then tackled by the RL
based on DC-OPF (Optimal Power Flow). Designs of the key elements of the RL
framework (rewards, states, etc.) are also discussed in detail. Experiments on
the IEEE 118-bus system by both shallow and deep neural networks demonstrate
promising results in terms of reduced system collapse rates.","I.2.1; I.2.8; I.6.3,cs.LG,cs.SY,eess.SY,math.DS,stat.ML"
"Normalization Before Shaking Toward Learning Symmetrically Distributed Representation Without Margin in Speech Emotion Recognition. Regularization is crucial to the success of many practical deep learning
models, in particular in a more often than not scenario where there are only a
few to a moderate number of accessible training samples. In addition to weight
decay, data augmentation and dropout, regularization based on multi-branch
architectures, such as Shake-Shake regularization, has been proven successful
in many applications and attracted more and more attention. However, beyond
model-based representation augmentation, it is unclear how Shake-Shake
regularization helps to provide further improvement on classification tasks,
let alone the baffling interaction between batch normalization and shaking. In
this work, we present our investigation on Shake-Shake regularization, drawing
connections to the vicinal risk minimization principle and discriminative
feature learning in verification tasks. Furthermore, we identify a strong
resemblance between batch normalized residual blocks and batch normalized
recurrent neural networks, where both of them share a similar convergence
behavior, which could be mitigated by a proper initialization of batch
normalization. Based on the findings, our experiments on speech emotion
recognition demonstrate simultaneously an improvement on the classification
accuracy and a reduction on the generalization gap both with statistical
significance.","cs.HC,cs.LG,cs.MM,cs.SD,eess.AS"
"Enhancing Transformer for Video Understanding Using Gated Multi-Level Attention and Temporal Adversarial Training. The introduction of Transformer model has led to tremendous advancements in
sequence modeling, especially in text domain. However, the use of
attention-based models for video understanding is still relatively unexplored.
In this paper, we introduce Gated Adversarial Transformer (GAT) to enhance the
applicability of attention-based models to videos. GAT uses a multi-level
attention gate to model the relevance of a frame based on local and global
contexts. This enables the model to understand the video at various
granularities. Further, GAT uses adversarial training to improve model
generalization. We propose temporal attention regularization scheme to improve
the robustness of attention modules to adversarial examples. We illustrate the
performance of GAT on the large-scale YoutTube-8M data set on the task of video
categorization. We further show ablation studies along with quantitative and
qualitative analysis to showcase the improvement.","cs.CV,cs.LG,cs.MM"
"Expert Graphs: Synthesizing New Expertise via Collaboration. Consider multiple experts with overlapping expertise working on a
classification problem under uncertain input. What constitutes a consistent set
of opinions? How can we predict the opinions of experts on missing sub-domains?
In this paper, we define a framework of to analyze this problem, termed ""expert
graphs."" In an expert graph, vertices represent classes and edges represent
binary opinions on the topics of their vertices. We derive necessary conditions
for expert graph validity and use them to create ""synthetic experts"" which
describe opinions consistent with the observed opinions of other experts. We
show this framework to be equivalent to the well-studied linear ordering
polytope. We show our conditions are not sufficient for describing all expert
graphs on cliques, but are sufficient for cycles.","cs.AI,cs.DM,cs.IT,cs.LG,econ.TH,math.IT"
"DFineNet: Ego-Motion Estimation and Depth Refinement from Sparse, Noisy Depth Input with RGB Guidance. Depth estimation is an important capability for autonomous vehicles to
understand and reconstruct 3D environments as well as avoid obstacles during
the execution. Accurate depth sensors such as LiDARs are often heavy, expensive
and can only provide sparse depth while lighter depth sensors such as stereo
cameras are noiser in comparison. We propose an end-to-end learning algorithm
that is capable of using sparse, noisy input depth for refinement and depth
completion. Our model also produces the camera pose as a byproduct, making it a
great solution for autonomous systems. We evaluate our approach on both indoor
and outdoor datasets. Empirical results show that our method performs well on
the KITTI~\cite{kitti_geiger2012we} dataset when compared to other competing
methods, while having superior performance in dealing with sparse, noisy input
depth on the TUM~\cite{sturm12iros} dataset.","cs.CV,cs.RO"
"Variational Inference for Sparse and Undirected Models. Undirected graphical models are applied in genomics, protein structure
prediction, and neuroscience to identify sparse interactions that underlie
discrete data. Although Bayesian methods for inference would be favorable in
these contexts, they are rarely used because they require doubly intractable
Monte Carlo sampling. Here, we develop a framework for scalable Bayesian
inference of discrete undirected models based on two new methods. The first is
Persistent VI, an algorithm for variational inference of discrete undirected
models that avoids doubly intractable MCMC and approximations of the partition
function. The second is Fadeout, a reparameterization approach for variational
inference under sparsity-inducing priors that captures a posteriori
correlations between parameters and hyperparameters with noncentered
parameterizations. We find that, together, these methods for variational
inference substantially improve learning of sparse undirected graphical models
in simulated and real problems from physics and biology.","cond-mat.dis-nn,physics.data-an,q-bio.QM,stat.ML"
"An Empirical Study of the L2-Boost technique with Echo State Networks. A particular case of Recurrent Neural Network (RNN) was introduced at the
beginning of the 2000s under the name of Echo State Networks (ESNs). The ESN
model overcomes the limitations during the training of the RNNs while
introducing no significant disadvantages. Although the model presents some
well-identified drawbacks when the parameters are not well initialised. The
performance of an ESN is highly dependent on its internal parameters and
pattern of connectivity of the hidden-hidden weights Often, the tuning of the
network parameters can be hard and can impact in the accuracy of the models.
  In this work, we investigate the performance of a specific boosting technique
(called L2-Boost) with ESNs as single predictors. The L2-Boost technique has
been shown to be an effective tool to combine ""weak"" predictors in regression
problems. In this study, we use an ensemble of random initialized ESNs (without
control their parameters) as ""weak"" predictors of the boosting procedure. We
evaluate our approach on five well-know time-series benchmark problems.
Additionally, we compare this technique with a baseline approach that consists
of averaging the prediction of an ensemble of ESNs.","cs.LG,cs.NE"
"A Comparative Analysis of Forecasting Financial Time Series Using ARIMA, LSTM, and BiLSTM. Machine and deep learning-based algorithms are the emerging approaches in
addressing prediction problems in time series. These techniques have been shown
to produce more accurate results than conventional regression-based modeling.
It has been reported that artificial Recurrent Neural Networks (RNN) with
memory, such as Long Short-Term Memory (LSTM), are superior compared to
Autoregressive Integrated Moving Average (ARIMA) with a large margin. The
LSTM-based models incorporate additional ""gates"" for the purpose of memorizing
longer sequences of input data. The major question is that whether the gates
incorporated in the LSTM architecture already offers a good prediction and
whether additional training of data would be necessary to further improve the
prediction.
  Bidirectional LSTMs (BiLSTMs) enable additional training by traversing the
input data twice (i.e., 1) left-to-right, and 2) right-to-left). The research
question of interest is then whether BiLSTM, with additional training
capability, outperforms regular unidirectional LSTM. This paper reports a
behavioral analysis and comparison of BiLSTM and LSTM models. The objective is
to explore to what extend additional layers of training of data would be
beneficial to tune the involved parameters. The results show that additional
training of data and thus BiLSTM-based modeling offers better predictions than
regular LSTM-based models. More specifically, it was observed that BiLSTM
models provide better predictions compared to ARIMA and LSTM models. It was
also observed that BiLSTM models reach the equilibrium much slower than
LSTM-based models.","cs.CE,cs.LG,cs.PF,stat.ML"
"Model-Based Reinforcement Learning with a Generative Model is Minimax Optimal. This work considers the sample and computational complexity of obtaining an
$\epsilon$-optimal policy in a discounted Markov Decision Process (MDP), given
only access to a generative model. In this work, we study the effectiveness of
the most natural plug-in approach to model-based planning: we build the maximum
likelihood estimate of the transition model in the MDP from observations and
then find an optimal policy in this empirical MDP. We ask arguably the most
basic and unresolved question in model based planning: is the naive ""plug-in""
approach, non-asymptotically, minimax optimal in the quality of the policy it
finds, given a fixed sample size? Here, the non-asymptotic regime refers to
when the sample size is sublinear in the model size.
  With access to a generative model, we resolve this question in the strongest
possible sense: our main result shows that \emph{any} high accuracy solution in
the plug-in model constructed with $N$ samples, provides an $\epsilon$-optimal
policy in the true underlying MDP (where $\epsilon$ is the minimax accuracy
with $N$ samples at every state, action pair). In comparison, all prior
(non-asymptotically) minimax optimal results use model free approaches, such as
the Variance Reduced Q-value iteration algorithm (Sidford et al 2018), while
the best known model-based results (e.g. Azar et al 2013) require larger sample
sizes in their dependence on the planning horizon or the state space. Notably,
we show that the model-based approach allows the use of \emph{any} efficient
planning algorithm in the empirical MDP, which simplifies algorithm design as
this approach does not tie the algorithm to the sampling procedure. The core of
our analysis is avnovel ""absorbing MDP"" construction to address the statistical
dependency issues that arise in the analysis of model-based planning
approaches, a construction which may be helpful more generally.","cs.LG,math.PR,stat.ML"
"Automatic Generation of Constrained Furniture Layouts. Efficient authoring of vast virtual environments hinges on algorithms that
are able to automatically generate content while also being controllable. We
propose a method to automatically generate furniture layouts for indoor
environments. Our method is simple, efficient, human-interpretable and amenable
to a wide variety of constraints. We model the composition of rooms into
classes of objects and learn joint (co-occurrence) statistics from a database
of training layouts. We generate new layouts by performing a sequence of
conditional sampling steps, exploiting the statistics learned from the
database. The generated layouts are specified as 3D object models, along with
their positions and orientations. We show they are of equivalent perceived
quality to the training layouts, and compare favorably to a state-of-the-art
method. We incorporate constraints using a general mechanism -- rejection
sampling -- which provides great flexibility at the cost of extra computation.
We demonstrate the versatility of our method by applying a wide variety of
constraints relevant to real-world applications.","cs.CV,cs.GR"
"Kernels for sequentially ordered data. We present a novel framework for kernel learning with sequential data of any
kind, such as time series, sequences of graphs, or strings. Our approach is
based on signature features which can be seen as an ordered variant of sample
(cross-)moments; it allows to obtain a ""sequentialized"" version of any static
kernel. The sequential kernels are efficiently computable for discrete
sequences and are shown to approximate a continuous moment form in a sampling
sense.
  A number of known kernels for sequences arise as ""sequentializations"" of
suitable static kernels: string kernels may be obtained as a special case, and
alignment kernels are closely related up to a modification that resolves their
open non-definiteness issue. Our experiments indicate that our signature-based
sequential kernel framework may be a promising approach to learning with
sequential data, such as time series, that allows to avoid extensive manual
pre-processing.","cs.DM,cs.LG,math.ST,stat.ME,stat.ML,stat.TH"
"Investigation of Color Constancy for Ubiquitous Wireless LAN/Camera Positioning: An Initial Outcome. This paper present our color constancy investigation in the hybridization of
Wireless LAN and Camera positioning in the mobile phone. Five typical color
constancy schemes are analyzed in different location environment. The results
can be used to combine with RF signals from Wireless LAN positioning by using
model fitting approach in order to establish absolute positioning output. There
is no conventional searching algorithm required, thus it is expected to reduce
the complexity of computation. Finally we present our preliminary results to
illustrate the indoor positioning algorithm performance evaluation for an
indoor environment set-up.","cs.CV,cs.HC"
"MarlRank: Multi-agent Reinforced Learning to Rank. When estimating the relevancy between a query and a document, ranking models
largely neglect the mutual information among documents. A common wisdom is that
if two documents are similar in terms of the same query, they are more likely
to have similar relevance score. To mitigate this problem, in this paper, we
propose a multi-agent reinforced ranking model, named MarlRank. In particular,
by considering each document as an agent, we formulate the ranking process as a
multi-agent Markov Decision Process (MDP), where the mutual interactions among
documents are incorporated in the ranking process. To compute the ranking list,
each document predicts its relevance to a query considering not only its own
query-document features but also its similar documents features and actions. By
defining reward as a function of NDCG, we can optimize our model directly on
the ranking performance measure. Our experimental results on two LETOR
benchmark datasets show that our model has significant performance gains over
the state-of-art baselines. We also find that the NDCG shows an overall
increasing trend along with the step of interactions, which demonstrates that
the mutual information among documents helps improve the ranking performance.","cs.IR,cs.LG,stat.ML"
"Texture image classification based on a pseudo-parabolic diffusion model. This work proposes a novel method based on a pseudo-parabolic diffusion
process to be employed for texture recognition. The proposed operator is
applied over a range of time scales giving rise to a family of images
transformed by nonlinear filters. Therefore each of those images are encoded by
a local descriptor (we use local binary patterns for that purpose) and they are
summarized by a simple histogram, yielding in this way the image feature
vector. The proposed approach is tested on the classification of well
established benchmark texture databases and on a practical task of plant
species recognition. In both cases, it is compared with several
state-of-the-art methodologies employed for texture recognition. Our proposal
outperforms those methods in terms of classification accuracy, confirming its
competitiveness. The good performance can be justified to a large extent by the
ability of the pseudo-parabolic operator to smooth possibly noisy details
inside homogeneous regions of the image at the same time that it preserves
discontinuities that convey critical information for the object description.
Such results also confirm that model-based approaches like the proposed one can
still be competitive with the omnipresent learning-based approaches, especially
when the user does not have access to a powerful computational structure and a
large amount of labeled data for training.","cs.CV,cs.NA,math.NA"
"Graph Feature Gating Networks. Graph neural networks (GNNs) have received tremendous attention due to their
power in learning effective representations for graphs. Most GNNs follow a
message-passing scheme where the node representations are updated by
aggregating and transforming the information from the neighborhood. Meanwhile,
they adopt the same strategy in aggregating the information from different
feature dimensions. However, suggested by social dimension theory and spectral
embedding, there are potential benefits to treat the dimensions differently
during the aggregation process. In this work, we investigate to enable
heterogeneous contributions of feature dimensions in GNNs. In particular, we
propose a general graph feature gating network (GFGN) based on the graph signal
denoising problem and then correspondingly introduce three graph filters under
GFGN to allow different levels of contributions from feature dimensions.
Extensive experiments on various real-world datasets demonstrate the
effectiveness and robustness of the proposed frameworks.","cs.AI,cs.LG"
"Eikonal Region-based Active Contours for Image Segmentation. The minimal path model based on the Eikonal partial differential equation
(PDE) has served as a fundamental tool for the applications of image
segmentation and boundary detection in the passed three decades. However, the
existing minimal paths-based image segmentation approaches commonly rely on the
image boundary features, potentially limiting their performance in some
situations. In this paper, we introduce a new variational image segmentation
model based on the minimal path framework and the Eikonal PDE, where the
region-based functional that defines the homogeneity criteria can be taken into
account for estimating the associated geodesic paths. This is done by
establishing a geodesic curve interpretation to the region-based active contour
evolution problem. The image segmentation processing is carried out in an
iterative manner in our approach. A crucial ingredient in each iteration is to
construct an asymmetric Randers geodesic metric using a sufficiently small
vector field, such that a set of geodesic paths can be tracked from the
geodesic distance map which is the solution to an Eikonal PDE. The object
boundary can be delineated by the concatenation of the final geodesic paths. We
invoke the Finsler variant of the fast marching method to estimate the geodesic
distance map, yielding an efficient implementation of the proposed Eikonal
region-based active contour model. Experimental results on both of the
synthetic and real images exhibit that our model indeed achieves encouraging
segmentation performance.","cs.CG,cs.CV"
"MUREL: Multimodal Relational Reasoning for Visual Question Answering. Multimodal attentional networks are currently state-of-the-art models for
Visual Question Answering (VQA) tasks involving real images. Although attention
allows to focus on the visual content relevant to the question, this simple
mechanism is arguably insufficient to model complex reasoning features required
for VQA or other high-level tasks.
  In this paper, we propose MuRel, a multimodal relational network which is
learned end-to-end to reason over real images. Our first contribution is the
introduction of the MuRel cell, an atomic reasoning primitive representing
interactions between question and image regions by a rich vectorial
representation, and modeling region relations with pairwise combinations.
Secondly, we incorporate the cell into a full MuRel network, which
progressively refines visual and question interactions, and can be leveraged to
define visualization schemes finer than mere attention maps.
  We validate the relevance of our approach with various ablation studies, and
show its superiority to attention-based methods on three datasets: VQA 2.0,
VQA-CP v2 and TDIUC. Our final MuRel network is competitive to or outperforms
state-of-the-art results in this challenging context.
  Our code is available: https://github.com/Cadene/murel.bootstrap.pytorch","cs.AI,cs.CL,cs.CV,cs.LG"
"Good Artists Copy, Great Artists Steal: Model Extraction Attacks Against Image Translation Generative Adversarial Networks. Machine learning models are typically made available to potential client
users via inference APIs. Model extraction attacks occur when a malicious
client uses information gleaned from queries to the inference API of a victim
model $F_V$ to build a surrogate model $F_A$ that has comparable functionality.
Recent research has shown successful model extraction attacks against image
classification, and NLP models. In this paper, we show the first model
extraction attack against real-world generative adversarial network (GAN) image
translation models. We present a framework for conducting model extraction
attacks against image translation models, and show that the adversary can
successfully extract functional surrogate models. The adversary is not required
to know $F_V$'s architecture or any other information about it beyond its
intended image translation task, and queries $F_V$'s inference interface using
data drawn from the same domain as the training data for $F_V$. We evaluate the
effectiveness of our attacks using three different instances of two popular
categories of image translation: (1) Selfie-to-Anime and (2) Monet-to-Photo
(image style transfer), and (3) Super-Resolution (super resolution). Using
standard performance metrics for GANs, we show that our attacks are effective
in each of the three cases -- the differences between $F_V$ and $F_A$, compared
to the target are in the following ranges: Selfie-to-Anime: FID $13.36-68.66$,
Monet-to-Photo: FID $3.57-4.40$, and Super-Resolution: SSIM: $0.06-0.08$ and
PSNR: $1.43-4.46$. Furthermore, we conducted a large scale (125 participants)
user study on Selfie-to-Anime and Monet-to-Photo to show that human perception
of the images produced by the victim and surrogate models can be considered
equivalent, within an equivalence bound of Cohen's $d=0.3$.","cs.CR,cs.CV,cs.LG"
"A Hierarchical Two-tier Approach to Hyper-parameter Optimization in Reinforcement Learning. Optimization of hyper-parameters in reinforcement learning (RL) algorithms is
a key task, because they determine how the agent will learn its policy by
interacting with its environment, and thus what data is gathered. In this work,
an approach that uses Bayesian optimization to perform a two-step optimization
is proposed: first, categorical RL structure hyper-parameters are taken as
binary variables and optimized with an acquisition function tailored for such
variables. Then, at a lower level of abstraction, solution-level
hyper-parameters are optimized by resorting to the expected improvement
acquisition function, while using the best categorical hyper-parameters found
in the optimization at the upper-level of abstraction. This two-tier approach
is validated in a simulated control task. Results obtained are promising and
open the way for more user-independent applications of reinforcement learning.","cs.AI,cs.LG,stat.ML"
"Generalised learning of time-series: Ornstein-Uhlenbeck processes. In machine learning, statistics, econometrics and statistical physics
cross-validation (CV) is used as a standard approach in quantifying the
generalization performance of a statistical model. In practice, direct usage of
CV is avoided for time-series due to several issues. A direct application of CV
in time-series leads to the loss of serial correlations, a requirement of
preserving any non-stationarity and the prediction of the past data using
future data. In this work, we propose a meta-algorithm called reconstructive
cross-validation (rCV ) that avoids all these issues. At first, k folds are
formed with non-overlapping randomly selected subsets of the original
time-series. Then, we generate k new partial time-series by removing data
points from a given fold: every new partial time-series have missing points at
random from a different entire fold. A suitable imputation or a smoothing
technique is used to reconstruct k time-series. We call these reconstructions
secondary models. Thereafter, we build the primary k time-series models using
new time-series coming from the secondary models. The performance of the
primary models is evaluated simultaneously by computing the deviations from the
originally removed data points and out-of-sample (OSS) data. These amounts to
reconstruction and prediction errors. If the secondary models use a technique
that retains the data points exactly, such as Gaussian process regression,
there will be no errors present on the data points that are not removed. By
this procedure serial correlations are retained, any non-stationarity is
preserved within models and there will be no prediction of past data using the
future data points. The cross-validation in time-series model can be practised
with rCV. Moreover, we can build time-series learning curves by repeating rCV
procedure with different k values.","68Q32,68T05,G.3; J.2,cond-mat.stat-mech,cs.LG,stat.ME,stat.ML"
"Physics-based Deep Learning. This digital book contains a practical and comprehensive introduction of
everything related to deep learning in the context of physical simulations. As
much as possible, all topics come with hands-on code examples in the form of
Jupyter notebooks to quickly get started. Beyond standard supervised learning
from data, we'll look at physical loss constraints, more tightly coupled
learning algorithms with differentiable simulations, as well as reinforcement
learning and uncertainty modeling. We live in exciting times: these methods
have a huge potential to fundamentally change what computer simulations can
achieve.","cs.LG,physics.comp-ph"
"Rethinking CNN Models for Audio Classification. In this paper, we show that ImageNet-Pretrained standard deep CNN models can
be used as strong baseline networks for audio classification. Even though there
is a significant difference between audio Spectrogram and standard ImageNet
image samples, transfer learning assumptions still hold firmly. To understand
what enables the ImageNet pretrained models to learn useful audio
representations, we systematically study how much of pretrained weights is
useful for learning spectrograms. We show (1) that for a given standard model
using pretrained weights is better than using randomly initialized weights (2)
qualitative results of what the CNNs learn from the spectrograms by visualizing
the gradients. Besides, we show that even though we use the pretrained model
weights for initialization, there is variance in performance in various output
runs of the same model. This variance in performance is due to the random
initialization of linear classification layer and random mini-batch orderings
in multiple runs. This brings significant diversity to build stronger ensemble
models with an overall improvement in accuracy. An ensemble of ImageNet
pretrained DenseNet achieves 92.89% validation accuracy on the ESC-50 dataset
and 87.42% validation accuracy on the UrbanSound8K dataset which is the current
state-of-the-art on both of these datasets.","cs.CV,cs.SD,eess.AS"
"Compositional Abstraction Error and a Category of Causal Models. Interventional causal models describe several joint distributions over some
variables used to describe a system, one for each intervention setting. They
provide a formal recipe for how to move between the different joint
distributions and make predictions about the variables upon intervening on the
system. Yet, it is difficult to formalise how we may change the underlying
variables used to describe the system, say moving from fine-grained to
coarse-grained variables. Here, we argue that compositionality is a desideratum
for such model transformations and the associated errors: When abstracting a
reference model M iteratively, first obtaining M' and then further simplifying
that to obtain M'', we expect the composite transformation from M to M'' to
exist and its error to be bounded by the errors incurred by each individual
transformation step. Category theory, the study of mathematical objects via
compositional transformations between them, offers a natural language to
develop our framework for model transformations and abstractions. We introduce
a category of finite interventional causal models and, leveraging theory of
enriched categories, prove the desired compositionality properties for our
framework.","cs.AI,cs.LG,cs.LO,math.CT,stat.ML"
"Physics-based machine learning for modeling stochastic IP3-dependent calcium dynamics. We present a machine learning method for model reduction which incorporates
domain-specific physics through candidate functions. Our method estimates an
effective probability distribution and differential equation model from
stochastic simulations of a reaction network. The close connection between
reduced and fine scale descriptions allows approximations derived from the
master equation to be introduced into the learning problem. This representation
is shown to improve generalization and allows a large reduction in network size
for a classic model of inositol trisphosphate (IP3) dependent calcium
oscillations in non-excitable cells.","68T07,I.2.6; I.2.1; I.2.0,cs.LG,physics.chem-ph,physics.comp-ph,q-bio.NC"
"Tree Tensor Networks for Generative Modeling. Matrix product states (MPS), a tensor network designed for one-dimensional
quantum systems, has been recently proposed for generative modeling of natural
data (such as images) in terms of `Born machine'. However, the exponential
decay of correlation in MPS restricts its representation power heavily for
modeling complex data such as natural images. In this work, we push forward the
effort of applying tensor networks to machine learning by employing the Tree
Tensor Network (TTN) which exhibits balanced performance in expressibility and
efficient training and sampling. We design the tree tensor network to utilize
the 2-dimensional prior of the natural images and develop sweeping learning and
sampling algorithms which can be efficiently implemented utilizing Graphical
Processing Units (GPU). We apply our model to random binary patterns and the
binary MNIST datasets of handwritten digits. We show that TTN is superior to
MPS for generative modeling in keeping correlation of pixels in natural images,
as well as giving better log-likelihood scores in standard datasets of
handwritten digits. We also compare its performance with state-of-the-art
generative models such as the Variational AutoEncoders, Restricted Boltzmann
machines, and PixelCNN. Finally, we discuss the future development of Tensor
Network States in machine learning problems.","cond-mat.stat-mech,cs.LG,quant-ph,stat.ML"
"Alternating Maximization: Unifying Framework for 8 Sparse PCA Formulations and Efficient Parallel Codes. Given a multivariate data set, sparse principal component analysis (SPCA)
aims to extract several linear combinations of the variables that together
explain the variance in the data as much as possible, while controlling the
number of nonzero loadings in these combinations. In this paper we consider 8
different optimization formulations for computing a single sparse loading
vector; these are obtained by combining the following factors: we employ two
norms for measuring variance (L2, L1) and two sparsity-inducing norms (L0, L1),
which are used in two different ways (constraint, penalty). Three of our
formulations, notably the one with L0 constraint and L1 variance, have not been
considered in the literature. We give a unifying reformulation which we propose
to solve via a natural alternating maximization (AM) method. We show the the AM
method is nontrivially equivalent to GPower (Journ\'{e}e et al; JMLR
11:517--553, 2010) for all our formulations. Besides this, we provide 24
efficient parallel SPCA implementations: 3 codes (multi-core, GPU and cluster)
for each of the 8 problems. Parallelism in the methods is aimed at i) speeding
up computations (our GPU code can be 100 times faster than an efficient serial
code written in C++), ii) obtaining solutions explaining more variance and iii)
dealing with big data problems (our cluster code is able to solve a 357 GB
problem in about a minute).","cs.LG,math.OC,stat.ML"
"Virtuously Safe Reinforcement Learning. We show that when a third party, the adversary, steps into the two-party
setting (agent and operator) of safely interruptible reinforcement learning, a
trade-off has to be made between the probability of following the optimal
policy in the limit, and the probability of escaping a dangerous situation
created by the adversary. So far, the work on safely interruptible agents has
assumed a perfect perception of the agent about its environment (no adversary),
and therefore implicitly set the second probability to zero, by explicitly
seeking a value of one for the first probability. We show that (1) agents can
be made both interruptible and adversary-resilient, and (2) the
interruptibility can be made safe in the sense that the agent itself will not
seek to avoid it. We also solve the problem that arises when the agent does not
go completely greedy, i.e. issues with safe exploration in the limit.
Resilience to perturbed perception, safe exploration in the limit, and safe
interruptibility are the three pillars of what we call \emph{virtuously safe
reinforcement learning}.","cs.AI,cs.GT,cs.LG,stat.ML"
"Tensor Programs IIb: Architectural Universality of Neural Tangent Kernel Training Dynamics. Yang (2020a) recently showed that the Neural Tangent Kernel (NTK) at
initialization has an infinite-width limit for a large class of architectures
including modern staples such as ResNet and Transformers. However, their
analysis does not apply to training. Here, we show the same neural networks (in
the so-called NTK parametrization) during training follow a kernel gradient
descent dynamics in function space, where the kernel is the infinite-width NTK.
This completes the proof of the *architectural universality* of NTK behavior.
To achieve this result, we apply the Tensor Programs technique: Write the
entire SGD dynamics inside a Tensor Program and analyze it via the Master
Theorem. To facilitate this proof, we develop a graphical notation for Tensor
Programs.","cs.LG,cs.NE,math.PR"
"Self-Referencing Embedded Strings (SELFIES): A 100% robust molecular string representation. The discovery of novel materials and functional molecules can help to solve
some of society's most urgent challenges, ranging from efficient energy
harvesting and storage to uncovering novel pharmaceutical drug candidates.
Traditionally matter engineering -- generally denoted as inverse design -- was
based massively on human intuition and high-throughput virtual screening. The
last few years have seen the emergence of significant interest in
computer-inspired designs based on evolutionary or deep learning methods. The
major challenge here is that the standard strings molecular representation
SMILES shows substantial weaknesses in that task because large fractions of
strings do not correspond to valid molecules. Here, we solve this problem at a
fundamental level and introduce SELFIES (SELF-referencIng Embedded Strings), a
string-based representation of molecules which is 100\% robust. Every SELFIES
string corresponds to a valid molecule, and SELFIES can represent every
molecule. SELFIES can be directly applied in arbitrary machine learning models
without the adaptation of the models; each of the generated molecule candidates
is valid. In our experiments, the model's internal memory stores two orders of
magnitude more diverse molecules than a similar test with SMILES. Furthermore,
as all molecules are valid, it allows for explanation and interpretation of the
internal working of the generative models.","cs.LG,physics.chem-ph,quant-ph,stat.ML"
"Negotiating Team Formation Using Deep Reinforcement Learning. When autonomous agents interact in the same environment, they must often
cooperate to achieve their goals. One way for agents to cooperate effectively
is to form a team, make a binding agreement on a joint plan, and execute it.
However, when agents are self-interested, the gains from team formation must be
allocated appropriately to incentivize agreement. Various approaches for
multi-agent negotiation have been proposed, but typically only work for
particular negotiation protocols. More general methods usually require human
input or domain-specific data, and so do not scale. To address this, we propose
a framework for training agents to negotiate and form teams using deep
reinforcement learning. Importantly, our method makes no assumptions about the
specific negotiation protocol, and is instead completely experience driven. We
evaluate our approach on both non-spatial and spatially extended team-formation
negotiation environments, demonstrating that our agents beat hand-crafted bots
and reach negotiation outcomes consistent with fair solutions predicted by
cooperative game theory. Additionally, we investigate how the physical location
of agents influences negotiation outcomes.","I.2.6,cs.AI,cs.LG,cs.MA"
"Spline-PINN: Approaching PDEs without Data using Fast, Physics-Informed Hermite-Spline CNNs. Partial Differential Equations (PDEs) are notoriously difficult to solve. In
general, closed-form solutions are not available and numerical approximation
schemes are computationally expensive. In this paper, we propose to approach
the solution of PDEs based on a novel technique that combines the advantages of
two recently emerging machine learning based approaches. First,
physics-informed neural networks (PINNs) learn continuous solutions of PDEs and
can be trained with little to no ground truth data. However, PINNs do not
generalize well to unseen domains. Second, convolutional neural networks
provide fast inference and generalize but either require large amounts of
training data or a physics-constrained loss based on finite differences that
can lead to inaccuracies and discretization artifacts. We leverage the
advantages of both of these approaches by using Hermite spline kernels in order
to continuously interpolate a grid-based state representation that can be
handled by a CNN. This allows for training without any precomputed training
data using a physics-informed loss function only and provides fast, continuous
solutions that generalize to unseen domains. We demonstrate the potential of
our method at the examples of the incompressible Navier-Stokes equation and the
damped wave equation. Our models are able to learn several intriguing phenomena
such as Karman vortex streets, the Magnus effect, Doppler effect, interference
patterns and wave reflections. Our quantitative assessment and an interactive
real-time demo show that we are narrowing the gap in accuracy of unsupervised
ML based methods to industrial CFD solvers while being orders of magnitude
faster.","cs.CE,cs.LG,physics.flu-dyn"
"Perceptual Evaluation of Adversarial Attacks for CNN-based Image Classification. Deep neural networks (DNNs) have recently achieved state-of-the-art
performance and provide significant progress in many machine learning tasks,
such as image classification, speech processing, natural language processing,
etc. However, recent studies have shown that DNNs are vulnerable to adversarial
attacks. For instance, in the image classification domain, adding small
imperceptible perturbations to the input image is sufficient to fool the DNN
and to cause misclassification. The perturbed image, called \textit{adversarial
example}, should be visually as close as possible to the original image.
However, all the works proposed in the literature for generating adversarial
examples have used the $L_{p}$ norms ($L_{0}$, $L_{2}$ and $L_{\infty}$) as
distance metrics to quantify the similarity between the original image and the
adversarial example. Nonetheless, the $L_{p}$ norms do not correlate with human
judgment, making them not suitable to reliably assess the perceptual
similarity/fidelity of adversarial examples. In this paper, we present a
database for visual fidelity assessment of adversarial examples. We describe
the creation of the database and evaluate the performance of fifteen
state-of-the-art full-reference (FR) image fidelity assessment metrics that
could substitute $L_{p}$ norms. The database as well as subjective scores are
publicly available to help designing new metrics for adversarial examples and
to facilitate future research works.","cs.CR,cs.CV,cs.LG,eess.IV,stat.ML"
"Combinatorial Optimization with Physics-Inspired Graph Neural Networks. We demonstrate how graph neural networks can be used to solve combinatorial
optimization problems. Our approach is broadly applicable to canonical NP-hard
problems in the form of quadratic unconstrained binary optimization problems,
such as maximum cut, minimum vertex cover, maximum independent set, as well as
Ising spin glasses and higher-order generalizations thereof in the form of
polynomial unconstrained binary optimization problems. We apply a relaxation
strategy to the problem Hamiltonian to generate a differentiable loss function
with which we train the graph neural network and apply a simple projection to
integer variables once the unsupervised training process has completed. We
showcase our approach with numerical results for the canonical maximum cut and
maximum independent set problems. We find that the graph neural network
optimizer performs on par or outperforms existing solvers, with the ability to
scale beyond the state of the art to problems with millions of variables.","cond-mat.dis-nn,cs.AI,cs.LG,math.OC,quant-ph"
"Enforcing constraints for interpolation and extrapolation in Generative Adversarial Networks. We suggest ways to enforce given constraints in the output of a Generative
Adversarial Network (GAN) generator both for interpolation and extrapolation
(prediction). For the case of dynamical systems, given a time series, we wish
to train GAN generators that can be used to predict trajectories starting from
a given initial condition. In this setting, the constraints can be in algebraic
and/or differential form. Even though we are predominantly interested in the
case of extrapolation, we will see that the tasks of interpolation and
extrapolation are related. However, they need to be treated differently.
  For the case of interpolation, the incorporation of constraints is built into
the training of the GAN. The incorporation of the constraints respects the
primary game-theoretic setup of a GAN so it can be combined with existing
algorithms. However, it can exacerbate the problem of instability during
training that is well-known for GANs. We suggest adding small noise to the
constraints as a simple remedy that has performed well in our numerical
experiments.
  The case of extrapolation (prediction) is more involved. During training, the
GAN generator learns to interpolate a noisy version of the data and we enforce
the constraints. This approach has connections with model reduction that we can
utilize to improve the efficiency and accuracy of the training. Depending on
the form of the constraints, we may enforce them also during prediction through
a projection step. We provide examples of linear and nonlinear systems of
differential equations to illustrate the various constructions.","37M10,62M45,65L05,68Q32,68T05,cs.LG,stat.ML"
"Overlapping Spaces for Compact Graph Representations. Various non-trivial spaces are becoming popular for embedding structured data
such as graphs, texts, or images. Following spherical and hyperbolic spaces,
more general product spaces have been proposed. However, searching for the best
configuration of product space is a resource-intensive procedure, which reduces
the practical applicability of the idea. We generalize the concept of product
space and introduce an overlapping space that does not have the configuration
search problem. The main idea is to allow subsets of coordinates to be shared
between spaces of different types (Euclidean, hyperbolic, spherical). As a
result, parameter optimization automatically learns the optimal configuration.
Additionally, overlapping spaces allow for more compact representations since
their geometry is more complex. Our experiments confirm that overlapping spaces
outperform the competitors in graph embedding tasks. Here, we consider both
distortion setup, where the aim is to preserve distances, and ranking setup,
where the relative order should be preserved. The proposed method effectively
solves the problem and outperforms the competitors in both settings. We also
perform an empirical analysis in a realistic information retrieval task, where
we compare all spaces by incorporating them into DSSM. In this case, the
proposed overlapping space consistently achieves nearly optimal results without
any configuration tuning. This allows for reducing training time, which can be
significant in large-scale applications.","I.2.6,cs.IR,cs.LG,stat.ML"
"R-FORCE: Robust Learning for Random Recurrent Neural Networks. Random Recurrent Neural Networks (RRNN) are the simplest recurrent networks
to model and extract features from sequential data. The simplicity however
comes with a price; RRNN are known to be susceptible to diminishing/exploding
gradient problem when trained with gradient-descent based optimization. To
enhance robustness of RRNN, alternative training approaches have been proposed.
Specifically, FORCE learning approach proposed a recursive least squares
alternative to train RRNN and was shown to be applicable even for the
challenging task of target-learning, where the network is tasked with
generating dynamic patterns with no guiding input. While FORCE training
indicates that solving target-learning is possible, it appears to be effective
only in a specific regime of network dynamics (edge-of-chaos). We thereby
investigate whether initialization of RRNN connectivity according to a tailored
distribution can guarantee robust FORCE learning. We are able to generate such
distribution by inference of four generating principles constraining the
spectrum of the network Jacobian to remain in stability region. This
initialization along with FORCE learning provides a robust training method,
i.e., Robust-FORCE (R-FORCE). We validate R-FORCE performance on various target
functions for a wide range of network configurations and compare with
alternative methods. Our experiments indicate that R-FORCE facilitates
significantly more stable and accurate target-learning for a wide class of
RRNN. Such stability becomes critical in modeling multi-dimensional sequences
as we demonstrate on modeling time-series of human body joints during physical
movements.","cs.LG,cs.NE,q-bio.NC"
"Quantum-assisted associative adversarial network: Applying quantum annealing in deep learning. We present an algorithm for learning a latent variable generative model via
generative adversarial learning where the canonical uniform noise input is
replaced by samples from a graphical model. This graphical model is learned by
a Boltzmann machine which learns low-dimensional feature representation of data
extracted by the discriminator. A quantum annealer, the D-Wave 2000Q, is used
to sample from this model. This algorithm joins a growing family of algorithms
that use a quantum annealing subroutine in deep learning, and provides a
framework to test the advantages of quantum-assisted learning in GANs. Fully
connected, symmetric bipartite and Chimera graph topologies are compared on a
reduced stochastically binarized MNIST dataset, for both classical and quantum
annealing sampling methods. The quantum-assisted associative adversarial
network successfully learns a generative model of the MNIST dataset for all
topologies, and is also applied to the LSUN dataset bedrooms class for the
Chimera topology. Evaluated using the Fr\'{e}chet inception distance and
inception score, the quantum and classical versions of the algorithm are found
to have equivalent performance for learning an implicit generative model of the
MNIST dataset.","cs.LG,quant-ph,stat.ML"
"Helix: Accelerating Human-in-the-loop Machine Learning. Data application developers and data scientists spend an inordinate amount of
time iterating on machine learning (ML) workflows -- by modifying the data
pre-processing, model training, and post-processing steps -- via
trial-and-error to achieve the desired model performance. Existing work on
accelerating machine learning focuses on speeding up one-shot execution of
workflows, failing to address the incremental and dynamic nature of typical ML
development. We propose Helix, a declarative machine learning system that
accelerates iterative development by optimizing workflow execution end-to-end
and across iterations. Helix minimizes the runtime per iteration via program
analysis and intelligent reuse of previous results, which are selectively
materialized -- trading off the cost of materialization for potential future
benefits -- to speed up future iterations. Additionally, Helix offers a
graphical interface to visualize workflow DAGs and compare versions to
facilitate iterative development. Through two ML applications, in
classification and in structured prediction, attendees will experience the
succinctness of Helix programming interface and the speed and ease of iterative
development using Helix. In our evaluations, Helix achieved up to an order of
magnitude reduction in cumulative run time compared to state-of-the-art machine
learning tools.","cs.DB,cs.LG,stat.ML"
"Instance-dependent $\ell_\infty$-bounds for policy evaluation in tabular reinforcement learning. Markov reward processes (MRPs) are used to model stochastic phenomena arising
in operations research, control engineering, robotics, and artificial
intelligence, as well as communication and transportation networks. In many of
these cases, such as in the policy evaluation problem encountered in
reinforcement learning, the goal is to estimate the long-term value function of
such a process without access to the underlying population transition and
reward functions. Working with samples generated under the synchronous model,
we study the problem of estimating the value function of an infinite-horizon,
discounted MRP on finitely many states in the $\ell_\infty$-norm. We analyze
both the standard plug-in approach to this problem and a more robust variant,
and establish non-asymptotic bounds that depend on the (unknown) problem
instance, as well as data-dependent bounds that can be evaluated based on the
observations of state-transitions and rewards. We show that these approaches
are minimax-optimal up to constant factors over natural sub-classes of MRPs.
Our analysis makes use of a leave-one-out decoupling argument tailored to the
policy evaluation problem, one which may be of independent interest.","cs.LG,math.OC,math.PR,math.ST,stat.ML,stat.TH"
"Using Sentences as Semantic Representations in Large Scale Zero-Shot Learning. Zero-shot learning aims to recognize instances of unseen classes, for which
no visual instance is available during training, by learning multimodal
relations between samples from seen classes and corresponding class semantic
representations. These class representations usually consist of either
attributes, which do not scale well to large datasets, or word embeddings,
which lead to poorer performance. A good trade-off could be to employ short
sentences in natural language as class descriptions. We explore different
solutions to use such short descriptions in a ZSL setting and show that while
simple methods cannot achieve very good results with sentences alone, a
combination of usual word embeddings and sentences can significantly outperform
current state-of-the-art.","cs.CV,cs.MM"
"No-Reference Quality Assessment for 360-degree Images by Analysis of Multi-frequency Information and Local-global Naturalness. 360-degree/omnidirectional images (OIs) have achieved remarkable attentions
due to the increasing applications of virtual reality (VR). Compared to
conventional 2D images, OIs can provide more immersive experience to consumers,
benefitting from the higher resolution and plentiful field of views (FoVs).
Moreover, observing OIs is usually in the head mounted display (HMD) without
references. Therefore, an efficient blind quality assessment method, which is
specifically designed for 360-degree images, is urgently desired. In this
paper, motivated by the characteristics of the human visual system (HVS) and
the viewing process of VR visual contents, we propose a novel and effective
no-reference omnidirectional image quality assessment (NR OIQA) algorithm by
Multi-Frequency Information and Local-Global Naturalness (MFILGN).
Specifically, inspired by the frequency-dependent property of visual cortex, we
first decompose the projected equirectangular projection (ERP) maps into
wavelet subbands. Then, the entropy intensities of low and high frequency
subbands are exploited to measure the multi-frequency information of OIs.
Besides, except for considering the global naturalness of ERP maps, owing to
the browsed FoVs, we extract the natural scene statistics features from each
viewport image as the measure of local naturalness. With the proposed
multi-frequency information measurement and local-global naturalness
measurement, we utilize support vector regression as the final image quality
regressor to train the quality evaluation model from visual quality-related
features to human ratings. To our knowledge, the proposed model is the first
no-reference quality assessment method for 360-degreee images that combines
multi-frequency information and image naturalness. Experimental results on two
publicly available OIQA databases demonstrate that our proposed MFILGN
outperforms state-of-the-art approaches.","cs.CG,cs.CV,cs.MM,eess.IV"
"Learning stochastic decision trees. We give a quasipolynomial-time algorithm for learning stochastic decision
trees that is optimally resilient to adversarial noise. Given an
$\eta$-corrupted set of uniform random samples labeled by a size-$s$ stochastic
decision tree, our algorithm runs in time
$n^{O(\log(s/\varepsilon)/\varepsilon^2)}$ and returns a hypothesis with error
within an additive $2\eta + \varepsilon$ of the Bayes optimal. An additive
$2\eta$ is the information-theoretic minimum.
  Previously no non-trivial algorithm with a guarantee of $O(\eta) +
\varepsilon$ was known, even for weaker noise models. Our algorithm is
furthermore proper, returning a hypothesis that is itself a decision tree;
previously no such algorithm was known even in the noiseless setting.","cs.DS,cs.LG,stat.ML"
"Explaining Deep Neural Networks and Beyond: A Review of Methods and Applications. With the broader and highly successful usage of machine learning in industry
and the sciences, there has been a growing demand for Explainable AI.
Interpretability and explanation methods for gaining a better understanding
about the problem solving abilities and strategies of nonlinear Machine
Learning, in particular, deep neural networks, are therefore receiving
increased attention. In this work we aim to (1) provide a timely overview of
this active emerging field, with a focus on 'post-hoc' explanations, and
explain its theoretical foundations, (2) put interpretability algorithms to a
test both from a theory and comparative evaluation perspective using extensive
simulations, (3) outline best practice aspects i.e. how to best include
interpretation methods into the standard usage of machine learning and (4)
demonstrate successful usage of explainable AI in a representative selection of
application scenarios. Finally, we discuss challenges and possible future
directions of this exciting foundational field of machine learning.","cs.AI,cs.CV,cs.LG,cs.NE,stat.ML"
"Spliced Binned-Pareto Distribution for Robust Modeling of Heavy-tailed Time Series. This work proposes a novel method to robustly and accurately model time
series with heavy-tailed noise, in non-stationary scenarios. In many practical
application time series have heavy-tailed noise that significantly impacts the
performance of classical forecasting models; in particular, accurately modeling
a distribution over extreme events is crucial to performing accurate time
series anomaly detection. We propose a Spliced Binned-Pareto distribution which
is both robust to extreme observations and allows accurate modeling of the full
distribution. Our method allows the capture of time dependencies in the higher
order moments of the distribution such as the tail heaviness. We compare the
robustness and the accuracy of the tail estimation of our method to other state
of the art methods on Twitter mentions count time series.","cs.LG,stat.CO,stat.ML"
"Parameter Estimation with Dense and Convolutional Neural Networks Applied to the FitzHugh-Nagumo ODE. Machine learning algorithms have been successfully used to approximate
nonlinear maps under weak assumptions on the structure and properties of the
maps. We present deep neural networks using dense and convolutional layers to
solve an inverse problem, where we seek to estimate parameters of a
FitzHugh-Nagumo model, which consists of a nonlinear system of ordinary
differential equations (ODEs). We employ the neural networks to approximate
reconstruction maps for model parameter estimation from observational data,
where the data comes from the solution of the ODE and takes the form of a time
series representing dynamically spiking membrane potential of a biological
neuron. We target this dynamical model because of the computational challenges
it poses in an inference setting, namely, having a highly nonlinear and
nonconvex data misfit term and permitting only weakly informative priors on
parameters. These challenges cause traditional optimization to fail and
alternative algorithms to exhibit large computational costs. We quantify the
prediction errors of model parameters obtained from the neural networks and
investigate the effects of network architectures with and without the presence
of noise in observational data. We generalize our framework for neural
network-based reconstruction maps to simultaneously estimate ODE parameters and
parameters of autocorrelated observational noise. Our results demonstrate that
deep neural networks have the potential to estimate parameters in dynamical
models and stochastic processes, and they are capable of predicting parameters
accurately for the FitzHugh-Nagumo model.","cs.LG,math.DS,stat.ML"
"Efficient Super Resolution Using Binarized Neural Network. Deep convolutional neural networks (DCNNs) have recently demonstrated
high-quality results in single-image super-resolution (SR). DCNNs often suffer
from over-parametrization and large amounts of redundancy, which results in
inefficient inference and high memory usage, preventing massive applications on
mobile devices. As a way to significantly reduce model size and computation
time, binarized neural network has only been shown to excel on semantic-level
tasks such as image classification and recognition. However, little effort of
network quantization has been spent on image enhancement tasks like SR, as
network quantization is usually assumed to sacrifice pixel-level accuracy. In
this work, we explore an network-binarization approach for SR tasks without
sacrificing much reconstruction accuracy. To achieve this, we binarize the
convolutional filters in only residual blocks, and adopt a learnable weight for
each binary filter. We evaluate this idea on several state-of-the-art
DCNN-based architectures, and show that binarized SR networks achieve
comparable qualitative and quantitative results as their real-weight
counterparts. Moreover, the proposed binarized strategy could help reduce model
size by 80% when applying on SRResNet, and could potentially speed up inference
by 5 times.","cs.AI,cs.CV"
"A deep learning framework for solution and discovery in solid mechanics. We present the application of a class of deep learning, known as Physics
Informed Neural Networks (PINN), to learning and discovery in solid mechanics.
We explain how to incorporate the momentum balance and constitutive relations
into PINN, and explore in detail the application to linear elasticity, and
illustrate its extension to nonlinear problems through an example that
showcases von~Mises elastoplasticity. While common PINN algorithms are based on
training one deep neural network (DNN), we propose a multi-network model that
results in more accurate representation of the field variables. To validate the
model, we test the framework on synthetic data generated from analytical and
numerical reference solutions. We study convergence of the PINN model, and show
that Isogeometric Analysis (IGA) results in superior accuracy and convergence
characteristics compared with classic low-order Finite Element Method (FEM). We
also show the applicability of the framework for transfer learning, and find
vastly accelerated convergence during network re-training. Finally, we find
that honoring the physics leads to improved robustness: when trained only on a
few parameters, we find that the PINN model can accurately predict the solution
for a wide range of parameters new to the network---thus pointing to an
important application of this framework to sensitivity analysis and surrogate
modeling.","74B05,74L05,74L10 (secondary),74S05,74S30 (primary),J.2,cs.CE,cs.LG,stat.ML"
"Universal Differential Equations for Scientific Machine Learning. In the context of science, the well-known adage ""a picture is worth a
thousand words"" might well be ""a model is worth a thousand datasets.""
Scientific models, such as Newtonian physics or biological gene regulatory
networks, are human-driven simplifications of complex phenomena that serve as
surrogates for the countless experiments that validated the models. Recently,
machine learning has been able to overcome the inaccuracies of approximate
modeling by directly learning the entire set of nonlinear interactions from
data. However, without any predetermined structure from the scientific basis
behind the problem, machine learning approaches are flexible but
data-expensive, requiring large databases of homogeneous labeled training data.
A central challenge is reconciling data that is at odds with simplified models
without requiring ""big data"".
  In this work we develop a new methodology, universal differential equations
(UDEs), which augments scientific models with machine-learnable structures for
scientifically-based learning. We show how UDEs can be utilized to discover
previously unknown governing equations, accurately extrapolate beyond the
original data, and accelerate model simulation, all in a time and
data-efficient manner. This advance is coupled with open-source software that
allows for training UDEs which incorporate physical constraints, delayed
interactions, implicitly-defined events, and intrinsic stochasticity in the
model. Our examples show how a diverse set of computationally-difficult
modeling issues across scientific disciplines, from automatically discovering
biological mechanisms to accelerating the training of physics-informed neural
networks and large-eddy simulations, can all be transformed into UDE training
problems that are efficiently solved by a single software methodology.","cs.LG,math.DS,q-bio.QM,stat.ML"
"UniT: Multimodal Multitask Learning with a Unified Transformer. We propose UniT, a Unified Transformer model to simultaneously learn the most
prominent tasks across different domains, ranging from object detection to
natural language understanding and multimodal reasoning. Based on the
transformer encoder-decoder architecture, our UniT model encodes each input
modality with an encoder and makes predictions on each task with a shared
decoder over the encoded input representations, followed by task-specific
output heads. The entire model is jointly trained end-to-end with losses from
each task. Compared to previous efforts on multi-task learning with
transformers, we share the same model parameters across all tasks instead of
separately fine-tuning task-specific models and handle a much higher variety of
tasks across different domains. In our experiments, we learn 7 tasks jointly
over 8 datasets, achieving strong performance on each task with significantly
fewer parameters. Our code is available in MMF at https://mmf.sh.","cs.CL,cs.CV"
"Accurate, Efficient and Scalable Graph Embedding. The Graph Convolutional Network (GCN) model and its variants are powerful
graph embedding tools for facilitating classification and clustering on graphs.
However, a major challenge is to reduce the complexity of layered GCNs and make
them parallelizable and scalable on very large graphs -- state-of the art
techniques are unable to achieve scalability without losing accuracy and
efficiency. In this paper, we propose novel parallelization techniques for
graph sampling-based GCNs that achieve superior scalable performance on very
large graphs without compromising accuracy. Specifically, our GCN guarantees
work-efficient training and produces order of magnitude savings in computation
and communication. To scale GCN training on tightly-coupled shared memory
systems, we develop parallelization strategies for the key steps in training:
For the graph sampling step, we exploit parallelism within and across multiple
sampling instances, and devise an efficient data structure for concurrent
accesses that provides theoretical guarantee of near-linear speedup with number
of processing units. For the feature propagation step within the sampled graph,
we improve cache utilization and reduce DRAM communication by data
partitioning. We prove that our partitioning strategy is a 2-approximation for
minimizing the communication time compared to the optimal strategy. We
demonstrate that our parallel graph embedding outperforms state-of-the-art
methods in scalability (with respect to number of processors, graph size and
GCN model size), efficiency and accuracy on several large datasets. On a
40-core Xeon platform, our parallel training achieves $64\times$ speedup (with
AVX) in the sampling step and $25\times$ speedup in the feature propagation
step, compared to the serial implementation, resulting in a net speedup of
$21\times$.","cs.LG,cs.PF,stat.ML"
"Deep Learning on Graphs: A Survey. Deep learning has been shown to be successful in a number of domains, ranging
from acoustics, images, to natural language processing. However, applying deep
learning to the ubiquitous graph data is non-trivial because of the unique
characteristics of graphs. Recently, substantial research efforts have been
devoted to applying deep learning methods to graphs, resulting in beneficial
advances in graph analysis techniques. In this survey, we comprehensively
review the different types of deep learning methods on graphs. We divide the
existing methods into five categories based on their model architectures and
training strategies: graph recurrent neural networks, graph convolutional
networks, graph autoencoders, graph reinforcement learning, and graph
adversarial methods. We then provide a comprehensive overview of these methods
in a systematic manner mainly by following their development history. We also
analyze the differences and compositions of different methods. Finally, we
briefly outline the applications in which they have been used and discuss
potential future research directions.","cs.LG,cs.SI,stat.ML"
"Change point detection for graphical models in the presence of missing values. We propose estimation methods for change points in high-dimensional
covariance structures with an emphasis on challenging scenarios with missing
values. We advocate three imputation like methods and investigate their
implications on common losses used for change point detection. We also discuss
how model selection methods have to be adapted to the setting of incomplete
data. The methods are compared in a simulation study and applied to a time
series from an environmental monitoring system. An implementation of our
proposals within the R-package hdcd is available via the Supplementary
materials.","cs.LG,stat.AP,stat.ME,stat.ML"
"Towards Scalable Verification of Deep Reinforcement Learning. Deep neural networks (DNNs) have gained significant popularity in recent
years, becoming the state of the art in a variety of domains. In particular,
deep reinforcement learning (DRL) has recently been employed to train DNNs that
realize control policies for various types of real-world systems. In this work,
we present the whiRL 2.0 tool, which implements a new approach for verifying
complex properties of interest for DRL systems. To demonstrate the benefits of
whiRL 2.0, we apply it to case studies from the communication networks domain
that have recently been used to motivate formal verification of DRL systems,
and which exhibit characteristics that are conducive for scalable verification.
We propose techniques for performing k-induction and semi-automated invariant
inference on such systems, and leverage these techniques for proving safety and
liveness properties that were previously impossible to verify due to the
scalability barriers of prior approaches. Furthermore, we show how our proposed
techniques provide insights into the inner workings and the generalizability of
DRL systems. whiRL 2.0 is publicly available online.","cs.LG,cs.NI,math.OC"
"The Mathematics Behind Spectral Clustering And The Equivalence To PCA. Spectral clustering is a popular algorithm that clusters points using the
eigenvalues and eigenvectors of Laplacian matrices derived from the data. For
years, spectral clustering has been working mysteriously. This paper explains
spectral clustering by dividing it into two categories based on whether the
graph Laplacian is fully connected or not. For a fully connected graph, this
paper demonstrates the dimension reduction part by offering an objective
function: the covariance between the original data points' similarities and the
mapped data points' similarities. For a multi-connected graph, this paper
proves that with a proper $k$, the first $k$ eigenvectors are the indicators of
the connected components. This paper also proves there is an equivalence
between spectral embedding and PCA.","cs.LG,stat.ML"
"Instance-Dependent Complexity of Contextual Bandits and Reinforcement Learning: A Disagreement-Based Perspective. In the classical multi-armed bandit problem, instance-dependent algorithms
attain improved performance on ""easy"" problems with a gap between the best and
second-best arm. Are similar guarantees possible for contextual bandits? While
positive results are known for certain special cases, there is no general
theory characterizing when and how instance-dependent regret bounds for
contextual bandits can be achieved for rich, general classes of policies. We
introduce a family of complexity measures that are both sufficient and
necessary to obtain instance-dependent regret bounds. We then introduce new
oracle-efficient algorithms which adapt to the gap whenever possible, while
also attaining the minimax rate in the worst case. Finally, we provide
structural results that tie together a number of complexity measures previously
proposed throughout contextual bandits, reinforcement learning, and active
learning and elucidate their role in determining the optimal instance-dependent
regret. In a large-scale empirical evaluation, we find that our approach often
gives superior results for challenging exploration problems.
  Turning our focus to reinforcement learning with function approximation, we
develop new oracle-efficient algorithms for reinforcement learning with rich
observations that obtain optimal gap-dependent sample complexity.","cs.LG,math.ST,stat.ML,stat.TH"
"MLPerf Mobile Inference Benchmark. MLPerf Mobile is the first industry-standard open-source mobile benchmark
developed by industry members and academic researchers to allow
performance/accuracy evaluation of mobile devices with different AI chips and
software stacks. The benchmark draws from the expertise of leading mobile-SoC
vendors, ML-framework providers, and model producers. In this paper, we
motivate the drive to demystify mobile-AI performance and present MLPerf
Mobile's design considerations, architecture, and implementation. The benchmark
comprises a suite of models that operate under standard models, data sets,
quality metrics, and run rules. For the first iteration, we developed an app to
provide an ""out-of-the-box"" inference-performance benchmark for computer vision
and natural-language processing on mobile devices. MLPerf Mobile can serve as a
framework for integrating future models, for customizing quality-target
thresholds to evaluate system performance, for comparing software frameworks,
and for assessing heterogeneous-hardware capabilities for machine learning, all
fairly and faithfully with fully reproducible results.","cs.DC,cs.LG"
"Robust learning from corrupted EEG with dynamic spatial filtering. Building machine learning models using EEG recorded outside of the laboratory
setting requires methods robust to noisy data and randomly missing channels.
This need is particularly great when working with sparse EEG montages (1-6
channels), often encountered in consumer-grade or mobile EEG devices. Neither
classical machine learning models nor deep neural networks trained end-to-end
on EEG are typically designed or tested for robustness to corruption, and
especially to randomly missing channels. While some studies have proposed
strategies for using data with missing channels, these approaches are not
practical when sparse montages are used and computing power is limited (e.g.,
wearables, cell phones). To tackle this problem, we propose dynamic spatial
filtering (DSF), a multi-head attention module that can be plugged in before
the first layer of a neural network to handle missing EEG channels by learning
to focus on good channels and to ignore bad ones. We tested DSF on public EEG
data encompassing ~4,000 recordings with simulated channel corruption and on a
private dataset of ~100 at-home recordings of mobile EEG with natural
corruption. Our proposed approach achieves the same performance as baseline
models when no noise is applied, but outperforms baselines by as much as 29.4%
accuracy when significant channel corruption is present. Moreover, DSF outputs
are interpretable, making it possible to monitor channel importance in
real-time. This approach has the potential to enable the analysis of EEG in
challenging settings where channel corruption hampers the reading of brain
signals.","cs.LG,eess.SP,q-bio.NC,q-bio.QM,stat.ML"
"An Information-theoretic On-line Learning Principle for Specialization in Hierarchical Decision-Making Systems. Information-theoretic bounded rationality describes utility-optimizing
decision-makers whose limited information-processing capabilities are
formalized by information constraints. One of the consequences of bounded
rationality is that resource-limited decision-makers can join together to solve
decision-making problems that are beyond the capabilities of each individual.
Here, we study an information-theoretic principle that drives division of labor
and specialization when decision-makers with information constraints are joined
together. We devise an on-line learning rule of this principle that learns a
partitioning of the problem space such that it can be solved by specialized
linear policies. We demonstrate the approach for decision-making problems whose
complexity exceeds the capabilities of individual decision-makers, but can be
solved by combining the decision-makers optimally. The strength of the model is
that it is abstract and principled, yet has direct applications in
classification, regression, reinforcement learning and adaptive control.","cs.IT,cs.LG,math.IT,stat.ML"
"A Novel Semi-supervised Framework for Call Center Agent Malpractice Detection via Neural Feature Learning. This work presents a practical solution to the problem of call center agent
malpractice. A semi-supervised framework comprising of non-linear power
transformation, neural feature learning and k-means clustering is outlined. We
put these building blocks together and tune the parameters so that the best
performance was obtained. The data used in the experiments is obtained from our
in-house call center. It is made up of recorded agent-customer conversations
which have been annotated using a convolutional neural network based segmenter.
The methods provided a means of tuning the parameters of the neural network to
achieve a desirable result. We show that, using our proposed framework, it is
possible to significantly reduce the malpractice classification error of a
k-means-only clustering model which would serve the same purpose. Additionally,
by presenting the amount of silence per call as a key performance indicator, we
show that the proposed system has enhanced agents performance at our call
center since deployment.","cs.LG,cs.SD,eess.AS"
"Prediction of Atomization Energy Using Graph Kernel and Active Learning. Data-driven prediction of molecular properties presents unique challenges to
the design of machine learning methods concerning data
structure/dimensionality, symmetry adaption, and confidence management. In this
paper, we present a kernel-based pipeline that can learn and predict the
atomization energy of molecules with high accuracy. The framework employs
Gaussian process regression to perform predictions based on the similarity
between molecules, which is computed using the marginalized graph kernel. To
apply the marginalized graph kernel, a spatial adjacency rule is first employed
to convert molecules into graphs whose vertices and edges are labeled by
elements and interatomic distances, respectively. We then derive formulas for
the efficient evaluation of the kernel. Specific functional components for the
marginalized graph kernel are proposed, while the effect of the associated
hyperparameters on accuracy and predictive confidence are examined. We show
that the graph kernel is particularly suitable for predicting extensive
properties because its convolutional structure coincides with that of the
covariance formula between sums of random variables. Using an active learning
procedure, we demonstrate that the proposed method can achieve a mean absolute
error of 0.62 +- 0.01 kcal/mol using as few as 2000 training samples on the QM7
data set.","cond-mat.mtrl-sci,cs.CE,cs.LG,physics.comp-ph,stat.ML"
"High-dimensional Multivariate Time Series Forecasting in IoT Applications using Embedding Non-stationary Fuzzy Time Series. In Internet of things (IoT), data is continuously recorded from different
data sources and devices can suffer faults in their embedded electronics, thus
leading to a high-dimensional data sets and concept drift events. Therefore,
methods that are capable of high-dimensional non-stationary time series are of
great value in IoT applications. Fuzzy Time Series (FTS) models stand out as
data-driven non-parametric models of easy implementation and high accuracy.
Unfortunately, FTS encounters difficulties when dealing with data sets of many
variables and scenarios with concept drift. We present a new approach to handle
high-dimensional non-stationary time series, by projecting the original
high-dimensional data into a low dimensional embedding space and using FTS
approach. Combining these techniques enables a better representation of the
complex content of non-stationary multivariate time series and accurate
forecasts. Our model is able to explain 98% of the variance and reach 11.52% of
RMSE, 2.68% of MAE and 2.91% of MAPE.","I.2.6,cs.AI,cs.LG,cs.SY,eess.SP,eess.SY"
"Unsupervised Learning of Semantic Orientation from a Hundred-Billion-Word Corpus. The evaluative character of a word is called its semantic orientation. A
positive semantic orientation implies desirability (e.g., ""honest"", ""intrepid"")
and a negative semantic orientation implies undesirability (e.g., ""disturbing"",
""superfluous""). This paper introduces a simple algorithm for unsupervised
learning of semantic orientation from extremely large corpora. The method
involves issuing queries to a Web search engine and using pointwise mutual
information to analyse the results. The algorithm is empirically evaluated
using a training corpus of approximately one hundred billion words -- the
subset of the Web that is indexed by the chosen search engine. Tested with
3,596 words (1,614 positive and 1,982 negative), the algorithm attains an
accuracy of 80%. The 3,596 test words include adjectives, adverbs, nouns, and
verbs. The accuracy is comparable with the results achieved by Hatzivassiloglou
and McKeown (1997), using a complex four-stage supervised learning algorithm
that is restricted to determining the semantic orientation of adjectives.","H.3.1; H.3.3; I.2.6; I.2.7,cs.IR,cs.LG"
"Learning Attribute-Structure Co-Evolutions in Dynamic Graphs. Most graph neural network models learn embeddings of nodes in static
attributed graphs for predictive analysis. Recent attempts have been made to
learn temporal proximity of the nodes. We find that real dynamic attributed
graphs exhibit complex co-evolution of node attributes and graph structure.
Learning node embeddings for forecasting change of node attributes and birth
and death of links over time remains an open problem. In this work, we present
a novel framework called CoEvoGNN for modeling dynamic attributed graph
sequence. It preserves the impact of earlier graphs on the current graph by
embedding generation through the sequence. It has a temporal self-attention
mechanism to model long-range dependencies in the evolution. Moreover, CoEvoGNN
optimizes model parameters jointly on two dynamic tasks, attribute inference
and link prediction over time. So the model can capture the co-evolutionary
patterns of attribute change and link formation. This framework can adapt to
any graph neural algorithms so we implemented and investigated three methods
based on it: CoEvoGCN, CoEvoGAT, and CoEvoSAGE. Experiments demonstrate the
framework (and its methods) outperform strong baselines on predicting an entire
unseen graph snapshot of personal attributes and interpersonal links in dynamic
social graphs and financial graphs.","cs.LG,stat.ML"
"Bandit Learning for Dynamic Colonel Blotto Game with a Budget Constraint. We consider a dynamic Colonel Blotto game (CBG) in which one of the players
is the learner and has limited troops (budget) to allocate over a finite time
horizon. At each stage, the learner strategically determines the budget and its
distribution to allocate among the battlefields based on past observations. The
other player is the adversary, who chooses its budget allocation strategies
randomly from some fixed but unknown distribution. The learner's objective is
to minimize the regret, which is defined as the difference between the optimal
payoff in terms of the best dynamic policy and the realized payoff by following
a learning algorithm. The dynamic CBG is analyzed under the framework of
combinatorial bandit and bandit with knapsacks. We first convert the dynamic
CBG with the budget constraint to a path planning problem on a graph. We then
devise an efficient dynamic policy for the learner that uses a combinatorial
bandit algorithm Edge on the path planning graph as a subroutine for another
algorithm LagrangeBwK. A high-probability regret bound is derived, and it is
shown that under the proposed policy, the learner's regret in the
budget-constrained dynamic CBG matches (up to a logarithmic factor) that of the
repeated CBG without budget constraints.","cs.GT,cs.LG,cs.MA,cs.SY,eess.SY"
"Deep Unrolled Recovery in Sparse Biological Imaging. Deep algorithm unrolling has emerged as a powerful model-based approach to
develop deep architectures that combine the interpretability of iterative
algorithms with the performance gains of supervised deep learning, especially
in cases of sparse optimization. This framework is well-suited to applications
in biological imaging, where physics-based models exist to describe the
measurement process and the information to be recovered is often highly
structured. Here, we review the method of deep unrolling, and show how it
improves source localization in several biological imaging settings.","cs.CV,cs.IT,cs.LG,eess.SP,math.IT"
"Dialog-based Interactive Image Retrieval. Existing methods for interactive image retrieval have demonstrated the merit
of integrating user feedback, improving retrieval results. However, most
current systems rely on restricted forms of user feedback, such as binary
relevance responses, or feedback based on a fixed set of relative attributes,
which limits their impact. In this paper, we introduce a new approach to
interactive image search that enables users to provide feedback via natural
language, allowing for more natural and effective interaction. We formulate the
task of dialog-based interactive image retrieval as a reinforcement learning
problem, and reward the dialog system for improving the rank of the target
image during each dialog turn. To mitigate the cumbersome and costly process of
collecting human-machine conversations as the dialog system learns, we train
our system with a user simulator, which is itself trained to describe the
differences between target and candidate images. The efficacy of our approach
is demonstrated in a footwear retrieval application. Experiments on both
simulated and real-world data show that 1) our proposed learning framework
achieves better accuracy than other supervised and reinforcement learning
baselines and 2) user feedback based on natural language rather than
pre-specified attributes leads to more effective retrieval results, and a more
natural and expressive communication interface.","cs.AI,cs.CV"
"Efficient, Simple and Automated Negative Sampling for Knowledge Graph Embedding. Negative sampling, which samples negative triplets from non-observed ones in
knowledge graph (KG), is an essential step in KG embedding. Recently,
generative adversarial network (GAN), has been introduced in negative sampling.
By sampling negative triplets with large gradients, these methods avoid the
problem of vanishing gradient and thus obtain better performance. However, they
make the original model more complex and harder to train. In this paper,
motivated by the observation that negative triplets with large gradients are
important but rare, we propose to directly keep track of them with the cache.
In this way, our method acts as a ""distilled"" version of previous GAN-based
methods, which does not waste training time on additional parameters to fit the
full distribution of negative triplets. However, how to sample from and update
the cache are two critical questions. We propose to solve these issues by
automated machine learning techniques. The automated version also covers
GAN-based methods as special cases. Theoretical explanation of NSCaching is
also provided, justifying the superior over fixed sampling scheme. Besides, we
further extend NSCaching with skip-gram model for graph embedding. Finally,
extensive experiments show that our method can gain significant improvements on
various KG embedding models and the skip-gram model, and outperforms the
state-of-the-art negative sampling methods.","cs.AI,cs.DB,cs.LG"
"Manifold Oblique Random Forests: Towards Closing the Gap on Convolutional Deep Networks. Decision forests (Forests), in particular random forests and gradient
boosting trees, have demonstrated state-of-the-art accuracy compared to other
methods in many supervised learning scenarios. In particular, Forests dominate
other methods in tabular data, that is, when the feature space is unstructured,
so that the signal is invariant to a permutation of the feature indices.
However, in structured data lying on a manifold (such as images, text, and
speech) deep networks (Networks), specifically convolutional deep networks
(ConvNets), tend to outperform Forests. We conjecture that at least part of the
reason for this is that the input to Networks is not simply the feature
magnitudes, but also their indices. In contrast, naive Forest implementations
fail to explicitly consider feature indices. A recently proposed Forest
approach demonstrates that Forests, for each node, implicitly sample a random
matrix from some specific distribution. These Forests, like some classes of
Networks, learn by partitioning the feature space into convex polytopes
corresponding to linear functions. We build on that approach and show that one
can choose distributions in a manifold-aware fashion to incorporate feature
locality. We demonstrate the empirical performance on data whose features live
on three different manifolds: a torus, images, and time-series. Moreover, we
demonstrate its strength in multivariate simulated settings and also show
superiority in predicting surgical outcome in epilepsy patients and predicting
movement direction from raw stereotactic EEG data from non-motor brain regions.
In all simulations and real data, Manifold Oblique Random Forest (MORF)
algorithm outperforms approaches that ignore feature space structure and
challenges the performance of ConvNets. Moreover, MORF runs fast and maintains
interpretability and theoretical justification.","68T05,cs.LG,stat.ML"
"Transfer Learning for Protein Structure Classification at Low Resolution. Structure determination is key to understanding protein function at a
molecular level. Whilst significant advances have been made in predicting
structure and function from amino acid sequence, researchers must still rely on
expensive, time-consuming analytical methods to visualise detailed protein
conformation. In this study, we demonstrate that it is possible to make
accurate ($\geq$80%) predictions of protein class and architecture from
structures determined at low ($>$3A) resolution, using a deep convolutional
neural network trained on high-resolution ($\leq$3A) structures represented as
2D matrices. Thus, we provide proof of concept for high-speed, low-cost protein
structure classification at low resolution, and a basis for extension to
prediction of function. We investigate the impact of the input representation
on classification performance, showing that side-chain information may not be
necessary for fine-grained structure predictions. Finally, we confirm that
high-resolution, low-resolution and NMR-determined structures inhabit a common
feature space, and thus provide a theoretical foundation for boosting with
single-image super-resolution.","I.2; I.4,cs.CV,cs.LG,q-bio.BM"
"Visual-and-Language Navigation: A Survey and Taxonomy. An agent that can understand natural-language instruction and carry out
corresponding actions in the visual world is one of the long-term challenges of
Artificial Intelligent (AI). Due to multifarious instructions from humans, it
requires the agent can link natural language to vision and action in
unstructured, previously unseen environments. If the instruction given by human
is a navigation task, this challenge is called Visual-and-Language Navigation
(VLN). It is a booming multi-disciplinary field of increasing importance and
with extraordinary practicality. Instead of focusing on the details of specific
methods, this paper provides a comprehensive survey on VLN tasks and makes a
classification carefully according the different characteristics of language
instructions in these tasks. According to when the instructions are given, the
tasks can be divided into single-turn and multi-turn. For single-turn tasks, we
further divided them into goal-orientation and route-orientation based on
whether the instructions contain a route. For multi-turn tasks, we divided them
into imperative task and interactive task based on whether the agent responses
to the instructions. This taxonomy enable researchers to better grasp the key
point of a specific task and identify directions for future research.","cs.CV,cs.MM"
"Silas: High Performance, Explainable and Verifiable Machine Learning. This paper introduces a new classification tool named Silas, which is built
to provide a more transparent and dependable data analytics service. A focus of
Silas is on providing a formal foundation of decision trees in order to support
logical analysis and verification of learned prediction models. This paper
describes the distinct features of Silas: The Model Audit module formally
verifies the prediction model against user specifications, the Enforcement
Learning module trains prediction models that are guaranteed correct, the Model
Insight and Prediction Insight modules reason about the prediction model and
explain the decision-making of predictions. We also discuss implementation
details ranging from programming paradigm to memory management that help
achieve high-performance computation.","cs.LG,cs.LO,stat.ML"
"Temporally Coherent Embeddings for Self-Supervised Video Representation Learning. This paper presents TCE: Temporally Coherent Embeddings for self-supervised
video representation learning. The proposed method exploits inherent structure
of unlabeled video data to explicitly enforce temporal coherency in the
embedding space, rather than indirectly learning it through ranking or
predictive proxy tasks. In the same way that high-level visual information in
the world changes smoothly, we believe that nearby frames in learned
representations will benefit from demonstrating similar properties. Using this
assumption, we train our TCE model to encode videos such that adjacent frames
exist close to each other and videos are separated from one another. Using TCE
we learn robust representations from large quantities of unlabeled video data.
We thoroughly analyse and evaluate our self-supervised learned TCE models on a
downstream task of video action recognition using multiple challenging
benchmarks (Kinetics400, UCF101, HMDB51). With a simple but effective 2D-CNN
backbone and only RGB stream inputs, TCE pre-trained representations outperform
all previous selfsupervised 2D-CNN and 3D-CNN pre-trained on UCF101. The code
and pre-trained models for this paper can be downloaded at:
https://github.com/csiro-robotics/TCE","I.2.6,cs.CV,cs.LG,eess.IV"
"An extension of the angular synchronization problem to the heterogeneous setting. Given an undirected measurement graph $G = ([n], E)$, the classical angular
synchronization problem consists of recovering unknown angles
$\theta_1,\dots,\theta_n$ from a collection of noisy pairwise measurements of
the form $(\theta_i - \theta_j) \mod 2\pi$, for each $\{i,j\} \in E$. This
problem arises in a variety of applications, including computer vision, time
synchronization of distributed networks, and ranking from preference
relationships. In this paper, we consider a generalization to the setting where
there exist $k$ unknown groups of angles $\theta_{l,1}, \dots,\theta_{l,n}$,
for $l=1,\dots,k$. For each $ \{i,j\} \in E$, we are given noisy pairwise
measurements of the form $\theta_{\ell,i} - \theta_{\ell,j}$ for an unknown
$\ell \in \{1,2,\ldots,k\}$. This can be thought of as a natural extension of
the angular synchronization problem to the heterogeneous setting of multiple
groups of angles, where the measurement graph has an unknown edge-disjoint
decomposition $G = G_1 \cup G_2 \ldots \cup G_k$, where the $G_i$'s denote the
subgraphs of edges corresponding to each group. We propose a probabilistic
generative model for this problem, along with a spectral algorithm for which we
provide a detailed theoretical analysis in terms of robustness against both
sampling sparsity and noise. The theoretical findings are complemented by a
comprehensive set of numerical experiments, showcasing the efficacy of our
algorithm under various parameter regimes. Finally, we consider an application
of bi-synchronization to the graph realization problem, and provide along the
way an iterative graph disentangling procedure that uncovers the subgraphs
$G_i$, $i=1,\ldots,k$ which is of independent interest, as it is shown to
improve the final recovery accuracy across all the experiments considered.","cs.IT,cs.LG,cs.NA,math.IT,math.NA,stat.ML"
"Symmetric Spaces for Graph Embeddings: A Finsler-Riemannian Approach. Learning faithful graph representations as sets of vertex embeddings has
become a fundamental intermediary step in a wide range of machine learning
applications. We propose the systematic use of symmetric spaces in
representation learning, a class encompassing many of the previously used
embedding targets. This enables us to introduce a new method, the use of
Finsler metrics integrated in a Riemannian optimization scheme, that better
adapts to dissimilar structures in the graph. We develop a tool to analyze the
embeddings and infer structural properties of the data sets. For
implementation, we choose Siegel spaces, a versatile family of symmetric
spaces. Our approach outperforms competitive baselines for graph reconstruction
tasks on various synthetic and real-world datasets. We further demonstrate its
applicability on two downstream tasks, recommender systems and node
classification.","I.2,cs.CG,cs.LG"
"Fully Learnable Deep Wavelet Transform for Unsupervised Monitoring of High-Frequency Time Series. High-Frequency (HF) signal are ubiquitous in the industrial world and are of
great use for the monitoring of industrial assets. Most deep learning tools are
designed for inputs of fixed and/or very limited size and many successful
applications of deep learning to the industrial context use as inputs extracted
features, which is a manually and often arduously obtained compact
representation of the original signal. In this paper, we propose a fully
unsupervised deep learning framework that is able to extract meaningful and
sparse representation of raw HF signals. We embed in our architecture important
properties of the fast discrete wavelet transformation (FDWT) such as (1) the
cascade algorithm, (2) the quadrature mirror filter property that relates
together the wavelet, the scaling and transposed filter functions, and (3) the
coefficient denoising. Using deep learning, we make this architecture fully
learnable: both the wavelet bases and the wavelet coefficient denoising are
learnable. To achieve this objective, we introduce a new activation function
that performs a learnable hard-thresholding of the wavelet coefficients. With
our framework, the denoising FDWT becomes a fully learnable unsupervised tool
that does neither require any type of pre- nor post-processing, nor any prior
knowledge on wavelet transform. We demonstrate the benefit of embedding all
these properties on three machine-learning tasks performed on open source sound
datasets. We achieve results well above baseline and we perform an ablation
study of the impact of each property on the performance of the architecture.","cs.LG,cs.SD,eess.AS"
"Defending SVMs against Poisoning Attacks: the Hardness and DBSCAN Approach. Adversarial machine learning has attracted a great amount of attention in
recent years. In a poisoning attack, the adversary can inject a small number of
specially crafted samples into the training data which make the decision
boundary severely deviate and cause unexpected misclassification. Due to the
great importance and popular use of support vector machines (SVM), we consider
defending SVM against poisoning attacks in this paper. We study two commonly
used strategies for defending: designing robust SVM algorithms and data
sanitization. Though several robust SVM algorithms have been proposed before,
most of them either are in lack of adversarial-resilience, or rely on strong
assumptions about the data distribution or the attacker's behavior. Moreover,
the research on their complexities is still quite limited. We are the first, to
the best of our knowledge, to prove that even the simplest hard-margin
one-class SVM with outliers problem is NP-complete, and has no fully PTAS
unless P$=$NP (that means it is hard to achieve an even approximate algorithm).
For the data sanitization defense, we link it to the intrinsic dimensionality
of data; in particular, we provide a sampling theorem in doubling metrics for
explaining the effectiveness of DBSCAN (as a density-based outlier removal
method) for defending against poisoning attacks. In our empirical experiments,
we compare several defenses including the DBSCAN and robust SVM methods, and
investigate the influences from the intrinsic dimensionality and data density
to their performances.","cs.CG,cs.CR,cs.LG,stat.ML"
"Safe Exploration for Identifying Linear Systems via Robust Optimization. Safely exploring an unknown dynamical system is critical to the deployment of
reinforcement learning (RL) in physical systems where failures may have
catastrophic consequences. In scenarios where one knows little about the
dynamics, diverse transition data covering relevant regions of state-action
space is needed to apply either model-based or model-free RL. Motivated by the
cooling of Google's data centers, we study how one can safely identify the
parameters of a system model with a desired accuracy and confidence level. In
particular, we focus on learning an unknown linear system with Gaussian noise
assuming only that, initially, a nominal safe action is known. Define safety as
satisfying specific linear constraints on the state space (e.g., requirements
on process variable) that must hold over the span of an entire trajectory, and
given a Probably Approximately Correct (PAC) style bound on the estimation
error of model parameters, we show how to compute safe regions of action space
by gradually growing a ball around the nominal safe action. One can apply any
exploration strategy where actions are chosen from such safe regions.
Experiments on a stylized model of data center cooling dynamics show how
computing proper safe regions can increase the sample efficiency of safe
exploration.","cs.LG,cs.SY"
"Seeing What a GAN Cannot Generate. Despite the success of Generative Adversarial Networks (GANs), mode collapse
remains a serious issue during GAN training. To date, little work has focused
on understanding and quantifying which modes have been dropped by a model. In
this work, we visualize mode collapse at both the distribution level and the
instance level. First, we deploy a semantic segmentation network to compare the
distribution of segmented objects in the generated images with the target
distribution in the training set. Differences in statistics reveal object
classes that are omitted by a GAN. Second, given the identified omitted object
classes, we visualize the GAN's omissions directly. In particular, we compare
specific differences between individual photos and their approximate inversions
by a GAN. To this end, we relax the problem of inversion and solve the
tractable problem of inverting a GAN layer instead of the entire generator.
Finally, we use this framework to analyze several recent GANs trained on
multiple datasets and identify their typical failure cases.","cs.CV,cs.GR,cs.LG,eess.IV"
"Meta Decision Trees for Explainable Recommendation Systems. We tackle the problem of building explainable recommendation systems that are
based on a per-user decision tree, with decision rules that are based on single
attribute values. We build the trees by applying learned regression functions
to obtain the decision rules as well as the values at the leaf nodes. The
regression functions receive as input the embedding of the user's training set,
as well as the embedding of the samples that arrive at the current node. The
embedding and the regressors are learned end-to-end with a loss that encourages
the decision rules to be sparse. By applying our method, we obtain a
collaborative filtering solution that provides a direct explanation to every
rating it provides. With regards to accuracy, it is competitive with other
algorithms. However, as expected, explainability comes at a cost and the
accuracy is typically slightly lower than the state of the art result reported
in the literature.","cs.IR,cs.LG,stat.ML"
"Clustering acoustic emission data streams with sequentially appearing clusters using mixture models. The interpretation of unlabeled acoustic emission (AE) data classically
relies on general-purpose clustering methods. While several external criteria
have been used in the past to select the hyperparameters of those algorithms,
few studies have paid attention to the development of dedicated objective
functions in clustering methods able to cope with the specificities of AE data.
We investigate how to explicitly represent clusters onsets in mixture models in
general, and in Gaussian Mixture Models (GMM) in particular. By modifying the
internal criterion of such models, we propose the first clustering method able
to provide, through parameters estimated by an expectation-maximization
procedure, information about when clusters occur (onsets), how they grow
(kinetics) and their level of activation through time. This new objective
function accommodates continuous timestamps of AE signals and, thus, their
order of occurrence. The method, called GMMSEQ, is experimentally validated to
characterize the loosening phenomenon in bolted structure under vibrations. A
comparison with three standard clustering methods on raw streaming data from
five experimental campaigns shows that GMMSEQ not only provides useful
qualitative information about the timeline of clusters, but also shows better
performance in terms of cluster characterization. In view of developing an open
acoustic emission initiative and according to the FAIR principles, the datasets
and the codes are made available to reproduce the research of this paper.","cs.LG,cs.SD,stat.AP,stat.ME,stat.ML"
"Semi-supervised Learning for Aggregated Multilayer Graphs Using Diffuse Interface Methods and Fast Matrix Vector Products. We generalize a graph-based multiclass semi-supervised classification
technique based on diffuse interface methods to multilayer graphs. Besides the
treatment of various applications with an inherent multilayer structure, we
present a very flexible approach that interprets high-dimensional data in a
low-dimensional multilayer graph representation. Highly efficient numerical
methods involving the spectral decomposition of the corresponding differential
graph operators as well as fast matrix-vector products based on the
nonequispaced fast Fourier transform (NFFT) enable the rapid treatment of large
and high-dimensional data sets. We perform various numerical tests putting a
special focus on image segmentation. In particular, we test the performance of
our method on data sets with up to 10 million nodes per layer as well as up to
104 dimensions resulting in graphs with up to 52 layers. While all presented
numerical experiments can be run on an average laptop computer, the linear
dependence per iteration step of the runtime on the network size in all stages
of our algorithm makes it scalable to even larger and higher-dimensional
problems.","05C50,62H30,65F15,65T50,68R10,68T05,cs.LG,cs.NA,math.NA,stat.ML"
"InfoSSM: Interpretable Unsupervised Learning of Nonparametric State-Space Model for Multi-modal Dynamics. The goal of system identification is to learn about underlying physics
dynamics behind the time-series data. To model the probabilistic and
nonparametric dynamics model, Gaussian process (GP) have been widely used; GP
can estimate the uncertainty of prediction and avoid over-fitting. Traditional
GPSSMs, however, are based on Gaussian transition model, thus often have
difficulty in describing a more complex transition model, e.g. aircraft
motions. To resolve the challenge, this paper proposes a framework using
multiple GP transition models which is capable of describing multi-modal
dynamics. Furthermore, we extend the model to the information-theoretic
framework, the so-called InfoSSM, by introducing a mutual information
regularizer helping the model to learn interpretable and distinguishable
multiple dynamics models. Two illustrative numerical experiments in simple
Dubins vehicle and high-fidelity flight simulator are presented to demonstrate
the performance and interpretability of the proposed model. Finally, this paper
introduces a framework using InfoSSM with Bayesian filtering for air traffic
control tracking.","cs.LG,stat.AP,stat.ML"
"MM-Deacon: Multimodal molecular domain embedding analysis via contrastive learning. Molecular representation learning plays an essential role in cheminformatics.
Recently, language model-based approaches have been popular as an alternative
to traditional expert-designed features to encode molecules. However, these
approaches only utilize a single modality for representing molecules. Driven by
the fact that a given molecule can be described through different modalities
such as Simplified Molecular Line Entry System (SMILES), The International
Union of Pure and Applied Chemistry (IUPAC), and The IUPAC International
Chemical Identifier (InChI), we propose a multimodal molecular embedding
generation approach called MM-Deacon (multimodal molecular domain embedding
analysis via contrastive learning). MM-Deacon is trained using SMILES and IUPAC
molecule representations as two different modalities. First, SMILES and IUPAC
strings are encoded by using two different transformer-based language models
independently, then the contrastive loss is utilized to bring these encoded
representations from different modalities closer to each other if they belong
to the same molecule, and to push embeddings farther from each other if they
belong to different molecules. We evaluate the robustness of our molecule
embeddings on molecule clustering, cross-modal molecule search, drug similarity
assessment and drug-drug interaction tasks.","cs.CL,cs.LG,physics.chem-ph"
"Multilayer bootstrap network for unsupervised speaker recognition. We apply multilayer bootstrap network (MBN), a recent proposed unsupervised
learning method, to unsupervised speaker recognition. The proposed method first
extracts supervectors from an unsupervised universal background model, then
reduces the dimension of the high-dimensional supervectors by multilayer
bootstrap network, and finally conducts unsupervised speaker recognition by
clustering the low-dimensional data. The comparison results with 2 unsupervised
and 1 supervised speaker recognition techniques demonstrate the effectiveness
and robustness of the proposed method.","cs.LG,cs.SD"
"Uncertainty-aware Self-supervised 3D Data Association. 3D object trackers usually require training on large amounts of annotated
data that is expensive and time-consuming to collect. Instead, we propose
leveraging vast unlabeled datasets by self-supervised metric learning of 3D
object trackers, with a focus on data association. Large scale annotations for
unlabeled data are cheaply obtained by automatic object detection and
association across frames. We show how these self-supervised annotations can be
used in a principled manner to learn point-cloud embeddings that are effective
for 3D tracking. We estimate and incorporate uncertainty in self-supervised
tracking to learn more robust embeddings, without needing any labeled data. We
design embeddings to differentiate objects across frames, and learn them using
uncertainty-aware self-supervised training. Finally, we demonstrate their
ability to perform accurate data association across frames, towards effective
and accurate 3D tracking. Project videos and code are at
https://jianrenw.github.io/Self-Supervised-3D-Data-Association.","cs.CV,cs.RO"
"Johnson-Lindenstrauss Lemma, Linear and Nonlinear Random Projections, Random Fourier Features, and Random Kitchen Sinks: Tutorial and Survey. This is a tutorial and survey paper on the Johnson-Lindenstrauss (JL) lemma
and linear and nonlinear random projections. We start with linear random
projection and then justify its correctness by JL lemma and its proof. Then,
sparse random projections with $\ell_1$ norm and interpolation norm are
introduced. Two main applications of random projection, which are low-rank
matrix approximation and approximate nearest neighbor search by random
projection onto hypercube, are explained. Random Fourier Features (RFF) and
Random Kitchen Sinks (RKS) are explained as methods for nonlinear random
projection. Some other methods for nonlinear random projection, including
extreme learning machine, randomly weighted neural networks, and ensemble of
random projections, are also introduced.","cs.DS,cs.LG,math.PR,stat.ML"
"Local Linear Forests. Random forests are a powerful method for non-parametric regression, but are
limited in their ability to fit smooth signals, and can show poor predictive
performance in the presence of strong, smooth effects. Taking the perspective
of random forests as an adaptive kernel method, we pair the forest kernel with
a local linear regression adjustment to better capture smoothness. The
resulting procedure, local linear forests, enables us to improve on asymptotic
rates of convergence for random forests with smooth signals, and provides
substantial gains in accuracy on both real and simulated data. We prove a
central limit theorem valid under regularity conditions on the forest and
smoothness constraints, and propose a computationally efficient construction
for confidence intervals. Moving to a causal inference application, we discuss
the merits of local regression adjustments for heterogeneous treatment effect
estimation, and give an example on a dataset exploring the effect word choice
has on attitudes to the social safety net. Last, we include simulation results
on real and generated data.","cs.LG,econ.EM,math.ST,stat.ML,stat.TH"
"Fourier Neural Operator for Parametric Partial Differential Equations. The classical development of neural networks has primarily focused on
learning mappings between finite-dimensional Euclidean spaces. Recently, this
has been generalized to neural operators that learn mappings between function
spaces. For partial differential equations (PDEs), neural operators directly
learn the mapping from any functional parametric dependence to the solution.
Thus, they learn an entire family of PDEs, in contrast to classical methods
which solve one instance of the equation. In this work, we formulate a new
neural operator by parameterizing the integral kernel directly in Fourier
space, allowing for an expressive and efficient architecture. We perform
experiments on Burgers' equation, Darcy flow, and Navier-Stokes equation. The
Fourier neural operator is the first ML-based method to successfully model
turbulent flows with zero-shot super-resolution. It is up to three orders of
magnitude faster compared to traditional PDE solvers. Additionally, it achieves
superior accuracy compared to previous learning-based solvers under fixed
resolution.","cs.LG,cs.NA,math.NA"
"Power Grid Cascading Failure Mitigation by Reinforcement Learning. This paper proposes a cascading failure mitigation strategy based on
Reinforcement Learning (RL). The motivation of the Multi-Stage Cascading
Failure (MSCF) problem and its connection with the challenge of climate change
are introduced. The bottom-level corrective control of the MCSF problem is
formulated based on DCOPF (Direct Current Optimal Power Flow). Then, to
mitigate the MSCF issue by a high-level RL-based strategy, physics-informed
reward, action, and state are devised. Besides, both shallow and deep neural
network architectures are tested. Experiments on the IEEE 118-bus system by the
proposed mitigation strategy demonstrate a promising performance in reducing
system collapses.","68T01,68T07,F.2.2; K.3.2,cs.LG,math.DS,math.OC"
"AlphaNet: An Attention Guided Deep Network for Automatic Image Matting. In this paper, we propose an end to end solution for image matting i.e
high-precision extraction of foreground objects from natural images. Image
matting and background detection can be achieved easily through chroma keying
in a studio setting when the background is either pure green or blue.
Nonetheless, image matting in natural scenes with complex and uneven depth
backgrounds remains a tedious task that requires human intervention. To achieve
complete automatic foreground extraction in natural scenes, we propose a method
that assimilates semantic segmentation and deep image matting processes into a
single network to generate detailed semantic mattes for image composition task.
The contribution of our proposed method is two-fold, firstly it can be
interpreted as a fully automated semantic image matting method and secondly as
a refinement of existing semantic segmentation models. We propose a novel model
architecture as a combination of segmentation and matting that unifies the
function of upsampling and downsampling operators with the notion of attention.
As shown in our work, attention guided downsampling and upsampling can extract
high-quality boundary details, unlike other normal downsampling and upsampling
techniques. For achieving the same, we utilized an attention guided
encoder-decoder framework which does unsupervised learning for generating an
attention map adaptively from the data to serve and direct the upsampling and
downsampling operators. We also construct a fashion e-commerce focused dataset
with high-quality alpha mattes to facilitate the training and evaluation for
image matting.","I.2.10; I.4.8; I.5.1,cs.CV,cs.LG,eess.IV"
"Discovering PDEs from Multiple Experiments. Automated model discovery of partial differential equations (PDEs) usually
considers a single experiment or dataset to infer the underlying governing
equations. In practice, experiments have inherent natural variability in
parameters, initial and boundary conditions that cannot be simply averaged out.
We introduce a randomised adaptive group Lasso sparsity estimator to promote
grouped sparsity and implement it in a deep learning based PDE discovery
framework. It allows to create a learning bias that implies the a priori
assumption that all experiments can be explained by the same underlying PDE
terms with potentially different coefficients. Our experimental results show
more generalizable PDEs can be found from multiple highly noisy datasets, by
this grouped sparsity promotion rather than simply performing independent model
discoveries.","cs.LG,physics.comp-ph,stat.ML"
"Local Unsupervised Learning for Image Analysis. Local Hebbian learning is believed to be inferior in performance to
end-to-end training using a backpropagation algorithm. We question this popular
belief by designing a local algorithm that can learn convolutional filters at
scale on large image datasets. These filters combined with patch normalization
and very steep non-linearities result in a good classification accuracy for
shallow networks trained locally, as opposed to end-to-end. The filters learned
by our algorithm contain both orientation selective units and unoriented color
units, resembling the responses of pyramidal neurons located in the cytochrome
oxidase 'interblob' and 'blob' regions in the primary visual cortex of
primates. It is shown that convolutional networks with patch normalization
significantly outperform standard convolutional networks on the task of
recovering the original classes when shadows are superimposed on top of
standard CIFAR-10 images. Patch normalization approximates the retinal
adaptation to the mean light intensity, important for human vision. We also
demonstrate a successful transfer of learned representations between CIFAR-10
and ImageNet 32x32 datasets. All these results taken together hint at the
possibility that local unsupervised training might be a powerful tool for
learning general representations (without specifying the task) directly from
unlabeled data.","cs.CV,cs.LG,cs.NE,q-bio.NC,stat.ML"
"Medical Image Segmentation with Limited Supervision: A Review of Deep Network Models. Despite the remarkable performance of deep learning methods on various tasks,
most cutting-edge models rely heavily on large-scale annotated training
examples, which are often unavailable for clinical and health care tasks. The
labeling costs for medical images are very high, especially in medical image
segmentation, which typically requires intensive pixel/voxel-wise labeling.
Therefore, the strong capability of learning and generalizing from limited
supervision, including a limited amount of annotations, sparse annotations, and
inaccurate annotations, is crucial for the successful application of deep
learning models in medical image segmentation. However, due to its intrinsic
difficulty, segmentation with limited supervision is challenging and specific
model design and/or learning strategies are needed. In this paper, we provide a
systematic and up-to-date review of the solutions above, with summaries and
comments about the methodologies. We also highlight several problems in this
field, discussed future directions observing further investigations.","cs.CV,cs.LG"
"Pay Attention to the cough: Early Diagnosis of COVID-19 using Interpretable Symptoms Embeddings with Cough Sound Signal Processing. COVID-19 (coronavirus disease 2019) pandemic caused by SARS-CoV-2 has led to
a treacherous and devastating catastrophe for humanity. At the time of writing,
no specific antivirus drugs or vaccines are recommended to control infection
transmission and spread. The current diagnosis of COVID-19 is done by
Reverse-Transcription Polymer Chain Reaction (RT-PCR) testing. However, this
method is expensive, time-consuming, and not easily available in straitened
regions. An interpretable and COVID-19 diagnosis AI framework is devised and
developed based on the cough sounds features and symptoms metadata to overcome
these limitations. The proposed framework's performance was evaluated using a
medical dataset containing Symptoms and Demographic data of 30000 audio
segments, 328 cough sounds from 150 patients with four cough classes (
COVID-19, Asthma, Bronchitis, and Healthy). Experiments' results show that the
model captures the better and robust feature embedding to distinguish between
COVID-19 patient coughs and several types of non-COVID-19 coughs with higher
specificity and accuracy of 95.04 $\pm$ 0.18% and 96.83$\pm$ 0.18%
respectively, all the while maintaining interpretability.","cs.LG,cs.SD,eess.AS"
"PISA: Pixelwise Image Saliency by Aggregating Complementary Appearance Contrast Measures with Edge-Preserving Coherence. Driven by recent vision and graphics applications such as image segmentation
and object recognition, computing pixel-accurate saliency values to uniformly
highlight foreground objects becomes increasingly important. In this paper, we
propose a unified framework called PISA, which stands for Pixelwise Image
Saliency Aggregating various bottom-up cues and priors. It generates spatially
coherent yet detail-preserving, pixel-accurate and fine-grained saliency, and
overcomes the limitations of previous methods which use homogeneous
superpixel-based and color only treatment. PISA aggregates multiple saliency
cues in a global context such as complementary color and structure contrast
measures with their spatial priors in the image domain. The saliency confidence
is further jointly modeled with a neighborhood consistence constraint into an
energy minimization formulation, in which each pixel will be evaluated with
multiple hypothetical saliency levels. Instead of using global discrete
optimization methods, we employ the cost-volume filtering technique to solve
our formulation, assigning the saliency levels smoothly while preserving the
edge-aware structure details. In addition, a faster version of PISA is
developed using a gradient-driven image sub-sampling strategy to greatly
improve the runtime efficiency while keeping comparable detection accuracy.
Extensive experiments on a number of public datasets suggest that PISA
convincingly outperforms other state-of-the-art approaches. In addition, with
this work we also create a new dataset containing $800$ commodity images for
evaluating saliency detection. The dataset and source code of PISA can be
downloaded at http://vision.sysu.edu.cn/project/PISA/","68U10,cs.CV"
"Dimension Free Generalization Bounds for Non Linear Metric Learning. In this work we study generalization guarantees for the metric learning
problem, where the metric is induced by a neural network type embedding of the
data. Specifically, we provide uniform generalization bounds for two regimes --
the sparse regime, and a non-sparse regime which we term \emph{bounded
amplification}. The sparse regime bounds correspond to situations where
$\ell_1$-type norms of the parameters are small. Similarly to the situation in
classification, solutions satisfying such bounds can be obtained by an
appropriate regularization of the problem. On the other hand, unregularized SGD
optimization of a metric learning loss typically does not produce sparse
solutions. We show that despite this lack of sparsity, by relying on a
different, new property of the solutions, it is still possible to provide
dimension free generalization guarantees. Consequently, these bounds can
explain generalization in non sparse real experimental situations. We
illustrate the studied phenomena on the MNIST and 20newsgroups datasets.","cs.LG,math.ST,stat.ML,stat.TH"
"Federated Transfer Reinforcement Learning for Autonomous Driving. Reinforcement learning (RL) is widely used in autonomous driving tasks and
training RL models typically involves in a multi-step process: pre-training RL
models on simulators, uploading the pre-trained model to real-life robots, and
fine-tuning the weight parameters on robot vehicles. This sequential process is
extremely time-consuming and more importantly, knowledge from the fine-tuned
model stays local and can not be re-used or leveraged collaboratively. To
tackle this problem, we present an online federated RL transfer process for
real-time knowledge extraction where all the participant agents make
corresponding actions with the knowledge learned by others, even when they are
acting in very different environments. To validate the effectiveness of the
proposed approach, we constructed a real-life collision avoidance system with
Microsoft Airsim simulator and NVIDIA JetsonTX2 car agents, which cooperatively
learn from scratch to avoid collisions in indoor environment with obstacle
objects. We demonstrate that with the proposed framework, the simulator car
agents can transfer knowledge to the RC cars in real-time, with 27% increase in
the average distance with obstacles and 42% decrease in the collision counts.","cs.AI,cs.LG,cs.RO"
"Using Graph Neural Networks to model the performance of Deep Neural Networks. With the unprecedented proliferation of machine learning software, there is
an ever-increasing need to generate efficient code for such applications.
State-of-the-art deep-learning compilers like TVM and Halide incorporate a
learning-based performance model to search the space of valid implementations
of a given deep learning algorithm. For a given application, the model
generates a performance metric such as the run time without executing the
application on hardware. Such models speed up the compilation process by
obviating the need to benchmark an enormous number of candidate
implementations, referred to as schedules, on hardware. Existing performance
models employ feed-forward networks, recurrent networks, or decision tree
ensembles to estimate the performance of different implementations of a neural
network. Graphs present a natural and intuitive way to model deep-learning
networks where each node represents a computational stage or operation.
Incorporating the inherent graph structure of these workloads in the
performance model can enable a better representation and learning of
inter-stage interactions. The accuracy of a performance model has direct
implications on the efficiency of the search strategy, making it a crucial
component of this class of deep-learning compilers. In this work, we develop a
novel performance model that adopts a graph representation. In our model, each
stage of computation represents a node characterized by features that capture
the operations performed by the stage. The interaction between nodes is
achieved using graph convolutions. Experimental evaluation shows a 7:75x and
12x reduction in prediction error compared to the Halide and TVM models,
respectively.","cs.LG,cs.SE"
"Growing an architecture for a neural network. We propose a new kind of automatic architecture search algorithm. The
algorithm alternates pruning connections and adding neurons, and it is not
restricted to layered architectures only. Here architecture is an arbitrary
oriented graph with some weights (along with some biases and an activation
function), so there may be no layered structure in such a network. The
algorithm minimizes the complexity of staying within a given error. We
demonstrate our algorithm on the brightness prediction problem of the next
point through the previous points on an image. Our second test problem is the
approximation of the bivariate function defining the brightness of a black and
white image. Our optimized networks significantly outperform the standard
solution for neural network architectures in both cases.","68T07,cs.LG,cs.NE"
"A Tunable Model for Graph Generation Using LSTM and Conditional VAE. With the development of graph applications, generative models for graphs have
been more crucial. Classically, stochastic models that generate graphs with a
pre-defined probability of edges and nodes have been studied. Recently, some
models that reproduce the structural features of graphs by learning from actual
graph data using machine learning have been studied. However, in these
conventional studies based on machine learning, structural features of graphs
can be learned from data, but it is not possible to tune features and generate
graphs with specific features. In this paper, we propose a generative model
that can tune specific features, while learning structural features of a graph
from data. With a dataset of graphs with various features generated by a
stochastic model, we confirm that our model can generate a graph with specific
features.","cs.LG,cs.NI,cs.SI"
"Towards Explainable Bit Error Tolerance of Resistive RAM-Based Binarized Neural Networks. Non-volatile memory, such as resistive RAM (RRAM), is an emerging
energy-efficient storage, especially for low-power machine learning models on
the edge. It is reported, however, that the bit error rate of RRAMs can be up
to 3.3% in the ultra low-power setting, which might be crucial for many use
cases. Binary neural networks (BNNs), a resource efficient variant of neural
networks (NNs), can tolerate a certain percentage of errors without a loss in
accuracy and demand lower resources in computation and storage. The bit error
tolerance (BET) in BNNs can be achieved by flipping the weight signs during
training, as proposed by Hirtzlin et al., but their method has a significant
drawback, especially for fully connected neural networks (FCNN): The FCNNs
overfit to the error rate used in training, which leads to low accuracy under
lower error rates. In addition, the underlying principles of BET are not
investigated. In this work, we improve the training for BET of BNNs and aim to
explain this property. We propose straight-through gradient approximation to
improve the weight-sign-flip training, by which BNNs adapt less to the bit
error rates. To explain the achieved robustness, we define a metric that aims
to measure BET without fault injection. We evaluate the metric and find that it
correlates with accuracy over error rate for all FCNNs tested. Finally, we
explore the influence of a novel regularizer that optimizes with respect to
this metric, with the aim of providing a configurable trade-off in accuracy and
BET.","68T05,I.2.6; B.7.1,cs.ET,cs.LG,stat.ML"
"Few-Shot Adversarial Learning of Realistic Neural Talking Head Models. Several recent works have shown how highly realistic human head images can be
obtained by training convolutional neural networks to generate them. In order
to create a personalized talking head model, these works require training on a
large dataset of images of a single person. However, in many practical
scenarios, such personalized talking head models need to be learned from a few
image views of a person, potentially even a single image. Here, we present a
system with such few-shot capability. It performs lengthy meta-learning on a
large dataset of videos, and after that is able to frame few- and one-shot
learning of neural talking head models of previously unseen people as
adversarial training problems with high capacity generators and discriminators.
Crucially, the system is able to initialize the parameters of both the
generator and the discriminator in a person-specific way, so that training can
be based on just a few images and done quickly, despite the need to tune tens
of millions of parameters. We show that such an approach is able to learn
highly realistic and personalized talking head models of new people and even
portrait paintings.","cs.CV,cs.GR,cs.LG"
"Auto-Encoding Molecular Conformations. In this work we introduce an Autoencoder for molecular conformations. Our
proposed model converts the discrete spatial arrangements of atoms in a given
molecular graph (conformation) into and from a continuous fixed-sized latent
representation. We demonstrate that in this latent representation, similar
conformations cluster together while distinct conformations split apart.
Moreover, by training a probabilistic model on a large dataset of molecular
conformations, we demonstrate how our model can be used to generate diverse
sets of energetically favorable conformations for a given molecule. Finally, we
show that the continuous representation allows us to utilize optimization
methods to find molecules that have conformations with favourable spatial
properties.","cs.LG,physics.chem-ph,q-bio.QM"
"Multi-Threshold Attention U-Net (MTAU) based Model for Multimodal Brain Tumor Segmentation in MRI scans. Gliomas are one of the most frequent brain tumors and are classified into
high grade and low grade gliomas. The segmentation of various regions such as
tumor core, enhancing tumor etc. plays an important role in determining
severity and prognosis. Here, we have developed a multi-threshold model based
on attention U-Net for identification of various regions of the tumor in
magnetic resonance imaging (MRI). We propose a multi-path segmentation and
built three separate models for the different regions of interest. The proposed
model achieved mean Dice Coefficient of 0.59, 0.72, and 0.61 for enhancing
tumor, whole tumor and tumor core respectively on the training dataset. The
same model gave mean Dice Coefficient of 0.57, 0.73, and 0.61 on the validation
dataset and 0.59, 0.72, and 0.57 on the test dataset.","cs.AI,cs.CV,eess.IV,physics.med-ph"
"Bayesian Optimisation for Sequential Experimental Design with Applications in Additive Manufacturing. Bayesian optimization (BO) is an approach to globally optimizing black-box
objective functions that are expensive to evaluate. BO-powered experimental
design has found wide application in materials science, chemistry, experimental
physics, drug development, etc. This work aims to bring attention to the
benefits of applying BO in designing experiments and to provide a BO manual,
covering both methodology and software, for the convenience of anyone who wants
to apply or learn BO. In particular, we briefly explain the BO technique,
review all the applications of BO in additive manufacturing, compare and
exemplify the features of different open BO libraries, unlock new potential
applications of BO to other types of data (e.g., preferential output). This
article is aimed at readers with some understanding of Bayesian methods, but
not necessarily with knowledge of additive manufacturing; the software
performance overview and implementation instructions are instrumental for any
experimental-design practitioner. Moreover, our review in the field of additive
manufacturing highlights the current knowledge and technological trends of BO.","cs.CE,cs.LG"
"Sparsity Based Methods for Overparameterized Variational Problems. Two complementary approaches have been extensively used in signal and image
processing leading to novel results, the sparse representation methodology and
the variational strategy. Recently, a new sparsity based model has been
proposed, the cosparse analysis framework, which may potentially help in
bridging sparse approximation based methods to the traditional total-variation
minimization. Based on this, we introduce a sparsity based framework for
solving overparameterized variational problems. The latter has been used to
improve the estimation of optical flow and also for general denoising of
signals and images. However, the recovery of the space varying parameters
involved was not adequately addressed by traditional variational methods. We
first demonstrate the efficiency of the new framework for one dimensional
signals in recovering a piecewise linear and polynomial function. Then, we
illustrate how the new technique can be used for denoising and segmentation of
images.","35A15,47N10,49N45,65D18,65J22,65M20,68U10,94A12,cs.CV,stat.ML"
"An Explainable Probabilistic Classifier for Categorical Data Inspired to Quantum Physics. This paper presents Sparse Tensor Classifier (STC), a supervised
classification algorithm for categorical data inspired by the notion of
superposition of states in quantum physics. By regarding an observation as a
superposition of features, we introduce the concept of wave-particle duality in
machine learning and propose a generalized framework that unifies the classical
and the quantum probability. We show that STC possesses a wide range of
desirable properties not available in most other machine learning methods but
it is at the same time exceptionally easy to comprehend and use. Empirical
evaluation of STC on structured data and text classification demonstrates that
our methodology achieves state-of-the-art performances compared to both
standard classifiers and deep learning, at the additional benefit of requiring
minimal data pre-processing and hyper-parameter tuning. Moreover, STC provides
a native explanation of its predictions both for single instances and for each
target label globally.","68Txx,81Pxx,I.2,cs.AI,cs.LG,quant-ph"
"Decision Tree Learning with Spatial Modal Logics. Symbolic learning represents the most straightforward approach to
interpretable modeling, but its applications have been hampered by a single
structural design choice: the adoption of propositional logic as the underlying
language. Recently, more-than-propositional symbolic learning methods have
started to appear, in particular for time-dependent data. These methods exploit
the expressive power of modal temporal logics in powerful learning algorithms,
such as temporal decision trees, whose classification capabilities are
comparable with the best non-symbolic ones, while producing models with
explicit knowledge representation.
  With the intent of following the same approach in the case of spatial data,
in this paper we: i) present a theory of spatial decision tree learning; ii)
describe a prototypical implementation of a spatial decision tree learning
algorithm based, and strictly extending, the classical C4.5 algorithm; and iii)
perform a series of experiments in which we compare the predicting power of
spatial decision trees with that of classical propositional decision trees in
several versions, for a multi-class image classification problem, on publicly
available datasets. Our results are encouraging, showing clear improvements in
the performances from the propositional to the spatial models, which in turn
show higher levels of interpretability.","cs.AI,cs.LG,cs.LO"
"Off-Policy Exploitability-Evaluation in Two-Player Zero-Sum Markov Games. Off-policy evaluation (OPE) is the problem of evaluating new policies using
historical data obtained from a different policy. In the recent OPE context,
most studies have focused on single-player cases, and not on multi-player
cases. In this study, we propose OPE estimators constructed by the doubly
robust and double reinforcement learning estimators in two-player zero-sum
Markov games. The proposed estimators project exploitability that is often used
as a metric for determining how close a policy profile (i.e., a tuple of
policies) is to a Nash equilibrium in two-player zero-sum games. We prove the
exploitability estimation error bounds for the proposed estimators. We then
propose the methods to find the best candidate policy profile by selecting the
policy profile that minimizes the estimated exploitability from a given policy
profile class. We prove the regret bounds of the policy profiles selected by
our methods. Finally, we demonstrate the effectiveness and performance of the
proposed estimators through experiments.","cs.GT,cs.LG,econ.EM,stat.ML"
"A Survey on Deep Reinforcement Learning for Data Processing and Analytics. Data processing and analytics are fundamental and pervasive. Algorithms play
a vital role in data processing and analytics where many algorithm designs have
incorporated heuristics and general rules from human knowledge and experience
to improve their effectiveness. Recently, reinforcement learning, deep
reinforcement learning (DRL) in particular, is increasingly explored and
exploited in many areas because it can learn better strategies in complicated
environments it is interacting with than statically designed algorithms.
Motivated by this trend, we provide a comprehensive review of recent works
focusing on utilizing deep reinforcement learning to improve data processing
and analytics. First, we present an introduction to key concepts, theories, and
methods in deep reinforcement learning. Next, we discuss deep reinforcement
learning deployment on database systems, facilitating data processing and
analytics in various aspects, including data organization, scheduling, tuning,
and indexing. Then, we survey the application of deep reinforcement learning in
data processing and analytics, ranging from data preparation, natural language
interface to healthcare, fintech, etc. Finally, we discuss important open
challenges and future research directions of using deep reinforcement learning
in data processing and analytics.","cs.AI,cs.DB,cs.LG"
"Tiny-Inception-ResNet-v2: Using Deep Learning for Eliminating Bonded Labors of Brick Kilns in South Asia. This paper proposes to employ a Inception-ResNet inspired deep learning
architecture called Tiny-Inception-ResNet-v2 to eliminate bonded labor by
identifying brick kilns within ""Brick-Kiln-Belt"" of South Asia. The framework
is developed by training a network on the satellite imagery consisting of 11
different classes of South Asian region. The dataset developed during the
process includes the geo-referenced images of brick kilns, houses, roads,
tennis courts, farms, sparse trees, dense trees, orchards, parking lots, parks
and barren lands. The dataset is made publicly available for further research.
Our proposed network architecture with very fewer learning parameters
outperforms all state-of-the-art architectures employed for recognition of
brick kilns. Our proposed solution would enable regional monitoring and
evaluation mechanisms for the Sustainable Development Goals.","cs.CV,cs.GR,cs.LG"
"DiffGCN: Graph Convolutional Networks via Differential Operators and Algebraic Multigrid Pooling. Graph Convolutional Networks (GCNs) have shown to be effective in handling
unordered data like point clouds and meshes. In this work we propose novel
approaches for graph convolution, pooling and unpooling, inspired from finite
differences and algebraic multigrid frameworks. We form a parameterized
convolution kernel based on discretized differential operators, leveraging the
graph mass, gradient and Laplacian. This way, the parameterization does not
depend on the graph structure, only on the meaning of the network convolutions
as differential operators. To allow hierarchical representations of the input,
we propose pooling and unpooling operations that are based on algebraic
multigrid methods, which are mainly used to solve partial differential
equations on unstructured grids. To motivate and explain our method, we compare
it to standard convolutional neural networks, and show their similarities and
relations in the case of a regular grid. Our proposed method is demonstrated in
various experiments like classification and part-segmentation, achieving on par
or better than state of the art results. We also analyze the computational cost
of our method compared to other GCNs.","cs.CV,cs.GR,cs.LG"
"Generating Human Action Videos by Coupling 3D Game Engines and Probabilistic Graphical Models. Deep video action recognition models have been highly successful in recent
years but require large quantities of manually annotated data, which are
expensive and laborious to obtain. In this work, we investigate the generation
of synthetic training data for video action recognition, as synthetic data have
been successfully used to supervise models for a variety of other computer
vision tasks. We propose an interpretable parametric generative model of human
action videos that relies on procedural generation, physics models and other
components of modern game engines. With this model we generate a diverse,
realistic, and physically plausible dataset of human action videos, called PHAV
for ""Procedural Human Action Videos"". PHAV contains a total of 39,982 videos,
with more than 1,000 examples for each of 35 action categories. Our video
generation approach is not limited to existing motion capture sequences: 14 of
these 35 categories are procedurally defined synthetic actions. In addition,
each video is represented with 6 different data modalities, including RGB,
optical flow and pixel-level semantic labels. These modalities are generated
almost simultaneously using the Multiple Render Targets feature of modern GPUs.
In order to leverage PHAV, we introduce a deep multi-task (i.e. that considers
action classes from multiple datasets) representation learning architecture
that is able to simultaneously learn from synthetic and real video datasets,
even when their action categories differ. Our experiments on the UCF-101 and
HMDB-51 benchmarks suggest that combining our large set of synthetic videos
with small real-world datasets can boost recognition performance. Our approach
also significantly outperforms video representations produced by fine-tuning
state-of-the-art unsupervised generative models of videos.","cs.CV,cs.LG,cs.MM"
"Fine-tuning Vision Transformers for the Prediction of State Variables in Ising Models. Transformers are state-of-the-art deep learning models that are composed of
stacked attention and point-wise, fully connected layers designed for handling
sequential data. Transformers are not only ubiquitous throughout Natural
Language Processing (NLP), but, recently, they have inspired a new wave of
Computer Vision (CV) applications research. In this work, a Vision Transformer
(ViT) is applied to predict the state variables of 2-dimensional Ising model
simulations. Our experiments show that ViT outperform state-of-the-art
Convolutional Neural Networks (CNN) when using a small number of microstate
images from the Ising model corresponding to various boundary conditions and
temperatures. This work opens the possibility of applying ViT to other
simulations, and raises interesting research directions on how attention maps
can learn about the underlying physics governing different phenomena.","cond-mat.stat-mech,cs.CV,cs.LG,physics.comp-ph"
"Bayesian Optimisation for Sequential Experimental Design with Applications in Additive Manufacturing. Bayesian optimization (BO) is an approach to globally optimizing black-box
objective functions that are expensive to evaluate. BO-powered experimental
design has found wide application in materials science, chemistry, experimental
physics, drug development, etc. This work aims to bring attention to the
benefits of applying BO in designing experiments and to provide a BO manual,
covering both methodology and software, for the convenience of anyone who wants
to apply or learn BO. In particular, we briefly explain the BO technique,
review all the applications of BO in additive manufacturing, compare and
exemplify the features of different open BO libraries, unlock new potential
applications of BO to other types of data (e.g., preferential output). This
article is aimed at readers with some understanding of Bayesian methods, but
not necessarily with knowledge of additive manufacturing; the software
performance overview and implementation instructions are instrumental for any
experimental-design practitioner. Moreover, our review in the field of additive
manufacturing highlights the current knowledge and technological trends of BO.","cs.CE,cs.LG"
"GP3: A Sampling-based Analysis Framework for Gaussian Processes. Although machine learning is increasingly applied in control approaches, only
few methods guarantee certifiable safety, which is necessary for real world
applications. These approaches typically rely on well-understood learning
algorithms, which allow formal theoretical analysis. Gaussian process
regression is a prominent example among those methods, which attracts growing
attention due to its strong Bayesian foundations. Even though many problems
regarding the analysis of Gaussian processes have a similar structure, specific
approaches are typically tailored for them individually, without strong focus
on computational efficiency. Thereby, the practical applicability and
performance of these approaches is limited. In order to overcome this issue, we
propose a novel framework called GP3, general purpose computation on graphics
processing units for Gaussian processes, which allows to solve many of the
existing problems efficiently. By employing interval analysis, local Lipschitz
constants are computed in order to extend properties verified on a grid to
continuous state spaces. Since the computation is completely parallelizable,
the computational benefits of GPU processing are exploited in combination with
multi-resolution sampling in order to allow high resolution analysis.","cs.LG,cs.SY,eess.SY,stat.ML"
"Algorithmic Recourse: from Counterfactual Explanations to Interventions. As machine learning is increasingly used to inform consequential
decision-making (e.g., pre-trial bail and loan approval), it becomes important
to explain how the system arrived at its decision, and also suggest actions to
achieve a favorable decision. Counterfactual explanations -- ""how the world
would have (had) to be different for a desirable outcome to occur"" -- aim to
satisfy these criteria. Existing works have primarily focused on designing
algorithms to obtain counterfactual explanations for a wide range of settings.
However, one of the main objectives of ""explanations as a means to help a
data-subject act rather than merely understand"" has been overlooked. In
layman's terms, counterfactual explanations inform an individual where they
need to get to, but not how to get there. In this work, we rely on causal
reasoning to caution against the use of counterfactual explanations as a
recommendable set of actions for recourse. Instead, we propose a shift of
paradigm from recourse via nearest counterfactual explanations to recourse
through minimal interventions, moving the focus from explanations to
recommendations. Finally, we provide the reader with an extensive discussion on
how to realistically achieve recourse beyond structural interventions.","cs.AI,cs.LG,stat.ML"
"Stacked Auto Encoder Based Deep Reinforcement Learning for Online Resource Scheduling in Large-Scale MEC Networks. An online resource scheduling framework is proposed for minimizing the sum of
weighted task latency for all the Internet of things (IoT) users, by optimizing
offloading decision, transmission power and resource allocation in the
large-scale mobile edge computing (MEC) system. Towards this end, a deep
reinforcement learning (DRL) based solution is proposed, which includes the
following components. Firstly, a related and regularized stacked auto encoder
(2r-SAE) with unsupervised learning is applied to perform data compression and
representation for high dimensional channel quality information (CQI) data,
which can reduce the state space for DRL. Secondly, we present an adaptive
simulated annealing based approach (ASA) as the action search method of DRL, in
which an adaptive h-mutation is used to guide the search direction and an
adaptive iteration is proposed to enhance the search efficiency during the DRL
process. Thirdly, a preserved and prioritized experience replay (2p-ER) is
introduced to assist the DRL to train the policy network and find the optimal
offloading policy. Numerical results are provided to demonstrate that the
proposed algorithm can achieve near-optimal performance while significantly
decreasing the computational time compared with existing benchmarks.","cs.LG,cs.NI,stat.ML"
"Probing transfer learning with a model of synthetic correlated datasets. Transfer learning can significantly improve the sample efficiency of neural
networks, by exploiting the relatedness between a data-scarce target task and a
data-abundant source task. Despite years of successful applications, transfer
learning practice often relies on ad-hoc solutions, while theoretical
understanding of these procedures is still limited. In the present work, we
re-think a solvable model of synthetic data as a framework for modeling
correlation between data-sets. This setup allows for an analytic
characterization of the generalization performance obtained when transferring
the learned feature map from the source to the target task. Focusing on the
problem of training two-layer networks in a binary classification setting, we
show that our model can capture a range of salient features of transfer
learning with real data. Moreover, by exploiting parametric control over the
correlation between the two data-sets, we systematically investigate under
which conditions the transfer of features is beneficial for generalization.","cond-mat.dis-nn,cs.LG"
"Study of Efficient Technique Based On 2D Tsallis Entropy For Image Thresholding. Thresholding is an important task in image processing. It is a main tool in
pattern recognition, image segmentation, edge detection and scene analysis. In
this paper, we present a new thresholding technique based on two-dimensional
Tsallis entropy. The two-dimensional Tsallis entropy was obtained from the
twodimensional histogram which was determined by using the gray value of the
pixels and the local average gray value of the pixels, the work it was applied
a generalized entropy formalism that represents a recent development in
statistical mechanics. The effectiveness of the proposed method is demonstrated
by using examples from the real-world and synthetic images. The performance
evaluation of the proposed technique in terms of the quality of the thresholded
images are presented. Experimental results demonstrate that the proposed method
achieve better result than the Shannon method.","68U10,cs.CV"
"Graph Message Passing with Cross-location Attentions for Long-term ILI Prediction. Forecasting influenza-like illness (ILI) is of prime importance to
epidemiologists and health-care providers. Early prediction of epidemic
outbreaks plays a pivotal role in disease intervention and control. Most
existing work has either limited long-term prediction performance or lacks a
comprehensive ability to capture spatio-temporal dependencies in data. Accurate
and early disease forecasting models would markedly improve both epidemic
prevention and managing the onset of an epidemic. In this paper, we design a
cross-location attention based graph neural network (Cola-GNN) for learning
time series embeddings and location aware attentions. We propose a graph
message passing framework to combine learned feature embeddings and an
attention matrix to model disease propagation over time. We compare the
proposed method with state-of-the-art statistical approaches and deep learning
models on real-world epidemic-related datasets from United States and Japan.
The proposed method shows strong predictive performance and leads to
interpretable results for long-term epidemic predictions.","cs.LG,cs.SI,stat.ML"
"Decentralized Multi-Agent Reinforcement Learning for Task Offloading Under Uncertainty. Multi-Agent Reinforcement Learning (MARL) is a challenging subarea of
Reinforcement Learning due to the non-stationarity of the environments and the
large dimensionality of the combined action space. Deep MARL algorithms have
been applied to solve different task offloading problems. However, in
real-world applications, information required by the agents (i.e. rewards and
states) are subject to noise and alterations. The stability and the robustness
of deep MARL to practical challenges is still an open research problem. In this
work, we apply state-of-the art MARL algorithms to solve task offloading with
reward uncertainty. We show that perturbations in the reward signal can induce
decrease in the performance compared to learning with perfect rewards. We
expect this paper to stimulate more research in studying and addressing the
practical challenges of deploying deep MARL solutions in wireless
communications systems.","cs.LG,cs.MA"
"Statistical mechanics of unsupervised feature learning in a restricted Boltzmann machine with binary synapses. Revealing hidden features in unlabeled data is called unsupervised feature
learning, which plays an important role in pretraining a deep neural network.
Here we provide a statistical mechanics analysis of the unsupervised learning
in a restricted Boltzmann machine with binary synapses. A message passing
equation to infer the hidden feature is derived, and furthermore, variants of
this equation are analyzed. A statistical analysis by replica theory describes
the thermodynamic properties of the model. Our analysis confirms an entropy
crisis preceding the non-convergence of the message passing equation,
suggesting a discontinuous phase transition as a key characteristic of the
restricted Boltzmann machine. Continuous phase transition is also confirmed
depending on the embedded feature strength in the data. The mean-field result
under the replica symmetric assumption agrees with that obtained by running
message passing algorithms on single instances of finite sizes. Interestingly,
in an approximate Hopfield model, the entropy crisis is absent, and a
continuous phase transition is observed instead. We also develop an iterative
equation to infer the hyper-parameter (temperature) hidden in the data, which
in physics corresponds to iteratively imposing Nishimori condition. Our study
provides insights towards understanding the thermodynamic properties of the
restricted Boltzmann machine learning, and moreover important theoretical basis
to build simplified deep networks.","cond-mat.dis-nn,cond-mat.stat-mech,cs.LG,cs.NE,q-bio.NC"
"Evaluating CLIP: Towards Characterization of Broader Capabilities and Downstream Implications. Recently, there have been breakthroughs in computer vision (""CV"") models that
are more generalizable with the advent of models such as CLIP and ALIGN. In
this paper, we analyze CLIP and highlight some of the challenges such models
pose. CLIP reduces the need for task specific training data, potentially
opening up many niche tasks to automation. CLIP also allows its users to
flexibly specify image classification classes in natural language, which we
find can shift how biases manifest. Additionally, through some preliminary
probes we find that CLIP can inherit biases found in prior computer vision
systems. Given the wide and unpredictable domain of uses for such models, this
raises questions regarding what sufficiently safe behaviour for such systems
may look like. These results add evidence to the growing body of work calling
for a change in the notion of a 'better' model--to move beyond simply looking
at higher accuracy at task-oriented capability evaluations, and towards a
broader 'better' that takes into account deployment-critical features such as
different use contexts, and people who interact with the model when thinking
about model deployment.","cs.AI,cs.CV,cs.CY"
"MLMOD Package: Machine Learning Methods for Data-Driven Modeling in LAMMPS. We discuss a software package for incorporating into simulations data-driven
models trained using machine learning methods. These can be used for (i)
modeling dynamics and time-step integration, (ii) modeling interactions between
system components, and (iii) computing quantities of interest characterizing
system state. The package allows for use of machine learning methods with
general model classes including Neural Networks, Gaussian Process Regression,
Kernel Models, and other approaches. We discuss in this whitepaper our
prototype C++ package, aims, and example usage.","cond-mat.mes-hall,cond-mat.soft,cs.LG,cs.NA,math.NA"
"Learning Temporal Causal Sequence Relationships from Real-Time Time-Series. We aim to mine temporal causal sequences that explain observed events
(consequents) in time-series traces. Causal explanations of key events in a
time-series has applications in design debugging, anomaly detection, planning,
root-cause analysis and many more. We make use of decision trees and interval
arithmetic to mine sequences that explain defining events in the time-series.
We propose modified decision tree construction metrics to handle the
non-determinism introduced by the temporal dimension. The mined sequences are
expressed in a readable temporal logic language that is easy to interpret. The
application of the proposed methodology is illustrated through various
examples.","cs.AI,cs.LG,cs.LO"
"DeepThermal: Combustion Optimization for Thermal Power Generating Units Using Offline Reinforcement Learning. Thermal power generation plays a dominant role in the world's electricity
supply. It consumes large amounts of coal worldwide, and causes serious air
pollution. Optimizing the combustion efficiency of a thermal power generating
unit (TPGU) is a highly challenging and critical task in the energy industry.
We develop a new data-driven AI system, namely DeepThermal, to optimize the
combustion control strategy for TPGUs. At its core, is a new model-based
offline reinforcement learning (RL) framework, called MORE, which leverages
logged historical operational data of a TPGU to solve a highly complex
constrained Markov decision process problem via purely offline training. MORE
aims at simultaneously improving the long-term reward (increase combustion
efficiency and reduce pollutant emission) and controlling operational risks
(safety constraints satisfaction). In DeepThermal, we first learn a data-driven
combustion process simulator from the offline dataset. The RL agent of MORE is
then trained by combining real historical data as well as carefully filtered
and processed simulation data through a novel restrictive exploration scheme.
DeepThermal has been successfully deployed in four large coal-fired thermal
power plants in China. Real-world experiments show that DeepThermal effectively
improves the combustion efficiency of a TPGU. We also report and demonstrate
the superior performance of MORE by comparing with the state-of-the-art
algorithms on the standard offline RL benchmarks. To the best knowledge of the
authors, DeepThermal is the first AI application that has been used to solve
real-world complex mission-critical control tasks using the offline RL
approach.","I.2,cs.AI,cs.LG,cs.SY,eess.SY"
"Transductive Multi-view Zero-Shot Learning. Most existing zero-shot learning approaches exploit transfer learning via an
intermediate-level semantic representation shared between an annotated
auxiliary dataset and a target dataset with different classes and no
annotation. A projection from a low-level feature space to the semantic
representation space is learned from the auxiliary dataset and is applied
without adaptation to the target dataset. In this paper we identify two
inherent limitations with these approaches. First, due to having disjoint and
potentially unrelated classes, the projection functions learned from the
auxiliary dataset/domain are biased when applied directly to the target
dataset/domain. We call this problem the projection domain shift problem and
propose a novel framework, transductive multi-view embedding, to solve it. The
second limitation is the prototype sparsity problem which refers to the fact
that for each target class, only a single prototype is available for zero-shot
learning given a semantic representation. To overcome this problem, a novel
heterogeneous multi-view hypergraph label propagation method is formulated for
zero-shot learning in the transductive embedding space. It effectively exploits
the complementary information offered by different semantic representations and
takes advantage of the manifold structures of multiple representation spaces in
a coherent manner. We demonstrate through extensive experiments that the
proposed approach (1) rectifies the projection shift between the auxiliary and
target domains, (2) exploits the complementarity of multiple semantic
representations, (3) significantly outperforms existing methods for both
zero-shot and N-shot recognition on three image and video benchmark datasets,
and (4) enables novel cross-view annotation tasks.","cs.CV,cs.DS,cs.MM"
"Structural Learning for Template-free Protein Folding. The thesis is aimed to solve the template-free protein folding problem by
tackling two important components: efficient sampling in vast conformation
space, and design of knowledge-based potentials with high accuracy. We have
proposed the first-order and second-order CRF-Sampler to sample structures from
the continuous local dihedral angles space by modeling the lower and higher
order conditional dependency between neighboring dihedral angles given the
primary sequence information. A framework combining the Conditional Random
Fields and the energy function is introduced to guide the local conformation
sampling using long range constraints with the energy function.
  The relationship between the sequence profile and the local dihedral angle
distribution is nonlinear. Hence we proposed the CNF-Folder to model this
complex relationship by applying a novel machine learning model Conditional
Neural Fields which utilizes the structural graphical model with the neural
network. CRF-Samplers and CNF-Folder perform very well in CASP8 and CASP9.
  Further, a novel pairwise distance statistical potential (EPAD) is designed
to capture the dependency of the energy profile on the positions of the
interacting amino acids as well as the types of those amino acids, opposing the
common assumption that this energy profile depends only on the types of amino
acids. EPAD has also been successfully applied in the CASP 10 Free Modeling
experiment with CNF-Folder, especially outstanding on some uncommon structured
targets.","cs.CE,cs.LG,q-bio.QM"
"Learning with Molecules beyond Graph Neural Networks. We demonstrate a deep learning framework which is inherently based in the
highly expressive language of relational logic, enabling to, among other
things, capture arbitrarily complex graph structures. We show how Graph Neural
Networks and similar models can be easily covered in the framework by
specifying the underlying propagation rules in the relational logic. The
declarative nature of the used language then allows to easily modify and extend
the propagation schemes into complex structures, such as the molecular rings
which we choose for a short demonstration in this paper.","cs.AI,cs.LG,cs.LO,cs.NE"
"Monocular LSD-SLAM Integration within AR System. In this paper, we cover the process of integrating Large-Scale Direct
Simultaneous Localization and Mapping (LSD-SLAM) algorithm into our existing AR
stereo engine, developed for our modified ""Augmented Reality Oculus Rift"". With
that, we are able to track one of our realworld cameras which are mounted on
the rift, within a complete unknown environment. This makes it possible to
achieve a constant and full augmentation, synchronizing our 3D movement (x, y,
z) in both worlds, the real world and the virtual world. The development for
the basic AR setup using the Oculus Rift DK1 and two fisheye cameras is fully
documented in our previous paper. After an introduction to image-based
registration, we detail the LSD-SLAM algorithm and document our code
implementing our integration. The AR stereo engine with Oculus Rift support can
be accessed via the GIT repository https://github.com/MaXvanHeLL/ARift.git and
the modified LSD-SLAM project used for the integration is available here
https://github.com/MaXvanHeLL/LSD-SLAM.git.","cs.CV,cs.GR,cs.SE"
"Fairness in Forecasting and Learning Linear Dynamical Systems. In machine learning, training data often capture the behaviour of multiple
subgroups of some underlying human population. When the amounts of training
data for the subgroups are not controlled carefully, under-representation bias
arises. We introduce two natural notions of subgroup fairness and instantaneous
fairness to address such under-representation bias in time-series forecasting
problems. In particular, we consider the subgroup-fair and instant-fair
learning of a linear dynamical system (LDS) from multiple trajectories of
varying lengths, and the associated forecasting problems. We provide globally
convergent methods for the learning problems using hierarchies of
convexifications of non-commutative polynomial optimisation problems. Our
empirical results on a biased data set motivated by insurance applications and
the well-known COMPAS data set demonstrate both the beneficial impact of
fairness considerations on statistical performance and encouraging effects of
exploiting sparsity on run time.","cs.LG,math.DS,math.ST,stat.ML,stat.TH"
"Graph-based Incident Aggregation for Large-Scale Online Service Systems. As online service systems continue to grow in terms of complexity and volume,
how service incidents are managed will significantly impact company revenue and
user trust. Due to the cascading effect, cloud failures often come with an
overwhelming number of incidents from dependent services and devices. To pursue
efficient incident management, related incidents should be quickly aggregated
to narrow down the problem scope. To this end, in this paper, we propose GRLIA,
an incident aggregation framework based on graph representation learning over
the cascading graph of cloud failures. A representation vector is learned for
each unique type of incident in an unsupervised and unified manner, which is
able to simultaneously encode the topological and temporal correlations among
incidents. Thus, it can be easily employed for online incident aggregation. In
particular, to learn the correlations more accurately, we try to recover the
complete scope of failures' cascading impact by leveraging fine-grained system
monitoring data, i.e., Key Performance Indicators (KPIs). The proposed
framework is evaluated with real-world incident data collected from a
large-scale online service system of Huawei Cloud. The experimental results
demonstrate that GRLIA is effective and outperforms existing methods.
Furthermore, our framework has been successfully deployed in industrial
practice.","cs.LG,cs.SE"
"Improving Discrete Latent Representations With Differentiable Approximation Bridges. Modern neural network training relies on piece-wise (sub-)differentiable
functions in order to use backpropagation to update model parameters. In this
work, we introduce a novel method to allow simple non-differentiable functions
at intermediary layers of deep neural networks. We do so by training with a
differentiable approximation bridge (DAB) neural network which approximates the
non-differentiable forward function and provides gradient updates during
backpropagation. We present strong empirical results (performing over 600
experiments) in four different domains: unsupervised (image) representation
learning, variational (image) density estimation, image classification, and
sequence sorting to demonstrate that our proposed method improves state of the
art performance. We demonstrate that training with DAB aided discrete
non-differentiable functions improves image reconstruction quality and
posterior linear separability by 10% against the Gumbel-Softmax relaxed
estimator [37, 26] as well as providing a 9% improvement in the test
variational lower bound in comparison to the state of the art RELAX [16]
discrete estimator. We also observe an accuracy improvement of 77% in neural
sequence sorting and a 25% improvement against the straight-through estimator
[5] in an image classification setting. The DAB network is not used for
inference and expands the class of functions that are usable in neural
networks.","cs.AI,cs.CV,cs.LG,stat.ML"
"Tangent-Space Gradient Optimization of Tensor Network for Machine Learning. The gradient-based optimization method for deep machine learning models
suffers from gradient vanishing and exploding problems, particularly when the
computational graph becomes deep. In this work, we propose the tangent-space
gradient optimization (TSGO) for the probabilistic models to keep the gradients
from vanishing or exploding. The central idea is to guarantee the orthogonality
between the variational parameters and the gradients. The optimization is then
implemented by rotating parameter vector towards the direction of gradient. We
explain and testify TSGO in tensor network (TN) machine learning, where the TN
describes the joint probability distribution as a normalized state $\left| \psi
\right\rangle $ in Hilbert space. We show that the gradient can be restricted
in the tangent space of $\left\langle \psi \right.\left| \psi \right\rangle =
1$ hyper-sphere. Instead of additional adaptive methods to control the learning
rate in deep learning, the learning rate of TSGO is naturally determined by the
angle $\theta $ as $\eta = \tan \theta $. Our numerical results reveal better
convergence of TSGO in comparison to the off-the-shelf Adam.","cond-mat.dis-nn,cs.LG,stat.ML"
"Heterogeneous Graph Transformer. Recent years have witnessed the emerging success of graph neural networks
(GNNs) for modeling structured data. However, most GNNs are designed for
homogeneous graphs, in which all nodes and edges belong to the same types,
making them infeasible to represent heterogeneous structures. In this paper, we
present the Heterogeneous Graph Transformer (HGT) architecture for modeling
Web-scale heterogeneous graphs. To model heterogeneity, we design node- and
edge-type dependent parameters to characterize the heterogeneous attention over
each edge, empowering HGT to maintain dedicated representations for different
types of nodes and edges. To handle dynamic heterogeneous graphs, we introduce
the relative temporal encoding technique into HGT, which is able to capture the
dynamic structural dependency with arbitrary durations. To handle Web-scale
graph data, we design the heterogeneous mini-batch graph sampling
algorithm---HGSampling---for efficient and scalable training. Extensive
experiments on the Open Academic Graph of 179 million nodes and 2 billion edges
show that the proposed HGT model consistently outperforms all the
state-of-the-art GNN baselines by 9%--21% on various downstream tasks.","cs.LG,cs.SI,stat.ML"
"Data-driven Prediction of General Hamiltonian Dynamics via Learning Exactly-Symplectic Maps. We consider the learning and prediction of nonlinear time series generated by
a latent symplectic map. A special case is (not necessarily separable)
Hamiltonian systems, whose solution flows give such symplectic maps. For this
special case, both generic approaches based on learning the vector field of the
latent ODE and specialized approaches based on learning the Hamiltonian that
generates the vector field exist. Our method, however, is different as it does
not rely on the vector field nor assume its existence; instead, it directly
learns the symplectic evolution map in discrete time. Moreover, we do so by
representing the symplectic map via a generating function, which we approximate
by a neural network (hence the name GFNN). This way, our approximation of the
evolution map is always \emph{exactly} symplectic. This additional geometric
structure allows the local prediction error at each step to accumulate in a
controlled fashion, and we will prove, under reasonable assumptions, that the
global prediction error grows at most \emph{linearly} with long prediction
time, which significantly improves an otherwise exponential growth. In
addition, as a map-based and thus purely data-driven method, GFNN avoids two
additional sources of inaccuracies common in vector-field based approaches,
namely the error in approximating the vector field by finite difference of the
data, and the error in numerical integration of the vector field for making
predictions. Numerical experiments further demonstrate our claims.","cs.LG,cs.NA,math.DS,math.NA,physics.comp-ph"
"DyLoc: Dynamic Localization for Massive MIMO Using Predictive Recurrent Neural Networks. This paper presents a data-driven localization framework with high precision
in time-varying complex multipath environments, such as dense urban areas and
indoors, where GPS and model-based localization techniques come short. We
consider the angle-delay profile (ADP), a linear transformation of channel
state information (CSI), in massive MIMO systems and show that ADPs preserve
users' motion when stacked temporally. We discuss that given a static
environment, future frames of ADP time-series are predictable employing a video
frame prediction algorithm. We express that a deep convolutional neural network
(DCNN) can be employed to learn the background static scattering environment.
To detect foreground changes in the environment, corresponding to path blockage
or addition, we introduce an algorithm taking advantage of the trained DCNN.
Furthermore, we present DyLoc, a data-driven framework to recover distorted
ADPs due to foreground changes and to obtain precise location estimations. We
evaluate the performance of DyLoc in several dynamic scenarios employing
DeepMIMO dataset to generate geo-tagged CSI datasets for indoor and outdoor
environments. We show that previous DCNN-based techniques fail to perform with
desirable accuracy in dynamic environments, while DyLoc pursues localization
precisely. Moreover, simulations show that as the environment gets richer in
terms of the number of multipath, DyLoc gets more robust to foreground changes.","cs.CV,eess.SP"
"Beyond Low-frequency Information in Graph Convolutional Networks. Graph neural networks (GNNs) have been proven to be effective in various
network-related tasks. Most existing GNNs usually exploit the low-frequency
signals of node features, which gives rise to one fundamental question: is the
low-frequency information all we need in the real world applications? In this
paper, we first present an experimental investigation assessing the roles of
low-frequency and high-frequency signals, where the results clearly show that
exploring low-frequency signal only is distant from learning an effective node
representation in different scenarios. How can we adaptively learn more
information beyond low-frequency information in GNNs? A well-informed answer
can help GNNs enhance the adaptability. We tackle this challenge and propose a
novel Frequency Adaptation Graph Convolutional Networks (FAGCN) with a
self-gating mechanism, which can adaptively integrate different signals in the
process of message passing. For a deeper understanding, we theoretically
analyze the roles of low-frequency signals and high-frequency signals on
learning node representations, which further explains why FAGCN can perform
well on different types of networks. Extensive experiments on six real-world
networks validate that FAGCN not only alleviates the over-smoothing problem,
but also has advantages over the state-of-the-arts.","cs.LG,cs.SI"
"Multimodal Multitask Representation Learning for Pathology Biobank Metadata Prediction. Metadata are general characteristics of the data in a well-curated and
condensed format, and have been proven to be useful for decision making,
knowledge discovery, and also heterogeneous data organization of biobank. Among
all data types in the biobank, pathology is the key component of the biobank
and also serves as the gold standard of diagnosis. To maximize the utility of
biobank and allow the rapid progress of biomedical science, it is essential to
organize the data with well-populated pathology metadata. However, manual
annotation of such information is tedious and time-consuming. In the study, we
develop a multimodal multitask learning framework to predict four major
slide-level metadata of pathology images. The framework learns generalizable
representations across tissue slides, pathology reports, and case-level
structured data. We demonstrate improved performance across all four tasks with
the proposed method compared to a single modal single task baseline on two test
sets, one external test set from a distinct data source (TCGA) and one internal
held-out test set (TTH). In the test sets, the performance improvements on the
averaged area under receiver operating characteristic curve across the four
tasks are 16.48% and 9.05% on TCGA and TTH, respectively. Such pathology
metadata prediction system may be adopted to mitigate the effort of expert
annotation and ultimately accelerate the data-driven research by better
utilization of the pathology biobank.","cs.CV,cs.LG"
"Non-stationary Online Regression. Online forecasting under a changing environment has been a problem of
increasing importance in many real-world applications. In this paper, we
consider the meta-algorithm presented in \citet{zhang2017dynamic} combined with
different subroutines. We show that an expected cumulative error of order
$\tilde{O}(n^{1/3} C_n^{2/3})$ can be obtained for non-stationary online linear
regression where the total variation of parameter sequence is bounded by $C_n$.
Our paper extends the result of online forecasting of one-dimensional
time-series as proposed in \cite{baby2019online} to general $d$-dimensional
non-stationary linear regression. We improve the rate $O(\sqrt{n C_n})$
obtained by Zhang et al. 2017 and Besbes et al. 2015. We further extend our
analysis to non-stationary online kernel regression. Similar to the
non-stationary online regression case, we use the meta-procedure of Zhang et
al. 2017 combined with Kernel-AWV (Jezequel et al. 2020) to achieve an expected
cumulative controlled by the effective dimension of the RKHS and the total
variation of the sequence. To the best of our knowledge, this work is the first
extension of non-stationary online regression to non-stationary kernel
regression. Lastly, we evaluate our method empirically with several existing
benchmarks and also compare it with the theoretical bound obtained in this
paper.","cs.LG,math.ST,stat.TH"
"Probabilistic Attention for Interactive Segmentation. We provide a probabilistic interpretation of attention and show that the
standard dot-product attention in transformers is a special case of Maximum A
Posteriori (MAP) inference. The proposed approach suggests the use of
Expectation Maximization algorithms for online adaptation of key and value
model parameters. This approach is useful for cases in which external agents,
e.g., annotators, provide inference-time information about the correct values
of some tokens, e.g, the semantic category of some pixels, and we need for this
new information to propagate to other tokens in a principled manner. We
illustrate the approach on an interactive semantic segmentation task in which
annotators and models collaborate online to improve annotation efficiency.
Using standard benchmarks, we observe that key adaptation boosts model
performance ($\sim10\%$ mIoU) in the low feedback regime and value propagation
improves model responsiveness in the high feedback regime. A PyTorch layer
implementation of our probabilistic attention model will be made publicly
available here: https://github.com/apple/ml-probabilistic-attention.","68T07,I.5.1; I.4.6; I.2.6,cs.AI,cs.CV,cs.LG"
"Monte Carlo Simulation of SDEs using GANs. Generative adversarial networks (GANs) have shown promising results when
applied on partial differential equations and financial time series generation.
We investigate if GANs can also be used to approximate one-dimensional Ito
stochastic differential equations (SDEs). We propose a scheme that approximates
the path-wise conditional distribution of SDEs for large time steps. Standard
GANs are only able to approximate processes in distribution, yielding a weak
approximation to the SDE. A conditional GAN architecture is proposed that
enables strong approximation. We inform the discriminator of this GAN with the
map between the prior input to the generator and the corresponding output
samples, i.e. we introduce a `supervised GAN'. We compare the input-output map
obtained with the standard GAN and supervised GAN and show experimentally that
the standard GAN may fail to provide a path-wise approximation. The GAN is
trained on a dataset obtained with exact simulation. The architecture was
tested on geometric Brownian motion (GBM) and the Cox-Ingersoll-Ross (CIR)
process. The supervised GAN outperformed the Euler and Milstein schemes in
strong error on a discretisation with large time steps. It also outperformed
the standard conditional GAN when approximating the conditional distribution.
We also demonstrate how standard GANs may give rise to non-parsimonious
input-output maps that are sensitive to perturbations, which motivates the need
for constraints and regularisation on GAN generators.","cs.LG,cs.NA,math.NA,q-fin.CP"
"ScheduleNet: Learn to solve multi-agent scheduling problems with reinforcement learning. We propose ScheduleNet, a RL-based real-time scheduler, that can solve
various types of multi-agent scheduling problems. We formulate these problems
as a semi-MDP with episodic reward (makespan) and learn ScheduleNet, a
decentralized decision-making policy that can effectively coordinate multiple
agents to complete tasks. The decision making procedure of ScheduleNet
includes: (1) representing the state of a scheduling problem with the
agent-task graph, (2) extracting node embeddings for agent and tasks nodes, the
important relational information among agents and tasks, by employing the
type-aware graph attention (TGA), and (3) computing the assignment probability
with the computed node embeddings. We validate the effectiveness of ScheduleNet
as a general learning-based scheduler for solving various types of multi-agent
scheduling tasks, including multiple salesman traveling problem (mTSP) and job
shop scheduling problem (JSP).","cs.AI,cs.LG,cs.MA,cs.SY,eess.SY"
"Enabling Lightweight Fine-tuning for Pre-trained Language Model Compression based on Matrix Product Operators. This paper presents a novel pre-trained language models (PLM) compression
approach based on the matrix product operator (short as MPO) from quantum
many-body physics. It can decompose an original matrix into central tensors
(containing the core information) and auxiliary tensors (with only a small
proportion of parameters). With the decomposed MPO structure, we propose a
novel fine-tuning strategy by only updating the parameters from the auxiliary
tensors, and design an optimization algorithm for MPO-based approximation over
stacked network architectures. Our approach can be applied to the original or
the compressed PLMs in a general way, which derives a lighter network and
significantly reduces the parameters to be fine-tuned. Extensive experiments
have demonstrated the effectiveness of the proposed approach in model
compression, especially the reduction in finetuning parameters (91% reduction
on average).","cs.AI,cs.LG,quant-ph"
"Context-aware Active Multi-Step Reinforcement Learning. Reinforcement learning has attracted great attention recently, especially
policy gradient algorithms, which have been demonstrated on challenging
decision making and control tasks. In this paper, we propose an active
multi-step TD algorithm with adaptive stepsizes to learn actor and critic.
Specifically, our model consists of two components: active stepsize learning
and adaptive multi-step TD algorithm. Firstly, we divide the time horizon into
chunks and actively select state and action inside each chunk. Then given the
selected samples, we propose the adaptive multi-step TD, which generalizes
TD($\lambda$), but adaptively switch on/off the backups from future returns of
different steps. Particularly, the adaptive multi-step TD introduces a
context-aware mechanism, here a binary classifier, which decides whether or not
to turn on its future backups based on the context changes. Thus, our model is
kind of combination of active learning and multi-step TD algorithm, which has
the capacity for learning off-policy without the need of importance sampling.
We evaluate our approach on both discrete and continuous space tasks in an
off-policy setting respectively, and demonstrate competitive results compared
to other reinforcement learning baselines.","I.2.6,cs.AI,cs.LG,stat.ML"
"Multi-resolution Networks For Flexible Irregular Time Series Modeling (Multi-FIT). Missing values, irregularly collected samples, and multi-resolution signals
commonly occur in multivariate time series data, making predictive tasks
difficult. These challenges are especially prevalent in the healthcare domain,
where patients' vital signs and electronic records are collected at different
frequencies and have occasionally missing information due to the imperfections
in equipment or patient circumstances. Researchers have handled each of these
issues differently, often handling missing data through mean value imputation
and then using sequence models over the multivariate signals while ignoring the
different resolution of signals. We propose a unified model named
Multi-resolution Flexible Irregular Time series Network (Multi-FIT). The
building block for Multi-FIT is the FIT network. The FIT network creates an
informative dense representation at each time step using signal information
such as last observed value, time difference since the last observed time stamp
and overall mean for the signal. Vertical FIT (FIT-V) is a variant of FIT which
also models the relationship between different temporal signals while creating
the informative dense representations for the signal. The multi-FIT model uses
multiple FIT networks for sets of signals with different resolutions, further
facilitating the construction of flexible representations. Our model has three
main contributions: a.) it does not impute values but rather creates
informative representations to provide flexibility to the model for creating
task-specific representations b.) it models the relationship between different
signals in the form of support signals c.) it models different resolutions in
parallel before merging them for the final prediction task. The FIT, FIT-V and
Multi-FIT networks improve upon the state-of-the-art models for three
predictive tasks, including the forecasting of patient survival.","cs.LG,eess.SP,stat.ML"
"Stochastic Normalizing Flows for Inverse Problems: a Markov Chains Viewpoint. To overcome topological constraints and improve the expressiveness of
normalizing flow architectures, Wu, K\""ohler and No\'e introduced stochastic
normalizing flows which combine deterministic, learnable flow transformations
with stochastic sampling methods. In this paper, we consider stochastic
normalizing flows from a Markov chain point of view. In particular, we replace
transition densities by general Markov kernels and establish proofs via
Radon-Nikodym derivatives which allows to incorporate distributions without
densities in a sound way. Further, we generalize the results for sampling from
posterior distributions as required in inverse problems. The performance of the
proposed conditional stochastic normalizing flow is demonstrated by numerical
examples.","cs.LG,math.PR"
"Learning Compositional Representations of Interacting Systems with Restricted Boltzmann Machines: Comparative Study of Lattice Proteins. A Restricted Boltzmann Machine (RBM) is an unsupervised machine-learning
bipartite graphical model that jointly learns a probability distribution over
data and extracts their relevant statistical features. As such, RBM were
recently proposed for characterizing the patterns of coevolution between amino
acids in protein sequences and for designing new sequences. Here, we study how
the nature of the features learned by RBM changes with its defining parameters,
such as the dimensionality of the representations (size of the hidden layer)
and the sparsity of the features. We show that for adequate values of these
parameters, RBM operate in a so-called compositional phase in which visible
configurations sampled from the RBM are obtained by recombining these features.
We then compare the performance of RBM with other standard representation
learning algorithms, including Principal or Independent Component Analysis,
autoencoders (AE), variational auto-encoders (VAE), and their sparse variants.
We show that RBM, due to the stochastic mapping between data configurations and
representations, better capture the underlying interactions in the system and
are significantly more robust with respect to sample size than deterministic
methods such as PCA or ICA. In addition, this stochastic mapping is not
prescribed a priori as in VAE, but learned from data, which allows RBM to show
good performance even with shallow architectures. All numerical results are
illustrated on synthetic lattice-protein data, that share similar statistical
features with real protein sequences, and for which ground-truth interactions
are known.","cs.LG,physics.data-an,q-bio.BM"
"Object-Aware Multi-Branch Relation Networks for Spatio-Temporal Video Grounding. Spatio-temporal video grounding aims to retrieve the spatio-temporal tube of
a queried object according to the given sentence. Currently, most existing
grounding methods are restricted to well-aligned segment-sentence pairs. In
this paper, we explore spatio-temporal video grounding on unaligned data and
multi-form sentences. This challenging task requires to capture critical object
relations to identify the queried target. However, existing approaches cannot
distinguish notable objects and remain in ineffective relation modeling between
unnecessary objects. Thus, we propose a novel object-aware multi-branch
relation network for object-aware relation discovery. Concretely, we first
devise multiple branches to develop object-aware region modeling, where each
branch focuses on a crucial object mentioned in the sentence. We then propose
multi-branch relation reasoning to capture critical object relationships
between the main branch and auxiliary branches. Moreover, we apply a diversity
loss to make each branch only pay attention to its corresponding object and
boost multi-branch learning. The extensive experiments show the effectiveness
of our proposed method.","cs.CV,cs.MM"
"How to find a unicorn: a novel model-free, unsupervised anomaly detection method for time series. Recognition of anomalous events is a challenging but critical task in many
scientific and industrial fields, especially when the properties of anomalies
are unknown. In this paper, we introduce a new anomaly concept called ""unicorn""
or unique event and present a new, model-free, unsupervised detection algorithm
to detect unicorns. The key component of the new algorithm is the Temporal
Outlier Factor (TOF) to measure the uniqueness of events in continuous data
sets from dynamic systems. The concept of unique events differs significantly
from traditional outliers in many aspects: while repetitive outliers are no
longer unique events, a unique event is not necessarily an outlier; it does not
necessarily fall out from the distribution of normal activity. The performance
of our algorithm was examined in recognizing unique events on different types
of simulated data sets with anomalies and it was compared with the Local
Outlier Factor (LOF) and discord discovery algorithms. TOF had superior
performance compared to LOF and discord algorithms even in recognizing
traditional outliers and it also recognized unique events that those did not.
The benefits of the unicorn concept and the new detection method were
illustrated by example data sets from very different scientific fields. Our
algorithm successfully recognized unique events in those cases where they were
already known such as the gravitational waves of a binary black hole merger on
LIGO detector data and the signs of respiratory failure on ECG data series.
Furthermore, unique events were found on the LIBOR data set of the last 30
years.","cs.LG,eess.SP,physics.data-an,stat.ML"
"Decision Tree Learning with Spatial Modal Logics. Symbolic learning represents the most straightforward approach to
interpretable modeling, but its applications have been hampered by a single
structural design choice: the adoption of propositional logic as the underlying
language. Recently, more-than-propositional symbolic learning methods have
started to appear, in particular for time-dependent data. These methods exploit
the expressive power of modal temporal logics in powerful learning algorithms,
such as temporal decision trees, whose classification capabilities are
comparable with the best non-symbolic ones, while producing models with
explicit knowledge representation.
  With the intent of following the same approach in the case of spatial data,
in this paper we: i) present a theory of spatial decision tree learning; ii)
describe a prototypical implementation of a spatial decision tree learning
algorithm based, and strictly extending, the classical C4.5 algorithm; and iii)
perform a series of experiments in which we compare the predicting power of
spatial decision trees with that of classical propositional decision trees in
several versions, for a multi-class image classification problem, on publicly
available datasets. Our results are encouraging, showing clear improvements in
the performances from the propositional to the spatial models, which in turn
show higher levels of interpretability.","cs.AI,cs.LG,cs.LO"
"Vertex Nomination, Consistent Estimation, and Adversarial Modification. Given a pair of graphs $G_1$ and $G_2$ and a vertex set of interest in $G_1$,
the vertex nomination (VN) problem seeks to find the corresponding vertices of
interest in $G_2$ (if they exist) and produce a rank list of the vertices in
$G_2$, with the corresponding vertices of interest in $G_2$ concentrating,
ideally, at the top of the rank list. In this paper, we define and derive the
analogue of Bayes optimality for VN with multiple vertices of interest, and we
define the notion of maximal consistency classes in vertex nomination. This
theory forms the foundation for a novel VN adversarial contamination model, and
we demonstrate with real and simulated data that there are VN schemes that
perform effectively in the uncontaminated setting, and adversarial network
contamination adversely impacts the performance of our VN scheme. We further
define a network regularization method for mitigating the impact of the
adversarial contamination, and we demonstrate the effectiveness of
regularization in both real and synthetic data.","cs.LG,cs.SI,stat.CO,stat.ML"
"VATT: Transformers for Multimodal Self-Supervised Learning from Raw Video, Audio and Text. We present a framework for learning multimodal representations from unlabeled
data using convolution-free Transformer architectures. Specifically, our
Video-Audio-Text Transformer (VATT) takes raw signals as inputs and extracts
multimodal representations that are rich enough to benefit a variety of
downstream tasks. We train VATT end-to-end from scratch using multimodal
contrastive losses and evaluate its performance by the downstream tasks of
video action recognition, audio event classification, image classification, and
text-to-video retrieval. Furthermore, we study a modality-agnostic
single-backbone Transformer by sharing weights among the three modalities. We
show that the convolution-free VATT outperforms state-of-the-art ConvNet-based
architectures in the downstream tasks. Especially, VATT's vision Transformer
achieves the top-1 accuracy of 82.1% on Kinetics-400, 83.6% on Kinetics-600,and
41.1% on Moments in Time, new records while avoiding supervised pre-training.
Transferring to image classification leads to 78.7% top-1 accuracy on ImageNet
compared to 64.7% by training the same Transformer from scratch, showing the
generalizability of our model despite the domain gap between videos and images.
VATT's audio Transformer also sets a new record on waveform-based audio event
recognition by achieving the mAP of 39.4% on AudioSet without any supervised
pre-training. VATT's source code is publicly available.","cs.AI,cs.CV,cs.LG,cs.MM,eess.IV"
"On the Global Convergence of Imitation Learning: A Case for Linear Quadratic Regulator. We study the global convergence of generative adversarial imitation learning
for linear quadratic regulators, which is posed as minimax optimization. To
address the challenges arising from non-convex-concave geometry, we analyze the
alternating gradient algorithm and establish its Q-linear rate of convergence
to a unique saddle point, which simultaneously recovers the globally optimal
policy and reward function. We hope our results may serve as a small step
towards understanding and taming the instability in imitation learning as well
as in more general non-convex-concave alternating minimax optimization that
arises from reinforcement learning and generative adversarial learning.","cs.AI,cs.LG,math.OC,stat.ML"
"Reinforcement Learning in FlipIt. Reinforcement learning has shown much success in games such as chess,
backgammon and Go. However, in most of these games, agents have full knowledge
of the environment at all times. In this paper, we describe a deep learning
model that successfully optimizes its score using reinforcement learning in a
game with incomplete and imperfect information. We apply our model to FlipIt, a
two-player game in which both players, the attacker and the defender, compete
for ownership of a shared resource and only receive information on the current
state (such as the current owner of the resource, or the time since the
opponent last moved, etc.) upon making a move. Our model is a deep neural
network combined with Q-learning and is trained to maximize the defender's time
of ownership of the resource. Despite the imperfect observations, our model
successfully learns an optimal cost-effective counter-strategy and shows the
advantages of the use of deep reinforcement learning in game theoretic
scenarios. Our results show that it outperforms the Greedy strategy against
distributions such as periodic and exponential distributions without any prior
knowledge of the opponent's strategy, and we generalize the model to $n$-player
games.","91A06,91A20,I.2.6,cs.AI,cs.GT,cs.LG"
"High-dimensional covariance estimation based on Gaussian graphical models. Undirected graphs are often used to describe high dimensional distributions.
Under sparsity conditions, the graph can be estimated using
$\ell_1$-penalization methods. We propose and study the following method. We
combine a multiple regression approach with ideas of thresholding and
refitting: first we infer a sparse undirected graphical model structure via
thresholding of each among many $\ell_1$-norm penalized regression functions;
we then estimate the covariance matrix and its inverse using the maximum
likelihood estimator. We show that under suitable conditions, this approach
yields consistent estimation in terms of graphical structure and fast
convergence rates with respect to the operator and Frobenius norm for the
covariance matrix and its inverse. We also derive an explicit bound for the
Kullback Leibler divergence.","math.ST,stat.ML,stat.TH"
"Fit without fear: remarkable mathematical phenomena of deep learning through the prism of interpolation. In the past decade the mathematical theory of machine learning has lagged far
behind the triumphs of deep neural networks on practical challenges. However,
the gap between theory and practice is gradually starting to close. In this
paper I will attempt to assemble some pieces of the remarkable and still
incomplete mathematical mosaic emerging from the efforts to understand the
foundations of deep learning. The two key themes will be interpolation, and its
sibling, over-parameterization. Interpolation corresponds to fitting data, even
noisy data, exactly. Over-parameterization enables interpolation and provides
flexibility to select a right interpolating model.
  As we will see, just as a physical prism separates colors mixed within a ray
of light, the figurative prism of interpolation helps to disentangle
generalization and optimization properties within the complex picture of modern
Machine Learning. This article is written with belief and hope that clearer
understanding of these issues brings us a step closer toward a general theory
of deep learning and machine learning.","cs.LG,math.ST,stat.ML,stat.TH"
"Automatic selection of clustering algorithms using supervised graph embedding. The widespread adoption of machine learning (ML) techniques and the extensive
expertise required to apply them have led to increased interest in automated ML
solutions that reduce the need for human intervention. One of the main
challenges in applying ML to previously unseen problems is algorithm selection
- the identification of high-performing algorithm(s) for a given dataset, task,
and evaluation measure. This study addresses the algorithm selection challenge
for data clustering, a fundamental task in data mining that is aimed at
grouping similar objects. We present MARCO-GE, a novel meta-learning approach
for the automated recommendation of clustering algorithms. MARCO-GE first
transforms datasets into graphs and then utilizes a graph convolutional neural
network technique to extract their latent representation. Using the embedding
representations obtained, MARCO-GE trains a ranking meta-model capable of
accurately recommending top-performing algorithms for a new dataset and
clustering evaluation measure. Extensive evaluation on 210 datasets, 13
clustering algorithms, and 10 clustering measures demonstrates the
effectiveness of our approach and its superiority in terms of predictive and
generalization performance over state-of-the-art clustering meta-learning
approaches.","cs.IR,cs.LG,stat.ML"
"A Knowledge Distillation Ensemble Framework for Predicting Short and Long-term Hospitalisation Outcomes from Electronic Health Records Data. The ability to perform accurate prognosis of patients is crucial for
proactive clinical decision making, informed resource management and
personalised care. Existing outcome prediction models suffer from a low recall
of infrequent positive outcomes. We present a highly-scalable and robust
machine learning framework to automatically predict adversity represented by
mortality and ICU admission from time-series vital signs and laboratory results
obtained within the first 24 hours of hospital admission. The stacked platform
comprises two components: a) an unsupervised LSTM Autoencoder that learns an
optimal representation of the time-series, using it to differentiate the less
frequent patterns which conclude with an adverse event from the majority
patterns that do not, and b) a gradient boosting model, which relies on the
constructed representation to refine prediction, incorporating static features
of demographics, admission details and clinical summaries. The model is used to
assess a patient's risk of adversity over time and provides visual
justifications of its prediction based on the patient's static features and
dynamic signals. Results of three case studies for predicting mortality and ICU
admission show that the model outperforms all existing outcome prediction
models, achieving PR-AUC of 0.891 (95$%$ CI: 0.878 - 0.969) in predicting
mortality in ICU and general ward settings and 0.908 (95$%$ CI: 0.870-0.935) in
predicting ICU admission.","cs.CY,cs.LG"
"Variable-lag Granger Causality and Transfer Entropy for Time Series Analysis. Granger causality is a fundamental technique for causal inference in time
series data, commonly used in the social and biological sciences. Typical
operationalizations of Granger causality make a strong assumption that every
time point of the effect time series is influenced by a combination of other
time series with a fixed time delay. The assumption of fixed time delay also
exists in Transfer Entropy, which is considered to be a non-linear version of
Granger causality. However, the assumption of the fixed time delay does not
hold in many applications, such as collective behavior, financial markets, and
many natural phenomena. To address this issue, we develop Variable-lag Granger
causality and Variable-lag Transfer Entropy, generalizations of both Granger
causality and Transfer Entropy that relax the assumption of the fixed time
delay and allow causes to influence effects with arbitrary time delays. In
addition, we propose methods for inferring both variable-lag Granger causality
and Transfer Entropy relations. In our approaches, we utilize an optimal
warping path of Dynamic Time Warping (DTW) to infer variable-lag causal
relations. We demonstrate our approaches on an application for studying
coordinated collective behavior and other real-world casual-inference datasets
and show that our proposed approaches perform better than several existing
methods in both simulated and real-world datasets. Our approaches can be
applied in any domain of time series analysis. The software of this work is
available in the R-CRAN package: VLTimeCausality.","62-07,68T05,91-08,G.3; I.2.3; I.2.6; J.4,cs.LG,econ.EM,physics.data-an,stat.ME,stat.ML"
"Regularization for convolutional kernel tensors to avoid unstable gradient problem in convolutional neural networks. Convolutional neural networks are very popular nowadays. Training neural
networks is not an easy task. Each convolution corresponds to a structured
transformation matrix. In order to help avoid the exploding/vanishing gradient
problem, it is desirable that the singular values of each transformation matrix
are not large/small in the training process. We propose three new
regularization terms for a convolutional kernel tensor to constrain the
singular values of each transformation matrix. We show how to carry out the
gradient type methods, which provides new insight about the training of
convolutional neural networks.","cs.LG,cs.NA,math.NA"
"An Approach to Partial Observability in Games: Learning to Both Act and Observe. Reinforcement learning (RL) is successful at learning to play games where the
entire environment is visible. However, RL approaches are challenged in complex
games like Starcraft II and in real-world environments where the entire
environment is not visible. In these more complex games with more limited
visual information, agents must choose where to look and how to optimally use
their limited visual information in order to succeed at the game. We verify
that with a relatively simple model the agent can learn where to look in
scenarios with a limited visual bandwidth. We develop a method for masking part
of the environment in Atari games to force the RL agent to learn both where to
look and how to play the game in order to study where the RL agent learns to
look. In addition, we develop a neural network architecture and method for
allowing the agent to choose where to look and what action to take in the Pong
game. Further, we analyze the strategies the agent learns to better understand
how the RL agent learns to play the game.","cs.AI,cs.CV,cs.GT,cs.LG"
"Align, then memorise: the dynamics of learning with feedback alignment. Direct Feedback Alignment (DFA) is emerging as an efficient and biologically
plausible alternative to the ubiquitous backpropagation algorithm for training
deep neural networks. Despite relying on random feedback weights for the
backward pass, DFA successfully trains state-of-the-art models such as
Transformers. On the other hand, it notoriously fails to train convolutional
networks. An understanding of the inner workings of DFA to explain these
diverging results remains elusive. Here, we propose a theory for the success of
DFA. We first show that learning in shallow networks proceeds in two steps: an
alignment phase, where the model adapts its weights to align the approximate
gradient with the true gradient of the loss function, is followed by a
memorisation phase, where the model focuses on fitting the data. This two-step
process has a degeneracy breaking effect: out of all the low-loss solutions in
the landscape, a network trained with DFA naturally converges to the solution
which maximises gradient alignment. We also identify a key quantity underlying
alignment in deep linear networks: the conditioning of the alignment matrices.
The latter enables a detailed understanding of the impact of data structure on
alignment, and suggests a simple explanation for the well-known failure of DFA
to train convolutional neural networks. Numerical experiments on MNIST and
CIFAR10 clearly demonstrate degeneracy breaking in deep non-linear networks and
show that the align-then-memorise process occurs sequentially from the bottom
layers of the network to the top.","cond-mat.dis-nn,cs.LG,cs.NE,stat.ML"
"Variational Transport: A Convergent Particle-BasedAlgorithm for Distributional Optimization. We consider the optimization problem of minimizing a functional defined over
a family of probability distributions, where the objective functional is
assumed to possess a variational form. Such a distributional optimization
problem arises widely in machine learning and statistics, with Monte-Carlo
sampling, variational inference, policy optimization, and generative
adversarial network as examples. For this problem, we propose a novel
particle-based algorithm, dubbed as variational transport, which approximately
performs Wasserstein gradient descent over the manifold of probability
distributions via iteratively pushing a set of particles. Specifically, we
prove that moving along the geodesic in the direction of functional gradient
with respect to the second-order Wasserstein distance is equivalent to applying
a pushforward mapping to a probability distribution, which can be approximated
accurately by pushing a set of particles. Specifically, in each iteration of
variational transport, we first solve the variational problem associated with
the objective functional using the particles, whose solution yields the
Wasserstein gradient direction. Then we update the current distribution by
pushing each particle along the direction specified by such a solution. By
characterizing both the statistical error incurred in estimating the
Wasserstein gradient and the progress of the optimization algorithm, we prove
that when the objective function satisfies a functional version of the
Polyak-\L{}ojasiewicz (PL) (Polyak, 1963) and smoothness conditions,
variational transport converges linearly to the global minimum of the objective
functional up to a certain statistical error, which decays to zero sublinearly
as the number of particles goes to infinity.","cs.LG,math.OC,math.ST,stat.ML,stat.TH"
"Estimating $$-mixing coefficients. The literature on statistical learning for time series assumes the asymptotic
independence or ``mixing' of the data-generating process. These mixing
assumptions are never tested, nor are there methods for estimating mixing rates
from data. We give an estimator for the $\beta$-mixing rate based on a single
stationary sample path and show it is $L_1$-risk consistent.","cs.LG,math.PR,stat.ML"
"Pathwise Conditioning of Gaussian Processes. As Gaussian processes are used to answer increasingly complex questions,
analytic solutions become scarcer and scarcer. Monte Carlo methods act as a
convenient bridge for connecting intractable mathematical expressions with
actionable estimates via sampling. Conventional approaches for simulating
Gaussian process posteriors view samples as draws from marginal distributions
of process values at finite sets of input locations. This distribution-centric
characterization leads to generative strategies that scale cubically in the
size of the desired random vector. These methods are prohibitively expensive in
cases where we would, ideally, like to draw high-dimensional vectors or even
continuous sample paths. In this work, we investigate a different line of
reasoning: rather than focusing on distributions, we articulate Gaussian
conditionals at the level of random variables. We show how this pathwise
interpretation of conditioning gives rise to a general family of approximations
that lend themselves to efficiently sampling Gaussian process posteriors.
Starting from first principles, we derive these methods and analyze the
approximation errors they introduce. We, then, ground these results by
exploring the practical implications of pathwise conditioning in various
applied settings, such as global optimization and reinforcement learning.","cs.LG,math.ST,stat.ML,stat.TH"
"Medium Access using Distributed Reinforcement Learning for IoTs with Low-Complexity Wireless Transceivers. This paper proposes a distributed Reinforcement Learning (RL) based framework
that can be used for synthesizing MAC layer wireless protocols in IoT networks
with low-complexity wireless transceivers. The proposed framework does not rely
on complex hardware capabilities such as carrier sensing and its associated
algorithmic complexities that are often not supported in wireless transceivers
of low-cost and low-energy IoT devices. In this framework, the access protocols
are first formulated as Markov Decision Processes (MDP) and then solved using
RL. A distributed and multi-Agent RL framework is used as the basis for
protocol synthesis. Distributed behavior makes the nodes independently learn
optimal transmission strategies without having to rely on full network level
information and direct knowledge of behavior of other nodes. The nodes learn to
minimize packet collisions such that optimal throughput can be attained and
maintained for loading conditions that are higher than what the known benchmark
protocols (such as ALOHA) for IoT devices without complex transceivers. In
addition, the nodes are observed to be able to learn to act optimally in the
presence of heterogeneous loading and network topological conditions. Finally,
the proposed learning approach allows the wireless bandwidth to be fairly
distributed among network nodes in a way that is not dependent on such
heterogeneities. Via simulation experiments, the paper demonstrates the
performance of the learning paradigm and its abilities to make nodes adapt
their optimal transmission strategies on the fly in response to various network
dynamics.","cs.DC,cs.LG,cs.NI,cs.SY,eess.SY"
"PDP: A General Neural Framework for Learning Constraint Satisfaction Solvers. There have been recent efforts for incorporating Graph Neural Network models
for learning full-stack solvers for constraint satisfaction problems (CSP) and
particularly Boolean satisfiability (SAT). Despite the unique representational
power of these neural embedding models, it is not clear how the search strategy
in the learned models actually works. On the other hand, by fixing the search
strategy (e.g. greedy search), we would effectively deprive the neural models
of learning better strategies than those given. In this paper, we propose a
generic neural framework for learning CSP solvers that can be described in
terms of probabilistic inference and yet learn search strategies beyond greedy
search. Our framework is based on the idea of propagation, decimation and
prediction (and hence the name PDP) in graphical models, and can be trained
directly toward solving CSP in a fully unsupervised manner via energy
minimization, as shown in the paper. Our experimental results demonstrate the
effectiveness of our framework for SAT solving compared to both neural and the
state-of-the-art baselines.","cs.LG,cs.LO,cs.NE,stat.ML"
"DreamNLP: Novel NLP System for Clinical Report Metadata Extraction using Count Sketch Data Streaming Algorithm: Preliminary Results. Extracting information from electronic health records (EHR) is a challenging
task since it requires prior knowledge of the reports and some natural language
processing algorithm (NLP). With the growing number of EHR implementations,
such knowledge is increasingly challenging to obtain in an efficient manner. We
address this challenge by proposing a novel methodology to analyze large sets
of EHRs using a modified Count Sketch data streaming algorithm termed DreamNLP.
By using DreamNLP, we generate a dictionary of frequently occurring terms or
heavy hitters in the EHRs using low computational memory compared to
conventional counting approach other NLP programs use. We demonstrate the
extraction of the most important breast diagnosis features from the EHRs in a
set of patients that underwent breast imaging. Based on the analysis,
extraction of these terms would be useful for defining important features for
downstream tasks such as machine learning for precision medicine.","E.1; E.2; F.2.2; I.2.7,cs.LG,eess.AS,stat.ML"
"Fuzzy soft rough K-Means clustering approach for gene expression data. Clustering is one of the widely used data mining techniques for medical
diagnosis. Clustering can be considered as the most important unsupervised
learning technique. Most of the clustering methods group data based on distance
and few methods cluster data based on similarity. The clustering algorithms
classify gene expression data into clusters and the functionally related genes
are grouped together in an efficient manner. The groupings are constructed such
that the degree of relationship is strong among members of the same cluster and
weak among members of different clusters. In this work, we focus on a
similarity relationship among genes with similar expression patterns so that a
consequential and simple analytical decision can be made from the proposed
Fuzzy Soft Rough K-Means algorithm. The algorithm is developed based on Fuzzy
Soft sets and Rough sets. Comparative analysis of the proposed work is made
with bench mark algorithms like K-Means and Rough K-Means and efficiency of the
proposed algorithm is illustrated in this work by using various cluster
validity measures such as DB index and Xie-Beni index.","cs.CE,cs.LG"
"A Stochastic Grammar for Natural Shapes. We consider object detection using a generic model for natural shapes. A
common approach for object recognition involves matching object models directly
to images. Another approach involves building intermediate representations via
a generic grouping processes. We argue that these two processes (model-based
recognition and grouping) may use similar computational mechanisms. By defining
a generic model for shapes we can use model-based techniques to implement a
mid-level vision grouping process.",cs.CV
"Extraction of Discrete Spectra Modes from Video Data Using a Deep Convolutional Koopman Network. Recent deep learning extensions in Koopman theory have enabled compact,
interpretable representations of nonlinear dynamical systems which are amenable
to linear analysis. Deep Koopman networks attempt to learn the Koopman
eigenfunctions which capture the coordinate transformation to globally
linearize system dynamics. These eigenfunctions can be linked to underlying
system modes which govern the dynamical behavior of the system. While many
related techniques have demonstrated their efficacy on canonical systems and
their associated state variables, in this work the system dynamics are observed
optically (i.e. in video format). We demonstrate the ability of a deep
convolutional Koopman network (CKN) in automatically identifying independent
modes for dynamical systems with discrete spectra. Practically, this affords
flexibility in system data collection as the data are easily obtainable
observable variables. The learned models are able to successfully and robustly
identify the underlying modes governing the system, even with a redundantly
large embedding space. Modal disaggregation is encouraged using a simple
masking procedure. All of the systems analyzed in this work use an identical
network architecture.","cs.CV,math.DS,physics.flu-dyn"
"Detect the Interactions that Matter in Matter: Geometric Attention for Many-Body Systems. Attention mechanisms are developing into a viable alternative to
convolutional layers as elementary building block of NNs. Their main advantage
is that they are not restricted to capture local dependencies in the input, but
can draw arbitrary connections. This unprecedented capability coincides with
the long-standing problem of modeling global atomic interactions in molecular
force fields and other many-body problems. In its original formulation,
however, attention is not applicable to the continuous domains in which the
atoms live. For this purpose we propose a variant to describe geometric
relations for arbitrary atomic configurations in Euclidean space that also
respects all relevant physical symmetries. We furthermore demonstrate, how the
successive application of our learned attention matrices effectively translates
the molecular geometry into a set of individual atomic contributions
on-the-fly.","cs.LG,physics.chem-ph"
"Adversarial Attack on Graph Structured Data. Deep learning on graph structures has shown exciting results in various
applications. However, few attentions have been paid to the robustness of such
models, in contrast to numerous research work for image or text adversarial
attack and defense. In this paper, we focus on the adversarial attacks that
fool the model by modifying the combinatorial structure of data. We first
propose a reinforcement learning based attack method that learns the
generalizable attack policy, while only requiring prediction labels from the
target classifier. Also, variants of genetic algorithms and gradient methods
are presented in the scenario where prediction confidence or gradients are
available. We use both synthetic and real-world data to show that, a family of
Graph Neural Network models are vulnerable to these attacks, in both
graph-level and node-level classification tasks. We also show such attacks can
be used to diagnose the learned classifiers.","cs.CR,cs.LG,cs.SI,stat.ML"
"Sharp threshold for alignment of graph databases with Gaussian weights. We study the fundamental limits for reconstruction in weighted graph (or
matrix) database alignment. We consider a model of two graphs where $\pi^*$ is
a planted uniform permutation and all pairs of edge weights $(A_{i,j},
B_{\pi^*(i),\pi^*(j)})_{1 \leq i<j \leq n}$ are i.i.d. pairs of Gaussian
variables with zero mean, unit variance and correlation parameter $\rho \in
[0,1]$. We prove that there is a sharp threshold for exact recovery of $\pi^*$:
if $n \rho^2 \geq (4+\epsilon) \log n + \omega(1)$ for some $\epsilon>0$, there
is an estimator $\hat{\pi}$ -- namely the MAP estimator -- based on the
observation of databases $A,B$ that achieves exact reconstruction with high
probability. Conversely, if $n \rho^2 \leq 4 \log n - \log \log n - \omega(1)$,
then any estimator $\hat{\pi}$ verifies $\hat{\pi}=\pi$ with probability
$o(1)$.
  This result shows that the information-theoretic threshold for exact recovery
is the same as the one obtained for detection in a recent work by Wu et al.
(2020): in other words, for Gaussian weighted graph alignment, the problem of
reconstruction is not more difficult than that of detection. Though the
reconstruction task was already well understood for vector-shaped database
alignment (that is taking signal of the form $(u_i, v_{\pi^*(i)})_{1 \leq i\leq
n}$ where $(u_i, v_{\pi^*(i)})$ are i.i.d. pairs in $\mathbb{R}^{d_u} \times
\mathbb{R}^{d_v}$), its formulation for graph (or matrix) databases brings a
drastically different problem for which the hard phase is conjectured to be
wide.
  The proofs build upon the analysis of the MAP estimator and the second moment
method, together with the study of the correlation structure of energies of
permutations.","cs.LG,math.PR,stat.ML"
"Semi-Supervised Hierarchical Drug Embedding in Hyperbolic Space. Learning accurate drug representation is essential for tasks such as
computational drug repositioning and prediction of drug side-effects. A drug
hierarchy is a valuable source that encodes human knowledge of drug relations
in a tree-like structure where drugs that act on the same organs, treat the
same disease, or bind to the same biological target are grouped together.
However, its utility in learning drug representations has not yet been
explored, and currently described drug representations cannot place novel
molecules in a drug hierarchy. Here, we develop a semi-supervised drug
embedding that incorporates two sources of information: (1) underlying chemical
grammar that is inferred from molecular structures of drugs and drug-like
molecules (unsupervised), and (2) hierarchical relations that are encoded in an
expert-crafted hierarchy of approved drugs (supervised). We use the Variational
Auto-Encoder (VAE) framework to encode the chemical structures of molecules and
use the knowledge-based drug-drug similarity to induce the clustering of drugs
in hyperbolic space. The hyperbolic space is amenable for encoding hierarchical
concepts. Both quantitative and qualitative results support that the learned
drug embedding can accurately reproduce the chemical structure and induce the
hierarchical relations among drugs. Furthermore, our approach can infer the
pharmacological properties of novel molecules by retrieving similar drugs from
the embedding space. We demonstrate that the learned drug embedding can be used
to find new uses for existing drugs and to discover side-effects. We show that
it significantly outperforms baselines in both tasks.","cs.LG,q-bio.MN,q-bio.QM,stat.ML"
"Improving Stability of LS-GANs for Audio and Speech Signals. In this paper we address the instability issue of generative adversarial
network (GAN) by proposing a new similarity metric in unitary space of Schur
decomposition for 2D representations of audio and speech signals. We show that
encoding departure from normality computed in this vector space into the
generator optimization formulation helps to craft more comprehensive
spectrograms. We demonstrate the effectiveness of binding this metric for
enhancing stability in training with less mode collapse compared to baseline
GANs. Experimental results on subsets of UrbanSound8k and Mozilla common voice
datasets have shown considerable improvements on the quality of the generated
samples measured by the Fr\'echet inception distance. Moreover, reconstructed
signals from these samples, have achieved higher signal to noise ratio compared
to regular LS-GANs.","cs.LG,cs.SD,eess.AS,stat.ML"
"Molecule Attention Transformer. Designing a single neural network architecture that performs competitively
across a range of molecule property prediction tasks remains largely an open
challenge, and its solution may unlock a widespread use of deep learning in the
drug discovery industry. To move towards this goal, we propose Molecule
Attention Transformer (MAT). Our key innovation is to augment the attention
mechanism in Transformer using inter-atomic distances and the molecular graph
structure. Experiments show that MAT performs competitively on a diverse set of
molecular prediction tasks. Most importantly, with a simple self-supervised
pretraining, MAT requires tuning of only a few hyperparameter values to achieve
state-of-the-art performance on downstream tasks. Finally, we show that
attention weights learned by MAT are interpretable from the chemical point of
view.","cs.LG,physics.comp-ph,stat.ML"
"LaserNet: An Efficient Probabilistic 3D Object Detector for Autonomous Driving. In this paper, we present LaserNet, a computationally efficient method for 3D
object detection from LiDAR data for autonomous driving. The efficiency results
from processing LiDAR data in the native range view of the sensor, where the
input data is naturally compact. Operating in the range view involves well
known challenges for learning, including occlusion and scale variation, but it
also provides contextual information based on how the sensor data was captured.
Our approach uses a fully convolutional network to predict a multimodal
distribution over 3D boxes for each point and then it efficiently fuses these
distributions to generate a prediction for each object. Experiments show that
modeling each detection as a distribution rather than a single deterministic
box leads to better overall detection performance. Benchmark results show that
this approach has significantly lower runtime than other recent detectors and
that it achieves state-of-the-art performance when compared on a large dataset
that has enough data to overcome the challenges of training on the range view.","cs.CV,cs.LG,cs.RO"
"Solution of Definite Integrals using Functional Link Artificial Neural Networks. This paper discusses a new method to solve definite integrals using
artificial neural networks. The objective is to build a neural network that
would be a novel alternative to pre-established numerical methods and with the
help of a learning algorithm, be able to solve definite integrals, by
minimising a well constructed error function. The proposed algorithm, with
respect to existing numerical methods, is effective and precise and well-suited
for purposes which require integration of higher order polynomials. The
observations have been recorded and illustrated in tabular and graphical form.","cs.LG,cs.NA"
"GIF: Generative Interpretable Faces. Photo-realistic visualization and animation of expressive human faces have
been a long standing challenge. 3D face modeling methods provide parametric
control but generates unrealistic images, on the other hand, generative 2D
models like GANs (Generative Adversarial Networks) output photo-realistic face
images, but lack explicit control. Recent methods gain partial control, either
by attempting to disentangle different factors in an unsupervised manner, or by
adding control post hoc to a pre-trained model. Unconditional GANs, however,
may entangle factors that are hard to undo later. We condition our generative
model on pre-defined control parameters to encourage disentanglement in the
generation process. Specifically, we condition StyleGAN2 on FLAME, a generative
3D face model. While conditioning on FLAME parameters yields unsatisfactory
results, we find that conditioning on rendered FLAME geometry and photometric
details works well. This gives us a generative 2D face model named GIF
(Generative Interpretable Faces) that offers FLAME's parametric control. Here,
interpretable refers to the semantic meaning of different parameters. Given
FLAME parameters for shape, pose, expressions, parameters for appearance,
lighting, and an additional style vector, GIF outputs photo-realistic face
images. We perform an AMT based perceptual study to quantitatively and
qualitatively evaluate how well GIF follows its conditioning. The code, data,
and trained model are publicly available for research purposes at
http://gif.is.tue.mpg.de.","cs.AI,cs.CV,cs.GR,cs.LG,stat.AP"
"Constructing fast approximate eigenspaces with application to the fast graph Fourier transforms. We investigate numerically efficient approximations of eigenspaces associated
to symmetric and general matrices. The eigenspaces are factored into a fixed
number of fundamental components that can be efficiently manipulated (we
consider extended orthogonal Givens or scaling and shear transformations). The
number of these components controls the trade-off between approximation
accuracy and the computational complexity of projecting on the eigenspaces. We
write minimization problems for the single fundamental components and provide
closed-form solutions. Then we propose algorithms that iterative update all
these components until convergence. We show results on random matrices and an
application on the approximation of graph Fourier transforms for directed and
undirected graphs.","cs.LG,cs.NA,eess.SP,math.NA,stat.ML"
"Exact Rate-Distortion in Autoencoders via Echo Noise. Compression is at the heart of effective representation learning. However,
lossy compression is typically achieved through simple parametric models like
Gaussian noise to preserve analytic tractability, and the limitations this
imposes on learning are largely unexplored. Further, the Gaussian prior
assumptions in models such as variational autoencoders (VAEs) provide only an
upper bound on the compression rate in general. We introduce a new noise
channel, \emph{Echo noise}, that admits a simple, exact expression for mutual
information for arbitrary input distributions. The noise is constructed in a
data-driven fashion that does not require restrictive distributional
assumptions. With its complex encoding mechanism and exact rate regularization,
Echo leads to improved bounds on log-likelihood and dominates $\beta$-VAEs
across the achievable range of rate-distortion trade-offs. Further, we show
that Echo noise can outperform flow-based methods without the need to train
additional distributional transformations.","cs.IT,cs.LG,math.IT,stat.ML"
"A Hybrid mmWave and Camera System for Long-Range Depth Imaging. mmWave radars offer excellent depth resolution owing to their high bandwidth
at mmWave radio frequencies. Yet, they suffer intrinsically from poor angular
resolution, that is an order-of-magnitude worse than camera systems, and are
therefore not a capable 3-D imaging solution in isolation. We propose
Metamoran, a system that combines the complimentary strengths of radar and
camera systems to obtain depth images at high azimuthal resolutions at
distances of several tens of meters with high accuracy, all from a single fixed
vantage point. Metamoran enables rich long-range depth imaging outdoors with
applications to roadside safety infrastructure, surveillance and wide-area
mapping. Our key insight is to use the high azimuth resolution from cameras
using computer vision techniques, including image segmentation and monocular
depth estimation, to obtain object shapes and use these as priors for our novel
specular beamforming algorithm. We also design this algorithm to work in
cluttered environments with weak reflections and in partially occluded
scenarios. We perform a detailed evaluation of Metamoran's depth imaging and
sensing capabilities in 200 diverse scenes at a major U.S. city. Our evaluation
shows that Metamoran estimates the depth of an object up to 60~m away with a
median error of 28~cm, an improvement of 13$\times$ compared to a naive
radar+camera baseline and 23$\times$ compared to monocular depth estimation.","cs.CV,cs.NI,cs.RO,eess.SP"
"Deep Learning for Material recognition: most recent advances and open challenges. Recognizing material from color images is still a challenging problem today.
While deep neural networks provide very good results on object recognition and
has been the topic of a huge amount of papers in the last decade, their
adaptation to material images still requires some works to reach equivalent
accuracies. Nevertheless, recent studies achieve very good results in material
recognition with deep learning and we propose, in this paper, to review most of
them by focusing on three aspects: material image datasets, influence of the
context and ad hoc descriptors for material appearance. Every aspect is
introduced by a systematic manner and results from representative works are
cited. We also present our own studies in this area and point out some open
challenges for future works.","cs.CV,physics.comp-ph"
"Tensor Networks for Probabilistic Sequence Modeling. Tensor networks are a powerful modeling framework developed for computational
many-body physics, which have only recently been applied within machine
learning. In this work we utilize a uniform matrix product state (u-MPS) model
for probabilistic modeling of sequence data. We first show that u-MPS enable
sequence-level parallelism, with length-n sequences able to be evaluated in
depth O(log n). We then introduce a novel generative algorithm giving trained
u-MPS the ability to efficiently sample from a wide variety of conditional
distributions, each one defined by a regular expression. Special cases of this
algorithm correspond to autoregressive and fill-in-the-blank sampling, but more
complex regular expressions permit the generation of richly structured data in
a manner that has no direct analogue in neural generative models. Experiments
on sequence modeling with synthetic and real text data show u-MPS outperforming
a variety of baselines and effectively generalizing their predictions in the
presence of limited data.","cs.LG,quant-ph,stat.ML"
"Approximate Cross-Validation for Structured Models. Many modern data analyses benefit from explicitly modeling dependence
structure in data -- such as measurements across time or space, ordered words
in a sentence, or genes in a genome. A gold standard evaluation technique is
structured cross-validation (CV), which leaves out some data subset (such as
data within a time interval or data in a geographic region) in each fold. But
CV here can be prohibitively slow due to the need to re-run already-expensive
learning algorithms many times. Previous work has shown approximate
cross-validation (ACV) methods provide a fast and provably accurate alternative
in the setting of empirical risk minimization. But this existing ACV work is
restricted to simpler models by the assumptions that (i) data across CV folds
are independent and (ii) an exact initial model fit is available. In structured
data analyses, both these assumptions are often untrue. In the present work, we
address (i) by extending ACV to CV schemes with dependence structure between
the folds. To address (ii), we verify -- both theoretically and empirically --
that ACV quality deteriorates smoothly with noise in the initial fit. We
demonstrate the accuracy and computational benefits of our proposed methods on
a diverse set of real-world applications.","cs.LG,stat.CO,stat.ME,stat.ML"
"Recurrent Deep Divergence-based Clustering for simultaneous feature learning and clustering of variable length time series. The task of clustering unlabeled time series and sequences entails a
particular set of challenges, namely to adequately model temporal relations and
variable sequence lengths. If these challenges are not properly handled, the
resulting clusters might be of suboptimal quality. As a key solution, we
present a joint clustering and feature learning framework for time series based
on deep learning. For a given set of time series, we train a recurrent network
to represent, or embed, each time series in a vector space such that a
divergence-based clustering loss function can discover the underlying cluster
structure in an end-to-end manner. Unlike previous approaches, our model
inherently handles multivariate time series of variable lengths and does not
require specification of a distance-measure in the input space. On a diverse
set of benchmark datasets we illustrate that our proposed Recurrent Deep
Divergence-based Clustering approach outperforms, or performs comparable to,
previous approaches.","cs.LG,stat.ML"
"AI Feynman 2.0: Pareto-optimal symbolic regression exploiting graph modularity. We present an improved method for symbolic regression that seeks to fit data
to formulas that are Pareto-optimal, in the sense of having the best accuracy
for a given complexity. It improves on the previous state-of-the-art by
typically being orders of magnitude more robust toward noise and bad data, and
also by discovering many formulas that stumped previous methods. We develop a
method for discovering generalized symmetries (arbitrary modularity in the
computational graph of a formula) from gradient properties of a neural network
fit. We use normalizing flows to generalize our symbolic regression method to
probability distributions from which we only have samples, and employ
statistical hypothesis testing to accelerate robust brute-force search.","cs.AI,cs.IT,cs.LG,math.IT,physics.comp-ph,stat.ML"
"What is Local Optimality in Nonconvex-Nonconcave Minimax Optimization?. Minimax optimization has found extensive applications in modern machine
learning, in settings such as generative adversarial networks (GANs),
adversarial training and multi-agent reinforcement learning. As most of these
applications involve continuous nonconvex-nonconcave formulations, a very basic
question arises---""what is a proper definition of local optima?""
  Most previous work answers this question using classical notions of
equilibria from simultaneous games, where the min-player and the max-player act
simultaneously. In contrast, most applications in machine learning, including
GANs and adversarial training, correspond to sequential games, where the order
of which player acts first is crucial (since minimax is in general not equal to
maximin due to the nonconvex-nonconcave nature of the problems). The main
contribution of this paper is to propose a proper mathematical definition of
local optimality for this sequential setting---local minimax, as well as to
present its properties and existence results. Finally, we establish a strong
connection to a basic local search algorithm---gradient descent ascent (GDA):
under mild conditions, all stable limit points of GDA are exactly local minimax
points up to some degenerate points.","cs.LG,math.OC,stat.ML"
"Modification method for single-stage object detectors that allows to exploit the temporal behaviour of a scene to improve detection accuracy. A simple modification method for single-stage generic object detection neural
networks, such as YOLO and SSD, is proposed, which allows for improving the
detection accuracy on video data by exploiting the temporal behavior of the
scene in the detection pipeline. It is shown that, using this method, the
detection accuracy of the base network can be considerably improved, especially
for occluded and hidden objects. It is shown that a modified network is more
prone to detect hidden objects with more confidence than an unmodified one. A
weakly supervised training method is proposed, which allows for training a
modified network without requiring any additional annotated data.","68T07,68T45,I.4.8; I.4.9; I.2.10,cs.CV"
"An End-to-End Neural Network for Polyphonic Piano Music Transcription. We present a supervised neural network model for polyphonic piano music
transcription. The architecture of the proposed model is analogous to speech
recognition systems and comprises an acoustic model and a music language model.
The acoustic model is a neural network used for estimating the probabilities of
pitches in a frame of audio. The language model is a recurrent neural network
that models the correlations between pitch combinations over time. The proposed
model is general and can be used to transcribe polyphonic music without
imposing any constraints on the polyphony. The acoustic and language model
predictions are combined using a probabilistic graphical model. Inference over
the output variables is performed using the beam search algorithm. We perform
two sets of experiments. We investigate various neural network architectures
for the acoustic models and also investigate the effect of combining acoustic
and music language model predictions using the proposed architecture. We
compare performance of the neural network based acoustic models with two
popular unsupervised acoustic models. Results show that convolutional neural
network acoustic models yields the best performance across all evaluation
metrics. We also observe improved performance with the application of the music
language models. Finally, we present an efficient variant of beam search that
improves performance and reduces run-times by an order of magnitude, making the
model suitable for real-time applications.","cs.LG,cs.SD,stat.ML"
"VMAV-C: A Deep Attention-based Reinforcement Learning Algorithm for Model-based Control. Recent breakthroughs in Go play and strategic games have witnessed the great
potential of reinforcement learning in intelligently scheduling in uncertain
environment, but some bottlenecks are also encountered when we generalize this
paradigm to universal complex tasks. Among them, the low efficiency of data
utilization in model-free reinforcement algorithms is of great concern. In
contrast, the model-based reinforcement learning algorithms can reveal
underlying dynamics in learning environments and seldom suffer the data
utilization problem. To address the problem, a model-based reinforcement
learning algorithm with attention mechanism embedded is proposed as an
extension of World Models in this paper. We learn the environment model through
Mixture Density Network Recurrent Network(MDN-RNN) for agents to interact, with
combinations of variational auto-encoder(VAE) and attention incorporated in
state value estimates during the process of learning policy. In this way, agent
can learn optimal policies through less interactions with actual environment,
and final experiments demonstrate the effectiveness of our model in control
problem.","cs.AI,cs.LG,cs.NE"
"Learning and Fast Adaptation for Grid Emergency Control via Deep Meta Reinforcement Learning. As power systems are undergoing a significant transformation with more
uncertainties, less inertia and closer to operation limits, there is increasing
risk of large outages. Thus, there is an imperative need to enhance grid
emergency control to maintain system reliability and security. Towards this
end, great progress has been made in developing deep reinforcement learning
(DRL) based grid control solutions in recent years. However, existing DRL-based
solutions have two main limitations: 1) they cannot handle well with a wide
range of grid operation conditions, system parameters, and contingencies; 2)
they generally lack the ability to fast adapt to new grid operation conditions,
system parameters, and contingencies, limiting their applicability for
real-world applications. In this paper, we mitigate these limitations by
developing a novel deep meta reinforcement learning (DMRL) algorithm. The DMRL
combines the meta strategy optimization together with DRL, and trains policies
modulated by a latent space that can quickly adapt to new scenarios. We test
the developed DMRL algorithm on the IEEE 300-bus system. We demonstrate fast
adaptation of the meta-trained DRL polices with latent variables to new
operating conditions and scenarios using the proposed method and achieve
superior performance compared to the state-of-the-art DRL and model predictive
control (MPC) methods.","cs.LG,cs.SY,eess.SY"
"Nonparametric Trace Regression in High Dimensions via Sign Series Representation. Learning of matrix-valued data has recently surged in a range of scientific
and business applications. Trace regression is a widely used method to model
effects of matrix predictors and has shown great success in matrix learning.
However, nearly all existing trace regression solutions rely on two
assumptions: (i) a known functional form of the conditional mean, and (ii) a
global low-rank structure in the entire range of the regression function, both
of which may be violated in practice. In this article, we relax these
assumptions by developing a general framework for nonparametric trace
regression models via structured sign series representations of high
dimensional functions. The new model embraces both linear and nonlinear trace
effects, and enjoys rank invariance to order-preserving transformations of the
response. In the context of matrix completion, our framework leads to a
substantially richer model based on what we coin as the ""sign rank"" of a
matrix. We show that the sign series can be statistically characterized by
weighted classification tasks. Based on this connection, we propose a learning
reduction approach to learn the regression model via a series of classifiers,
and develop a parallelable computation algorithm to implement sign series
aggregations. We establish the excess risk bounds, estimation error rates, and
sample complexities. Our proposal provides a broad nonparametric paradigm to
many important matrix learning problems, including matrix regression, matrix
completion, multi-task learning, and compressed sensing. We demonstrate the
advantages of our method through simulations and two applications, one on brain
connectivity study and the other on high-rank image completion.","cs.LG,math.ST,stat.ME,stat.ML,stat.TH"
"GAttANet: Global attention agreement for convolutional neural networks. Transformer attention architectures, similar to those developed for natural
language processing, have recently proved efficient also in vision, either in
conjunction with or as a replacement for convolutional layers. Typically,
visual attention is inserted in the network architecture as a (series of)
feedforward self-attention module(s), with mutual key-query agreement as the
main selection and routing operation. However efficient, this strategy is only
vaguely compatible with the way that attention is implemented in biological
brains: as a separate and unified network of attentional selection regions,
receiving inputs from and exerting modulatory influence on the entire hierarchy
of visual regions. Here, we report experiments with a simple such attention
system that can improve the performance of standard convolutional networks,
with relatively few additional parameters. Each spatial position in each layer
of the network produces a key-query vector pair; all queries are then pooled
into a global attention query. On the next iteration, the match between each
key and the global attention query modulates the network's activations --
emphasizing or silencing the locations that agree or disagree (respectively)
with the global attention system. We demonstrate the usefulness of this
brain-inspired Global Attention Agreement network (GAttANet) for various
convolutional backbones (from a simple 5-layer toy model to a standard ResNet50
architecture) and datasets (CIFAR10, CIFAR100, Imagenet-1k). Each time, our
global attention system improves accuracy over the corresponding baseline.","cs.CV,q-bio.NC"
"Shared Space Transfer Learning for analyzing multi-site fMRI data. Multi-voxel pattern analysis (MVPA) learns predictive models from task-based
functional magnetic resonance imaging (fMRI) data, for distinguishing when
subjects are performing different cognitive tasks -- e.g., watching movies or
making decisions. MVPA works best with a well-designed feature set and an
adequate sample size. However, most fMRI datasets are noisy, high-dimensional,
expensive to collect, and with small sample sizes. Further, training a robust,
generalized predictive model that can analyze homogeneous cognitive tasks
provided by multi-site fMRI datasets has additional challenges. This paper
proposes the Shared Space Transfer Learning (SSTL) as a novel transfer learning
(TL) approach that can functionally align homogeneous multi-site fMRI datasets,
and so improve the prediction performance in every site. SSTL first extracts a
set of common features for all subjects in each site. It then uses TL to map
these site-specific features to a site-independent shared space in order to
improve the performance of the MVPA. SSTL uses a scalable optimization
procedure that works effectively for high-dimensional fMRI datasets. The
optimization procedure extracts the common features for each site by using a
single-iteration algorithm and maps these site-specific common features to the
site-independent shared space. We evaluate the effectiveness of the proposed
method for transferring between various cognitive tasks. Our comprehensive
experiments validate that SSTL achieves superior performance to other
state-of-the-art analysis techniques.","cs.AI,cs.LG,eess.IV,math.FA,q-bio.NC"
"How Are Learned Perception-Based Controllers Impacted by the Limits of Robust Control?. The difficulty of optimal control problems has classically been characterized
in terms of system properties such as minimum eigenvalues of
controllability/observability gramians. We revisit these characterizations in
the context of the increasing popularity of data-driven techniques like
reinforcement learning (RL), and in control settings where input observations
are high-dimensional images and transition dynamics are unknown. Specifically,
we ask: to what extent are quantifiable control and perceptual difficulty
metrics of a task predictive of the performance and sample complexity of
data-driven controllers? We modulate two different types of partial
observability in a cartpole ""stick-balancing"" problem -- (i) the height of one
visible fixation point on the cartpole, which can be used to tune fundamental
limits of performance achievable by any controller, and by (ii) the level of
perception noise in the fixation point position inferred from depth or RGB
images of the cartpole. In these settings, we empirically study two popular
families of controllers: RL and system identification-based $H_\infty$ control,
using visually estimated system state. Our results show that the fundamental
limits of robust control have corresponding implications for the
sample-efficiency and performance of learned perception-based controllers.
Visit our project website https://jxu.ai/rl-vs-control-web for more
information.","cs.CV,cs.LG,cs.RO"
"A New Basis for Sparse Principal Component Analysis. Previous versions of sparse principal component analysis (PCA) have presumed
that the eigen-basis (a $p \times k$ matrix) is approximately sparse. We
propose a method that presumes the $p \times k$ matrix becomes approximately
sparse after a $k \times k$ rotation. The simplest version of the algorithm
initializes with the leading $k$ principal components. Then, the principal
components are rotated with an $k \times k$ orthogonal rotation to make them
approximately sparse. Finally, soft-thresholding is applied to the rotated
principal components. This approach differs from prior approaches because it
uses an orthogonal rotation to approximate a sparse basis. One consequence is
that a sparse component need not to be a leading eigenvector, but rather a
mixture of them. In this way, we propose a new (rotated) basis for sparse PCA.
In addition, our approach avoids ""deflation"" and multiple tuning parameters
required for that. Our sparse PCA framework is versatile; for example, it
extends naturally to a two-way analysis of a data matrix for simultaneous
dimensionality reduction of rows and columns. We provide evidence showing that
for the same level of sparsity, the proposed sparse PCA method is more stable
and can explain more variance compared to alternative methods. Through three
applications -- sparse coding of images, analysis of transcriptome sequencing
data, and large-scale clustering of social networks, we demonstrate the modern
usefulness of sparse PCA in exploring multivariate data.","cs.CV,cs.LG,stat.CO,stat.ME,stat.ML"
"Single Node Injection Attack against Graph Neural Networks. Node injection attack on Graph Neural Networks (GNNs) is an emerging and
practical attack scenario that the attacker injects malicious nodes rather than
modifying original nodes or edges to affect the performance of GNNs. However,
existing node injection attacks ignore extremely limited scenarios, namely the
injected nodes might be excessive such that they may be perceptible to the
target GNN. In this paper, we focus on an extremely limited scenario of single
node injection evasion attack, i.e., the attacker is only allowed to inject one
single node during the test phase to hurt GNN's performance. The discreteness
of network structure and the coupling effect between network structure and node
features bring great challenges to this extremely limited scenario. We first
propose an optimization-based method to explore the performance upper bound of
single node injection evasion attack. Experimental results show that 100%,
98.60%, and 94.98% nodes on three public datasets are successfully attacked
even when only injecting one node with one edge, confirming the feasibility of
single node injection evasion attack. However, such an optimization-based
method needs to be re-optimized for each attack, which is computationally
unbearable. To solve the dilemma, we further propose a Generalizable Node
Injection Attack model, namely G-NIA, to improve the attack efficiency while
ensuring the attack performance. Experiments are conducted across three
well-known GNNs. Our proposed G-NIA significantly outperforms state-of-the-art
baselines and is 500 times faster than the optimization-based method when
inferring.","cs.CR,cs.LG"
"Online Baum-Welch algorithm for Hierarchical Imitation Learning. The options framework for hierarchical reinforcement learning has increased
its popularity in recent years and has made improvements in tackling the
scalability problem in reinforcement learning. Yet, most of these recent
successes are linked with a proper options initialization or discovery. When an
expert is available, the options discovery problem can be addressed by learning
an options-type hierarchical policy directly from expert demonstrations. This
problem is referred to as hierarchical imitation learning and can be handled as
an inference problem in a Hidden Markov Model, which is done via an
Expectation-Maximization type algorithm. In this work, we propose a novel
online algorithm to perform hierarchical imitation learning in the options
framework. Further, we discuss the benefits of such an algorithm and compare it
with its batch version in classical reinforcement learning benchmarks. We show
that this approach works well in both discrete and continuous environments and,
under certain conditions, it outperforms the batch version.","cs.LG,cs.SY,eess.SY,stat.ML"
"Unsupervised Hierarchical Concept Learning. Discovering concepts (or temporal abstractions) in an unsupervised manner
from demonstration data in the absence of an environment is an important
problem. Organizing these discovered concepts hierarchically at different
levels of abstraction is useful in discovering patterns, building ontologies,
and generating tutorials from demonstration data. However, recent work to
discover such concepts without access to any environment does not discover
relationships (or a hierarchy) between these discovered concepts. In this
paper, we present a Transformer-based concept abstraction architecture UNHCLE
(pronounced uncle) that extracts a hierarchy of concepts in an unsupervised way
from demonstration data. We empirically demonstrate how UNHCLE discovers
meaningful hierarchies using datasets from Chess and Cooking domains. Finally,
we show how UNHCLE learns meaningful language labels for concepts by using
demonstration data augmented with natural language for cooking and chess. All
of our code is available at https://github.com/UNHCLE/UNHCLE","cs.AI,cs.CL,cs.LG"
"Deep Multimodal Learning: An Effective Method for Video Classification. Videos have become ubiquitous on the Internet. And video analysis can provide
lots of information for detecting and recognizing objects as well as help
people understand human actions and interactions with the real world. However,
facing data as huge as TB level, effective methods should be applied. Recurrent
neural network (RNN) architecture has wildly been used on many sequential
learning problems such as Language Model, Time-Series Analysis, etc. In this
paper, we propose some variations of RNN such as stacked bidirectional LSTM/GRU
network with attention mechanism to categorize large-scale video data. We also
explore different multimodal fusion methods. Our model combines both visual and
audio information on both video and frame level and received great result.
Ensemble methods are also applied. Because of its multimodal characteristics,
we decide to call this method Deep Multimodal Learning(DML). Our DML-based
model was trained on Google Cloud and our own server and was tested in a
well-known video classification competition on Kaggle held by Google.","cs.CV,cs.MM"
"ShapeVis: High-dimensional Data Visualization at Scale. We present ShapeVis, a scalable visualization technique for point cloud data
inspired from topological data analysis. Our method captures the underlying
geometric and topological structure of the data in a compressed graphical
representation. Much success has been reported by the data visualization
technique Mapper, that discreetly approximates the Reeb graph of a filter
function on the data. However, when using standard dimensionality reduction
algorithms as the filter function, Mapper suffers from considerable
computational cost. This makes it difficult to scale to high-dimensional data.
Our proposed technique relies on finding a subset of points called landmarks
along the data manifold to construct a weighted witness-graph over it. This
graph captures the structural characteristics of the point cloud, and its
weights are determined using a Finite Markov Chain. We further compress this
graph by applying induced maps from standard community detection algorithms.
Using techniques borrowed from manifold tearing, we prune and reinstate edges
in the induced graph based on their modularity to summarize the shape of data.
We empirically demonstrate how our technique captures the structural
characteristics of real and synthetic data sets. Further, we compare our
approach with Mapper using various filter functions like t-SNE, UMAP, LargeVis
and show that our algorithm scales to millions of data points while preserving
the quality of data visualization.","cs.HC,cs.LG,stat.ML"
"Real-time Locational Marginal Price Forecasting Using Generative Adversarial Network. In this paper, we propose a model-free unsupervised learning approach to
forecast real-time locational marginal prices (RTLMPs) in wholesale electricity
markets. By organizing system-wide hourly RTLMP data into a 3-dimensional (3D)
tensor consisting of a series of time-indexed matrices, we formulate the RTLMP
forecasting problem as a problem of generating the next matrix with forecasted
RTLMPs given the historical RTLMP tensor, and propose a generative adversarial
network (GAN) model to forecast RTLMPs. The proposed formulation preserves the
spatio-temporal correlations among system-wide RTLMPs in the format of
historical RTLMP tensor. The proposed GAN model learns the spatio-temporal
correlations using the historical RTLMP tensors and generate RTLMPs that are
statistically similar and temporally coherent to the historical RTLMP tensor.
The proposed approach forecasts system-wide RTLMPs using only publicly
available historical price data, without involving confidential information of
system model, such as system parameters, topology, or operating conditions. The
effectiveness of the proposed approach is verified through case studies using
historical RTLMP data in Southwest Power Pool (SPP).","cs.LG,eess.SP"
"Learning to Represent Programs with Graphs. Learning tasks on source code (i.e., formal languages) have been considered
recently, but most work has tried to transfer natural language methods and does
not capitalize on the unique opportunities offered by code's known syntax. For
example, long-range dependencies induced by using the same variable or function
in distant locations are often not considered. We propose to use graphs to
represent both the syntactic and semantic structure of code and use graph-based
deep learning methods to learn to reason over program structures.
  In this work, we present how to construct graphs from source code and how to
scale Gated Graph Neural Networks training to such large graphs. We evaluate
our method on two tasks: VarNaming, in which a network attempts to predict the
name of a variable given its usage, and VarMisuse, in which the network learns
to reason about selecting the correct variable that should be used at a given
program location. Our comparison to methods that use less structured program
representations shows the advantages of modeling known structure, and suggests
that our models learn to infer meaningful names and to solve the VarMisuse task
in many cases. Additionally, our testing showed that VarMisuse identifies a
number of bugs in mature open-source projects.","cs.AI,cs.LG,cs.PL,cs.SE"
"Introducing a Generative Adversarial Network Model for Lagrangian Trajectory Simulation. We introduce a generative adversarial network (GAN) model to simulate the
3-dimensional Lagrangian motion of particles trapped in the recirculation zone
of a buoyancy-opposed flame. The GAN model comprises a stochastic recurrent
neural network, serving as a generator, and a convoluted neural network,
serving as a discriminator. Adversarial training was performed to the point
where the best-trained discriminator failed to distinguish the ground truth
from the trajectory produced by the best-trained generator. The model
performance was then benchmarked against a statistical analysis performed on
both the simulated trajectories and the ground truth, with regard to the
accuracy and generalization criteria.","cs.CV,cs.LG,physics.data-an,stat.CO,stat.ML"
"Predicting Blood Pressure Response to Fluid Bolus Therapy Using Attention-Based Neural Networks for Clinical Interpretability. Determining whether hypotensive patients in intensive care units (ICUs)
should receive fluid bolus therapy (FBT) has been an extremely challenging task
for intensive care physicians as the corresponding increase in blood pressure
has been hard to predict. Our study utilized regression models and
attention-based recurrent neural network (RNN) algorithms and a multi-clinical
information system large-scale database to build models that can predict the
successful response to FBT among hypotensive patients in ICUs. We investigated
both time-aggregated modeling using logistic regression algorithms with
regularization and time-series modeling using the long short term memory
network (LSTM) and the gated recurrent units network (GRU) with the attention
mechanism for clinical interpretability. Among all modeling strategies, the
stacked LSTM with the attention mechanism yielded the most predictable model
with the highest accuracy of 0.852 and area under the curve (AUC) value of
0.925. The study results may help identify hypotensive patients in ICUs who
will have sufficient blood pressure recovery after FBT.","cs.LG,physics.med-ph,q-bio.QM,stat.ML"
"Global-Local Item Embedding for Temporal Set Prediction. Temporal set prediction is becoming increasingly important as many companies
employ recommender systems in their online businesses, e.g., personalized
purchase prediction of shopping baskets. While most previous techniques have
focused on leveraging a user's history, the study of combining it with others'
histories remains untapped potential. This paper proposes Global-Local Item
Embedding (GLOIE) that learns to utilize the temporal properties of sets across
whole users as well as within a user by coining the names as global and local
information to distinguish the two temporal patterns. GLOIE uses Variational
Autoencoder (VAE) and dynamic graph-based model to capture global and local
information and then applies attention to integrate resulting item embeddings.
Additionally, we propose to use Tweedie output for the decoder of VAE as it can
easily model zero-inflated and long-tailed distribution, which is more suitable
for several real-world data distributions than Gaussian or multinomial
counterparts. When evaluated on three public benchmarks, our algorithm
consistently outperforms previous state-of-the-art methods in most ranking
metrics.","cs.IR,cs.LG"
"BridgeNet: A Joint Learning Network of Depth Map Super-Resolution and Monocular Depth Estimation. Depth map super-resolution is a task with high practical application
requirements in the industry. Existing color-guided depth map super-resolution
methods usually necessitate an extra branch to extract high-frequency detail
information from RGB image to guide the low-resolution depth map
reconstruction. However, because there are still some differences between the
two modalities, direct information transmission in the feature dimension or
edge map dimension cannot achieve satisfactory result, and may even trigger
texture copying in areas where the structures of the RGB-D pair are
inconsistent. Inspired by the multi-task learning, we propose a joint learning
network of depth map super-resolution (DSR) and monocular depth estimation
(MDE) without introducing additional supervision labels. For the interaction of
two subnetworks, we adopt a differentiated guidance strategy and design two
bridges correspondingly. One is the high-frequency attention bridge (HABdg)
designed for the feature encoding process, which learns the high-frequency
information of the MDE task to guide the DSR task. The other is the content
guidance bridge (CGBdg) designed for the depth map reconstruction process,
which provides the content guidance learned from DSR task for MDE task. The
entire network architecture is highly portable and can provide a paradigm for
associating the DSR and MDE tasks. Extensive experiments on benchmark datasets
demonstrate that our method achieves competitive performance. Our code and
models are available at https://rmcong.github.io/proj_BridgeNet.html.",cs.CV
"Predictive Learning on Hidden Tree-Structured Ising Models. We provide high-probability sample complexity guarantees for exact structure
recovery and accurate predictive learning using noise-corrupted samples from an
acyclic (tree-shaped) graphical model. The hidden variables follow a
tree-structured Ising model distribution, whereas the observable variables are
generated by a binary symmetric channel taking the hidden variables as its
input (flipping each bit independently with some constant probability $q\in
[0,1/2)$). In the absence of noise, predictive learning on Ising models was
recently studied by Bresler and Karzand (2020); this paper quantifies how noise
in the hidden model impacts the tasks of structure recovery and marginal
distribution estimation by proving upper and lower bounds on the sample
complexity. Our results generalize state-of-the-art bounds reported in prior
work, and they exactly recover the noiseless case ($q=0$). In fact, for any
tree with $p$ vertices and probability of incorrect recovery $\delta>0$, the
sufficient number of samples remains logarithmic as in the noiseless case,
i.e., $\mathcal{O}(\log(p/\delta))$, while the dependence on $q$ is
$\mathcal{O}\big( 1/(1-2q)^{4} \big)$, for both aforementioned tasks. We also
present a new equivalent of Isserlis' Theorem for sign-valued tree-structured
distributions, yielding a new low-complexity algorithm for higher-order moment
estimation.","cs.IT,cs.LG,math.IT,math.ST,stat.ML,stat.TH"
"Meta-Inductive Node Classification across Graphs. Semi-supervised node classification on graphs is an important research
problem, with many real-world applications in information retrieval such as
content classification on a social network and query intent classification on
an e-commerce query graph. While traditional approaches are largely
transductive, recent graph neural networks (GNNs) integrate node features with
network structures, thus enabling inductive node classification models that can
be applied to new nodes or even new graphs in the same feature space. However,
inter-graph differences still exist across graphs within the same domain. Thus,
training just one global model (e.g., a state-of-the-art GNN) to handle all new
graphs, whilst ignoring the inter-graph differences, can lead to suboptimal
performance.
  In this paper, we study the problem of inductive node classification across
graphs. Unlike existing one-model-fits-all approaches, we propose a novel
meta-inductive framework called MI-GNN to customize the inductive model to each
graph under a meta-learning paradigm. That is, MI-GNN does not directly learn
an inductive model; it learns the general knowledge of how to train a model for
semi-supervised node classification on new graphs. To cope with the differences
across graphs, MI-GNN employs a dual adaptation mechanism at both the graph and
task levels. More specifically, we learn a graph prior to adapt for the
graph-level differences, and a task prior to adapt for the task-level
differences conditioned on a graph. Extensive experiments on five real-world
graph collections demonstrate the effectiveness of our proposed model.","cs.LG,cs.SI"
"Deep Fundamental Matrix Estimation without Correspondences. Estimating fundamental matrices is a classic problem in computer vision.
Traditional methods rely heavily on the correctness of estimated key-point
correspondences, which can be noisy and unreliable. As a result, it is
difficult for these methods to handle image pairs with large occlusion or
significantly different camera poses. In this paper, we propose novel neural
network architectures to estimate fundamental matrices in an end-to-end manner
without relying on point correspondences. New modules and layers are introduced
in order to preserve mathematical properties of the fundamental matrix as a
homogeneous rank-2 matrix with seven degrees of freedom. We analyze performance
of the proposed models using various metrics on the KITTI dataset, and show
that they achieve competitive performance with traditional methods without the
need for extracting correspondences.","cs.CG,cs.CV,cs.GR,cs.LG,stat.ML"
"PA-GAN: Progressive Attention Generative Adversarial Network for Facial Attribute Editing. Facial attribute editing aims to manipulate attributes on the human face,
e.g., adding a mustache or changing the hair color. Existing approaches suffer
from a serious compromise between correct attribute generation and preservation
of the other information such as identity and background, because they edit the
attributes in the imprecise area. To resolve this dilemma, we propose a
progressive attention GAN (PA-GAN) for facial attribute editing. In our
approach, the editing is progressively conducted from high to low feature level
while being constrained inside a proper attribute area by an attention mask at
each level. This manner prevents undesired modifications to the irrelevant
regions from the beginning, and then the network can focus more on correctly
generating the attributes within a proper boundary at each level. As a result,
our approach achieves correct attribute editing with irrelevant details much
better preserved compared with the state-of-the-arts. Codes are released at
https://github.com/LynnHo/PA-GAN-Tensorflow.",cs.CV
"ES-ENAS: Blackbox Optimization over Hybrid Spaces via Combinatorial and Continuous Evolution. We consider the problem of efficient blackbox optimization over a large
hybrid search space, consisting of a mixture of a high dimensional continuous
space and a complex combinatorial space. Such examples arise commonly in
evolutionary computation, but also more recently, neuroevolution and
architecture search for Reinforcement Learning (RL) policies. In this paper, we
introduce ES-ENAS, a simple joint optimization procedure by combining
Evolutionary Strategies (ES) and combinatorial optimization techniques in a
highly scalable and intuitive way, inspired by the \textit{one-shot} or
\textit{supernet} paradigm introduced in Efficient Neural Architecture Search
(ENAS). Our main insight is noticing that ES is already a highly distributed
algorithm involving hundreds of blackbox evaluations which can not only be used
for training neural network weights, but also for feedback to a combinatorial
optimizer. Through this relatively simple marriage between two different lines
of research, we are able to gain the best of both worlds, and empirically
demonstrate our approach by optimizing BBOB functions over hybrid spaces as
well as combinatorial neural network architectures via edge pruning and
quantization on popular RL benchmarks. Due to the modularity of the algorithm,
we also are able incorporate a wide variety of popular techniques ranging from
use of different continuous and combinatorial optimizers, as well as
constrained optimization.","cs.LG,cs.NE,cs.RO"
"Time Series (re)sampling using Generative Adversarial Networks. We propose a novel bootstrap procedure for dependent data based on Generative
Adversarial networks (GANs). We show that the dynamics of common stationary
time series processes can be learned by GANs and demonstrate that GANs trained
on a single sample path can be used to generate additional samples from the
process. We find that temporal convolutional neural networks provide a suitable
design for the generator and discriminator, and that convincing samples can be
generated on the basis of a vector of iid normal noise. We demonstrate the
finite sample properties of GAN sampling and the suggested bootstrap using
simulations where we compare the performance to circular block bootstrapping in
the case of resampling an AR(1) time series processes. We find that resampling
using the GAN can outperform circular block bootstrapping in terms of empirical
coverage.","cs.LG,econ.EM,stat.ME,stat.ML"
"Crosslink-Net: Double-branch Encoder Segmentation Network via Fusing Vertical and Horizontal Convolutions. Accurate image segmentation plays a crucial role in medical image analysis,
yet it faces great challenges of various shapes, diverse sizes, and blurry
boundaries. To address these difficulties, square kernel-based encoder-decoder
architecture has been proposed and widely used, but its performance remains
still unsatisfactory. To further cope with these challenges, we present a novel
double-branch encoder architecture. Our architecture is inspired by two
observations: 1) Since the discrimination of features learned via square
convolutional kernels needs to be further improved, we propose to utilize
non-square vertical and horizontal convolutional kernels in the double-branch
encoder, so features learned by the two branches can be expected to complement
each other. 2) Considering that spatial attention can help models to better
focus on the target region in a large-sized image, we develop an attention loss
to further emphasize the segmentation on small-sized targets. Together, the
above two schemes give rise to a novel double-branch encoder segmentation
framework for medical image segmentation, namely Crosslink-Net. The experiments
validate the effectiveness of our model on four datasets. The code is released
at https://github.com/Qianyu1226/Crosslink-Net.","68T07,I.4.6,cs.AI,cs.CV"
"A Unified View of Stochastic Hamiltonian Sampling. In this work, we revisit the theoretical properties of Hamiltonian stochastic
differential equations (SDEs) for Bayesian posterior sampling, and we study the
two types of errors that arise from numerical SDE simulation: the
discretization error and the error due to noisy gradient estimates in the
context of data subsampling. We consider overlooked results describing the
ergodic convergence rates of numerical integration schemes, and we produce a
novel analysis for the effect of mini-batches through the lens of differential
operator splitting. In our analysis, the stochastic component of the proposed
Hamiltonian SDE is decoupled from the gradient noise, for which we make no
normality assumptions. This allows us to derive interesting connections among
different sampling schemes, including the original Hamiltonian Monte Carlo
(HMC) algorithm, and explain their performance. We show that for a careful
selection of numerical integrators, both errors vanish at a rate
$\mathcal{O}(\eta^2)$, where $\eta$ is the integrator step size. Our
theoretical results are supported by an empirical study on a variety of
regression and classification tasks for Bayesian neural networks.","cs.LG,stat.CO"
"Extracting Incentives from Black-Box Decisions. An algorithmic decision-maker incentivizes people to act in certain ways to
receive better decisions. These incentives can dramatically influence subjects'
behaviors and lives, and it is important that both decision-makers and
decision-recipients have clarity on which actions are incentivized by the
chosen model. While for linear functions, the changes a subject is incentivized
to make may be clear, we prove that for many non-linear functions (e.g. neural
networks, random forests), classical methods for interpreting the behavior of
models (e.g. input gradients) provide poor advice to individuals on which
actions they should take. In this work, we propose a mathematical framework for
understanding algorithmic incentives as the challenge of solving a Markov
Decision Process, where the state includes the set of input features, and the
reward is a function of the model's output. We can then leverage the many
toolkits for solving MDPs (e.g. tree-based planning, reinforcement learning) to
identify the optimal actions each individual is incentivized to take to improve
their decision under a given model. We demonstrate the utility of our method by
estimating the maximally-incentivized actions in two real-world settings: a
recidivism risk predictor we train using ProPublica's COMPAS dataset, and an
online credit scoring tool published by the Fair Isaac Corporation (FICO).","cs.AI,cs.CY,cs.LG,stat.ML"
"Deep Reinforcement Learning for Intelligent Transportation Systems: A Survey. Latest technological improvements increased the quality of transportation.
New data-driven approaches bring out a new research direction for all
control-based systems, e.g., in transportation, robotics, IoT and power
systems. Combining data-driven applications with transportation systems plays a
key role in recent transportation applications. In this paper, the latest deep
reinforcement learning (RL) based traffic control applications are surveyed.
Specifically, traffic signal control (TSC) applications based on (deep) RL,
which have been studied extensively in the literature, are discussed in detail.
Different problem formulations, RL parameters, and simulation environments for
TSC are discussed comprehensively. In the literature, there are also several
autonomous driving applications studied with deep RL models. Our survey
extensively summarizes existing works in this field by categorizing them with
respect to application types, control models and studied algorithms. In the
end, we discuss the challenges and open questions regarding deep RL-based
transportation applications.","cs.LG,cs.MA,cs.SY,eess.SP,eess.SY,stat.ML"
"Learned Interpretable Residual Extragradient ISTA for Sparse Coding. Recently, the study on learned iterative shrinkage thresholding algorithm
(LISTA) has attracted increasing attentions. A large number of experiments as
well as some theories have proved the high efficiency of LISTA for solving
sparse coding problems. However, existing LISTA methods are all serial
connection. To address this issue, we propose a novel extragradient based LISTA
(ELISTA), which has a residual structure and theoretical guarantees. In
particular, our algorithm can also provide the interpretability for Res-Net to
a certain extent. From a theoretical perspective, we prove that our method
attains linear convergence. In practice, extensive empirical results verify the
advantages of our method.","cs.LG,math.OC,stat.CO"
"Taming the Noise in Reinforcement Learning via Soft Updates. Model-free reinforcement learning algorithms, such as Q-learning, perform
poorly in the early stages of learning in noisy environments, because much
effort is spent unlearning biased estimates of the state-action value function.
The bias results from selecting, among several noisy estimates, the apparent
optimum, which may actually be suboptimal. We propose G-learning, a new
off-policy learning algorithm that regularizes the value estimates by
penalizing deterministic policies in the beginning of the learning process. We
show that this method reduces the bias of the value-function estimation,
leading to faster convergence to the optimal value and the optimal policy.
Moreover, G-learning enables the natural incorporation of prior domain
knowledge, when available. The stochastic nature of G-learning also makes it
avoid some exploration costs, a property usually attributed only to on-policy
algorithms. We illustrate these ideas in several examples, where G-learning
results in significant improvements of the convergence rate and the cost of the
learning process.","cs.IT,cs.LG,math.IT"
"Debiased Off-Policy Evaluation for Recommendation Systems. Efficient methods to evaluate new algorithms are critical for improving
interactive bandit and reinforcement learning systems such as recommendation
systems. A/B tests are reliable, but are time- and money-consuming, and entail
a risk of failure. In this paper, we develop an alternative method, which
predicts the performance of algorithms given historical data that may have been
generated by a different algorithm. Our estimator has the property that its
prediction converges in probability to the true performance of a counterfactual
algorithm at a rate of $\sqrt{N}$, as the sample size $N$ increases. We also
show a correct way to estimate the variance of our prediction, thus allowing
the analyst to quantify the uncertainty in the prediction. These properties
hold even when the analyst does not know which among a large number of
potentially important state variables are actually important. We validate our
method by a simulation experiment about reinforcement learning. We finally
apply it to improve advertisement design by a major advertisement company. We
find that our method produces smaller mean squared errors than state-of-the-art
methods.","cs.AI,cs.LG,econ.EM,stat.ME,stat.ML"
"Generative chemical transformer: attention makes neural machine learn molecular geometric structures via text. Chemical formula is an artificial language that expresses molecules as text.
Neural machines that have learned chemical language can be used as a tool for
inverse molecular design. Here, we propose a neural machine that creates
molecules that meet some desired conditions based on a deep understanding of
chemical language (generative chemical Transformer, GCT). Attention-mechanism
in GCT allows a deeper understanding of molecular structures, beyond the
limitations of chemical language itself that cause semantic discontinuity, by
paying attention to characters sparsely. We investigate the significance of
language models to inverse molecular design problems by quantitatively
evaluating the quality of generated molecules. GCT generates highly realistic
chemical strings that satisfy both a chemical rule and grammars of a language.
Molecules parsed from generated strings simultaneously satisfy the multiple
target properties and are various for a single condition set. GCT generates de
novo molecules, and this is done in a short time that human experts cannot.
These advances will contribute to improving the quality of human life by
accelerating the process of desired material discovery.","cs.LG,physics.chem-ph"
"Sequential Recommendation with Relation-Aware Kernelized Self-Attention. Recent studies identified that sequential Recommendation is improved by the
attention mechanism. By following this development, we propose Relation-Aware
Kernelized Self-Attention (RKSA) adopting a self-attention mechanism of the
Transformer with augmentation of a probabilistic model. The original
self-attention of Transformer is a deterministic measure without
relation-awareness. Therefore, we introduce a latent space to the
self-attention, and the latent space models the recommendation context from
relation as a multivariate skew-normal distribution with a kernelized
covariance matrix from co-occurrences, item characteristics, and user
information. This work merges the self-attention of the Transformer and the
sequential recommendation by adding a probabilistic model of the recommendation
task specifics. We experimented RKSA over the benchmark datasets, and RKSA
shows significant improvements compared to the recent baseline models. Also,
RKSA were able to produce a latent space model that answers the reasons for
recommendation.","cs.IR,cs.LG,stat.ML"
"Vision: A Deep Learning Approach to provide walking assistance to the visually impaired. Blind people face a lot of problems in their daily routines. They have to
struggle a lot just to do their day-to-day chores. In this paper, we have
proposed a system with the objective to help the visually impaired by providing
audio aid guiding them to avoid obstacles, which will assist them to move in
their surroundings. Object Detection using YOLO will help them detect the
nearby objects and Depth Estimation using monocular vision will tell the
approximate distance of the detected objects from the user. Despite a higher
accuracy, stereo vision has many hardware constraints, which makes monocular
vision the preferred choice for this application.","68T45,cs.CV,cs.LG"
"SensitiveLoss: Improving Accuracy and Fairness of Face Representations with Discrimination-Aware Deep Learning. We propose a discrimination-aware learning method to improve both accuracy
and fairness of biased face recognition algorithms. The most popular face
recognition benchmarks assume a distribution of subjects without paying much
attention to their demographic attributes. In this work, we perform a
comprehensive discrimination-aware experimentation of deep learning-based face
recognition. We also propose a general formulation of algorithmic
discrimination with application to face biometrics. The experiments include
tree popular face recognition models and three public databases composed of
64,000 identities from different demographic groups characterized by gender and
ethnicity. We experimentally show that learning processes based on the most
used face databases have led to popular pre-trained deep face models that
present a strong algorithmic discrimination. We finally propose a
discrimination-aware learning method, Sensitive Loss, based on the popular
triplet loss function and a sensitive triplet generator. Our approach works as
an add-on to pre-trained networks and is used to improve their performance in
terms of average accuracy and fairness. The method shows results comparable to
state-of-the-art de-biasing networks and represents a step forward to prevent
discriminatory effects by automatic systems.","cs.CV,cs.CY"
"Challenging common interpretability assumptions in feature attribution explanations. As machine learning and algorithmic decision making systems are increasingly
being leveraged in high-stakes human-in-the-loop settings, there is a pressing
need to understand the rationale of their predictions. Researchers have
responded to this need with explainable AI (XAI), but often proclaim
interpretability axiomatically without evaluation. When these systems are
evaluated, they are often tested through offline simulations with proxy metrics
of interpretability (such as model complexity). We empirically evaluate the
veracity of three common interpretability assumptions through a large scale
human-subjects experiment with a simple ""placebo explanation"" control. We find
that feature attribution explanations provide marginal utility in our task for
a human decision maker and in certain cases result in worse decisions due to
cognitive and contextual confounders. This result challenges the assumed
universal benefit of applying these methods and we hope this work will
underscore the importance of human evaluation in XAI research. Supplemental
materials -- including anonymized data from the experiment, code to replicate
the study, an interactive demo of the experiment, and the models used in the
analysis -- can be found at: https://doi.pizza/challenging-xai.","J.4; I.5.1; K.4,cs.CY,cs.HC,cs.LG"
"Physics-Enforced Modeling for Insertion Loss of Transmission Lines by Deep Neural Networks. In this paper, we investigate data-driven parameterized modeling of insertion
loss for transmission lines with respect to design parameters. We first show
that direct application of neural networks can lead to non-physics models with
negative insertion loss. To mitigate this problem, we propose two deep learning
solutions. One solution is to add a regulation term, which represents the
passive condition, to the final loss function to enforce the negative quantity
of insertion loss. In the second method, a third-order polynomial expression is
defined first, which ensures positiveness, to approximate the insertion loss,
then DeepONet neural network structure, which was proposed recently for
function and system modeling, was employed to model the coefficients of
polynomials. The resulting neural network is applied to predict the
coefficients of the polynomial expression. The experimental results on an
open-sourced SI/PI database of a PCB design show that both methods can ensure
the positiveness for the insertion loss. Furthermore, both methods can achieve
similar prediction results, while the polynomial-based DeepONet method is
faster than DeepONet based method in training time.","cs.LG,cs.NA,math.NA"
"Reducing Communication in Graph Neural Network Training. Graph Neural Networks (GNNs) are powerful and flexible neural networks that
use the naturally sparse connectivity information of the data. GNNs represent
this connectivity as sparse matrices, which have lower arithmetic intensity and
thus higher communication costs compared to dense matrices, making GNNs harder
to scale to high concurrencies than convolutional or fully-connected neural
networks.
  We introduce a family of parallel algorithms for training GNNs and show that
they can asymptotically reduce communication compared to previous parallel GNN
training methods. We implement these algorithms, which are based on 1D, 1.5D,
2D, and 3D sparse-dense matrix multiplication, using torch.distributed on
GPU-equipped clusters. Our algorithms optimize communication across the full
GNN training pipeline. We train GNNs on over a hundred GPUs on multiple
datasets, including a protein network with over a billion edges.","cs.DC,cs.LG,stat.ML"
"Objects that Sound. In this paper our objectives are, first, networks that can embed audio and
visual inputs into a common space that is suitable for cross-modal retrieval;
and second, a network that can localize the object that sounds in an image,
given the audio signal. We achieve both these objectives by training from
unlabelled video using only audio-visual correspondence (AVC) as the objective
function. This is a form of cross-modal self-supervision from video.
  To this end, we design new network architectures that can be trained for
cross-modal retrieval and localizing the sound source in an image, by using the
AVC task. We make the following contributions: (i) show that audio and visual
embeddings can be learnt that enable both within-mode (e.g. audio-to-audio) and
between-mode retrieval; (ii) explore various architectures for the AVC task,
including those for the visual stream that ingest a single image, or multiple
images, or a single image and multi-frame optical flow; (iii) show that the
semantic object that sounds within an image can be localized (using only the
sound, no motion or flow information); and (iv) give a cautionary tale on how
to avoid undesirable shortcuts in the data preparation.","cs.CV,cs.LG,cs.MM,cs.SD,eess.AS"
"Complex Transformer: A Framework for Modeling Complex-Valued Sequence. While deep learning has received a surge of interest in a variety of fields
in recent years, major deep learning models barely use complex numbers.
However, speech, signal and audio data are naturally complex-valued after
Fourier Transform, and studies have shown a potentially richer representation
of complex nets. In this paper, we propose a Complex Transformer, which
incorporates the transformer model as a backbone for sequence modeling; we also
develop attention and encoder-decoder network operating for complex input. The
model achieves state-of-the-art performance on the MusicNet dataset and an
In-phase Quadrature (IQ) signal dataset.","cs.LG,cs.SD,eess.AS,stat.ML"
"Pedestrian Detection in Thermal Images using Saliency Maps. Thermal images are mainly used to detect the presence of people at night or
in bad lighting conditions, but perform poorly at daytime. To solve this
problem, most state-of-the-art techniques employ a fusion network that uses
features from paired thermal and color images. Instead, we propose to augment
thermal images with their saliency maps, to serve as an attention mechanism for
the pedestrian detector especially during daytime. We investigate how such an
approach results in improved performance for pedestrian detection using only
thermal images, eliminating the need for paired color images. For our
experiments, we train the Faster R-CNN for pedestrian detection and report the
added effect of saliency maps generated using static and deep methods (PiCA-Net
and R3-Net). Our best performing model results in an absolute reduction of miss
rate by 13.4% and 19.4% over the baseline in day and night images respectively.
We also annotate and release pixel level masks of pedestrians on a subset of
the KAIST Multispectral Pedestrian Detection dataset, which is a first publicly
available dataset for salient pedestrian detection.","68T45,I.2.10,cs.CV"
"On Function Approximation in Reinforcement Learning: Optimism in the Face of Large State Spaces. The classical theory of reinforcement learning (RL) has focused on tabular
and linear representations of value functions. Further progress hinges on
combining RL with modern function approximators such as kernel functions and
deep neural networks, and indeed there have been many empirical successes that
have exploited such combinations in large-scale applications. There are
profound challenges, however, in developing a theory to support this
enterprise, most notably the need to take into consideration the
exploration-exploitation tradeoff at the core of RL in conjunction with the
computational and statistical tradeoffs that arise in modern
function-approximation-based learning systems. We approach these challenges by
studying an optimistic modification of the least-squares value iteration
algorithm, in the context of the action-value function
  represented by a kernel function or an overparameterized neural network. We
establish both polynomial runtime complexity and polynomial sample complexity
for this algorithm, without additional assumptions on the data-generating
model. In particular, we prove that the algorithm incurs an
$\tilde{\mathcal{O}}(\delta_{\mathcal{F}} H^2 \sqrt{T})$ regret, where
$\delta_{\mathcal{F}}$ characterizes the intrinsic complexity of the function
class $\mathcal{F}$, $H$ is the length of each episode, and $T$ is the total
number of episodes. Our regret bounds are independent of the number of states,
a result which exhibits clearly the benefit of function approximation in RL.","cs.AI,cs.LG,math.OC,math.ST,stat.ML,stat.TH"
"Theoretical Foundations of t-SNE for Visualizing High-Dimensional Clustered Data. This study investigates the theoretical foundations of t-distributed
stochastic neighbor embedding (t-SNE), a popular nonlinear dimension reduction
and data visualization method. A novel theoretical framework for the analysis
of t-SNE based on the gradient descent approach is presented. For the early
exaggeration stage of t-SNE, we show its asymptotic equivalence to a power
iteration based on the underlying graph Laplacian, characterize its limiting
behavior, and uncover its deep connection to Laplacian spectral clustering, and
fundamental principles including early stopping as implicit regularization. The
results explain the intrinsic mechanism and the empirical benefits of such a
computational strategy. For the embedding stage of t-SNE, we characterize the
kinematics of the low-dimensional map throughout the iterations, and identify
an amplification phase, featuring the intercluster repulsion and the expansive
behavior of the low-dimensional map. The general theory explains the fast
convergence rate and the exceptional empirical performance of t-SNE for
visualizing clustered data, brings forth the interpretations of the t-SNE
output, and provides theoretical guidance for selecting tuning parameters in
various applications.","cs.LG,math.ST,stat.ML,stat.TH"
"WheaCha: A Method for Explaining the Predictions of Code Summarization Models. The last decade has witnessed a rapid advance in machine learning models.
While the black-box nature of these systems allows powerful predictions, it
cannot be directly explained, posing a threat to the continuing democratization
of machine learning technology.
  Tackling the challenge of model explainability, research has made significant
progress in demystifying the image classification models. In the same spirit of
these works, this paper studies code summarization models, particularly, given
an input program for which a model makes a prediction, our goal is to reveal
the key features that the model uses for predicting the label of the program.
We realize our approach in HouYi, which we use to evaluate four prominent code
summarization models: extreme summarizer, code2vec, code2seq, and sequence GNN.
Results show that all models base their predictions on syntactic and lexical
properties with little to none semantic implication. Based on this finding, we
present a novel approach to explaining the predictions of code summarization
models through the lens of training data.
  Our work opens up this exciting, new direction of studying what models have
learned from source code.","cs.LG,cs.PL,cs.SE"
"Routing Networks: Adaptive Selection of Non-linear Functions for Multi-Task Learning. Multi-task learning (MTL) with neural networks leverages commonalities in
tasks to improve performance, but often suffers from task interference which
reduces the benefits of transfer. To address this issue we introduce the
routing network paradigm, a novel neural network and training algorithm. A
routing network is a kind of self-organizing neural network consisting of two
components: a router and a set of one or more function blocks. A function block
may be any neural network - for example a fully-connected or a convolutional
layer. Given an input the router makes a routing decision, choosing a function
block to apply and passing the output back to the router recursively,
terminating when a fixed recursion depth is reached. In this way the routing
network dynamically composes different function blocks for each input. We
employ a collaborative multi-agent reinforcement learning (MARL) approach to
jointly train the router and function blocks. We evaluate our model against
cross-stitch networks and shared-layer baselines on multi-task settings of the
MNIST, mini-imagenet, and CIFAR-100 datasets. Our experiments demonstrate a
significant improvement in accuracy, with sharper convergence. In addition,
routing networks have nearly constant per-task training cost while cross-stitch
networks scale linearly with the number of tasks. On CIFAR-100 (20 tasks) we
obtain cross-stitch performance levels with an 85% reduction in training time.","cs.CV,cs.LG,cs.NE"
"Modular Deep Reinforcement Learning for Continuous Motion Planning with Temporal Logic. This paper investigates the motion planning of autonomous dynamical systems
modeled by Markov decision processes (MDP) with unknown transition
probabilities over continuous state and action spaces. Linear temporal logic
(LTL) is used to specify high-level tasks over infinite horizon, which can be
converted into a limit deterministic generalized B\""uchi automaton (LDGBA) with
several accepting sets. The novelty is to design an embedded product MDP
(EP-MDP) between the LDGBA and the MDP by incorporating a synchronous
tracking-frontier function to record unvisited accepting sets of the automaton,
and to facilitate the satisfaction of the accepting conditions. The proposed
LDGBA-based reward shaping and discounting schemes for the model-free
reinforcement learning (RL) only depend on the EP-MDP states and can overcome
the issues of sparse rewards. Rigorous analysis shows that any RL method that
optimizes the expected discounted return is guaranteed to find an optimal
policy whose traces maximize the satisfaction probability. A modular deep
deterministic policy gradient (DDPG) is then developed to generate such
policies over continuous state and action spaces. The performance of our
framework is evaluated via an array of OpenAI gym environments.","cs.AI,cs.FL,cs.LG,cs.LO"
"AN-GCN: An Anonymous Graph Convolutional Network Defense Against Edge-Perturbing Attack. Recent studies have revealed the vulnerability of graph convolutional
networks (GCNs) to edge-perturbing attacks, such as maliciously inserting or
deleting graph edges. However, a theoretical proof of such vulnerability
remains a big challenge, and effective defense schemes are still open issues.
In this paper, we first generalize the formulation of edge-perturbing attacks
and strictly prove the vulnerability of GCNs to such attacks in node
classification tasks. Following this, an anonymous graph convolutional network,
named AN-GCN, is proposed to counter against edge-perturbing attacks.
Specifically, we present a node localization theorem to demonstrate how the GCN
locates nodes during its training phase. In addition, we design a staggered
Gaussian noise based node position generator, and devise a spectral graph
convolution based discriminator in detecting the generated node positions.
Further, we give the optimization of the above generator and discriminator.
AN-GCN can classify nodes without taking their position as input. It is
demonstrated that the AN-GCN is secure against edge-perturbing attacks in node
classification tasks, as AN-GCN classifies nodes without the edge information
and thus makes it impossible for attackers to perturb edges anymore. Extensive
evaluations demonstrated the effectiveness of the general edge-perturbing
attack model in manipulating the classification results of the target nodes.
More importantly, the proposed AN-GCN can achieve 82.7% in node classification
accuracy without the edge-reading permission, which outperforms the
state-of-the-art GCN.","cs.CR,cs.LG"
"Regularized K-means through hard-thresholding. We study a framework of regularized $K$-means methods based on direct
penalization of the size of the cluster centers. Different penalization
strategies are considered and compared through simulation and theoretical
analysis. Based on the results, we propose HT $K$-means, which uses an $\ell_0$
penalty to induce sparsity in the variables. Different techniques for selecting
the tuning parameter are discussed and compared. The proposed method stacks up
favorably with the most popular regularized $K$-means methods in an extensive
simulation study. Finally, HT $K$-means is applied to several real data
examples. Graphical displays are presented and used in these examples to gain
more insight into the datasets.","cs.LG,stat.ME,stat.ML"
"A Neural Stochastic Volatility Model. In this paper, we show that the recent integration of statistical models with
deep recurrent neural networks provides a new way of formulating volatility
(the degree of variation of time series) models that have been widely used in
time series analysis and prediction in finance. The model comprises a pair of
complementary stochastic recurrent neural networks: the generative network
models the joint distribution of the stochastic volatility process; the
inference network approximates the conditional distribution of the latent
variables given the observables. Our focus here is on the formulation of
temporal dynamics of volatility over time under a stochastic recurrent neural
network framework. Experiments on real-world stock price datasets demonstrate
that the proposed model generates a better volatility estimation and prediction
that outperforms mainstream methods, e.g., deterministic models such as GARCH
and its variants, and stochastic models namely the MCMC-based model
\emph{stochvol} as well as the Gaussian process volatility model \emph{GPVol},
on average negative log-likelihood.","cs.CE,cs.LG,q-fin.ST,stat.ML"
"Sampling and Recovery of Graph Signals based on Graph Neural Networks. We propose interpretable graph neural networks for sampling and recovery of
graph signals, respectively. To take informative measurements, we propose a new
graph neural sampling module, which aims to select those vertices that
maximally express their corresponding neighborhoods. Such expressiveness can be
quantified by the mutual information between vertices' features and
neighborhoods' features, which are estimated via a graph neural network. To
reconstruct an original graph signal from the sampled measurements, we propose
a graph neural recovery module based on the algorithm-unrolling technique.
Compared to previous analytical sampling and recovery, the proposed methods are
able to flexibly learn a variety of graph signal models from data by leveraging
the learning ability of neural networks; compared to previous
neural-network-based sampling and recovery, the proposed methods are designed
through exploiting specific graph properties and provide interpretability. We
further design a new multiscale graph neural network, which is a trainable
multiscale graph filter bank and can handle various graph-related learning
tasks. The multiscale network leverages the proposed graph neural sampling and
recovery modules to achieve multiscale representations of a graph. In the
experiments, we illustrate the effects of the proposed graph neural sampling
and recovery modules and find that the modules can flexibly adapt to various
graph structures and graph signals. In the task of active-sampling-based
semi-supervised learning, the graph neural sampling module improves the
classification accuracy over 10% in Cora dataset. We further validate the
proposed multiscale graph neural network on several standard datasets for both
vertex and graph classification. The results show that our method consistently
improves the classification accuracies.","cs.LG,cs.SI,eess.SP"
"Neighborhood Features Help Detecting Non-Technical Losses in Big Data Sets. Electricity theft is a major problem around the world in both developed and
developing countries and may range up to 40% of the total electricity
distributed. More generally, electricity theft belongs to non-technical losses
(NTL), which are losses that occur during the distribution of electricity in
power grids. In this paper, we build features from the neighborhood of
customers. We first split the area in which the customers are located into
grids of different sizes. For each grid cell we then compute the proportion of
inspected customers and the proportion of NTL found among the inspected
customers. We then analyze the distributions of features generated and show why
they are useful to predict NTL. In addition, we compute features from the
consumption time series of customers. We also use master data features of
customers, such as their customer class and voltage of their connection. We
compute these features for a Big Data base of 31M meter readings, 700K
customers and 400K inspection results. We then use these features to train four
machine learning algorithms that are particularly suitable for Big Data sets
because of their parallelizable structure: logistic regression, k-nearest
neighbors, linear support vector machine and random forest. Using the
neighborhood features instead of only analyzing the time series has resulted in
appreciable results for Big Data sets for varying NTL proportions of 1%-90%.
This work can therefore be deployed to a wide range of different regions around
the world.","cs.AI,cs.LG"
"Rapidly and accurately estimating brain strain and strain rate across head impact types with transfer learning and data fusion. Brain strain and strain rate are effective in predicting traumatic brain
injury (TBI) caused by head impacts. However, state-of-the-art finite element
modeling (FEM) demands considerable computational time in the computation,
limiting its application in real-time TBI risk monitoring. To accelerate,
machine learning head models (MLHMs) were developed, and the model accuracy was
found to decrease when the training/test datasets were from different head
impacts types. However, the size of dataset for specific impact types may not
be enough for model training. To address the computational cost of FEM, the
limited strain rate prediction, and the generalizability of MLHMs to on-field
datasets, we propose data fusion and transfer learning to develop a series of
MLHMs to predict the maximum principal strain (MPS) and maximum principal
strain rate (MPSR). We trained and tested the MLHMs on 13,623 head impacts from
simulations, American football, mixed martial arts, car crash, and compared
against the models trained on only simulations or only on-field impacts. The
MLHMs developed with transfer learning are significantly more accurate in
estimating MPS and MPSR than other models, with a mean absolute error (MAE)
smaller than 0.03 in predicting MPS and smaller than 7 (1/s) in predicting MPSR
on all impact datasets. The MLHMs can be applied to various head impact types
for rapidly and accurately calculating brain strain and strain rate. Besides
the clinical applications in real-time brain strain and strain rate monitoring,
this model helps researchers estimate the brain strain and strain rate caused
by head impacts more efficiently than FEM.","cs.LG,physics.med-ph,q-bio.QM,q-bio.TO"
"Identifying nonlinear dynamical systems via generative recurrent neural networks with applications to fMRI. A major tenet in theoretical neuroscience is that cognitive and behavioral
processes are ultimately implemented in terms of the neural system dynamics.
Accordingly, a major aim for the analysis of neurophysiological measurements
should lie in the identification of the computational dynamics underlying task
processing. Here we advance a state space model (SSM) based on generative
piecewise-linear recurrent neural networks (PLRNN) to assess dynamics from
neuroimaging data. In contrast to many other nonlinear time series models which
have been proposed for reconstructing latent dynamics, our model is easily
interpretable in neural terms, amenable to systematic dynamical systems
analysis of the resulting set of equations, and can straightforwardly be
transformed into an equivalent continuous-time dynamical system. The major
contributions of this paper are the introduction of a new observation model
suitable for functional magnetic resonance imaging (fMRI) coupled to the latent
PLRNN, an efficient stepwise training procedure that forces the latent model to
capture the 'true' underlying dynamics rather than just fitting (or predicting)
the observations, and of an empirical measure based on the Kullback-Leibler
divergence to evaluate from empirical time series how well this goal of
approximating the underlying dynamics has been achieved. We validate and
illustrate the power of our approach on simulated 'ground-truth' dynamical
(benchmark) systems as well as on actual experimental fMRI time series, and
demonstrate that the latent dynamics harbors task-related nonlinear structure
that a linear dynamical model fails to capture. Given that fMRI is one of the
most common techniques for measuring brain activity non-invasively in human
subjects, this approach may provide a novel step toward analyzing aberrant
(nonlinear) dynamics for clinical assessment or neuroscientific research.","cs.LG,q-bio.NC,stat.ML"
"Variational Inference for Continuous-Time Switching Dynamical Systems. Switching dynamical systems provide a powerful, interpretable modeling
framework for inference in time-series data in, e.g., the natural sciences or
engineering applications. Since many areas, such as biology or discrete-event
systems, are naturally described in continuous time, we present a model based
on an Markov jump process modulating a subordinated diffusion process. We
provide the exact evolution equations for the prior and posterior marginal
densities, the direct solutions of which are however computationally
intractable. Therefore, we develop a new continuous-time variational inference
algorithm, combining a Gaussian process approximation on the diffusion level
with posterior inference for Markov jump processes. By minimizing the path-wise
Kullback-Leibler divergence we obtain (i) Bayesian latent state estimates for
arbitrary points on the real axis and (ii) point estimates of unknown system
parameters, utilizing variational expectation maximization. We extensively
evaluate our algorithm under the model assumption and for real-world examples.","cs.LG,cs.SY,eess.SY,q-bio.QM,stat.ML"
"Minimax Defense against Gradient-based Adversarial Attacks. State-of-the-art adversarial attacks are aimed at neural network classifiers.
By default, neural networks use gradient descent to minimize their loss
function. The gradient of a classifier's loss function is used by
gradient-based adversarial attacks to generate adversarially perturbed images.
We pose the question whether another type of optimization could give neural
network classifiers an edge. Here, we introduce a novel approach that uses
minimax optimization to foil gradient-based adversarial attacks. Our minimax
classifier is the discriminator of a generative adversarial network (GAN) that
plays a minimax game with the GAN generator. In addition, our GAN generator
projects all points onto a manifold that is different from the original
manifold since the original manifold might be the cause of adversarial attacks.
To measure the performance of our minimax defense, we use adversarial attacks -
Carlini Wagner (CW), DeepFool, Fast Gradient Sign Method (FGSM) - on three
datasets: MNIST, CIFAR-10 and German Traffic Sign (TRAFFIC). Against CW
attacks, our minimax defense achieves 98.07% (MNIST-default 98.93%), 73.90%
(CIFAR-10-default 83.14%) and 94.54% (TRAFFIC-default 96.97%). Against DeepFool
attacks, our minimax defense achieves 98.87% (MNIST), 76.61% (CIFAR-10) and
94.57% (TRAFFIC). Against FGSM attacks, we achieve 97.01% (MNIST), 76.79%
(CIFAR-10) and 81.41% (TRAFFIC). Our Minimax adversarial approach presents a
significant shift in defense strategy for neural network classifiers.","cs.GT,cs.LG,stat.ML"
"Nested Invariance Pooling and RBM Hashing for Image Instance Retrieval. The goal of this work is the computation of very compact binary hashes for
image instance retrieval. Our approach has two novel contributions. The first
one is Nested Invariance Pooling (NIP), a method inspired from i-theory, a
mathematical theory for computing group invariant transformations with
feed-forward neural networks. NIP is able to produce compact and
well-performing descriptors with visual representations extracted from
convolutional neural networks. We specifically incorporate scale, translation
and rotation invariances but the scheme can be extended to any arbitrary sets
of transformations. We also show that using moments of increasing order
throughout nesting is important. The NIP descriptors are then hashed to the
target code size (32-256 bits) with a Restricted Boltzmann Machine with a novel
batch-level regularization scheme specifically designed for the purpose of
hashing (RBMH). A thorough empirical evaluation with state-of-the-art shows
that the results obtained both with the NIP descriptors and the NIP+RBMH hashes
are consistently outstanding across a wide range of datasets.","cs.CV,cs.IR"
"Multi-Decoder RNN Autoencoder Based on Variational Bayes Method. Clustering algorithms have wide applications and play an important role in
data analysis fields including time series data analysis. However, in time
series analysis, most of the algorithms used signal shape features or the
initial value of hidden variable of a neural network. Little has been discussed
on the methods based on the generative model of the time series. In this paper,
we propose a new clustering algorithm focusing on the generative process of the
signal with a recurrent neural network and the variational Bayes method. Our
experiments show that the proposed algorithm not only has a robustness against
for phase shift, amplitude and signal length variations but also provide a
flexible clustering based on the property of the variational Bayes method.","cs.LG,stat.AP,stat.ML"
"F2A2: Flexible Fully-decentralized Approximate Actor-critic for Cooperative Multi-agent Reinforcement Learning. Traditional centralized multi-agent reinforcement learning (MARL) algorithms
are sometimes unpractical in complicated applications, due to non-interactivity
between agents, curse of dimensionality and computation complexity. Hence,
several decentralized MARL algorithms are motivated. However, existing
decentralized methods only handle the fully cooperative setting where massive
information needs to be transmitted in training. The block coordinate gradient
descent scheme they used for successive independent actor and critic steps can
simplify the calculation, but it causes serious bias. In this paper, we propose
a flexible fully decentralized actor-critic MARL framework, which can combine
most of actor-critic methods, and handle large-scale general cooperative
multi-agent setting. A primal-dual hybrid gradient descent type algorithm
framework is designed to learn individual agents separately for
decentralization. From the perspective of each agent, policy improvement and
value evaluation are jointly optimized, which can stabilize multi-agent policy
learning. Furthermore, our framework can achieve scalability and stability for
large-scale environment and reduce information transmission, by the parameter
sharing mechanism and a novel modeling-other-agents methods based on
theory-of-mind and online supervised learning. Sufficient experiments in
cooperative Multi-agent Particle Environment and StarCraft II show that our
decentralized MARL instantiation algorithms perform competitively against
conventional centralized and decentralized methods.","cs.AI,cs.LG,cs.MA,stat.ML"
"MAgent: A Many-Agent Reinforcement Learning Platform for Artificial Collective Intelligence. We introduce MAgent, a platform to support research and development of
many-agent reinforcement learning. Unlike previous research platforms on single
or multi-agent reinforcement learning, MAgent focuses on supporting the tasks
and the applications that require hundreds to millions of agents. Within the
interactions among a population of agents, it enables not only the study of
learning algorithms for agents' optimal polices, but more importantly, the
observation and understanding of individual agent's behaviors and social
phenomena emerging from the AI society, including communication languages,
leaderships, altruism. MAgent is highly scalable and can host up to one million
agents on a single GPU server. MAgent also provides flexible configurations for
AI researchers to design their customized environments and agents. In this
demo, we present three environments designed on MAgent and show emerged
collective intelligence by learning from scratch.","cs.AI,cs.LG,cs.MA"
"Node Masking: Making Graph Neural Networks Generalize and Scale Better. Graph Neural Networks (GNNs) have received a lot of interest in the recent
times. From the early spectral architectures that could only operate on
undirected graphs per a transductive learning paradigm to the current state of
the art spatial ones that can apply inductively to arbitrary graphs, GNNs have
seen significant contributions from the research community. In this paper, we
utilize some theoretical tools to better visualize the operations performed by
state of the art spatial GNNs. We analyze the inner workings of these
architectures and introduce a simple concept, Node Masking, that allows them to
generalize and scale better. To empirically validate the concept, we perform
several experiments on some widely-used datasets for node classification in
both the transductive and inductive settings, hence laying down strong
benchmarks for future research.","cs.AI,cs.LG,stat.ML"
"Inverse-Dirichlet Weighting Enables Reliable Training of Physics Informed Neural Networks. We characterize and remedy a failure mode that may arise from multi-scale
dynamics with scale imbalances during training of deep neural networks, such as
Physics Informed Neural Networks (PINNs). PINNs are popular machine-learning
templates that allow for seamless integration of physical equation models with
data. Their training amounts to solving an optimization problem over a weighted
sum of data-fidelity and equation-fidelity objectives. Conflicts between
objectives can arise from scale imbalances, heteroscedasticity in the data,
stiffness of the physical equation, or from catastrophic interference during
sequential training. We explain the training pathology arising from this and
propose a simple yet effective inverse-Dirichlet weighting strategy to
alleviate the issue. We compare with Sobolev training of neural networks,
providing the baseline of analytically $\boldsymbol{\epsilon}$-optimal
training. We demonstrate the effectiveness of inverse-Dirichlet weighting in
various applications, including a multi-scale model of active turbulence, where
we show orders of magnitude improvement in accuracy and convergence over
conventional PINN training. For inverse modeling using sequential training, we
find that inverse-Dirichlet weighting protects a PINN against catastrophic
forgetting.","cs.LG,cs.NA,math.NA,physics.comp-ph,q-bio.QM"
"Compositional Learning of Image-Text Query for Image Retrieval. In this paper, we investigate the problem of retrieving images from a
database based on a multi-modal (image-text) query. Specifically, the query
text prompts some modification in the query image and the task is to retrieve
images with the desired modifications. For instance, a user of an E-Commerce
platform is interested in buying a dress, which should look similar to her
friend's dress, but the dress should be of white color with a ribbon sash. In
this case, we would like the algorithm to retrieve some dresses with desired
modifications in the query dress. We propose an autoencoder based model,
ComposeAE, to learn the composition of image and text query for retrieving
images. We adopt a deep metric learning approach and learn a metric that pushes
composition of source image and text query closer to the target images. We also
propose a rotational symmetry constraint on the optimization problem. Our
approach is able to outperform the state-of-the-art method TIRG \cite{TIRG} on
three benchmark datasets, namely: MIT-States, Fashion200k and Fashion IQ. In
order to ensure fair comparison, we introduce strong baselines by enhancing
TIRG method. To ensure reproducibility of the results, we publish our code
here: \url{https://github.com/ecom-research/ComposeAE}.","cs.CV,cs.IR"
"Copy mechanism and tailored training for character-based data-to-text generation. In the last few years, many different methods have been focusing on using
deep recurrent neural networks for natural language generation. The most widely
used sequence-to-sequence neural methods are word-based: as such, they need a
pre-processing step called delexicalization (conversely, relexicalization) to
deal with uncommon or unknown words. These forms of processing, however, give
rise to models that depend on the vocabulary used and are not completely
neural.
  In this work, we present an end-to-end sequence-to-sequence model with
attention mechanism which reads and generates at a character level, no longer
requiring delexicalization, tokenization, nor even lowercasing. Moreover, since
characters constitute the common ""building blocks"" of every text, it also
allows a more general approach to text generation, enabling the possibility to
exploit transfer learning for training. These skills are obtained thanks to two
major features: (i) the possibility to alternate between the standard
generation mechanism and a copy one, which allows to directly copy input facts
to produce outputs, and (ii) the use of an original training pipeline that
further improves the quality of the generated texts.
  We also introduce a new dataset called E2E+, designed to highlight the
copying capabilities of character-based models, that is a modified version of
the well-known E2E dataset used in the E2E Challenge. We tested our model
according to five broadly accepted metrics (including the widely used BLEU),
showing that it yields competitive performance with respect to both
character-based and word-based approaches.","68T50,cs.CL,cs.LG,cs.NE,stat.ML"
"Time-series Scenario Forecasting. Many applications require the ability to judge uncertainty of time-series
forecasts. Uncertainty is often specified as point-wise error bars around a
mean or median forecast. Due to temporal dependencies, such a method obscures
some information. We would ideally have a way to query the posterior
probability of the entire time-series given the predictive variables, or at a
minimum, be able to draw samples from this distribution. We use a Bayesian
dictionary learning algorithm to statistically generate an ensemble of
forecasts. We show that the algorithm performs as well as a physics-based
ensemble method for temperature forecasts for Houston. We conclude that the
method shows promise for scenario forecasting where physics-based methods are
absent.","cs.LG,stat.AP,stat.ML"
"FinGAT: Financial Graph Attention Networks for Recommending Top-K Profitable Stocks. Financial technology (FinTech) has drawn much attention among investors and
companies. While conventional stock analysis in FinTech targets at predicting
stock prices, less effort is made for profitable stock recommendation. Besides,
in existing approaches on modeling time series of stock prices, the
relationships among stocks and sectors (i.e., categories of stocks) are either
neglected or pre-defined. Ignoring stock relationships will miss the
information shared between stocks while using pre-defined relationships cannot
depict the latent interactions or influence of stock prices between stocks. In
this work, we aim at recommending the top-K profitable stocks in terms of
return ratio using time series of stock prices and sector information. We
propose a novel deep learning-based model, Financial Graph Attention Networks
(FinGAT), to tackle the task under the setting that no pre-defined
relationships between stocks are given. The idea of FinGAT is three-fold.
First, we devise a hierarchical learning component to learn short-term and
long-term sequential patterns from stock time series. Second, a fully-connected
graph between stocks and a fully-connected graph between sectors are
constructed, along with graph attention networks, to learn the latent
interactions among stocks and sectors. Third, a multi-task objective is devised
to jointly recommend the profitable stocks and predict the stock movement.
Experiments conducted on Taiwan Stock, S&P 500, and NASDAQ datasets exhibit
remarkable recommendation performance of our FinGAT, comparing to
state-of-the-art methods.","cs.CE,cs.IR,cs.LG,cs.SI"
"Directed Exploration for Reinforcement Learning. Efficient exploration is necessary to achieve good sample efficiency for
reinforcement learning in general. From small, tabular settings such as
gridworlds to large, continuous and sparse reward settings such as robotic
object manipulation tasks, exploration through adding an uncertainty bonus to
the reward function has been shown to be effective when the uncertainty is able
to accurately drive exploration towards promising states. However reward
bonuses can still be inefficient since they are non-stationary, which means
that we must wait for function approximators to catch up and converge again
when uncertainties change. We propose the idea of directed exploration, that is
learning a goal-conditioned policy where goals are simply other states, and
using that to directly try to reach states with large uncertainty. The
goal-conditioned policy is independent of uncertainty and is thus stationary.
We show in our experiments how directed exploration is more efficient at
exploration and more robust to how the uncertainty is computed than adding
bonuses to rewards.","cs.AI,cs.LG,stat.ML"
"Structure and Performance of Fully Connected Neural Networks: Emerging Complex Network Properties. Understanding the behavior of Artificial Neural Networks is one of the main
topics in the field recently, as black-box approaches have become usual since
the widespread of deep learning. Such high-dimensional models may manifest
instabilities and weird properties that resemble complex systems. Therefore, we
propose Complex Network (CN) techniques to analyze the structure and
performance of fully connected neural networks. For that, we build a dataset
with 4 thousand models and their respective CN properties. They are employed in
a supervised classification setup considering four vision benchmarks. Each
neural network is approached as a weighted and undirected graph of neurons and
synapses, and centrality measures are computed after training. Results show
that these measures are highly related to the network classification
performance. We also propose the concept of Bag-Of-Neurons (BoN), a CN-based
approach for finding topological signatures linking similar neurons. Results
suggest that six neuronal types emerge in such networks, independently of the
target domain, and are distributed differently according to classification
accuracy. We also tackle specific CN properties related to performance, such as
higher subgraph centrality on lower-performing models. Our findings suggest
that CN properties play a critical role in the performance of fully connected
neural networks, with topological patterns emerging independently on a wide
range of models.","68T07,I.2.6; I.5.4; J.2,cs.AI,cs.CV,cs.LG,physics.app-ph,physics.comp-ph"
"CM3: Cooperative Multi-goal Multi-stage Multi-agent Reinforcement Learning. A variety of cooperative multi-agent control problems require agents to
achieve individual goals while contributing to collective success. This
multi-goal multi-agent setting poses difficulties for recent algorithms, which
primarily target settings with a single global reward, due to two new
challenges: efficient exploration for learning both individual goal attainment
and cooperation for others' success, and credit-assignment for interactions
between actions and goals of different agents. To address both challenges, we
restructure the problem into a novel two-stage curriculum, in which
single-agent goal attainment is learned prior to learning multi-agent
cooperation, and we derive a new multi-goal multi-agent policy gradient with a
credit function for localized credit assignment. We use a function augmentation
scheme to bridge value and policy functions across the curriculum. The complete
architecture, called CM3, learns significantly faster than direct adaptations
of existing algorithms on three challenging multi-goal multi-agent problems:
cooperative navigation in difficult formations, negotiating multi-vehicle lane
changes in the SUMO traffic simulator, and strategic cooperation in a Checkers
environment.","cs.LG,cs.MA,stat.ML"
"XGBOD: Improving Supervised Outlier Detection with Unsupervised Representation Learning. A new semi-supervised ensemble algorithm called XGBOD (Extreme Gradient
Boosting Outlier Detection) is proposed, described and demonstrated for the
enhanced detection of outliers from normal observations in various practical
datasets. The proposed framework combines the strengths of both supervised and
unsupervised machine learning methods by creating a hybrid approach that
exploits each of their individual performance capabilities in outlier
detection. XGBOD uses multiple unsupervised outlier mining algorithms to
extract useful representations from the underlying data that augment the
predictive capabilities of an embedded supervised classifier on an improved
feature space. The novel approach is shown to provide superior performance in
comparison to competing individual detectors, the full ensemble and two
existing representation learning based algorithms across seven outlier
datasets.","cs.DB,cs.IR,cs.LG,stat.ML"
"Exploiting Local Geometry for Feature and Graph Construction for Better 3D Point Cloud Processing with Graph Neural Networks. We propose simple yet effective improvements in point representations and
local neighborhood graph construction within the general framework of graph
neural networks (GNNs) for 3D point cloud processing. As a first contribution,
we propose to augment the vertex representations with important local geometric
information of the points, followed by nonlinear projection using a MLP. As a
second contribution, we propose to improve the graph construction for GNNs for
3D point clouds. The existing methods work with a k-nn based approach for
constructing the local neighborhood graph. We argue that it might lead to
reduction in coverage in case of dense sampling by sensors in some regions of
the scene. The proposed methods aims to counter such problems and improve
coverage in such cases. As the traditional GNNs were designed to work with
general graphs, where vertices may have no geometric interpretations, we see
both our proposals as augmenting the general graphs to incorporate the
geometric nature of 3D point clouds. While being simple, we demonstrate with
multiple challenging benchmarks, with relatively clean CAD models, as well as
with real world noisy scans, that the proposed method achieves state of the art
results on benchmarks for 3D classification (ModelNet40) , part segmentation
(ShapeNet) and semantic segmentation (Stanford 3D Indoor Scenes Dataset). We
also show that the proposed network achieves faster training convergence, i.e.
~40% less epochs for classification. The project details are available at
https://siddharthsrivastava.github.io/publication/geomgcnn/","cs.CV,cs.RO"
"Learning to Extend Program Graphs to Work-in-Progress Code. Source code spends most of its time in a broken or incomplete state during
software development. This presents a challenge to machine learning for code,
since high-performing models typically rely on graph structured representations
of programs derived from traditional program analyses. Such analyses may be
undefined for broken or incomplete code. We extend the notion of program graphs
to work-in-progress code by learning to predict edge relations between tokens,
training on well-formed code before transferring to work-in-progress code. We
consider the tasks of code completion and localizing and repairing variable
misuse in a work-in-process scenario. We demonstrate that training
relation-aware models with fine-tuned edges consistently leads to improved
performance on both tasks.","cs.LG,cs.SE"
"Reducing Artificial Neural Network Complexity: A Case Study on Exoplanet Detection. Despite their successes in the field of self-learning AI, Convolutional
Neural Networks (CNNs) suffer from having too many trainable parameters,
impacting computational performance. Several approaches have been proposed to
reduce the number of parameters in the visual domain, the Inception
architecture [Szegedy et al., 2016] being a prominent example. This raises the
question whether the number of trainable parameters in CNNs can also be reduced
for 1D inputs, such as time-series data, without incurring a substantial loss
in classification performance. We propose and examine two methods for
complexity reduction in AstroNet [Shallue & Vanderburg, 2018], a CNN for
automatic classification of time-varying brightness data of stars to detect
exoplanets. The first method makes only a tactical reduction of layers in
AstroNet while the second method also modifies the original input data by means
of a Gaussian pyramid. We conducted our experiments with various degrees of
dropout regularization. Our results show only a non-substantial loss in
accuracy compared to the original AstroNet, while reducing training time up to
85 percent. These results show potential for similar reductions in other CNN
applications while largely retaining accuracy.","I.2.6,cs.LG,stat.ML"
"DistDGL: Distributed Graph Neural Network Training for Billion-Scale Graphs. Graph neural networks (GNN) have shown great success in learning from
graph-structured data. They are widely used in various applications, such as
recommendation, fraud detection, and search. In these domains, the graphs are
typically large, containing hundreds of millions of nodes and several billions
of edges. To tackle this challenge, we develop DistDGL, a system for training
GNNs in a mini-batch fashion on a cluster of machines. DistDGL is based on the
Deep Graph Library (DGL), a popular GNN development framework. DistDGL
distributes the graph and its associated data (initial features and embeddings)
across the machines and uses this distribution to derive a computational
decomposition by following an owner-compute rule. DistDGL follows a synchronous
training approach and allows ego-networks forming the mini-batches to include
non-local nodes. To minimize the overheads associated with distributed
computations, DistDGL uses a high-quality and light-weight min-cut graph
partitioning algorithm along with multiple balancing constraints. This allows
it to reduce communication overheads and statically balance the computations.
It further reduces the communication by replicating halo nodes and by using
sparse embedding updates. The combination of these design choices allows
DistDGL to train high-quality models while achieving high parallel efficiency
and memory scalability. We demonstrate our optimizations on both inductive and
transductive GNN models. Our results show that DistDGL achieves linear speedup
without compromising model accuracy and requires only 13 seconds to complete a
training epoch for a graph with 100 million nodes and 3 billion edges on a
cluster with 16 machines. DistDGL is now publicly available as part of
DGL:https://github.com/dmlc/dgl/tree/master/python/dgl/distributed.","cs.DC,cs.LG"
"Information Theoretic Feature Transformation Learning for Brain Interfaces. Objective: A variety of pattern analysis techniques for model training in
brain interfaces exploit neural feature dimensionality reduction based on
feature ranking and selection heuristics. In the light of broad evidence
demonstrating the potential sub-optimality of ranking based feature selection
by any criterion, we propose to extend this focus with an information theoretic
learning driven feature transformation concept. Methods: We present a maximum
mutual information linear transformation (MMI-LinT), and a nonlinear
transformation (MMI-NonLinT) framework derived by a general definition of the
feature transformation learning problem. Empirical assessments are performed
based on electroencephalographic (EEG) data recorded during a four class motor
imagery brain-computer interface (BCI) task. Exploiting state-of-the-art
methods for initial feature vector construction, we compare the proposed
approaches with conventional feature selection based dimensionality reduction
techniques which are widely used in brain interfaces. Furthermore, for the
multi-class problem, we present and exploit a hierarchical graphical model
based BCI decoding system. Results: Both binary and multi-class decoding
analyses demonstrate significantly better performances with the proposed
methods. Conclusion: Information theoretic feature transformations are capable
of tackling potential confounders of conventional approaches in various
settings. Significance: We argue that this concept provides significant
insights to extend the focus on feature selection heuristics to a broader
definition of feature transformation learning in brain interfaces.","cs.HC,cs.IT,cs.LG,eess.SP,math.IT,stat.ML"
"Behavioral Model Inference of Black-box Software using Deep Neural Networks. Many software engineering tasks, such as testing, and anomaly detection can
benefit from the ability to infer a behavioral model of the software.Most
existing inference approaches assume access to code to collect execution
sequences. In this paper, we investigate a black-box scenario, where the system
under analysis cannot be instrumented, in this granular fashion.This scenario
is particularly prevalent with control systems' log analysis in the form of
continuous signals. In this situation, an execution trace amounts to a
multivariate time-series of input and output signals, where different states of
the system correspond to different `phases` in the time-series. The main
challenge is to detect when these phase changes take place. Unfortunately, most
existing solutions are either univariate, make assumptions on the data
distribution, or have limited learning power.Therefore, we propose a hybrid
deep neural network that accepts as input a multivariate time series and
applies a set of convolutional and recurrent layers to learn the non-linear
correlations between signals and the patterns over time.We show how this
approach can be used to accurately detect state changes, and how the inferred
models can be successfully applied to transfer-learning scenarios, to
accurately process traces from different products with similar execution
characteristics. Our experimental results on two UAV autopilot case studies
indicate that our approach is highly accurate (over 90% F1 score for state
classification) and significantly improves baselines (by up to 102% for change
point detection).Using transfer learning we also show that up to 90% of the
maximum achievable F1 scores in the open-source case study can be achieved by
reusing the trained models from the industrial case and only fine tuning them
using as low as 5 labeled samples, which reduces the manual labeling effort by
98%.","cs.LG,cs.SE"
"Remaining Useful Life Estimation Under Uncertainty with Causal GraphNets. In this work, a novel approach for the construction and training of time
series models is presented that deals with the problem of learning on large
time series with non-equispaced observations, which at the same time may
possess features of interest that span multiple scales. The proposed method is
appropriate for constructing predictive models for non-stationary stochastic
time series.The efficacy of the method is demonstrated on a simulated
stochastic degradation dataset and on a real-world accelerated life testing
dataset for ball-bearings. The proposed method, which is based on GraphNets,
implicitly learns a model that describes the evolution of the system at the
level of a state-vector rather than of a raw observation. The proposed approach
is compared to a recurrent network with a temporal convolutional feature
extractor head (RNN-tCNN) which forms a known viable alternative for the
problem context considered. Finally, by taking advantage of recent advances in
the computation of reparametrization gradients for learning probability
distributions, a simple yet effective technique for representing prediction
uncertainty as a Gamma distribution over remaining useful life predictions is
employed.","J.2,cs.LG,stat.AP"
"Explainable Machine Learning-driven Strategy for Automated Trading Pattern Extraction. Financial markets are a source of non-stationary multidimensional time series
which has been drawing attention for decades. Each financial instrument has its
specific changing over time properties, making their analysis a complex task.
Improvement of understanding and development of methods for financial time
series analysis is essential for successful operation on financial markets. In
this study we propose a volume-based data pre-processing method for making
financial time series more suitable for machine learning pipelines. We use a
statistical approach for assessing the performance of the method. Namely, we
formally state the hypotheses, set up associated classification tasks, compute
effect sizes with confidence intervals, and run statistical tests to validate
the hypotheses. We additionally assess the trading performance of the proposed
method on historical data and compare it to a previously published approach.
Our analysis shows that the proposed volume-based method allows successful
classification of the financial time series patterns, and also leads to better
classification performance than a price action-based method, excelling
specifically on more liquid financial instruments. Finally, we propose an
approach for obtaining feature interactions directly from tree-based models on
example of CatBoost estimator, as well as formally assess the relatedness of
the proposed approach and SHAP feature interactions with a positive outcome.","cs.AI,cs.LG,cs.SY,eess.SY,q-fin.TR"
"A2D2: Audi Autonomous Driving Dataset. Research in machine learning, mobile robotics, and autonomous driving is
accelerated by the availability of high quality annotated data. To this end, we
release the Audi Autonomous Driving Dataset (A2D2). Our dataset consists of
simultaneously recorded images and 3D point clouds, together with 3D bounding
boxes, semantic segmentation, instance segmentation, and data extracted from
the automotive bus. Our sensor suite consists of six cameras and five LiDAR
units, providing full 360 degree coverage. The recorded data is time
synchronized and mutually registered. Annotations are for non-sequential
frames: 41,277 frames with semantic segmentation image and point cloud labels,
of which 12,497 frames also have 3D bounding box annotations for objects within
the field of view of the front camera. In addition, we provide 392,556
sequential frames of unannotated sensor data for recordings in three cities in
the south of Germany. These sequences contain several loops. Faces and vehicle
number plates are blurred due to GDPR legislation and to preserve anonymity.
A2D2 is made available under the CC BY-ND 4.0 license, permitting commercial
use subject to the terms of the license. Data and further information are
available at http://www.a2d2.audi.","cs.CV,cs.LG,eess.IV"
"Orthogonal Wasserstein GANs. Wasserstein-GANs have been introduced to address the deficiencies of
generative adversarial networks (GANs) regarding the problems of vanishing
gradients and mode collapse during the training, leading to improved
convergence behaviour and improved image quality. However, Wasserstein-GANs
require the discriminator to be Lipschitz continuous. In current
state-of-the-art Wasserstein-GANs this constraint is enforced via gradient norm
regularization. In this paper, we demonstrate that this regularization does not
encourage a broad distribution of spectral-values in the discriminator weights,
hence resulting in less fidelity in the learned distribution. We therefore
investigate the possibility of substituting this Lipschitz constraint with an
orthogonality constraint on the weight matrices. We compare three different
weight orthogonalization techniques with regards to their convergence
properties, their ability to ensure the Lipschitz condition and the achieved
quality of the learned distribution. In addition, we provide a comparison to
Wasserstein-GANs trained with current state-of-the-art methods, where we
demonstrate the potential of solely using orthogonality-based regularization.
In this context, we propose an improved training procedure for Wasserstein-GANs
which utilizes orthogonalization to further increase its generalization
capability. Finally, we provide a novel metric to evaluate the generalization
capabilities of the discriminators of different Wasserstein-GANs.","I.2.6,cs.LG,eess.IV,stat.ML"
"Multi-Grained Spatio-temporal Modeling for Lip-reading. Lip-reading aims to recognize speech content from videos via visual analysis
of speakers' lip movements. This is a challenging task due to the existence of
homophemes-words which involve identical or highly similar lip movements, as
well as diverse lip appearances and motion patterns among the speakers. To
address these challenges, we propose a novel lip-reading model which captures
not only the nuance between words but also styles of different speakers, by a
multi-grained spatio-temporal modeling of the speaking process. Specifically,
we first extract both frame-level fine-grained features and short-term
medium-grained features by the visual front-end, which are then combined to
obtain discriminative representations for words with similar phonemes. Next, a
bidirectional ConvLSTM augmented with temporal attention aggregates
spatio-temporal information in the entire input sequence, which is expected to
be able to capture the coarse-gained patterns of each word and robust to
various conditions in speaker identity, lighting conditions, and so on. By
making full use of the information from different levels in a unified
framework, the model is not only able to distinguish words with similar
pronunciations, but also becomes robust to appearance changes. We evaluate our
method on two challenging word-level lip-reading benchmarks and show the
effectiveness of the proposed method, which also demonstrate the above claims.","cs.CV,eess.AS"
"UneVEn: Universal Value Exploration for Multi-Agent Reinforcement Learning. VDN and QMIX are two popular value-based algorithms for cooperative MARL that
learn a centralized action value function as a monotonic mixing of per-agent
utilities. While this enables easy decentralization of the learned policy, the
restricted joint action value function can prevent them from solving tasks that
require significant coordination between agents at a given timestep. We show
that this problem can be overcome by improving the joint exploration of all
agents during training. Specifically, we propose a novel MARL approach called
Universal Value Exploration (UneVEn) that learns a set of related tasks
simultaneously with a linear decomposition of universal successor features.
With the policies of already solved related tasks, the joint exploration
process of all agents can be improved to help them achieve better coordination.
Empirical results on a set of exploration games, challenging cooperative
predator-prey tasks requiring significant coordination among agents, and
StarCraft II micromanagement benchmarks show that UneVEn can solve tasks where
other state-of-the-art MARL methods fail.","cs.AI,cs.LG,cs.MA"
"GeCo: Quality Counterfactual Explanations in Real Time. Machine learning is increasingly applied in high-stakes decision making that
directly affect people's lives, and this leads to an increased demand for
systems to explain their decisions. Explanations often take the form of
counterfactuals, which consists of conveying to the end user what she/he needs
to change in order to improve the outcome. Computing counterfactual
explanations is challenging, because of the inherent tension between a rich
semantics of the domain, and the need for real time response. In this paper we
present GeCo, the first system that can compute plausible and feasible
counterfactual explanations in real time. At its core, GeCo relies on a genetic
algorithm, which is customized to favor searching counterfactual explanations
with the smallest number of changes. To achieve real-time performance, we
introduce two novel optimizations: $\Delta$-representation of candidate
counterfactuals, and partial evaluation of the classifier. We compare
empirically GeCo against five other systems described in the literature, and
show that it is the only system that can achieve both high quality explanations
and real time answers.","cs.DB,cs.LG"
"Robust Hierarchical Graph Classification with Subgraph Attention. Graph neural networks get significant attention for graph representation and
classification in machine learning community. Attention mechanism applied on
the neighborhood of a node improves the performance of graph neural networks.
Typically, it helps to identify a neighbor node which plays more important role
to determine the label of the node under consideration. But in real world
scenarios, a particular subset of nodes together, but not the individual pairs
in the subset, may be important to determine the label of the graph. To address
this problem, we introduce the concept of subgraph attention for graphs. On the
other hand, hierarchical graph pooling has been shown to be promising in recent
literature. But due to noisy hierarchical structure of real world graphs, not
all the hierarchies of a graph play equal role for graph classification.
Towards this end, we propose a graph classification algorithm called
SubGattPool which jointly learns the subgraph attention and employs two
different types of hierarchical attention mechanisms to find the important
nodes in a hierarchy and the importance of individual hierarchies in a graph.
Experimental evaluation with different types of graph classification algorithms
shows that SubGattPool is able to improve the state-of-the-art or remains
competitive on multiple publicly available graph classification datasets. We
conduct further experiments on both synthetic and real world graph datasets to
justify the usefulness of different components of SubGattPool and to show its
consistent performance on other downstream tasks.","cs.LG,cs.SI"
"Multitask and Transfer Learning for Autotuning Exascale Applications. Multitask learning and transfer learning have proven to be useful in the
field of machine learning when additional knowledge is available to help a
prediction task. We aim at deriving methods following these paradigms for use
in autotuning, where the goal is to find the optimal performance parameters of
an application treated as a black-box function. We show comparative results
with state-of-the-art autotuning techniques. For instance, we observe an
average $1.5x$ improvement of the application runtime compared to the OpenTuner
and HpBandSter autotuners. We explain how our approaches can be more suitable
than some state-of-the-art autotuners for the tuning of any application in
general and of expensive exascale applications in particular.","cs.DC,cs.LG,stat.ML"
"Deep Interactive Region Segmentation and Captioning. With recent innovations in dense image captioning, it is now possible to
describe every object of the scene with a caption while objects are determined
by bounding boxes. However, interpretation of such an output is not trivial due
to the existence of many overlapping bounding boxes. Furthermore, in current
captioning frameworks, the user is not able to involve personal preferences to
exclude out of interest areas. In this paper, we propose a novel hybrid deep
learning architecture for interactive region segmentation and captioning where
the user is able to specify an arbitrary region of the image that should be
processed. To this end, a dedicated Fully Convolutional Network (FCN) named
Lyncean FCN (LFCN) is trained using our special training data to isolate the
User Intention Region (UIR) as the output of an efficient segmentation. In
parallel, a dense image captioning model is utilized to provide a wide variety
of captions for that region. Then, the UIR will be explained with the caption
of the best match bounding box. To the best of our knowledge, this is the first
work that provides such a comprehensive output. Our experiments show the
superiority of the proposed approach over state-of-the-art interactive
segmentation methods on several well-known datasets. In addition, replacement
of the bounding boxes with the result of the interactive segmentation leads to
a better understanding of the dense image captioning output as well as accuracy
enhancement for the object detection in terms of Intersection over Union (IoU).","68T45,cs.CV"
"FREDE: Linear-Space Anytime Graph Embeddings. Low-dimensional representations, or embeddings, of a graph's nodes facilitate
data mining tasks. Known embedding methods explicitly or implicitly rely on a
similarity measure among nodes. As the similarity matrix is quadratic, a
tradeoff between space complexity and embedding quality arises; past research
initially opted for heuristics and linear-transform factorizations, which allow
for linear space but compromise on quality; recent research has proposed a
quadratic-space solution as a viable option too.
  In this paper we observe that embedding methods effectively aim to preserve
the covariance among the rows of a similarity matrix, and raise the question:
is there a method that combines (i) linear space complexity, (ii) a nonlinear
transform as its basis, and (iii) nontrivial quality guarantees? We answer this
question in the affirmative, with FREDE(FREquent Directions Embedding), a
sketching-based method that iteratively improves on quality while processing
rows of the similarity matrix individually; thereby, it provides, at any
iteration, column-covariance approximation guarantees that are, in due course,
almost indistinguishable from those of the optimal row-covariance approximation
by SVD. Our experimental evaluation on variably sized networks shows that FREDE
performs as well as SVD and competitively against current state-of-the-art
methods in diverse data mining tasks, even when it derives an embedding based
on only 10% of node similarities.","cs.LG,cs.SI,stat.ML"
"Collaborative Total Variation: A General Framework for Vectorial TV Models. Even after over two decades, the total variation (TV) remains one of the most
popular regularizations for image processing problems and has sparked a
tremendous amount of research, particularly to move from scalar to
vector-valued functions. In this paper, we consider the gradient of a color
image as a three dimensional matrix or tensor with dimensions corresponding to
the spatial extend, the differences to other pixels, and the spectral channels.
The smoothness of this tensor is then measured by taking different norms along
the different dimensions. Depending on the type of these norms one obtains very
different properties of the regularization, leading to novel models for color
images. We call this class of regularizations collaborative total variation
(CTV). On the theoretical side, we characterize the dual norm, the
subdifferential and the proximal mapping of the proposed regularizers. We
further prove, with the help of the generalized concept of singular vectors,
that an $\ell^{\infty}$ channel coupling makes the most prior assumptions and
has the greatest potential to reduce color artifacts. Our practical
contributions consist of an extensive experimental section where we compare the
performance of a large number of collaborative TV methods for inverse problems
like denoising, deblurring and inpainting.","15A60,65F22,65K10,68U10,90C25,90C46,94A08,cs.CV,math.HO,math.NA,math.OC"
"Exact Reduction of Huge Action Spaces in General Reinforcement Learning. The reinforcement learning (RL) framework formalizes the notion of learning
with interactions. Many real-world problems have large state-spaces and/or
action-spaces such as in Go, StarCraft, protein folding, and robotics or are
non-Markovian, which cause significant challenges to RL algorithms. In this
work we address the large action-space problem by sequentializing actions,
which can reduce the action-space size significantly, even down to two actions
at the expense of an increased planning horizon. We provide explicit and exact
constructions and equivalence proofs for all quantities of interest for
arbitrary history-based processes. In the case of MDPs, this could help RL
algorithms that bootstrap. In this work we show how action-binarization in the
non-MDP case can significantly improve Extreme State Aggregation (ESA) bounds.
ESA allows casting any (non-MDP, non-ergodic, history-based) RL problem into a
fixed-sized non-Markovian state-space with the help of a surrogate Markovian
process. On the upside, ESA enjoys similar optimality guarantees as Markovian
models do. But a downside is that the size of the aggregated state-space
becomes exponential in the size of the action-space. In this work, we patch
this issue by binarizing the action-space. We provide an upper bound on the
number of states of this binarized ESA that is logarithmic in the original
action-space size, a double-exponential improvement.","cs.AI,cs.LG,cs.SY,eess.SY,stat.ML"
"MINOS: Multimodal Indoor Simulator for Navigation in Complex Environments. We present MINOS, a simulator designed to support the development of
multisensory models for goal-directed navigation in complex indoor
environments. The simulator leverages large datasets of complex 3D environments
and supports flexible configuration of multimodal sensor suites. We use MINOS
to benchmark deep-learning-based navigation methods, to analyze the influence
of environmental complexity on navigation performance, and to carry out a
controlled study of multimodality in sensorimotor learning. The experiments
show that current deep reinforcement learning approaches fail in large
realistic environments. The experiments also indicate that multimodality is
beneficial in learning to navigate cluttered scenes. MINOS is released
open-source to the research community at http://minosworld.org . A video that
shows MINOS can be found at https://youtu.be/c0mL9K64q84","cs.AI,cs.CV,cs.GR,cs.LG,cs.RO"
"Bayesian image segmentations by Potts prior and loopy belief propagation. This paper presents a Bayesian image segmentation model based on Potts prior
and loopy belief propagation. The proposed Bayesian model involves several
terms, including the pairwise interactions of Potts models, and the average
vectors and covariant matrices of Gauss distributions in color image modeling.
These terms are often referred to as hyperparameters in statistical machine
learning theory. In order to determine these hyperparameters, we propose a new
scheme for hyperparameter estimation based on conditional maximization of
entropy in the Potts prior. The algorithm is given based on loopy belief
propagation. In addition, we compare our conditional maximum entropy framework
with the conventional maximum likelihood framework, and also clarify how the
first order phase transitions in LBP's for Potts models influence our
hyperparameter estimation procedures.","cond-mat.dis-nn,cond-mat.stat-mech,cs.CV,cs.LG,stat.ML"
"Staircase Attention for Recurrent Processing of Sequences. Attention mechanisms have become a standard tool for sequence modeling tasks,
in particular by stacking self-attention layers over the entire input sequence
as in the Transformer architecture. In this work we introduce a novel attention
procedure called staircase attention that, unlike self-attention, operates
across the sequence (in time) recurrently processing the input by adding
another step of processing. A step in the staircase comprises of backward
tokens (encoding the sequence so far seen) and forward tokens (ingesting a new
part of the sequence), or an extreme Ladder version with a forward step of zero
that simply repeats the Transformer on each step of the ladder, sharing the
weights. We thus describe a family of such models that can trade off
performance and compute, by either increasing the amount of recurrence through
time, the amount of sequential processing via recurrence in depth, or both.
Staircase attention is shown to be able to solve tasks that involve tracking
that conventional Transformers cannot, due to this recurrence. Further, it is
shown to provide improved modeling power for the same size model (number of
parameters) compared to self-attentive Transformers on large language modeling
and dialogue tasks, yielding significant perplexity gains.","cs.CL,cs.LG"
"Prediction of Malignant & Benign Breast Cancer: A Data Mining Approach in Healthcare Applications. As much as data science is playing a pivotal role everywhere, healthcare also
finds it prominent application. Breast Cancer is the top rated type of cancer
amongst women; which took away 627,000 lives alone. This high mortality rate
due to breast cancer does need attention, for early detection so that
prevention can be done in time. As a potential contributor to state-of-art
technology development, data mining finds a multi-fold application in
predicting Brest cancer. This work focuses on different classification
techniques implementation for data mining in predicting malignant and benign
breast cancer. Breast Cancer Wisconsin data set from the UCI repository has
been used as experimental dataset while attribute clump thickness being used as
an evaluation class. The performances of these twelve algorithms: Ada Boost M
1, Decision Table, J Rip, Lazy IBK, Logistics Regression, Multiclass
Classifier, Multilayer Perceptron, Naive Bayes, Random forest and Random Tree
are analyzed on this data set. Keywords- Data Mining, Classification
Techniques, UCI repository, Breast Cancer, Classification Algorithms","cs.CY,cs.LG,stat.ML"
"An improved LogNNet classifier for IoT application. The internet of things devices suffer of low memory while good accuracy is
needed. Designing suitable algorithms is vital in this subject. This paper
proposes a feed forward LogNNet neural network which uses a semi-linear Henon
type discrete chaotic map to classify MNIST-10 dataset. The model is composed
of reservoir part and trainable classifier. The aim of reservoir part is
transforming the inputs to maximize the classification accuracy using a special
matrix filing method and a time series generated by the chaotic map. The
parameters of the chaotic map are optimized using particle swarm optimization
with random immigrants. The results show that the proposed LogNNet/Henon
classifier has higher accuracy and same RAM saving comparable to the original
version of LogNNet and has broad prospects for implementation in IoT devices.
In addition, the relation between the entropy and accuracy of the
classification is investigated. It is shown that there exists a direct relation
between the value of entropy and accuracy of the classification.","cs.CV,cs.LG,cs.NE,eess.IV,nlin.CD"
"Deep Within-Class Covariance Analysis for Robust Audio Representation Learning. Convolutional Neural Networks (CNNs) can learn effective features, though
have been shown to suffer from a performance drop when the distribution of the
data changes from training to test data. In this paper we analyze the internal
representations of CNNs and observe that the representations of unseen data in
each class, spread more (with higher variance) in the embedding space of the
CNN compared to representations of the training data. More importantly, this
difference is more extreme if the unseen data comes from a shifted
distribution. Based on this observation, we objectively evaluate the degree of
representation's variance in each class via eigenvalue decomposition on the
within-class covariance of the internal representations of CNNs and observe the
same behaviour. This can be problematic as larger variances might lead to
mis-classification if the sample crosses the decision boundary of its class. We
apply nearest neighbor classification on the representations and empirically
show that the embeddings with the high variance actually have significantly
worse KNN classification performances, although this could not be foreseen from
their end-to-end classification results. To tackle this problem, we propose
Deep Within-Class Covariance Analysis (DWCCA), a deep neural network layer that
significantly reduces the within-class covariance of a DNN's representation,
improving performance on unseen test data from a shifted distribution. We
empirically evaluate DWCCA on two datasets for Acoustic Scene Classification
(DCASE2016 and DCASE2017). We demonstrate that not only does DWCCA
significantly improve the network's internal representation, it also increases
the end-to-end classification accuracy, especially when the test set exhibits a
distribution shift. By adding DWCCA to a VGG network, we achieve around 6
percentage points improvement in the case of a distribution mismatch.","cs.AI,cs.LG,cs.SD,eess.AS"
"Hierarchical Reinforcement Learning Framework for Stochastic Spaceflight Campaign Design. This paper develops a hierarchical reinforcement learning architecture for
multi-mission spaceflight campaign design under uncertainty, including vehicle
design, infrastructure deployment planning, and space transportation
scheduling. This problem involves a high-dimensional design space and is
challenging especially with uncertainty present. To tackle this challenge, the
developed framework has a hierarchical structure with reinforcement learning
(RL) and network-based mixed-integer linear programming (MILP), where the
former optimizes campaign-level decisions (e.g., design of the vehicle used
throughout the campaign, destination demand assigned to each mission in the
campaign), whereas the latter optimizes the detailed mission-level decisions
(e.g., when to launch what from where to where). The framework is applied to a
set of human lunar exploration campaign scenarios with uncertain in-situ
resource utilization (ISRU) performance as a case study. The main value of this
work is its integration of the rapidly growing RL research and the existing
MILP-based space logistics methods through a hierarchical framework to handle
the otherwise intractable complexity of space mission design under uncertainty.
We expect this unique framework to be a critical steppingstone for the emerging
research direction of artificial intelligence for space mission design.","cs.LG,cs.SY,eess.SY,math.OC"
"Phenotype Inference with Semi-Supervised Mixed Membership Models. Disease phenotyping algorithms process observational clinical data to
identify patients with specific diseases. Supervised phenotyping methods
require significant quantities of expert-labeled data, while unsupervised
methods may learn non-disease phenotypes. To address these limitations, we
propose the Semi-Supervised Mixed Membership Model (SS3M) -- a probabilistic
graphical model for learning disease phenotypes from clinical data with
relatively few labels. We show SS3M can learn interpretable, disease-specific
phenotypes which capture the clinical characteristics of the diseases specified
by the labels provided.","cs.LG,q-bio.QM,stat.ML"
"Variational Encoding of Complex Dynamics. Often the analysis of time-dependent chemical and biophysical systems
produces high-dimensional time-series data for which it can be difficult to
interpret which individual features are most salient. While recent work from
our group and others has demonstrated the utility of time-lagged co-variate
models to study such systems, linearity assumptions can limit the compression
of inherently nonlinear dynamics into just a few characteristic components.
Recent work in the field of deep learning has led to the development of
variational autoencoders (VAE), which are able to compress complex datasets
into simpler manifolds. We present the use of a time-lagged VAE, or variational
dynamics encoder (VDE), to reduce complex, nonlinear processes to a single
embedding with high fidelity to the underlying dynamics. We demonstrate how the
VDE is able to capture nontrivial dynamics in a variety of examples, including
Brownian dynamics and atomistic protein folding. Additionally, we demonstrate a
method for analyzing the VDE model, inspired by saliency mapping, to determine
what features are selected by the VDE model to describe dynamics. The VDE
presents an important step in applying techniques from deep learning to more
accurately model and interpret complex biophysics.","physics.bio-ph,physics.chem-ph,physics.comp-ph,q-bio.BM,stat.ML"
"Usable Information and Evolution of Optimal Representations During Training. We introduce a notion of usable information contained in the representation
learned by a deep network, and use it to study how optimal representations for
the task emerge during training. We show that the implicit regularization
coming from training with Stochastic Gradient Descent with a high learning-rate
and small batch size plays an important role in learning minimal sufficient
representations for the task. In the process of arriving at a minimal
sufficient representation, we find that the content of the representation
changes dynamically during training. In particular, we find that semantically
meaningful but ultimately irrelevant information is encoded in the early
transient dynamics of training, before being later discarded. In addition, we
evaluate how perturbing the initial part of training impacts the learning
dynamics and the resulting representations. We show these effects on both
perceptual decision-making tasks inspired by neuroscience literature, as well
as on standard image classification tasks.","cs.IT,cs.LG,math.IT,stat.ML"
"Understanding the Role of Individual Units in a Deep Neural Network. Deep neural networks excel at finding hierarchical representations that solve
complex tasks over large data sets. How can we humans understand these learned
representations? In this work, we present network dissection, an analytic
framework to systematically identify the semantics of individual hidden units
within image classification and image generation networks. First, we analyze a
convolutional neural network (CNN) trained on scene classification and discover
units that match a diverse set of object concepts. We find evidence that the
network has learned many object classes that play crucial roles in classifying
scene classes. Second, we use a similar analytic method to analyze a generative
adversarial network (GAN) model trained to generate scenes. By analyzing
changes made when small sets of units are activated or deactivated, we find
that objects can be added and removed from the output scenes while adapting to
the context. Finally, we apply our analytic framework to understanding
adversarial attacks and to semantic image editing.","68T07,I.4; I.2,cs.CV,cs.LG,cs.NE"
"Can the Transformer Be Used as a Drop-in Replacement for RNNs in Text-Generating GANs?. In this paper we address the problem of fine-tuned text generation with a
limited computational budget. For that, we use a well-performing text
generative adversarial network (GAN) architecture - Diversity-Promoting GAN
(DPGAN), and attempted a drop-in replacement of the LSTM layer with a
self-attention-based Transformer layer in order to leverage their efficiency.
The resulting Self-Attention DPGAN (SADPGAN) was evaluated for performance,
quality and diversity of generated text and stability. Computational
experiments suggested that a transformer architecture is unable to drop-in
replace the LSTM layer, under-performing during the pre-training phase and
undergoing a complete mode collapse during the GAN tuning phase. Our results
suggest that the transformer architecture need to be adapted before it can be
used as a replacement for RNNs in text-generating GANs.","68T05,68T50,I.2.7,cs.CL,cs.LG"
"Drug-Drug Interaction Prediction with Wasserstein Adversarial Autoencoder-based Knowledge Graph Embeddings. Interaction between pharmacological agents can trigger unexpected adverse
events. Capturing richer and more comprehensive information about drug-drug
interactions (DDI) is one of the key tasks in public health and drug
development. Recently, several knowledge graph embedding approaches have
received increasing attention in the DDI domain due to their capability of
projecting drugs and interactions into a low-dimensional feature space for
predicting links and classifying triplets. However, existing methods only apply
a uniformly random mode to construct negative samples. As a consequence, these
samples are often too simplistic to train an effective model. In this paper, we
propose a new knowledge graph embedding framework by introducing adversarial
autoencoders (AAE) based on Wasserstein distances and Gumbel-Softmax relaxation
for drug-drug interactions tasks. In our framework, the autoencoder is employed
to generate high-quality negative samples and the hidden vector of the
autoencoder is regarded as a plausible drug candidate. Afterwards, the
discriminator learns the embeddings of drugs and interactions based on both
positive and negative triplets. Meanwhile, in order to solve vanishing gradient
problems on the discrete representation--an inherent flaw in traditional
generative models--we utilize the Gumbel-Softmax relaxation and the Wasserstein
distance to train the embedding model steadily. We empirically evaluate our
method on two tasks, link prediction and DDI classification. The experimental
results show that our framework can attain significant improvements and
noticeably outperform competitive baselines.","cs.CY,cs.LG,stat.ML"
"Is Simple Uniform Sampling Efficient for Center-Based Clustering With Outliers: When and Why?. Clustering has many important applications in computer science, but
real-world datasets often contain outliers. The presence of outliers can make
the clustering problems to be much more challenging. In this paper, we propose
a framework for solving three representative center-based clustering with
outliers problems: $k$-center/median/means clustering with outliers. The
framework actually is very simple, where we just need to take a small uniform
sample from the input and run an existing approximation algorithm on the
sample. However, our analysis is fundamentally different from the previous
(uniform and non-uniform) sampling based ideas. To explain the effectiveness of
uniform sampling in theory, we introduce a ""significance"" criterion and prove
that the performance of our framework depends on the significance degree of the
given instance. In particular, the sample size can be independent of the input
data size $n$ and the dimensionality $d$, if we assume the given instance is
sufficiently ""significant"", which is in fact a fairly appropriate assumption in
practice. Due to its simplicity, the uniform sampling approach also enjoys
several significant advantages over the non-uniform sampling approaches. The
experiments suggest that our framework can achieve comparable clustering
results with existing methods, but is much easier to implement and can greatly
reduce the running times. To the best of our knowledge, this is the first work
that systematically studies the effectiveness of uniform sampling from both
theoretical and experimental aspects.","cs.CG,cs.DB,cs.LG"
"Deep Transfer Learning for Static Malware Classification. We propose to apply deep transfer learning from computer vision to static
malware classification. In the transfer learning scheme, we borrow knowledge
from natural images or objects and apply to the target domain of static malware
detection. As a result, training time of deep neural networks is accelerated
while high classification performance is still maintained. We demonstrate the
effectiveness of our approach on three experiments and show that our proposed
method outperforms other classical machine learning methods measured in
accuracy, false positive rate, true positive rate and $F_1$ score (in binary
classification). We instrument an interpretation component to the algorithm and
provide interpretable explanations to enhance security practitioners' trust to
the model. We further discuss a convex combination scheme of transfer learning
and training from scratch for enhanced malware detection, and provide insights
of the algorithmic interpretation of vision-based malware classification
techniques.","cs.CR,cs.LG,stat.ML"
"Approximate Multi-Agent Fitted Q Iteration. We formulate an efficient approximation for multi-agent batch reinforcement
learning, the approximate multi-agent fitted Q iteration (AMAFQI). We present a
detailed derivation of our approach. We propose an iterative policy search and
show that it yields a greedy policy with respect to multiple approximations of
the centralized, standard Q-function. In each iteration and policy evaluation,
AMAFQI requires a number of computations that scales linearly with the number
of agents whereas the analogous number of computations increase exponentially
for the fitted Q iteration (FQI), one of the most commonly used approaches in
batch reinforcement learning. This property of AMAFQI is fundamental for the
design of a tractable multi-agent approach. We evaluate the performance of
AMAFQI and compare it to FQI in numerical simulations. Numerical examples
illustrate the significant computation time reduction when using AMAFQI instead
of FQI in multi-agent problems and corroborate the similar decision-making
performance of both approaches.","cs.LG,cs.SY,eess.SY"
"Learning of Discrete Graphical Models with Neural Networks. Graphical models are widely used in science to represent joint probability
distributions with an underlying conditional dependence structure. The inverse
problem of learning a discrete graphical model given i.i.d samples from its
joint distribution can be solved with near-optimal sample complexity using a
convex optimization method known as Generalized Regularized Interaction
Screening Estimator (GRISE). But the computational cost of GRISE becomes
prohibitive when the energy function of the true graphical model has
higher-order terms. We introduce NeurISE, a neural net based algorithm for
graphical model learning, to tackle this limitation of GRISE. We use neural
nets as function approximators in an Interaction Screening objective function.
The optimization of this objective then produces a neural-net representation
for the conditionals of the graphical model. NeurISE algorithm is seen to be a
better alternative to GRISE when the energy function of the true model has a
high order with a high degree of symmetry. In these cases NeurISE is able to
find the correct parsimonious representation for the conditionals without being
fed any prior information about the true model. NeurISE can also be used to
learn the underlying structure of the true model with some simple modifications
to its training procedure. In addition, we also show a variant of NeurISE that
can be used to learn a neural net representation for the full energy function
of the true model.","cond-mat.dis-nn,cs.LG,physics.data-an,stat.ML"
"Data-Driven Short-Term Voltage Stability Assessment Based on Spatial-Temporal Graph Convolutional Network. Post-fault dynamics of short-term voltage stability (SVS) present
spatial-temporal characteristics, but the existing data-driven methods for
online SVS assessment fail to incorporate such characteristics into their
models effectively. Confronted with this dilemma, this paper develops a novel
spatial-temporal graph convolutional network (STGCN) to address this problem.
The proposed STGCN utilizes graph convolution to integrate network topology
information into the learning model to exploit spatial information. Then, it
adopts one-dimensional convolution to exploit temporal information. In this
way, it models the spatial-temporal characteristics of SVS with complete
convolutional structures. After that, a node layer and a system layer are
strategically designed in the STGCN for SVS assessment. The proposed STGCN
incorporates the characteristics of SVS into the data-driven classification
model. It can result in higher assessment accuracy, better robustness and
adaptability than conventional methods. Besides, parameters in the system layer
can provide valuable information about the influences of individual buses on
SVS. Test results on the real-world Guangdong Power Grid in South China verify
the effectiveness of the proposed network.","cs.LG,cs.SY,eess.SY"
"An Adaptive Framework for Learning Unsupervised Depth Completion. We present a method to infer a dense depth map from a color image and
associated sparse depth measurements. Our main contribution lies in the design
of an annealing process for determining co-visibility (occlusions,
disocclusions) and the degree of regularization to impose on the model. We show
that regularization and co-visibility are related via the fitness (residual) of
model to data and both can be unified into a single framework to improve the
learning process. Our method is an adaptive weighting scheme that guides
optimization by measuring the residual at each pixel location over each
training step for (i) estimating a soft visibility mask and (ii) determining
the amount of regularization. We demonstrate the effectiveness our method by
applying it to several recent unsupervised depth completion methods and
improving their performance on public benchmark datasets, without incurring
additional trainable parameters or increase in inference time. Code available
at: https://github.com/alexklwong/adaframe-depth-completion.","cs.CV,cs.LG,cs.RO"
"INODE: Building an End-to-End Data Exploration System in Practice [Extended Vision]. A full-fledged data exploration system must combine different access
modalities with a powerful concept of guiding the user in the exploration
process, by being reactive and anticipative both for data discovery and for
data linking. Such systems are a real opportunity for our community to cater to
users with different domain and data science expertise. We introduce INODE --
an end-to-end data exploration system -- that leverages, on the one hand,
Machine Learning and, on the other hand, semantics for the purpose of Data
Management (DM). Our vision is to develop a classic unified, comprehensive
platform that provides extensive access to open datasets, and we demonstrate it
in three significant use cases in the fields of Cancer Biomarker Reearch,
Research and Innovation Policy Making, and Astrophysics. INODE offers
sustainable services in (a) data modeling and linking, (b) integrated query
processing using natural language, (c) guidance, and (d) data exploration
through visualization, thus facilitating the user in discovering new insights.
We demonstrate that our system is uniquely accessible to a wide range of users
from larger scientific communities to the public. Finally, we briefly
illustrate how this work paves the way for new research opportunities in DM.","I.2; H.2,cs.AI,cs.DB,cs.LG"
"Divergence Network: Graphical calculation method of divergence functions. In this paper, we introduce directed networks called `divergence network' in
order to perform graphical calculation of divergence functions. By using the
divergence networks, we can easily understand the geometric meaning of
calculation results and grasp relations among divergence functions intuitively.","cs.IT,cs.LG,math.IT,math.ST,stat.TH"
"Deep Spatial Learning with Molecular Vibration. Machine learning over-fitting caused by data scarcity greatly limits the
application of machine learning for molecules. Due to manufacturing processes
difference, big data is not always rendered available through computational
chemistry methods for some tasks, causing data scarcity problem for machine
learning algorithms. Here we propose to extract the natural features of
molecular structures and rationally distort them to augment the data
availability. This method allows a machine learning project to leverage the
powerful fit of physics-informed augmentation for providing significant boost
to predictive accuracy. Successfully verified by the prediction of rejection
rate and flux of thin film polyamide nanofiltration membranes, with the
relative error dropping from 16.34% to 6.71% and the coefficient of
determination rising from 0.16 to 0.75, the proposed deep spatial learning with
molecular vibration is widely instructive for molecular science. Experimental
comparison unequivocally demonstrates its superiority over common learning
algorithms.","cs.LG,physics.chem-ph"
"Learning Adversarially Robust Representations via Worst-Case Mutual Information Maximization. Training machine learning models that are robust against adversarial inputs
poses seemingly insurmountable challenges. To better understand adversarial
robustness, we consider the underlying problem of learning robust
representations. We develop a notion of representation vulnerability that
captures the maximum change of mutual information between the input and output
distributions, under the worst-case input perturbation. Then, we prove a
theorem that establishes a lower bound on the minimum adversarial risk that can
be achieved for any downstream classifier based on its representation
vulnerability. We propose an unsupervised learning method for obtaining
intrinsically robust representations by maximizing the worst-case mutual
information between the input and output distributions. Experiments on
downstream classification tasks support the robustness of the representations
found using unsupervised learning with our training principle.","cs.CR,cs.IT,cs.LG,math.IT,stat.ML"
"Discrete Function Bases and Convolutional Neural Networks. We discuss the notion of ""discrete function bases"" with a particular focus on
the discrete basis derived from the Legendre Delay Network (LDN). We
characterize the performance of these bases in a delay computation task, and as
fixed temporal convolutions in neural networks. Networks using fixed temporal
convolutions are conceptually simple and yield state-of-the-art results in
tasks such as psMNIST.
  Main Results
  (1) We present a numerically stable algorithm for constructing a matrix of
DLOPs L in O(qN)
  (2) The Legendre Delay Network (LDN) can be used to form a discrete function
basis with a basis transformation matrix H in O(qN).
  (3) If q < 300, convolving with the LDN basis online has a lower run-time
complexity than convolving with arbitrary FIR filters.
  (4) Sliding window transformations exist for some bases (Haar, cosine,
Fourier) and require O(q) operations per sample and O(N) memory.
  (5) LTI systems similar to the LDN can be constructed for many discrete
function bases; the LDN system is superior in terms of having a finite impulse
response.
  (6) We compare discrete function bases by linearly decoding delays from
signals represented with respect to these bases. Results are depicted in Figure
20. Overall, decoding errors are similar. The LDN basis has the highest and the
Fourier and cosine bases have the smallest errors.
  (7) The Fourier and cosine bases feature a uniform decoding error for all
delays. These bases should be used if the signal can be represented well in the
Fourier domain.
  (8) Neural network experiments suggest that fixed temporal convolutions can
outperform learned convolutions. The basis choice is not critical; we roughly
observe the same performance trends as in the delay task.
  (9) The LDN is the right choice for small q, if the O(q) Euler update is
feasible, and if the low O(q) memory requirement is of importance.","cs.LG,cs.SY,eess.SY"
"On the Possibility of Rewarding Structure Learning Agents: Mutual Information on Linguistic Random Sets. We present a first attempt to elucidate a theoretical and empirical approach
to design the reward provided by a natural language environment to some
structure learning agent. To this end, we revisit the Information Theory of
unsupervised induction of phrase-structure grammars to characterize the
behavior of simulated actions modeled as set-valued random variables (random
sets of linguistic samples) constituting semantic structures. Our results
showed empirical evidence of that simulated semantic structures (Open
Information Extraction triplets) can be distinguished from randomly constructed
ones by observing the Mutual Information among their constituents. This
suggests the possibility of rewarding structure learning agents without using
pretrained structural analyzers (oracle actors/experts).","cs.AI,cs.CL,cs.IT,cs.LG,math.IT,stat.ML"
"A Cluster-Matching-Based Method for Video Face Recognition. Face recognition systems are present in many modern solutions and thousands
of applications in our daily lives. However, current solutions are not easily
scalable, especially when it comes to the addition of new targeted people. We
propose a cluster-matching-based approach for face recognition in video. In our
approach, we use unsupervised learning to cluster the faces present in both the
dataset and targeted videos selected for face recognition. Moreover, we design
a cluster matching heuristic to associate clusters in both sets that is also
capable of identifying when a face belongs to a non-registered person. Our
method has achieved a recall of 99.435% and a precision of 99.131% in the task
of video face recognition. Besides performing face recognition, it can also be
used to determine the video segments where each person is present.","cs.AI,cs.CV,cs.LG,cs.MM"
"Complex Transformer: A Framework for Modeling Complex-Valued Sequence. While deep learning has received a surge of interest in a variety of fields
in recent years, major deep learning models barely use complex numbers.
However, speech, signal and audio data are naturally complex-valued after
Fourier Transform, and studies have shown a potentially richer representation
of complex nets. In this paper, we propose a Complex Transformer, which
incorporates the transformer model as a backbone for sequence modeling; we also
develop attention and encoder-decoder network operating for complex input. The
model achieves state-of-the-art performance on the MusicNet dataset and an
In-phase Quadrature (IQ) signal dataset.","cs.LG,cs.SD,eess.AS,stat.ML"
"Kernel k-Groups via Hartigan's Method. Energy statistics was proposed by Sz\' ekely in the 80's inspired by Newton's
gravitational potential in classical mechanics and it provides a model-free
hypothesis test for equality of distributions. In its original form, energy
statistics was formulated in Euclidean spaces. More recently, it was
generalized to metric spaces of negative type. In this paper, we consider a
formulation for the clustering problem using a weighted version of energy
statistics in spaces of negative type. We show that this approach leads to a
quadratically constrained quadratic program in the associated kernel space,
establishing connections with graph partitioning problems and kernel methods in
machine learning. To find local solutions of such an optimization problem, we
propose kernel k-groups, which is an extension of Hartigan's method to kernel
spaces. Kernel k-groups is cheaper than spectral clustering and has the same
computational cost as kernel k-means (which is based on Lloyd's heuristic) but
our numerical results show an improved performance, especially in higher
dimensions. Moreover, we verify the efficiency of kernel k-groups in community
detection in sparse stochastic block models which has fascinating applications
in several areas of science.","cs.CV,cs.DS,cs.LG,math.ST,stat.ML,stat.TH"
"A Representation Theory Perspective on Simultaneous Alignment and Classification. One of the difficulties in 3D reconstruction of molecules from images in
single particle Cryo-Electron Microscopy (Cryo-EM), in addition to high levels
of noise and unknown image orientations, is heterogeneity in samples: in many
cases, the samples contain a mixture of molecules, or multiple conformations of
one molecule. Many algorithms for the reconstruction of molecules from images
in heterogeneous Cryo-EM experiments are based on iterative approximations of
the molecules in a non-convex optimization that is prone to reaching suboptimal
local minima. Other algorithms require an alignment in order to perform
classification, or vice versa. The recently introduced Non-Unique Games
framework provides a representation theoretic approach to studying problems of
alignment over compact groups, and offers convex relaxations for alignment
problems which are formulated as semidefinite programs (SDPs) with certificates
of global optimality under certain circumstances. In this manuscript, we
propose to extend Non-Unique Games to the problem of simultaneous alignment and
classification with the goal of simultaneously classifying Cryo-EM images and
aligning them within their respective classes. Our proposed approach can also
be extended to the case of continuous heterogeneity.","cs.CV,math.OC"
"Visual Object Networks: Image Generation with Disentangled 3D Representation. Recent progress in deep generative models has led to tremendous breakthroughs
in image generation. However, while existing models can synthesize
photorealistic images, they lack an understanding of our underlying 3D world.
We present a new generative model, Visual Object Networks (VON), synthesizing
natural images of objects with a disentangled 3D representation. Inspired by
classic graphics rendering pipelines, we unravel our image formation process
into three conditionally independent factors---shape, viewpoint, and
texture---and present an end-to-end adversarial learning framework that jointly
models 3D shapes and 2D images. Our model first learns to synthesize 3D shapes
that are indistinguishable from real shapes. It then renders the object's 2.5D
sketches (i.e., silhouette and depth map) from its shape under a sampled
viewpoint. Finally, it learns to add realistic texture to these 2.5D sketches
to generate natural images. The VON not only generates images that are more
realistic than state-of-the-art 2D image synthesis methods, but also enables
many 3D operations such as changing the viewpoint of a generated image, editing
of shape and texture, linear interpolation in texture and shape space, and
transferring appearance across different objects and viewpoints.","cs.CV,cs.GR,stat.ML"
"Adaptive Stochastic ADMM for Decentralized Reinforcement Learning in Edge Industrial IoT. Edge computing provides a promising paradigm to support the implementation of
Industrial Internet of Things (IIoT) by offloading tasks to nearby edge nodes.
Meanwhile, the increasing network size makes it impractical for centralized
data processing due to limited bandwidth, and consequently a decentralized
learning scheme is preferable. Reinforcement learning (RL) has been widely
investigated and shown to be a promising solution for decision-making and
optimal control processes. For RL in a decentralized setup, edge nodes (agents)
connected through a communication network aim to work collaboratively to find a
policy to optimize the global reward as the sum of local rewards. However,
communication costs, scalability and adaptation in complex environments with
heterogeneous agents may significantly limit the performance of decentralized
RL. Alternating direction method of multipliers (ADMM) has a structure that
allows for decentralized implementation, and has shown faster convergence than
gradient descent based methods. Therefore, we propose an adaptive stochastic
incremental ADMM (asI-ADMM) algorithm and apply the asI-ADMM to decentralized
RL with edge-computing-empowered IIoT networks. We provide convergence
properties for proposed algorithms by designing a Lyapunov function and prove
that the asI-ADMM has $O(\frac{1}{k}) +O(\frac{1}{M})$ convergence rate where
$k$ and $ M$ are the number of iterations and batch samples, respectively.
Then, we test our algorithm with two supervised learning problems. For
performance evaluation, we simulate two applications in decentralized RL
settings with homogeneous and heterogeneous agents. The experiment results show
that our proposed algorithms outperform the state of the art in terms of
communication costs and scalability, and can well adapt to complex IoT
environments.","cs.LG,cs.SY,eess.SY"
"Competitive Multi-agent Inverse Reinforcement Learning with Sub-optimal Demonstrations. This paper considers the problem of inverse reinforcement learning in
zero-sum stochastic games when expert demonstrations are known to be not
optimal. Compared to previous works that decouple agents in the game by
assuming optimality in expert strategies, we introduce a new objective function
that directly pits experts against Nash Equilibrium strategies, and we design
an algorithm to solve for the reward function in the context of inverse
reinforcement learning with deep neural networks as model approximations. In
our setting the model and algorithm do not decouple by agent. In order to find
Nash Equilibrium in large-scale games, we also propose an adversarial training
algorithm for zero-sum stochastic games, and show the theoretical appeal of
non-existence of local optima in its objective function. In our numerical
experiments, we demonstrate that our Nash Equilibrium and inverse reinforcement
learning algorithms address games that are not amenable to previous approaches
using tabular representations. Moreover, with sub-optimal expert demonstrations
our algorithms recover both reward functions and strategies with good quality.","cs.LG,stat.ML"
"Unsupervised Domain Adaptation using Deep Networks with Cross-Grafted Stacks. Current deep domain adaptation methods used in computer vision have mainly
focused on learning discriminative and domain-invariant features across
different domains. In this paper, we present a novel approach that bridges the
domain gap by projecting the source and target domains into a common
association space through an unsupervised ``cross-grafted representation
stacking'' (CGRS) mechanism. Specifically, we construct variational
auto-encoders (VAE) for the two domains, and form bidirectional associations by
cross-grafting the VAEs' decoder stacks. Furthermore, generative adversarial
networks (GAN) are employed for label alignment (LA), mapping the target domain
data to the known label space of the source domain. The overall adaptation
process hence consists of three phases: feature representation learning by
VAEs, association generation, and association label alignment by GANs.
Experimental results demonstrate that our CGRS-LA approach outperforms the
state-of-the-art on a number of unsupervised domain adaptation benchmarks.",cs.CV
"Combining Ensemble Kalman Filter and Reservoir Computing to predict spatio-temporal chaotic systems from imperfect observations and models. Prediction of spatio-temporal chaotic systems is important in various fields,
such as Numerical Weather Prediction (NWP). While data assimilation methods
have been applied in NWP, machine learning techniques, such as Reservoir
Computing (RC), are recently recognized as promising tools to predict
spatio-temporal chaotic systems. However, the sensitivity of the skill of the
machine learning based prediction to the imperfectness of observations is
unclear. In this study, we evaluate the skill of RC with noisy and sparsely
distributed observations. We intensively compare the performances of RC and
Local Ensemble Transform Kalman Filter (LETKF) by applying them to the
prediction of the Lorenz 96 system. Although RC can successfully predict the
Lorenz 96 system if the system is perfectly observed, we find that RC is
vulnerable to observation sparsity compared with LETKF. To overcome this
limitation of RC, we propose to combine LETKF and RC. In our proposed method,
the system is predicted by RC that learned the analysis time series estimated
by LETKF. Our proposed method can successfully predict the Lorenz 96 system
using noisy and sparsely distributed observations. Most importantly, our method
can predict better than LETKF when the process-based model is imperfect.","cs.LG,physics.comp-ph,stat.ML"
"Video Face Manipulation Detection Through Ensemble of CNNs. In the last few years, several techniques for facial manipulation in videos
have been successfully developed and made available to the masses (i.e.,
FaceSwap, deepfake, etc.). These methods enable anyone to easily edit faces in
video sequences with incredibly realistic results and a very little effort.
Despite the usefulness of these tools in many fields, if used maliciously, they
can have a significantly bad impact on society (e.g., fake news spreading,
cyber bullying through fake revenge porn). The ability of objectively detecting
whether a face has been manipulated in a video sequence is then a task of
utmost importance. In this paper, we tackle the problem of face manipulation
detection in video sequences targeting modern facial manipulation techniques.
In particular, we study the ensembling of different trained Convolutional
Neural Network (CNN) models. In the proposed solution, different models are
obtained starting from a base network (i.e., EfficientNetB4) making use of two
different concepts: (i) attention layers; (ii) siamese training. We show that
combining these networks leads to promising face manipulation detection results
on two publicly available datasets with more than 119000 videos.","cs.CV,cs.MM,eess.IV"
"Towards Interpretable Sparse Graph Representation Learning with Laplacian Pooling. Recent work in graph neural networks (GNNs) has led to improvements in
molecular activity and property prediction tasks. Unfortunately, GNNs often
fail to capture the relative importance of interactions between molecular
substructures, in part due to the absence of efficient intermediate pooling
steps. To address these issues, we propose LaPool (Laplacian Pooling), a novel,
data-driven, and interpretable hierarchical graph pooling method that takes
into account both node features and graph structure to improve molecular
representation. We benchmark LaPool on molecular graph prediction and
understanding tasks and show that it outperforms recent GNNs. Interestingly,
LaPool also remains competitive on non-molecular tasks. Both quantitative and
qualitative assessments are done to demonstrate LaPool's improved
interpretability and highlight its potential benefits in drug design. Finally,
we demonstrate LaPool's utility for the generation of valid and novel molecules
by incorporating it into an adversarial autoencoder.","cs.LG,q-bio.BM,stat.ML"
"Enhancing Feature Tracking With Gyro Regularization. We present a deeply integrated method of exploiting low-cost gyroscopes to
improve general purpose feature tracking. Most previous methods use gyroscopes
to initialize and bound the search for features. In contrast, we use them to
regularize the tracking energy function so that they can directly assist in the
tracking of ambiguous and poor-quality features. We demonstrate that our simple
technique offers significant improvements in performance over conventional
template-based tracking methods, and is in fact competitive with more complex
and computationally expensive state-of-the-art trackers, but at a fraction of
the computational cost. Additionally, we show that the practice of initializing
template-based feature trackers like KLT (Kanade-Lucas-Tomasi) using
gyro-predicted optical flow offers no advantage over using a careful
optical-only initialization method, suggesting that some deeper level of
integration, like the method we propose, is needed in order to realize a
genuine improvement in tracking performance from these inertial sensors.","68T45,cs.CV"
"I2T2I: Learning Text to Image Synthesis with Textual Data Augmentation. Translating information between text and image is a fundamental problem in
artificial intelligence that connects natural language processing and computer
vision. In the past few years, performance in image caption generation has seen
significant improvement through the adoption of recurrent neural networks
(RNN). Meanwhile, text-to-image generation begun to generate plausible images
using datasets of specific categories like birds and flowers. We've even seen
image generation from multi-category datasets such as the Microsoft Common
Objects in Context (MSCOCO) through the use of generative adversarial networks
(GANs). Synthesizing objects with a complex shape, however, is still
challenging. For example, animals and humans have many degrees of freedom,
which means that they can take on many complex shapes. We propose a new
training method called Image-Text-Image (I2T2I) which integrates text-to-image
and image-to-text (image captioning) synthesis to improve the performance of
text-to-image synthesis. We demonstrate that %the capability of our method to
understand the sentence descriptions, so as to I2T2I can generate better
multi-categories images using MSCOCO than the state-of-the-art. We also
demonstrate that I2T2I can achieve transfer learning by using a pre-trained
image captioning module to generate human images on the MPII Human Pose","cs.CL,cs.CV"
"MCMC assisted by Belief Propagation. Markov Chain Monte Carlo (MCMC) and Belief Propagation (BP) are the most
popular algorithms for computational inference in Graphical Models (GM). In
principle, MCMC is an exact probabilistic method which, however, often suffers
from exponentially slow mixing. In contrast, BP is a deterministic method,
which is typically fast, empirically very successful, however in general
lacking control of accuracy over loopy graphs. In this paper, we introduce MCMC
algorithms correcting the approximation error of BP, i.e., we provide a way to
compensate for BP errors via a consecutive BP-aware MCMC. Our framework is
based on the Loop Calculus (LC) approach which allows expressing the BP error
as a sum of weighted generalized loops. Although the full series is
computationally intractable, it is known that a truncated series, summing up
all 2-regular loops, is computable in polynomial-time for planar pair-wise
binary GMs and it also provides a highly accurate approximation empirically.
Motivated by this, we first propose a polynomial-time approximation MCMC scheme
for the truncated series of general (non-planar) pair-wise binary models. Our
main idea here is to use the Worm algorithm, known to provide fast mixing in
other (related) problems, and then design an appropriate rejection scheme to
sample 2-regular loops. Furthermore, we also design an efficient rejection-free
MCMC scheme for approximating the full series. The main novelty underlying our
design is in utilizing the concept of cycle basis, which provides an efficient
decomposition of the generalized loops. In essence, the proposed MCMC schemes
run on transformed GM built upon the non-trivial BP solution, and our
experiments show that this synthesis of BP and MCMC outperforms both direct
MCMC and bare BP schemes.","cs.AI,cs.DS,stat.ML"
"LEEP: A New Measure to Evaluate Transferability of Learned Representations. We introduce a new measure to evaluate the transferability of representations
learned by classifiers. Our measure, the Log Expected Empirical Prediction
(LEEP), is simple and easy to compute: when given a classifier trained on a
source data set, it only requires running the target data set through this
classifier once. We analyze the properties of LEEP theoretically and
demonstrate its effectiveness empirically. Our analysis shows that LEEP can
predict the performance and convergence speed of both transfer and
meta-transfer learning methods, even for small or imbalanced data. Moreover,
LEEP outperforms recently proposed transferability measures such as negative
conditional entropy and H scores. Notably, when transferring from ImageNet to
CIFAR100, LEEP can achieve up to 30% improvement compared to the best competing
method in terms of the correlations with actual transfer accuracy.","cs.CV,cs.LG,stat.ML"
"Provenance Graph Kernel. Provenance is a record that describes how entities, activities, and agents
have influenced a piece of data; it is commonly represented as graphs with
relevant labels on both their nodes and edges. With the growing adoption of
provenance in a wide range of application domains, users are increasingly
confronted with an abundance of graph data, which may prove challenging to
process. Graph kernels, on the other hand, have been successfully used to
efficiently analyse graphs. In this paper, we introduce a novel graph kernel
called provenance kernel, which is inspired by and tailored for provenance
data. It decomposes a provenance graph into tree-patterns rooted at a given
node and considers the labels of edges and nodes up to a certain distance from
the root. We employ provenance kernels to classify provenance graphs from three
application domains. Our evaluation shows that they perform well in terms of
classification accuracy and yield competitive results when compared against
existing graph kernel methods and the provenance network analytics method while
more efficient in computing time. Moreover, the provenance types used by
provenance kernels also help improve the explainability of predictive models
built on them.","I.2.6,cs.AI,cs.DB,cs.LG"
"Estimation of Continuous Blood Pressure from PPG via a Federated Learning Approach. Ischemic heart disease is the highest cause of mortality globally each year.
This not only puts a massive strain on the lives of those affected but also on
the public healthcare systems. To understand the dynamics of the healthy and
unhealthy heart doctors commonly use electrocardiogram (ECG) and blood pressure
(BP) readings. These methods are often quite invasive, in particular when
continuous arterial blood pressure (ABP) readings are taken and not to mention
very costly. Using machine learning methods we seek to develop a framework that
is capable of inferring ABP from a single optical photoplethysmogram (PPG)
sensor alone. We train our framework across distributed models and data sources
to mimic a large-scale distributed collaborative learning experiment that could
be implemented across low-cost wearables. Our time series-to-time series
generative adversarial network (T2TGAN) is capable of high-quality continuous
ABP generation from a PPG signal with a mean error of 2.54 mmHg and a standard
deviation of 23.7 mmHg when estimating mean arterial pressure on a previously
unseen, noisy, independent dataset. To our knowledge, this framework is the
first example of a GAN capable of continuous ABP generation from an input PPG
signal that also uses a federated learning methodology.","cs.LG,eess.SP"
"Extract and Merge: Merging extracted humans from different images utilizing Mask R-CNN. Selecting human objects out of the various type of objects in images and
merging them with other scenes is manual and day-to-day work for photo editors.
Although recently Adobe photoshop released ""select subject"" tool which
automatically selects the foreground object in an image, but still requires
fine manual tweaking separately. In this work, we proposed an application
utilizing Mask R-CNN (for object detection and mask segmentation) that can
extract human instances from multiple images and merge them with a new
background. This application does not add any overhead to Mask R-CNN, running
at 5 frames per second. It can extract human instances from any number of
images or videos from merging them together. We also structured the code to
accept videos of different lengths as input and length of the output-video will
be equal to the longest input-video. We wanted to create a simple yet effective
application that can serve as a base for photo editing and do most
time-consuming work automatically, so, editors can focus more on the design
part. Other application could be to group people together in a single picture
with a new background from different images which could not be physically
together. We are showing single-person and multi-person extraction and
placement in two different backgrounds. Also, we are showing a video example
with single-person extraction.","cs.CV,cs.GR,cs.LG"
"Coordinate Independent Convolutional Networks -- Isometry and Gauge Equivariant Convolutions on Riemannian Manifolds. Motivated by the vast success of deep convolutional networks, there is a
great interest in generalizing convolutions to non-Euclidean manifolds. A major
complication in comparison to flat spaces is that it is unclear in which
alignment a convolution kernel should be applied on a manifold. The underlying
reason for this ambiguity is that general manifolds do not come with a
canonical choice of reference frames (gauge). Kernels and features therefore
have to be expressed relative to arbitrary coordinates. We argue that the
particular choice of coordinatization should not affect a network's inference
-- it should be coordinate independent. A simultaneous demand for coordinate
independence and weight sharing is shown to result in a requirement on the
network to be equivariant under local gauge transformations (changes of local
reference frames). The ambiguity of reference frames depends thereby on the
G-structure of the manifold, such that the necessary level of gauge
equivariance is prescribed by the corresponding structure group G. Coordinate
independent convolutions are proven to be equivariant w.r.t. those isometries
that are symmetries of the G-structure. The resulting theory is formulated in a
coordinate free fashion in terms of fiber bundles. To exemplify the design of
coordinate independent convolutions, we implement a convolutional network on
the M\""obius strip. The generality of our differential geometric formulation of
convolutional networks is demonstrated by an extensive literature review which
explains a large number of Euclidean CNNs, spherical CNNs and CNNs on general
surfaces as specific instances of coordinate independent convolutions.","cs.CG,cs.CV,cs.LG,stat.ML"
"Optimizing Quantum Variational Circuits with Deep Reinforcement Learning. Quantum Machine Learning (QML) is considered to be one of the most promising
applications of near term quantum devices. However, the optimization of quantum
machine learning models presents numerous challenges arising from the
imperfections of hardware and the fundamental obstacles in navigating an
exponentially scaling Hilbert space. In this work, we evaluate the potential of
contemporary methods in deep reinforcement learning to augment gradient based
optimization routines in quantum variational circuits. We find that
reinforcement learning augmented optimizers consistently outperform gradient
descent in noisy environments. All code and pretrained weights are available to
replicate the results or deploy the models at
https://github.com/lockwo/rl_qvc_opt.","cs.LG,quant-ph"
"Hybrid Quantum-Classical Graph Convolutional Network. The high energy physics (HEP) community has a long history of dealing with
large-scale datasets. To manage such voluminous data, classical machine
learning and deep learning techniques have been employed to accelerate physics
discovery. Recent advances in quantum machine learning (QML) have indicated the
potential of applying these techniques in HEP. However, there are only limited
results in QML applications currently available. In particular, the challenge
of processing sparse data, common in HEP datasets, has not been extensively
studied in QML models. This research provides a hybrid quantum-classical graph
convolutional network (QGCNN) for learning HEP data. The proposed framework
demonstrates an advantage over classical multilayer perceptron and
convolutional neural networks in the aspect of number of parameters. Moreover,
in terms of testing accuracy, the QGCNN shows comparable performance to a
quantum convolutional neural network on the same HEP dataset while requiring
less than $50\%$ of the parameters. Based on numerical simulation results,
studying the application of graph convolutional operations and other QML models
may prove promising in advancing HEP research and other scientific fields.","cs.CV,cs.LG,hep-ex,physics.data-an,quant-ph"
"Conterfactual Generative Zero-Shot Semantic Segmentation. zero-shot learning is an essential part of computer vision. As a classical
downstream task, zero-shot semantic segmentation has been studied because of
its applicant value. One of the popular zero-shot semantic segmentation methods
is based on the generative model Most new proposed works added structures on
the same architecture to enhance this model. However, we found that, from the
view of causal inference, the result of the original model has been influenced
by spurious statistical relationships. Thus the performance of the prediction
shows severe bias. In this work, we consider counterfactual methods to avoid
the confounder in the original model. Based on this method, we proposed a new
framework for zero-shot semantic segmentation. Our model is compared with
baseline models on two real-world datasets, Pascal-VOC and Pascal-Context. The
experiment results show proposed models can surpass previous confounded models
and can still make use of additional structures to improve the performance. We
also design a simple structure based on Graph Convolutional Networks (GCN) in
this work.","68T07,I.2.10,cs.CV"
"Unifying Cardiovascular Modelling with Deep Reinforcement Learning for Uncertainty Aware Control of Sepsis Treatment. Sepsis is a potentially life threatening inflammatory response to infection
or severe tissue damage. It has a highly variable clinical course, requiring
constant monitoring of the patient's state to guide the management of
intravenous fluids and vasopressors, among other interventions. Despite decades
of research, there's still debate among experts on optimal treatment. Here, we
combine for the first time, distributional deep reinforcement learning with
mechanistic physiological models to find personalized sepsis treatment
strategies. Our method handles partial observability by leveraging known
cardiovascular physiology, introducing a novel physiology-driven recurrent
autoencoder, and quantifies the uncertainty of its own results. Moreover, we
introduce a framework for uncertainty aware decision support with humans in the
loop. We show that our method learns physiologically explainable, robust
policies that are consistent with clinical knowledge. Further our method
consistently identifies high risk states that lead to death, which could
potentially benefit from more frequent vasopressor administration, providing
valuable guidance for future research","cs.AI,cs.LG,q-bio.QM"
"Fast Global Convergence for Low-rank Matrix Recovery via Riemannian Gradient Descent with Random Initialization. In this paper, we propose a new global analysis framework for a class of
low-rank matrix recovery problems on the Riemannian manifold. We analyze the
global behavior for the Riemannian optimization with random initialization. We
use the Riemannian gradient descent algorithm to minimize a least squares loss
function, and study the asymptotic behavior as well as the exact convergence
rate. We reveal a previously unknown geometric property of the low-rank matrix
manifold, which is the existence of spurious critical points for the simple
least squares function on the manifold. We show that under some assumptions,
the Riemannian gradient descent starting from a random initialization with high
probability avoids these spurious critical points and only converges to the
ground truth in nearly linear convergence rate, i.e.
$\mathcal{O}(\text{log}(\frac{1}{\epsilon})+ \text{log}(n))$ iterations to
reach an $\epsilon$-accurate solution. We use two applications as examples for
our global analysis. The first one is a rank-1 matrix recovery problem. The
second one is a generalization of the Gaussian phase retrieval problem. It only
satisfies the weak isometry property, but has behavior similar to that of the
first one except for an extra saddle set. Our convergence guarantee is nearly
optimal and almost dimension-free, which fully explains the numerical
observations. The global analysis can be potentially extended to other data
problems with random measurement structures and empirical least squares loss
functions.","cs.IT,cs.LG,math.IT,math.OC,math.ST,stat.ML,stat.TH"
"A statistical model for tensor PCA. We consider the Principal Component Analysis problem for large tensors of
arbitrary order $k$ under a single-spike (or rank-one plus noise) model. On the
one hand, we use information theory, and recent results in probability theory,
to establish necessary and sufficient conditions under which the principal
component can be estimated using unbounded computational resources. It turns
out that this is possible as soon as the signal-to-noise ratio $\beta$ becomes
larger than $C\sqrt{k\log k}$ (and in particular $\beta$ can remain bounded as
the problem dimensions increase).
  On the other hand, we analyze several polynomial-time estimation algorithms,
based on tensor unfolding, power iteration and message passing ideas from
graphical models. We show that, unless the signal-to-noise ratio diverges in
the system dimensions, none of these approaches succeeds. This is possibly
related to a fundamental limitation of computationally tractable estimators for
this problem.
  We discuss various initializations for tensor power iteration, and show that
a tractable initialization based on the spectrum of the matricized tensor
outperforms significantly baseline methods, statistically and computationally.
Finally, we consider the case in which additional side information is available
about the unknown signal. We characterize the amount of side information that
allows the iterative algorithms to converge to a good estimate.","cs.IT,cs.LG,math.IT,stat.ML"
"Supporting Financial Inclusion with Graph Machine Learning and Super-App Alternative Data. The presence of Super-Apps have changed the way we think about the
interactions between users and commerce. It then comes as no surprise that it
is also redefining the way banking is done. The paper investigates how
different interactions between users within a Super-App provide a new source of
information to predict borrower behavior. To this end, two experiments with
different graph-based methodologies are proposed, the first uses graph based
features as input in a classification model and the second uses graph neural
networks. Our results show that variables of centrality, behavior of
neighboring users and transactionality of a user constituted new forms of
knowledge that enhance statistical and financial performance of credit risk
models. Furthermore, opportunities are identified for Super-Apps to redefine
the definition of credit risk by contemplating all the environment that their
platforms entail, leading to a more inclusive financial system.","cs.CY,cs.LG,q-fin.GN"
"A3T-GCN: Attention Temporal Graph Convolutional Network for Traffic Forecasting. Accurate real-time traffic forecasting is a core technological problem
against the implementation of the intelligent transportation system. However,
it remains challenging considering the complex spatial and temporal
dependencies among traffic flows. In the spatial dimension, due to the
connectivity of the road network, the traffic flows between linked roads are
closely related. In terms of the temporal factor, although there exists a
tendency among adjacent time points in general, the importance of distant past
points is not necessarily smaller than that of recent past points since traffic
flows are also affected by external factors. In this study, an attention
temporal graph convolutional network (A3T-GCN) traffic forecasting method was
proposed to simultaneously capture global temporal dynamics and spatial
correlations. The A3T-GCN model learns the short-time trend in time series by
using the gated recurrent units and learns the spatial dependence based on the
topology of the road network through the graph convolutional network. Moreover,
the attention mechanism was introduced to adjust the importance of different
time points and assemble global temporal information to improve prediction
accuracy. Experimental results in real-world datasets demonstrate the
effectiveness and robustness of proposed A3T-GCN. The source code can be
visited at https://github.com/lehaifeng/T-GCN/A3T.","cs.LG,stat.ML"
"Learning Equational Theorem Proving. We develop Stratified Shortest Solution Imitation Learning (3SIL) to learn
equational theorem proving in a deep reinforcement learning (RL) setting. The
self-trained models achieve state-of-the-art performance in proving problems
generated by one of the top open conjectures in quasigroup theory, the Abelian
Inner Mapping (AIM) Conjecture. To develop the methods, we first use two
simpler arithmetic rewriting tasks that share tree-structured proof states and
sparse rewards with the AIM problems. On these tasks, 3SIL is shown to
significantly outperform several established RL and imitation learning methods.
The final system is then evaluated in a standalone and cooperative mode on the
AIM problems. The standalone 3SIL-trained system proves in 60 seconds more
theorems (70.2%) than the complex, hand-engineered Waldmeister system (65.5%).
In the cooperative mode, the final system is combined with the Prover9 system,
proving in 2 seconds what standalone Prover9 proves in 60 seconds.","cs.AI,cs.LG,cs.LO"
"Image Denoising and Super-Resolution using Residual Learning of Deep Convolutional Network. Image super-resolution and denoising are two important tasks in image
processing that can lead to improvement in image quality. Image
super-resolution is the task of mapping a low resolution image to a high
resolution image whereas denoising is the task of learning a clean image from a
noisy input. We propose and train a single deep learning network that we term
as SuRDCNN (super-resolution and denoising convolutional neural network), to
perform these two tasks simultaneously . Our model nearly replicates the
architecture of existing state-of-the-art deep learning models for
super-resolution and denoising. We use the proven strategy of residual
learning, as supported by state-of-the-art networks in this domain. Our trained
SuRDCNN is capable of super-resolving image in the presence of Gaussian noise,
Poisson noise or any random combination of both of these noises.","68T45,cs.CV"
"StructEdit: Learning Structural Shape Variations. Learning to encode differences in the geometry and (topological) structure of
the shapes of ordinary objects is key to generating semantically plausible
variations of a given shape, transferring edits from one shape to another, and
many other applications in 3D content creation. The common approach of encoding
shapes as points in a high-dimensional latent feature space suggests treating
shape differences as vectors in that space. Instead, we treat shape differences
as primary objects in their own right and propose to encode them in their own
latent space. In a setting where the shapes themselves are encoded in terms of
fine-grained part hierarchies, we demonstrate that a separate encoding of shape
deltas or differences provides a principled way to deal with inhomogeneities in
the shape space due to different combinatorial part structures, while also
allowing for compactness in the representation, as well as edit abstraction and
transfer. Our approach is based on a conditional variational autoencoder for
encoding and decoding shape deltas, conditioned on a source shape. We
demonstrate the effectiveness and robustness of our approach in multiple shape
modification and generation tasks, and provide comparison and ablation studies
on the PartNet dataset, one of the largest publicly available 3D datasets.","cs.CG,cs.CV,cs.GR"
"Road Damage Detection Using Deep Neural Networks with Images Captured Through a Smartphone. Research on damage detection of road surfaces using image processing
techniques has been actively conducted, achieving considerably high detection
accuracies. Many studies only focus on the detection of the presence or absence
of damage. However, in a real-world scenario, when the road managers from a
governing body need to repair such damage, they need to clearly understand the
type of damage in order to take effective action. In addition, in many of these
previous studies, the researchers acquire their own data using different
methods. Hence, there is no uniform road damage dataset available openly,
leading to the absence of a benchmark for road damage detection. This study
makes three contributions to address these issues. First, to the best of our
knowledge, for the first time, a large-scale road damage dataset is prepared.
This dataset is composed of 9,053 road damage images captured with a smartphone
installed on a car, with 15,435 instances of road surface damage included in
these road images. In order to generate this dataset, we cooperated with 7
municipalities in Japan and acquired road images for more than 40 hours. These
images were captured in a wide variety of weather and illuminance conditions.
In each image, we annotated the bounding box representing the location and type
of damage. Next, we used a state-of-the-art object detection method using
convolutional neural networks to train the damage detection model with our
dataset, and compared the accuracy and runtime speed on both, using a GPU
server and a smartphone. Finally, we demonstrate that the type of damage can be
classified into eight types with high accuracy by applying the proposed object
detection method. The road damage dataset, our experimental results, and the
developed smartphone application used in this study are publicly available
(https://github.com/sekilab/RoadDamageDetector/).","cs.CV,cs.CY"
"Necessary and Sufficient Conditions for Inverse Reinforcement Learning of Bayesian Stopping Time Problems. This paper presents an inverse reinforcement learning (IRL) framework for
Bayesian stopping time problems. By observing the actions of a Bayesian
decision maker, we provide a necessary and sufficient condition to identify if
these actions are consistent with optimizing a cost function; then we construct
set valued estimates of the cost function. To achieve this IRL objective, we
use novel ideas from Bayesian revealed preferences stemming from
microeconomics. To illustrate our IRL scheme,we consider two important examples
of stopping time problems, namely, sequential hypothesis testing and Bayesian
search. Finally, for finite datasets, we propose an IRL detection algorithm and
give finite sample bounds on its error probabilities. Also we discuss how to
identify $\epsilon$-optimal Bayesian decision makers and perform IRL.","cs.LG,cs.SY,econ.TH,eess.SY,stat.ML"
"Computing Valid p-value for Optimal Changepoint by Selective Inference using Dynamic Programming. There is a vast body of literature related to methods for detecting
changepoints (CP). However, less attention has been paid to assessing the
statistical reliability of the detected CPs. In this paper, we introduce a
novel method to perform statistical inference on the significance of the CPs,
estimated by a Dynamic Programming (DP)-based optimal CP detection algorithm.
Based on the selective inference (SI) framework, we propose an exact
(non-asymptotic) approach to compute valid p-values for testing the
significance of the CPs. Although it is well-known that SI has low statistical
power because of over-conditioning, we address this disadvantage by introducing
parametric programming techniques. Then, we propose an efficient method to
conduct SI with the minimum amount of conditioning, leading to high statistical
power. We conduct experiments on both synthetic and real-world datasets,
through which we offer evidence that our proposed method is more powerful than
existing methods, has decent performance in terms of computational efficiency,
and provides good results in many practical applications.","cs.LG,stat.ME,stat.ML"
"Fast algorithms for morphological operations using run-length encoded binary images. This paper presents innovative algorithms to efficiently compute erosions and
dilations of run-length encoded (RLE) binary images with arbitrary shaped
structuring elements. An RLE image is given by a set of runs, where a run is a
horizontal concatenation of foreground pixels. The proposed algorithms extract
the skeleton of the structuring element and build distance tables of the input
image, which are storing the distance to the next background pixel on the left
and right hand sides. This information is then used to speed up the
calculations of the erosion and dilation operator by enabling the use of
techniques which allow to skip the analysis of certain pixels whenever a hit or
miss occurs. Additionally the input image gets trimmed during the preprocessing
steps on the base of two primitive criteria. Experimental results show the
advantages over other algorithms. The source code of our algorithms is
available in C++.","65D19,68U10,94A08 (Primary) 65D18,94A12 (Secondary),I.4.3; I.5; I.4.10,cs.CV,cs.GR,cs.IT,math.IT"
"Stratospheric Aerosol Injection as a Deep Reinforcement Learning Problem. As global greenhouse gas emissions continue to rise, the use of stratospheric
aerosol injection (SAI), a form of solar geoengineering, is increasingly
considered in order to artificially mitigate climate change effects. However,
initial research in simulation suggests that naive SAI can have catastrophic
regional consequences, which may induce serious geostrategic conflicts. Current
geo-engineering research treats SAI control in low-dimensional approximation
only. We suggest treating SAI as a high-dimensional control problem, with
policies trained according to a context-sensitive reward function within the
Deep Reinforcement Learning (DRL) paradigm. In order to facilitate training in
simulation, we suggest to emulate HadCM3, a widely used General Circulation
Model, using deep learning techniques. We believe this is the first application
of DRL to the climate sciences.","cs.LG,physics.ao-ph,stat.ML"
"An Efficient Point of Gaze Estimator for Low-Resolution Imaging Systems Using Extracted Ocular Features Based Neural Architecture. A user's eyes provide means for Human Computer Interaction (HCI) research as
an important modal. The time to time scientific explorations of the eye has
already seen an upsurge of the benefits in HCI applications from gaze
estimation to the measure of attentiveness of a user looking at a screen for a
given time period. The eye tracking system as an assisting, interactive tool
can be incorporated by physically disabled individuals, fitted best for those
who have eyes as only a limited set of communication. The threefold objective
of this paper is - 1. To introduce a neural network based architecture to
predict users' gaze at 9 positions displayed in the 11.31{\deg} visual range on
the screen, through a low resolution based system such as a webcam in real time
by learning various aspects of eyes as an ocular feature set. 2.A collection of
coarsely supervised feature set obtained in real time which is also validated
through the user case study presented in the paper for 21 individuals ( 17 men
and 4 women ) from whom a 35k set of instances was derived with an accuracy
score of 82.36% and f1_score of 82.2% and 3.A detailed study over applicability
and underlying challenges of such systems. The experimental results verify the
feasibility and validity of the proposed eye gaze tracking model.","cs.CV,cs.HC,cs.LG"
"Profit Driven Decision Trees for Churn Prediction. Customer retention campaigns increasingly rely on predictive models to detect
potential churners in a vast customer base. From the perspective of machine
learning, the task of predicting customer churn can be presented as a binary
classification problem. Using data on historic behavior, classification
algorithms are built with the purpose of accurately predicting the probability
of a customer defecting. The predictive churn models are then commonly selected
based on accuracy related performance measures such as the area under the ROC
curve (AUC). However, these models are often not well aligned with the core
business requirement of profit maximization, in the sense that, the models fail
to take into account not only misclassification costs, but also the benefits
originating from a correct classification. Therefore, the aim is to construct
churn prediction models that are profitable and preferably interpretable too.
The recently developed expected maximum profit measure for customer churn
(EMPC) has been proposed in order to select the most profitable churn model. We
present a new classifier that integrates the EMPC metric directly into the
model construction. Our technique, called ProfTree, uses an evolutionary
algorithm for learning profit driven decision trees. In a benchmark study with
real-life data sets from various telecommunication service providers, we show
that ProfTree achieves significant profit improvements compared to classic
accuracy driven tree-based methods.","cs.LG,stat.AP,stat.ML"
"Enhanced Adversarial Strategically-Timed Attacks against Deep Reinforcement Learning. Recent deep neural networks based techniques, especially those equipped with
the ability of self-adaptation in the system level such as deep reinforcement
learning (DRL), are shown to possess many advantages of optimizing robot
learning systems (e.g., autonomous navigation and continuous robot arm
control.) However, the learning-based systems and the associated models may be
threatened by the risks of intentionally adaptive (e.g., noisy sensor
confusion) and adversarial perturbations from real-world scenarios. In this
paper, we introduce timing-based adversarial strategies against a DRL-based
navigation system by jamming in physical noise patterns on the selected time
frames. To study the vulnerability of learning-based navigation systems, we
propose two adversarial agent models: one refers to online learning; another
one is based on evolutionary learning. Besides, three open-source robot
learning and navigation control environments are employed to study the
vulnerability under adversarial timing attacks. Our experimental results show
that the adversarial timing attacks can lead to a significant performance drop,
and also suggest the necessity of enhancing the robustness of robot learning
systems.","cs.LG,eess.SP,stat.ML"
"Towards Understanding Cooperative Multi-Agent Q-Learning with Value Factorization. Value factorization is a popular and promising approach to scaling up
multi-agent reinforcement learning in cooperative settings. However, the
theoretical understanding of such methods is limited. In this paper, we
formalize a multi-agent fitted Q-iteration framework for analyzing factorized
multi-agent Q-learning. Based on this framework, we investigate linear value
factorization and reveal that multi-agent Q-learning with this simple
decomposition implicitly realizes a powerful counterfactual credit assignment,
but may not converge in some settings. Through further analysis, we find that
on-policy training or richer joint value function classes can improve its local
or global convergence properties, respectively. Finally, to support and extend
our theoretical implications to practical realization, we conduct an empirical
analysis of state-of-the-art deep multi-agent Q-learning algorithms on didactic
examples and a broad set of StarCraft II unit micromanagement tasks.","cs.AI,cs.LG,cs.MA,stat.ML"
"Operator Autoencoders: Learning Physical Operations on Encoded Molecular Graphs. Molecular dynamics simulations produce data with complex nonlinear dynamics.
If the timestep behavior of such a dynamic system can be represented by a
linear operator, future states can be inferred directly without expensive
simulations. The use of an autoencoder in combination with a physical timestep
operator allows both the relevant structural characteristics of the molecular
graphs and the underlying physics of the system to be isolated during the
training process. In this work, we develop a pipeline for establishing
graph-structured representations of time-series volumetric data from molecular
dynamics simulations. We then train an autoencoder to find nonlinear mappings
to a latent space where future timesteps can be predicted through application
of a linear operator trained in tandem with the autoencoder. Increasing the
dimensionality of the autoencoder output is shown to improve the accuracy of
the physical timestep operator.","cs.LG,physics.chem-ph"
"Energy-based Surprise Minimization for Multi-Agent Value Factorization. Multi-Agent Reinforcement Learning (MARL) has demonstrated significant
success in training decentralised policies in a centralised manner by making
use of value factorization methods. However, addressing surprise across
spurious states and approximation bias remain open problems for multi-agent
settings. Towards this goal, we introduce the Energy-based MIXer (EMIX), an
algorithm which minimizes surprise utilizing the energy across agents. Our
contributions are threefold; (1) EMIX introduces a novel surprise minimization
technique across multiple agents in the case of multi-agent
partially-observable settings. (2) EMIX highlights a practical use of energy
functions in MARL with theoretical guarantees and experiment validations of the
energy operator. Lastly, (3) EMIX extends Maxmin Q-learning for addressing
overestimation bias across agents in MARL. In a study of challenging StarCraft
II micromanagement scenarios, EMIX demonstrates consistent stable performance
for multiagent surprise minimization. Moreover, our ablation study highlights
the necessity of the energy-based scheme and the need for elimination of
overestimation bias in MARL. Our implementation of EMIX can be found at
karush17.github.io/emix-web/.","cs.LG,cs.MA,stat.ML"
"Local and Global Explanations of Agent Behavior: Integrating Strategy Summaries with Saliency Maps. With advances in reinforcement learning (RL), agents are now being developed
in high-stakes application domains such as healthcare and transportation.
Explaining the behavior of these agents is challenging, as the environments in
which they act have large state spaces, and their decision-making can be
affected by delayed rewards, making it difficult to analyze their behavior. To
address this problem, several approaches have been developed. Some approaches
attempt to convey the $\textit{global}$ behavior of the agent, describing the
actions it takes in different states. Other approaches devised $\textit{local}$
explanations which provide information regarding the agent's decision-making in
a particular state. In this paper, we combine global and local explanation
methods, and evaluate their joint and separate contributions, providing (to the
best of our knowledge) the first user study of combined local and global
explanations for RL agents. Specifically, we augment strategy summaries that
extract important trajectories of states from simulations of the agent with
saliency maps which show what information the agent attends to. Our results
show that the choice of what states to include in the summary (global
information) strongly affects people's understanding of agents: participants
shown summaries that included important states significantly outperformed
participants who were presented with agent behavior in a randomly set of chosen
world-states. We find mixed results with respect to augmenting demonstrations
with saliency maps (local information), as the addition of saliency maps did
not significantly improve performance in most cases. However, we do find some
evidence that saliency maps can help users better understand what information
the agent relies on in its decision making, suggesting avenues for future work
that can further improve explanations of RL agents.","cs.AI,cs.HC,cs.LG,cs.NE,stat.ML"
"Mind2Mind : transfer learning for GANs. Training generative adversarial networks (GANs) on high quality (HQ) images
involves important computing resources. This requirement represents a
bottleneck for the development of applications of GANs. We propose a transfer
learning technique for GANs that significantly reduces training time. Our
approach consists of freezing the low-level layers of both the critic and
generator of the original GAN. We assume an autoencoder constraint in order to
ensure the compatibility of the internal representations of the critic and the
generator. This assumption explains the gain in training time as it enables us
to bypass the low-level layers during the forward and backward passes. We
compare our method to baselines and observe a significant acceleration of the
training. It can reach two orders of magnitude on HQ datasets when compared
with StyleGAN. We prove rigorously, within the framework of optimal transport,
a theorem ensuring the convergence of the learning of the transferred GAN. We
moreover provide a precise bound for the convergence of the training in terms
of the distance between the source and target dataset.","68T45,cs.CV,cs.LG,stat.ML"
"Contextual-Bandit Anomaly Detection for IoT Data in Distributed Hierarchical Edge Computing. Advances in deep neural networks (DNN) greatly bolster real-time detection of
anomalous IoT data. However, IoT devices can hardly afford complex DNN models,
and offloading anomaly detection tasks to the cloud incurs long delay. In this
paper, we propose and build a demo for an adaptive anomaly detection approach
for distributed hierarchical edge computing (HEC) systems to solve this
problem, for both univariate and multivariate IoT data. First, we construct
multiple anomaly detection DNN models with increasing complexity, and associate
each model with a layer in HEC from bottom to top. Then, we design an adaptive
scheme to select one of these models on the fly, based on the contextual
information extracted from each input data. The model selection is formulated
as a contextual bandit problem characterized by a single-step Markov decision
process, and is solved using a reinforcement learning policy network. We build
an HEC testbed, implement our proposed approach, and evaluate it using real IoT
datasets. The demo shows that our proposed approach significantly reduces
detection delay (e.g., by 71.4% for univariate dataset) without sacrificing
accuracy, as compared to offloading detection tasks to the cloud. We also
compare it with other baseline schemes and demonstrate that it achieves the
best accuracy-delay tradeoff. Our demo is also available online:
https://rebrand.ly/91a71","cs.DC,cs.LG,cs.NI,stat.ML"
"Deep learning for molecular design - a review of the state of the art. In the space of only a few years, deep generative modeling has revolutionized
how we think of artificial creativity, yielding autonomous systems which
produce original images, music, and text. Inspired by these successes,
researchers are now applying deep generative modeling techniques to the
generation and optimization of molecules - in our review we found 45 papers on
the subject published in the past two years. These works point to a future
where such systems will be used to generate lead molecules, greatly reducing
resources spent downstream synthesizing and characterizing bad leads in the
lab. In this review we survey the increasingly complex landscape of models and
representation schemes that have been proposed. The four classes of techniques
we describe are recursive neural networks, autoencoders, generative adversarial
networks, and reinforcement learning. After first discussing some of the
mathematical fundamentals of each technique, we draw high level connections and
comparisons with other techniques and expose the pros and cons of each. Several
important high level themes emerge as a result of this work, including the
shift away from the SMILES string representation of molecules towards more
sophisticated representations such as graph grammars and 3D representations,
the importance of reward function design, the need for better standards for
benchmarking and testing, and the benefits of adversarial training and
reinforcement learning over maximum likelihood based training.","cs.LG,physics.chem-ph,stat.ML"
"A Markov Decision Process Approach to Active Meta Learning. In supervised learning, we fit a single statistical model to a given data
set, assuming that the data is associated with a singular task, which yields
well-tuned models for specific use, but does not adapt well to new contexts. By
contrast, in meta-learning, the data is associated with numerous tasks, and we
seek a model that may perform well on all tasks simultaneously, in pursuit of
greater generalization. One challenge in meta-learning is how to exploit
relationships between tasks and classes, which is overlooked by commonly used
random or cyclic passes through data. In this work, we propose actively
selecting samples on which to train by discerning covariates inside and between
meta-training sets. Specifically, we cast the problem of selecting a sample
from a number of meta-training sets as either a multi-armed bandit or a Markov
Decision Process (MDP), depending on how one encapsulates correlation across
tasks. We develop scheduling schemes based on Upper Confidence Bound (UCB),
Gittins Index and tabular Markov Decision Problems (MDPs) solved with linear
programming, where the reward is the scaled statistical accuracy to ensure it
is a time-invariant function of state and action. Across a variety of
experimental contexts, we observe significant reductions in sample complexity
of active selection scheme relative to cyclic or i.i.d. sampling, demonstrating
the merit of exploiting covariates in practice.","cs.LG,math.OC,stat.ML"
"Bilinear Supervised Hashing Based on 2D Image Features. Hashing has been recognized as an efficient representation learning method to
effectively handle big data due to its low computational complexity and memory
cost. Most of the existing hashing methods focus on learning the
low-dimensional vectorized binary features based on the high-dimensional raw
vectorized features. However, studies on how to obtain preferable binary codes
from the original 2D image features for retrieval is very limited. This paper
proposes a bilinear supervised discrete hashing (BSDH) method based on 2D image
features which utilizes bilinear projections to binarize the image matrix
features such that the intrinsic characteristics in the 2D image space are
preserved in the learned binary codes. Meanwhile, the bilinear projection
approximation and vectorization binary codes regression are seamlessly
integrated together to formulate the final robust learning framework.
Furthermore, a discrete optimization strategy is developed to alternatively
update each variable for obtaining the high-quality binary codes. In addition,
two 2D image features, traditional SURF-based FVLAD feature and CNN-based
AlexConv5 feature are designed for further improving the performance of the
proposed BSDH method. Results of extensive experiments conducted on four
benchmark datasets show that the proposed BSDH method almost outperforms all
competing hashing methods with different input features by different evaluation
protocols.","cs.CV,cs.MM"
"Video Captioning via Hierarchical Reinforcement Learning. Video captioning is the task of automatically generating a textual
description of the actions in a video. Although previous work (e.g.
sequence-to-sequence model) has shown promising results in abstracting a coarse
description of a short video, it is still very challenging to caption a video
containing multiple fine-grained actions with a detailed description. This
paper aims to address the challenge by proposing a novel hierarchical
reinforcement learning framework for video captioning, where a high-level
Manager module learns to design sub-goals and a low-level Worker module
recognizes the primitive actions to fulfill the sub-goal. With this
compositional framework to reinforce video captioning at different levels, our
approach significantly outperforms all the baseline methods on a newly
introduced large-scale dataset for fine-grained video captioning. Furthermore,
our non-ensemble model has already achieved the state-of-the-art results on the
widely-used MSR-VTT dataset.","cs.AI,cs.CL,cs.CV"
"Cross-Trajectory Representation Learning for Zero-Shot Generalization in RL. A highly desirable property of a reinforcement learning (RL) agent -- and a
major difficulty for deep RL approaches -- is the ability to generalize
policies learned on a few tasks over a high-dimensional observation space to
similar tasks not seen during training. Many promising approaches to this
challenge consider RL as a process of training two functions simultaneously: a
complex nonlinear encoder that maps high-dimensional observations to a latent
representation space, and a simple linear policy over this space. We posit that
a superior encoder for zero-shot generalization in RL can be trained by using
solely an auxiliary SSL objective if the training process encourages the
encoder to map behaviorally similar observations to similar representations, as
reward-based signal can cause overfitting in the encoder (Raileanu et al.,
2021). We propose Cross-Trajectory Representation Learning (CTRL), a method
that runs within an RL agent and conditions its encoder to recognize behavioral
similarity in observations by applying a novel SSL objective to pairs of
trajectories from the agent's policies. CTRL can be viewed as having the same
effect as inducing a pseudo-bisimulation metric but, crucially, avoids the use
of rewards and associated overfitting risks. Our experiments ablate various
components of CTRL and demonstrate that in combination with PPO it achieves
better generalization performance on the challenging Procgen benchmark suite
(Cobbe et al., 2020).","cs.AI,cs.LG"
"Classification and its applications for drug-target interaction identification. Classification is one of the most popular and widely used supervised learning
tasks, which categorizes objects into predefined classes based on known
knowledge. Classification has been an important research topic in machine
learning and data mining. Different classification methods have been proposed
and applied to deal with various real-world problems. Unlike unsupervised
learning such as clustering, a classifier is typically trained with labeled
data before being used to make prediction, and usually achieves higher accuracy
than unsupervised one.
  In this paper, we first define classification and then review several
representative methods. After that, we study in details the application of
classification to a critical problem in drug discovery, i.e., drug-target
prediction, due to the challenges in predicting possible interactions between
drugs and targets.","cs.LG,q-bio.MN,q-bio.QM"
"word2vec, node2vec, graph2vec, X2vec: Towards a Theory of Vector Embeddings of Structured Data. Vector representations of graphs and relational structures, whether
hand-crafted feature vectors or learned representations, enable us to apply
standard data analysis and machine learning techniques to the structures. A
wide range of methods for generating such embeddings have been studied in the
machine learning and knowledge representation literature. However, vector
embeddings have received relatively little attention from a theoretical point
of view.
  Starting with a survey of embedding techniques that have been used in
practice, in this paper we propose two theoretical approaches that we see as
central for understanding the foundations of vector embeddings. We draw
connections between the various approaches and suggest directions for future
research.","cs.DB,cs.DM,cs.LG,stat.ML"
"Saying What You're Looking For: Linguistics Meets Video Search. We present an approach to searching large video corpora for video clips which
depict a natural-language query in the form of a sentence. This approach uses
compositional semantics to encode subtle meaning that is lost in other systems,
such as the difference between two sentences which have identical words but
entirely different meaning: ""The person rode the horse} vs. \emph{The horse
rode the person"". Given a video-sentence pair and a natural-language parser,
along with a grammar that describes the space of sentential queries, we produce
a score which indicates how well the video depicts the sentence. We produce
such a score for each video clip in a corpus and return a ranked list of clips.
Furthermore, this approach addresses two fundamental problems simultaneously:
detecting and tracking objects, and recognizing whether those tracks depict the
query. Because both tracking and object detection are unreliable, this uses
knowledge about the intended sentential query to focus the tracker on the
relevant participants and ensures that the resulting tracks are described by
the sentential query. While earlier work was limited to single-word queries
which correspond to either verbs or nouns, we show how one can search for
complex queries which contain multiple phrases, such as prepositional phrases,
and modifiers, such as adverbs. We demonstrate this approach by searching for
141 queries involving people and horses interacting with each other in 10
full-length Hollywood movies.","cs.CL,cs.CV,cs.IR"
"Learning Direct Optimization for Scene Understanding. We develop a Learning Direct Optimization (LiDO) method for the refinement of
a latent variable model that describes input image x. Our goal is to explain a
single image x with an interpretable 3D computer graphics model having scene
graph latent variables z (such as object appearance, camera position). Given a
current estimate of z we can render a prediction of the image g(z), which can
be compared to the image x. The standard way to proceed is then to measure the
error E(x, g(z)) between the two, and use an optimizer to minimize the error.
However, it is unknown which error measure E would be most effective for
simultaneously addressing issues such as misaligned objects, occlusions,
textures, etc. In contrast, the LiDO approach trains a Prediction Network to
predict an update directly to correct z, rather than minimizing the error with
respect to z. Experiments show that our LiDO method converges rapidly as it
does not need to perform a search on the error landscape, produces better
solutions than error-based competitors, and is able to handle the mismatch
between the data and the fitted scene model. We apply LiDO to a realistic
synthetic dataset, and show that the method also transfers to work well with
real images.","cs.CV,cs.GR,cs.LG"
"Effects of Sampling Methods on Prediction Quality. The Case of Classifying Land Cover Using Decision Trees. Clever sampling methods can be used to improve the handling of big data and
increase its usefulness. The subject of this study is remote sensing,
specifically airborne laser scanning point clouds representing different
classes of ground cover. The aim is to derive a supervised learning model for
the classification using CARTs. In order to measure the effect of different
sampling methods on the classification accuracy, various experiments with
varying types of sampling methods, sample sizes, and accuracy metrics have been
designed. Numerical results for a subset of a large surveying project covering
the lower Rhine area in Germany are shown. General conclusions regarding
sampling design are drawn and presented.","cs.LG,stat.AP,stat.ML"
"Semi-supervised Learning for Aggregated Multilayer Graphs Using Diffuse Interface Methods and Fast Matrix Vector Products. We generalize a graph-based multiclass semi-supervised classification
technique based on diffuse interface methods to multilayer graphs. Besides the
treatment of various applications with an inherent multilayer structure, we
present a very flexible approach that interprets high-dimensional data in a
low-dimensional multilayer graph representation. Highly efficient numerical
methods involving the spectral decomposition of the corresponding differential
graph operators as well as fast matrix-vector products based on the
nonequispaced fast Fourier transform (NFFT) enable the rapid treatment of large
and high-dimensional data sets. We perform various numerical tests putting a
special focus on image segmentation. In particular, we test the performance of
our method on data sets with up to 10 million nodes per layer as well as up to
104 dimensions resulting in graphs with up to 52 layers. While all presented
numerical experiments can be run on an average laptop computer, the linear
dependence per iteration step of the runtime on the network size in all stages
of our algorithm makes it scalable to even larger and higher-dimensional
problems.","05C50,62H30,65F15,65T50,68R10,68T05,cs.LG,cs.NA,math.NA,stat.ML"
"ADA: A Game-Theoretic Perspective on Data Augmentation for Object Detection. The use of random perturbations of ground truth data, such as random
translation or scaling of bounding boxes, is a common heuristic used for data
augmentation that has been shown to prevent overfitting and improve
generalization. Since the design of data augmentation is largely guided by
reported best practices, it is difficult to understand if those design choices
are optimal. To provide a more principled perspective, we develop a
game-theoretic interpretation of data augmentation in the context of object
detection. We aim to find an optimal adversarial perturbations of the ground
truth data (i.e., the worst case perturbations) that forces the object bounding
box predictor to learn from the hardest distribution of perturbed examples for
better test-time performance. We establish that the game theoretic solution,
the Nash equilibrium, provides both an optimal predictor and optimal data
augmentation distribution. We show that our adversarial method of training a
predictor can significantly improve test time performance for the task of
object detection. On the ImageNet object detection task, our adversarial
approach improves performance by over 16\% compared to the best performing data
augmentation method","cs.AI,cs.CV,cs.GT"
"Health Status Prediction with Local-Global Heterogeneous Behavior Graph. Health management is getting increasing attention all over the world.
However, existing health management mainly relies on hospital examination and
treatment, which are complicated and untimely. The emerging of mobile devices
provides the possibility to manage people's health status in a convenient and
instant way. Estimation of health status can be achieved with various kinds of
data streams continuously collected from wearable sensors. However, these data
streams are multi-source and heterogeneous, containing complex temporal
structures with local contextual and global temporal aspects, which makes the
feature learning and data joint utilization challenging. We propose to model
the behavior-related multi-source data streams with a local-global graph, which
contains multiple local context sub-graphs to learn short term local context
information with heterogeneous graph neural networks and a global temporal
sub-graph to learn long term dependency with self-attention networks. Then
health status is predicted based on the structure-aware representation learned
from the local-global behavior graph. We take experiments on StudentLife
dataset, and extensive results demonstrate the effectiveness of our proposed
model.","cs.LG,cs.MM"
"Feedback graph regret bounds for Thompson Sampling and UCB. We study the stochastic multi-armed bandit problem with the graph-based
feedback structure introduced by Mannor and Shamir. We analyze the performance
of the two most prominent stochastic bandit algorithms, Thompson Sampling and
Upper Confidence Bound (UCB), in the graph-based feedback setting. We show that
these algorithms achieve regret guarantees that combine the graph structure and
the gaps between the means of the arm distributions. Surprisingly this holds
despite the fact that these algorithms do not explicitly use the graph
structure to select arms; they observe the additional feedback but do not
explore based on it. Towards this result we introduce a ""layering technique""
highlighting the commonalities in the two algorithms.","cs.DS,cs.LG,stat.ML"
"Multidimensional Scaling: Approximation and Complexity. Metric Multidimensional scaling (MDS) is a classical method for generating
meaningful (non-linear) low-dimensional embeddings of high-dimensional data.
MDS has a long history in the statistics, machine learning, and graph drawing
communities. In particular, the Kamada-Kawai force-directed graph drawing
method is equivalent to MDS and is one of the most popular ways in practice to
embed graphs into low dimensions. Despite its ubiquity, our theoretical
understanding of MDS remains limited as its objective function is highly
non-convex. In this paper, we prove that minimizing the Kamada-Kawai objective
is NP-hard and give a provable approximation algorithm for optimizing it, which
in particular is a PTAS on low-diameter graphs. We supplement this result with
experiments suggesting possible connections between our greedy approximation
algorithm and gradient-based methods.","cs.CG,cs.DS,cs.LG,stat.ML"
"Training like Playing: A Reinforcement Learning And Knowledge Graph-based framework for building Automatic Consultation System in Medical Field. We introduce a framework for AI-based medical consultation system with
knowledge graph embedding and reinforcement learning components and its
implement. Our implement of this framework leverages knowledge organized as a
graph to have diagnosis according to evidence collected from patients
recurrently and dynamically. According to experiment we designed for evaluating
its performance, it archives a good result. More importantly, for getting
better performance, researchers can implement it on this framework based on
their innovative ideas, well designed experiments and even clinical trials.","cs.LG,cs.SE"
"Mol-CycleGAN - a generative model for molecular optimization. Designing a molecule with desired properties is one of the biggest challenges
in drug development, as it requires optimization of chemical compound
structures with respect to many complex properties. To augment the compound
design process we introduce Mol-CycleGAN - a CycleGAN-based model that
generates optimized compounds with high structural similarity to the original
ones. Namely, given a molecule our model generates a structurally similar one
with an optimized value of the considered property. We evaluate the performance
of the model on selected optimization objectives related to structural
properties (presence of halogen groups, number of aromatic rings) and to a
physicochemical property (penalized logP). In the task of optimization of
penalized logP of drug-like molecules our model significantly outperforms
previous results.","cs.LG,physics.chem-ph,stat.ML"
"A Computationally Efficient Neural Network Invariant to the Action of Symmetry Subgroups. We introduce a method to design a computationally efficient $G$-invariant
neural network that approximates functions invariant to the action of a given
permutation subgroup $G \leq S_n$ of the symmetric group on input data. The key
element of the proposed network architecture is a new $G$-invariant
transformation module, which produces a $G$-invariant latent representation of
the input data. This latent representation is then processed with a multi-layer
perceptron in the network. We prove the universality of the proposed
architecture, discuss its properties and highlight its computational and memory
efficiency. Theoretical considerations are supported by numerical experiments
involving different network configurations, which demonstrate the effectiveness
and strong generalization properties of the proposed method in comparison to
other $G$-invariant neural networks.","I.2.6,cs.LG,cs.NE,stat.ML"
"Scalable Bayesian Preference Learning for Crowds. We propose a scalable Bayesian preference learning method for jointly
predicting the preferences of individuals as well as the consensus of a crowd
from pairwise labels. Peoples' opinions often differ greatly, making it
difficult to predict their preferences from small amounts of personal data.
Individual biases also make it harder to infer the consensus of a crowd when
there are few labels per item. We address these challenges by combining matrix
factorisation with Gaussian processes, using a Bayesian approach to account for
uncertainty arising from noisy and sparse data. Our method exploits input
features, such as text embeddings and user metadata, to predict preferences for
new items and users that are not in the training set. As previous solutions
based on Gaussian processes do not scale to large numbers of users, items or
pairwise labels, we propose a stochastic variational inference approach that
limits computational and memory costs. Our experiments on a recommendation task
show that our method is competitive with previous approaches despite our
scalable inference approximation. We demonstrate the method's scalability on a
natural language processing task with thousands of users and items, and show
improvements over the state of the art on this task. We make our software
publicly available for future work.","cs.CL,cs.HC,cs.LG,stat.ML"
"Detecting Nonlinear Causality in Multivariate Time Series with Sparse Additive Models. We propose a nonparametric method for detecting nonlinear causal relationship
within a set of multidimensional discrete time series, by using sparse additive
models (SpAMs). We show that, when the input to the SpAM is a $\beta$-mixing
time series, the model can be fitted by first approximating each unknown
function with a linear combination of a set of B-spline bases, and then solving
a group-lasso-type optimization problem with nonconvex regularization.
Theoretically, we characterize the oracle statistical properties of the
proposed sparse estimator in function estimation and model selection.
Numerically, we propose an efficient pathwise iterative shrinkage thresholding
algorithm (PISTA), which tames the nonconvexity and guarantees linear
convergence towards the desired sparse estimator with high probability.","cs.LG,stat.ME,stat.ML"
"Online Sequence Training of Recurrent Neural Networks with Connectionist Temporal Classification. Connectionist temporal classification (CTC) based supervised sequence
training of recurrent neural networks (RNNs) has shown great success in many
machine learning areas including end-to-end speech and handwritten character
recognition. For the CTC training, however, it is required to unroll (or
unfold) the RNN by the length of an input sequence. This unrolling requires a
lot of memory and hinders a small footprint implementation of online learning
or adaptation. Furthermore, the length of training sequences is usually not
uniform, which makes parallel training with multiple sequences inefficient on
shared memory models such as graphics processing units (GPUs). In this work, we
introduce an expectation-maximization (EM) based online CTC algorithm that
enables unidirectional RNNs to learn sequences that are longer than the amount
of unrolling. The RNNs can also be trained to process an infinitely long input
sequence without pre-segmentation or external reset. Moreover, the proposed
approach allows efficient parallel training on GPUs. For evaluation, phoneme
recognition and end-to-end speech recognition examples are presented on the
TIMIT and Wall Street Journal (WSJ) corpora, respectively. Our online model
achieves 20.7% phoneme error rate (PER) on the very long input sequence that is
generated by concatenating all 192 utterances in the TIMIT core test set. On
WSJ, a network can be trained with only 64 times of unrolling while sacrificing
4.5% relative word error rate (WER).","cs.LG,cs.NE"
"Joint Large-Scale Motion Estimation and Image Reconstruction. This article describes the implementation of the joint motion estimation and
image reconstruction framework presented by Burger, Dirks and Sch\""onlieb and
extends this framework to large-scale motion between consecutive image frames.
The variational framework uses displacements between consecutive frames based
on the optical flow approach to improve the image reconstruction quality on the
one hand and the motion estimation quality on the other. The energy functional
consists of a data-fidelity term with a general operator that connects the
input sequence to the solution, it has a total variation term for the image
sequence and is connected to the underlying flow using an optical flow term.
Additional spatial regularity for the flow is modeled by a total variation
regularizer for both components of the flow. The numerical minimization is
performed in an alternating manner using primal-dual techniques. The resulting
schemes are presented as pseudo-code together with a short numerical
evaluation.","65K10,65M06,68U10,I.4; G.1.6; G.4,cs.CV,math.OC"
"Large-Scale Extended Granger Causality for Classification of Marijuana Users From Functional MRI. It has been shown in the literature that marijuana use is associated with
changes in brain network connectivity. We propose large-scale Extended Granger
Causality (lsXGC) and investigate whether it can capture such changes using
resting-state fMRI. This method combines dimension reduction with source
time-series augmentation and uses predictive time-series modeling for
estimating directed causal relationships among fMRI time-series. It is a
multivariate approach, since it is capable of identifying the interdependence
of time-series in the presence of all other time-series of the underlying
dynamic system. Here, we investigate whether this model can serve as a
biomarker for classifying marijuana users from typical controls using 126 adult
subjects with a childhood diagnosis of ADHD from the Addiction Connectome
Preprocessed Initiative (ACPI) database. We use brain connections estimated by
lsXGC as features for classification. After feature extraction, we perform
feature selection by Kendall's-tau rank correlation coefficient followed by
classification using a support vector machine. As a reference method, we
compare our results with cross-correlation, which is typically used in the
literature as a standard measure of functional connectivity. Within a
cross-validation scheme of 100 different training/test (90%/10%) data splits,
we obtain a mean accuracy range of [0.714, 0.985] and a mean Area Under the
receiver operating characteristic Curve (AUC) range of [0.779, 0.999] across
all tested numbers of features for lsXGC, which is significantly better than
results obtained with cross-correlation, namely mean accuracy of [0.728, 0.912]
and mean AUC of [0.825, 0.969]. Our results suggest the applicability of lsXGC
as a potential biomarker for marijuana use.","cs.CE,cs.LG,eess.IV"
"SoilingNet: Soiling Detection on Automotive Surround-View Cameras. Cameras are an essential part of sensor suite in autonomous driving.
Surround-view cameras are directly exposed to external environment and are
vulnerable to get soiled. Cameras have a much higher degradation in performance
due to soiling compared to other sensors. Thus it is critical to accurately
detect soiling on the cameras, particularly for higher levels of autonomous
driving. We created a new dataset having multiple types of soiling namely
opaque and transparent. It will be released publicly as part of our WoodScape
dataset \cite{yogamani2019woodscape} to encourage further research. We
demonstrate high accuracy using a Convolutional Neural Network (CNN) based
architecture. We also show that it can be combined with the existing object
detection task in a multi-task learning framework. Finally, we make use of
Generative Adversarial Networks (GANs) to generate more images for data
augmentation and show that it works successfully similar to the style transfer.","cs.AI,cs.CV,cs.LG,cs.RO,stat.ML"
"Quantitative stability of optimal transport maps and linearization of the 2-Wasserstein space. This work studies an explicit embedding of the set of probability measures
into a Hilbert space, defined using optimal transport maps from a reference
probability density. This embedding linearizes to some extent the 2-Wasserstein
space, and enables the direct use of generic supervised and unsupervised
learning algorithms on measure data. Our main result is that the embedding is
(bi-)H\""older continuous, when the reference density is uniform over a convex
set, and can be equivalently phrased as a dimension-independent
H\""older-stability results for optimal transport maps.","cs.LG,cs.NA,math.MG,math.NA,stat.ML"
"GrADE: A graph based data-driven solver for time-dependent nonlinear partial differential equations. The physical world is governed by the laws of physics, often represented in
form of nonlinear partial differential equations (PDEs). Unfortunately,
solution of PDEs is non-trivial and often involves significant computational
time. With recent developments in the field of artificial intelligence and
machine learning, the solution of PDEs using neural network has emerged as a
domain with huge potential. However, most of the developments in this field are
based on either fully connected neural networks (FNN) or convolutional neural
networks (CNN). While FNN is computationally inefficient as the number of
network parameters can be potentially huge, CNN necessitates regular grid and
simpler domain. In this work, we propose a novel framework referred to as the
Graph Attention Differential Equation (GrADE) for solving time dependent
nonlinear PDEs. The proposed approach couples FNN, graph neural network, and
recently developed Neural ODE framework. The primary idea is to use graph
neural network for modeling the spatial domain, and Neural ODE for modeling the
temporal domain. The attention mechanism identifies important inputs/features
and assign more weightage to the same; this enhances the performance of the
proposed framework. Neural ODE, on the other hand, results in constant memory
cost and allows trading of numerical precision for speed. We also propose depth
refinement as an effective technique for training the proposed architecture in
lesser time with better accuracy. The effectiveness of the proposed framework
is illustrated using 1D and 2D Burgers' equations. Results obtained illustrate
the capability of the proposed framework in modeling PDE and its scalability to
larger domains without the need for retraining.","cs.LG,physics.comp-ph,stat.ML"
"Symmetry-Aware Actor-Critic for 3D Molecular Design. Automating molecular design using deep reinforcement learning (RL) has the
potential to greatly accelerate the search for novel materials. Despite recent
progress on leveraging graph representations to design molecules, such methods
are fundamentally limited by the lack of three-dimensional (3D) information. In
light of this, we propose a novel actor-critic architecture for 3D molecular
design that can generate molecular structures unattainable with previous
approaches. This is achieved by exploiting the symmetries of the design process
through a rotationally covariant state-action representation based on a
spherical harmonics series expansion. We demonstrate the benefits of our
approach on several 3D molecular design tasks, where we find that building in
such symmetries significantly improves generalization and the quality of
generated molecules.","cs.LG,physics.chem-ph,stat.ML"
"Finding Effective Security Strategies through Reinforcement Learning and Self-Play. We present a method to automatically find security strategies for the use
case of intrusion prevention. Following this method, we model the interaction
between an attacker and a defender as a Markov game and let attack and defense
strategies evolve through reinforcement learning and self-play without human
intervention. Using a simple infrastructure configuration, we demonstrate that
effective security strategies can emerge from self-play. This shows that
self-play, which has been applied in other domains with great success, can be
effective in the context of network security. Inspection of the converged
policies show that the emerged policies reflect common-sense knowledge and are
similar to strategies of humans. Moreover, we address known challenges of
reinforcement learning in this domain and present an approach that uses
function approximation, an opponent pool, and an autoregressive policy
representation. Through evaluations we show that our method is superior to two
baseline methods but that policy convergence in self-play remains a challenge.","cs.CR,cs.LG,cs.NI,stat.ML"
"A New Unbiased and Efficient Class of LSH-Based Samplers and Estimators for Partition Function Computation in Log-Linear Models. Log-linear models are arguably the most successful class of graphical models
for large-scale applications because of their simplicity and tractability.
Learning and inference with these models require calculating the partition
function, which is a major bottleneck and intractable for large state spaces.
Importance Sampling (IS) and MCMC-based approaches are lucrative. However, the
condition of having a ""good"" proposal distribution is often not satisfied in
practice.
  In this paper, we add a new dimension to efficient estimation via sampling.
We propose a new sampling scheme and an unbiased estimator that estimates the
partition function accurately in sub-linear time. Our samples are generated in
near-constant time using locality sensitive hashing (LSH), and so are
correlated and unnormalized. We demonstrate the effectiveness of our proposed
approach by comparing the accuracy and speed of estimating the partition
function against other state-of-the-art estimation techniques including IS and
the efficient variant of Gumbel-Max sampling. With our efficient sampling
scheme, we accurately train real-world language models using only 1-2% of
computations.","cs.DB,cs.DS,cs.LG,stat.ML"
"Fundamental Limits of Deep Graph Convolutional Networks. Graph convolutional networks (GCNs) are a widely used method for graph
representation learning. To elucidate the capabilities and limitations of GCNs,
we investigate their power, as a function of their number of layers, to
distinguish between different random graph models (corresponding to different
class-conditional distributions in a classification problem) on the basis of
the embeddings of their sample graphs. In particular, the graph models that we
consider arise from graphons, which are the most general possible
parameterizations of infinite exchangeable graph models and which are the
central objects of study in the theory of dense graph limits. We give a precise
characterization of the set of pairs of graphons that are indistinguishable by
a GCN with nonlinear activation functions coming from a certain broad class if
its depth is at least logarithmic in the size of the sample graph. This
characterization is in terms of a degree profile closeness property. Outside
this class, a very simple GCN architecture suffices for distinguishability. We
then exhibit a concrete, infinite class of graphons arising from stochastic
block models that are well-separated in terms of cut distance and are
indistinguishable by a GCN. These results theoretically match empirical
observations of several prior works. To prove our results, we exploit a
connection to random walks on graphs. Finally, we give empirical results on
synthetic and real graph classification datasets, indicating that
indistinguishable graph distributions arise in practice.","cs.IT,cs.LG,math.IT,math.PR,stat.ML"
"A survey on natural language processing (nlp) and applications in insurance. Text is the most widely used means of communication today. This data is
abundant but nevertheless complex to exploit within algorithms. For years,
scientists have been trying to implement different techniques that enable
computers to replicate some mechanisms of human reading. During the past five
years, research disrupted the capacity of the algorithms to unleash the value
of text data. It brings today, many opportunities for the insurance
industry.Understanding those methods and, above all, knowing how to apply them
is a major challenge and key to unleash the value of text data that have been
stored for many years. Processing language with computer brings many new
opportunities especially in the insurance sector where reports are central in
the information used by insurers. SCOR's Data Analytics team has been working
on the implementation of innovative tools or products that enable the use of
the latest research on text analysis. Understanding text mining techniques in
insurance enhances the monitoring of the underwritten risks and many processes
that finally benefit policyholders.This article proposes to explain
opportunities that Natural Language Processing (NLP) are providing to
insurance. It details different methods used today in practice traces back the
story of them. We also illustrate the implementation of certain methods using
open source libraries and python codes that we have developed to facilitate the
use of these techniques.After giving a general overview on the evolution of
text mining during the past few years,we share about how to conduct a full
study with text mining and share some examples to serve those models into
insurance products or services. Finally, we explained in more details every
step that composes a Natural Language Processing study to ensure the reader can
have a deep understanding on the implementation.","cs.CL,cs.LG,stat.ML"
"Anomaly Detection using Deep Learning based Image Completion. Automated surface inspection is an important task in many manufacturing
industries and often requires machine learning driven solutions. Supervised
approaches, however, can be challenging, since it is often difficult to obtain
large amounts of labeled training data. In this work, we instead perform
one-class unsupervised learning on fault-free samples by training a deep
convolutional neural network to complete images whose center regions are cut
out. Since the network is trained exclusively on fault-free data, it completes
the image patches with a fault-free version of the missing image region. The
pixel-wise reconstruction error within the cut out region is an anomaly image
which can be used for anomaly detection. Results on surface images of decorated
plastic parts demonstrate that this approach is suitable for detection of
visible anomalies and moreover surpasses all other tested methods.","68T45,cs.CV"
"DFTerNet: Towards 2-bit Dynamic Fusion Networks for Accurate Human Activity Recognition. Deep Convolutional Neural Networks (DCNNs) are currently popular in human
activity recognition applications. However, in the face of modern artificial
intelligence sensor-based games, many research achievements cannot be
practically applied on portable devices. DCNNs are typically resource-intensive
and too large to be deployed on portable devices, thus this limits the
practical application of complex activity detection. In addition, since
portable devices do not possess high-performance Graphic Processing Units
(GPUs), there is hardly any improvement in Action Game (ACT) experience.
Besides, in order to deal with multi-sensor collaboration, all previous human
activity recognition models typically treated the representations from
different sensor signal sources equally. However, distinct types of activities
should adopt different fusion strategies. In this paper, a novel scheme is
proposed. This scheme is used to train 2-bit Convolutional Neural Networks with
weights and activations constrained to {-0.5,0,0.5}. It takes into account the
correlation between different sensor signal sources and the activity types.
This model, which we refer to as DFTerNet, aims at producing a more reliable
inference and better trade-offs for practical applications. Our basic idea is
to exploit quantization of weights and activations directly in pre-trained
filter banks and adopt dynamic fusion strategies for different activity types.
Experiments demonstrate that by using dynamic fusion strategy can exceed the
baseline model performance by up to ~5% on activity recognition like
OPPORTUNITY and PAMAP2 datasets. Using the quantization method proposed, we
were able to achieve performances closer to that of full-precision counterpart.
These results were also verified using the UniMiB-SHAR dataset. In addition,
the proposed method can achieve ~9x acceleration on CPUs and ~11x memory
saving.","cs.AI,cs.CV,cs.HC,cs.LG,stat.ML"
"COUnty aggRegation mixup AuGmEntation (COURAGE) COVID-19 Prediction. The global spread of COVID-19, the disease caused by the novel coronavirus
SARS-CoV-2, has cast a significant threat to mankind. As the COVID-19 situation
continues to evolve, predicting localized disease severity is crucial for
advanced resource allocation. This paper proposes a method named COURAGE
(COUnty aggRegation mixup AuGmEntation) to generate a short-term prediction of
2-week-ahead COVID-19 related deaths for each county in the United States,
leveraging modern deep learning techniques. Specifically, our method adopts a
self-attention model from Natural Language Processing, known as the transformer
model, to capture both short-term and long-term dependencies within the time
series while enjoying computational efficiency. Our model fully utilizes
publicly available information of COVID-19 related confirmed cases, deaths,
community mobility trends and demographic information, and can produce
state-level prediction as an aggregation of the corresponding county-level
predictions. Our numerical experiments demonstrate that our model achieves the
state-of-the-art performance among the publicly available benchmark models.","cs.LG,stat.AP"
"Probabilistic Permutation Synchronization using the Riemannian Structure of the Birkhoff Polytope. We present an entirely new geometric and probabilistic approach to
synchronization of correspondences across multiple sets of objects or images.
In particular, we present two algorithms: (1) Birkhoff-Riemannian L-BFGS for
optimizing the relaxed version of the combinatorially intractable cycle
consistency loss in a principled manner, (2) Birkhoff-Riemannian Langevin Monte
Carlo for generating samples on the Birkhoff Polytope and estimating the
confidence of the found solutions. To this end, we first introduce the very
recently developed Riemannian geometry of the Birkhoff Polytope. Next, we
introduce a new probabilistic synchronization model in the form of a Markov
Random Field (MRF). Finally, based on the first order retraction operators, we
formulate our problem as simulating a stochastic differential equation and
devise new integrators. We show on both synthetic and real datasets that we
achieve high quality multi-graph matching results with faster convergence and
reliable confidence/uncertainty estimates.","cs.CV,cs.GR,cs.LG,cs.NA,cs.RO"
"DLBricks: Composable Benchmark Generation to Reduce Deep Learning Benchmarking Effort on CPUs (Extended). The past few years have seen a surge of applying Deep Learning (DL) models
for a wide array of tasks such as image classification, object detection,
machine translation, etc. While DL models provide an opportunity to solve
otherwise intractable tasks, their adoption relies on them being optimized to
meet latency and resource requirements. Benchmarking is a key step in this
process but has been hampered in part due to the lack of representative and
up-to-date benchmarking suites. This is exacerbated by the fast-evolving pace
of DL models.
  This paper proposes DLBricks, a composable benchmark generation design that
reduces the effort of developing, maintaining, and running DL benchmarks on
CPUs. DLBricks decomposes DL models into a set of unique runnable networks and
constructs the original model's performance using the performance of the
generated benchmarks. DLBricks leverages two key observations: DL layers are
the performance building blocks of DL models and layers are extensively
repeated within and across DL models. Since benchmarks are generated
automatically and the benchmarking time is minimized, DLBricks can keep
up-to-date with the latest proposed models, relieving the pressure of selecting
representative DL models. Moreover, DLBricks allows users to represent
proprietary models within benchmark suites. We evaluate DLBricks using $50$
MXNet models spanning $5$ DL tasks on $4$ representative CPU systems. We show
that DLBricks provides an accurate performance estimate for the DL models and
reduces the benchmarking time across systems (e.g. within $95\%$ accuracy and
up to $4.4\times$ benchmarking time speedup on Amazon EC2 c5.xlarge).","cs.LG,cs.PF,cs.SE,stat.ML"
"Logic-based Clustering and Learning for Time-Series Data. To effectively analyze and design cyberphysical systems (CPS), designers
today have to combat the data deluge problem, i.e., the burden of processing
intractably large amounts of data produced by complex models and experiments.
In this work, we utilize monotonic Parametric Signal Temporal Logic (PSTL) to
design features for unsupervised classification of time series data. This
enables using off-the-shelf machine learning tools to automatically cluster
similar traces with respect to a given PSTL formula. We demonstrate how this
technique produces interpretable formulas that are amenable to analysis and
understanding using a few representative examples. We illustrate this with case
studies related to automotive engine testing, highway traffic analysis, and
auto-grading massively open online courses.","cs.LG,cs.LO"
"Deep Learning Framework Applied for Predicting Anomaly of Respiratory Sounds. This paper proposes a robust deep learning framework used for classifying
anomaly of respiratory cycles. Initially, our framework starts with front-end
feature extraction step. This step aims to transform the respiratory input
sound into a two-dimensional spectrogram where both spectral and temporal
features are well presented. Next, an ensemble of C- DNN and Autoencoder
networks is then applied to classify into four categories of respiratory
anomaly cycles. In this work, we conducted experiments over 2017 Internal
Conference on Biomedical Health Informatics (ICBHI) benchmark dataset. As a
result, we achieve competitive performances with ICBHI average score of 0.49,
ICBHI harmonic score of 0.42.","cs.CV,cs.LG,cs.SD,eess.AS"
"Attention-Based Learning on Molecular Ensembles. The three-dimensional shape and conformation of small-molecule ligands are
critical for biomolecular recognition, yet encoding 3D geometry has not
improved ligand-based virtual screening approaches. We describe an end-to-end
deep learning approach that operates directly on small-molecule conformational
ensembles and identifies key conformational poses of small-molecules. Our
networks leverage two levels of representation learning: 1) individual
conformers are first encoded as spatial graphs using a graph neural network,
and 2) sampled conformational ensembles are represented as sets using an
attention mechanism to aggregate over individual instances. We demonstrate the
feasibility of this approach on a simple task based on bidentate coordination
of biaryl ligands, and show how attention-based pooling can elucidate key
conformational poses in tasks based on molecular geometry. This work
illustrates how set-based learning approaches may be further developed for
small molecule-based virtual screening.","cs.LG,physics.chem-ph"
"Boosting Template-based SSVEP Decoding by Cross-domain Transfer Learning. Objective: This study aims to establish a generalized transfer-learning
framework for boosting the performance of steady-state visual evoked potential
(SSVEP)-based brain-computer interfaces (BCIs) by leveraging cross-domain data
transferring. Approach: We enhanced the state-of-the-art template-based SSVEP
decoding through incorporating a least-squares transformation (LST)-based
transfer learning to leverage calibration data across multiple domains
(sessions, subjects, and EEG montages). Main results: Study results verified
the efficacy of LST in obviating the variability of SSVEPs when transferring
existing data across domains. Furthermore, the LST-based method achieved
significantly higher SSVEP-decoding accuracy than the standard task-related
component analysis (TRCA)-based method and the non-LST naive transfer-learning
method. Significance: This study demonstrated the capability of the LST-based
transfer learning to leverage existing data across subjects and/or devices with
an in-depth investigation of its rationale and behavior in various
circumstances. The proposed framework significantly improved the SSVEP decoding
accuracy over the standard TRCA approach when calibration data are limited. Its
performance in calibration reduction could facilitate plug-and-play SSVEP-based
BCIs and further practical applications.","cs.LG,eess.SP"
"Multiresolution Graph Variational Autoencoder. In this paper, we propose Multiresolution Graph Networks (MGN) and
Multiresolution Graph Variational Autoencoders (MGVAE) to learn and generate
graphs in a multiresolution and equivariant manner. At each resolution level,
MGN employs higher order message passing to encode the graph while learning to
partition it into mutually exclusive clusters and coarsening into a lower
resolution. MGVAE constructs a hierarchical generative model based on MGN to
variationally autoencode the hierarchy of coarsened graphs. Our proposed
framework is end-to-end permutation equivariant with respect to node ordering.
Our methods have been successful with several generative tasks including link
prediction on citation graphs, unsupervised molecular representation learning
to predict molecular properties, molecular generation, general graph generation
and graph-based image generation.","cs.LG,cs.SI,physics.chem-ph"
"Adversarial Robustness Guarantees for Random Deep Neural Networks. The reliability of deep learning algorithms is fundamentally challenged by
the existence of adversarial examples, which are incorrectly classified inputs
that are extremely close to a correctly classified input. We explore the
properties of adversarial examples for deep neural networks with random weights
and biases, and prove that for any $p\ge1$, the $\ell^p$ distance of any given
input from the classification boundary scales as one over the square root of
the dimension of the input times the $\ell^p$ norm of the input. The results
are based on the recently proved equivalence between Gaussian processes and
deep neural networks in the limit of infinite width of the hidden layers, and
are validated with experiments on both random deep neural networks and deep
neural networks trained on the MNIST and CIFAR10 datasets. The results
constitute a fundamental advance in the theoretical understanding of
adversarial examples, and open the way to a thorough theoretical
characterization of the relation between network architecture and robustness to
adversarial perturbations.","cond-mat.dis-nn,cs.LG,math-ph,math.MP,quant-ph,stat.ML"
"Deep Leakage from Gradients. Exchanging gradients is a widely used method in modern multi-node machine
learning system (e.g., distributed training, collaborative learning). For a
long time, people believed that gradients are safe to share: i.e., the training
data will not be leaked by gradient exchange. However, we show that it is
possible to obtain the private training data from the publicly shared
gradients. We name this leakage as Deep Leakage from Gradient and empirically
validate the effectiveness on both computer vision and natural language
processing tasks. Experimental results show that our attack is much stronger
than previous approaches: the recovery is pixel-wise accurate for images and
token-wise matching for texts. We want to raise people's awareness to rethink
the gradient's safety. Finally, we discuss several possible strategies to
prevent such deep leakage. The most effective defense method is gradient
pruning.","cs.CR,cs.LG,stat.ML"
"A Pilot Study on Visually Stimulated Cognitive Tasks for EEG-Based Dementia Recognition. In the status quo, dementia is yet to be cured. Precise diagnosis prior to
the onset of the symptoms can prevent the rapid progression of the emerging
cognitive impairment. Recent progress has shown that Electroencephalography
(EEG) is the promising and cost-effective test to facilitate the detection of
neurocognitive disorders. However, most of the existing works have been using
only resting-state EEG. The efficiencies of EEG signals from various cognitive
tasks, for dementia classification, have yet to be thoroughly investigated. In
this study, we designed four cognitive tasks that engage different cognitive
performances: attention, working memory, and executive function. We
investigated these tasks by using statistical analysis on both time and
frequency domains of EEG signals from three classes of human subjects: Dementia
(DEM), Mild Cognitive Impairment (MCI), and Normal Control (NC). We also
further evaluated the classification performances of two features extraction
methods: Principal Component Analysis (PCA) and Filter Bank Common Spatial
Pattern (FBCSP). We found that the working memory related tasks yielded good
performances for dementia recognition in both cases using PCA and FBCSP.
Moreover, FBCSP with features combination from four tasks revealed the best
sensitivity of 0.87 and the specificity of 0.80. To our best knowledge, this is
the first work that concurrently investigated several cognitive tasks for
dementia recognition using both statistical analysis and classification scores.
Our results yielded essential information to design and aid in conducting
further experimental tasks to early diagnose dementia patients.","cs.LG,eess.SP,q-bio.NC"
"Behavior Mimics Distribution: Combining Individual and Group Behaviors for Federated Learning. Federated Learning (FL) has become an active and promising distributed
machine learning paradigm. As a result of statistical heterogeneity, recent
studies clearly show that the performance of popular FL methods (e.g., FedAvg)
deteriorates dramatically due to the client drift caused by local updates. This
paper proposes a novel Federated Learning algorithm (called IGFL), which
leverages both Individual and Group behaviors to mimic distribution, thereby
improving the ability to deal with heterogeneity. Unlike existing FL methods,
our IGFL can be applied to both client and server optimization. As a
by-product, we propose a new attention-based federated learning in the server
optimization of IGFL. To the best of our knowledge, this is the first time to
incorporate attention mechanisms into federated optimization. We conduct
extensive experiments and show that IGFL can significantly improve the
performance of existing federated learning methods. Especially when the
distributions of data among individuals are diverse, IGFL can improve the
classification accuracy by about 13% compared with prior baselines.","cs.CV,cs.DC,cs.LG"
"A short note on the decision tree based neural turing machine. Turing machine and decision tree have developed independently for a long
time. With the recent development of differentiable models, there is an
intersection between them. Neural turing machine(NTM) opens door for the memory
network. It use differentiable attention mechanism to read/write external
memory bank. Differentiable forest brings differentiable properties to
classical decision tree. In this short note, we show the deep connection
between these two models. That is: differentiable forest is a special case of
NTM. Differentiable forest is actually decision tree based neural turing
machine. Based on this deep connection, we propose a response augmented
differential forest (RaDF). The controller of RaDF is differentiable forest,
the external memory of RaDF are response vectors which would be read/write by
leaf nodes.","cs.AI,cs.LG,cs.NE"
"Deep Snapshot HDR Imaging Using Multi-Exposure Color Filter Array. In this paper, we propose a deep snapshot high dynamic range (HDR) imaging
framework that can effectively reconstruct an HDR image from the RAW data
captured using a multi-exposure color filter array (ME-CFA), which consists of
a mosaic pattern of RGB filters with different exposure levels. To effectively
learn the HDR image reconstruction network, we introduce the idea of luminance
normalization that simultaneously enables effective loss computation and input
data normalization by considering relative local contrasts in the
""normalized-by-luminance"" HDR domain. This idea makes it possible to equally
handle the errors in both bright and dark areas regardless of absolute
luminance levels, which significantly improves the visual image quality in a
tone-mapped domain. Experimental results using two public HDR image datasets
demonstrate that our framework outperforms other snapshot methods and produces
high-quality HDR images with fewer visual artifacts.","cs.CV,cs.GR,eess.IV"
"subgraph2vec: Learning Distributed Representations of Rooted Sub-graphs from Large Graphs. In this paper, we present subgraph2vec, a novel approach for learning latent
representations of rooted subgraphs from large graphs inspired by recent
advancements in Deep Learning and Graph Kernels. These latent representations
encode semantic substructure dependencies in a continuous vector space, which
is easily exploited by statistical models for tasks such as graph
classification, clustering, link prediction and community detection.
subgraph2vec leverages on local information obtained from neighbourhoods of
nodes to learn their latent representations in an unsupervised fashion. We
demonstrate that subgraph vectors learnt by our approach could be used in
conjunction with classifiers such as CNNs, SVMs and relational data clustering
algorithms to achieve significantly superior accuracies. Also, we show that the
subgraph vectors could be used for building a deep learning variant of
Weisfeiler-Lehman graph kernel. Our experiments on several benchmark and
large-scale real-world datasets reveal that subgraph2vec achieves significant
improvements in accuracies over existing graph kernels on both supervised and
unsupervised learning tasks. Specifically, on two realworld program analysis
tasks, namely, code clone and malware detection, subgraph2vec outperforms
state-of-the-art kernels by more than 17% and 4%, respectively.","cs.AI,cs.CR,cs.LG,cs.SE"
"ImageNet-trained CNNs are biased towards texture; increasing shape bias improves accuracy and robustness. Convolutional Neural Networks (CNNs) are commonly thought to recognise
objects by learning increasingly complex representations of object shapes. Some
recent studies suggest a more important role of image textures. We here put
these conflicting hypotheses to a quantitative test by evaluating CNNs and
human observers on images with a texture-shape cue conflict. We show that
ImageNet-trained CNNs are strongly biased towards recognising textures rather
than shapes, which is in stark contrast to human behavioural evidence and
reveals fundamentally different classification strategies. We then demonstrate
that the same standard architecture (ResNet-50) that learns a texture-based
representation on ImageNet is able to learn a shape-based representation
instead when trained on ""Stylized-ImageNet"", a stylized version of ImageNet.
This provides a much better fit for human behavioural performance in our
well-controlled psychophysical lab setting (nine experiments totalling 48,560
psychophysical trials across 97 observers) and comes with a number of
unexpected emergent benefits such as improved object detection performance and
previously unseen robustness towards a wide range of image distortions,
highlighting advantages of a shape-based representation.","cs.AI,cs.CV,cs.LG,q-bio.NC,stat.ML"
"RobustSleepNet: Transfer learning for automated sleep staging at scale. Sleep disorder diagnosis relies on the analysis of polysomnography (PSG)
records. As a preliminary step of this examination, sleep stages are
systematically determined. In practice, sleep stage classification relies on
the visual inspection of 30-second epochs of polysomnography signals. Numerous
automatic approaches have been developed to replace this tedious and expensive
task. Although these methods demonstrated better performance than human sleep
experts on specific datasets, they remain largely unused in sleep clinics. The
main reason is that each sleep clinic uses a specific PSG montage that most
automatic approaches cannot handle out-of-the-box. Moreover, even when the PSG
montage is compatible, publications have shown that automatic approaches
perform poorly on unseen data with different demographics. To address these
issues, we introduce RobustSleepNet, a deep learning model for automatic sleep
stage classification able to handle arbitrary PSG montages. We trained and
evaluated this model in a leave-one-out-dataset fashion on a large corpus of 8
heterogeneous sleep staging datasets to make it robust to demographic changes.
When evaluated on an unseen dataset, RobustSleepNet reaches 97% of the F1 of a
model explicitly trained on this dataset. Hence, RobustSleepNet unlocks the
possibility to perform high-quality out-of-the-box automatic sleep staging with
any clinical setup. We further show that finetuning RobustSleepNet, using a
part of the unseen dataset, increases the F1 by 2% when compared to a model
trained specifically for this dataset. Therefore, finetuning might be used to
reach a state-of-the-art level of performance on a specific population.","cs.LG,eess.SP,stat.ML"
"Geometric-Algebra LMS Adaptive Filter and its Application to Rotation Estimation. This paper exploits Geometric (Clifford) Algebra (GA) theory in order to
devise and introduce a new adaptive filtering strategy. From a least-squares
cost function, the gradient is calculated following results from Geometric
Calculus (GC), the extension of GA to handle differential and integral
calculus. The novel GA least-mean-squares (GA-LMS) adaptive filter, which
inherits properties from standard adaptive filters and from GA, is developed to
recursively estimate a rotor (multivector), a hypercomplex quantity able to
describe rotations in any dimension. The adaptive filter (AF) performance is
assessed via a 3D point-clouds registration problem, which contains a rotation
estimation step. Calculating the AF computational complexity suggests that it
can contribute to reduce the cost of a full-blown 3D registration algorithm,
especially when the number of points to be processed grows. Moreover, the
employed GA/GC framework allows for easily applying the resulting filter to
estimating rotors in higher dimensions.","cs.CG,cs.CV"
"Empirical Studies on Symbolic Aggregation Approximation Under Statistical Perspectives for Knowledge Discovery in Time Series. Symbolic Aggregation approXimation (SAX) has been the de facto standard
representation methods for knowledge discovery in time series on a number of
tasks and applications. So far, very little work has been done in empirically
investigating the intrinsic properties and statistical mechanics in SAX words.
In this paper, we applied several statistical measurements and proposed a new
statistical measurement, i.e. information embedding cost (IEC) to analyze the
statistical behaviors of the symbolic dynamics. Our experiments on the
benchmark datasets and the clinical signals demonstrate that SAX can always
reduce the complexity while preserving the core information embedded in the
original time series with significant embedding efficiency. Our proposed IEC
score provide a priori to determine if SAX is adequate for specific dataset,
which can be generalized to evaluate other symbolic representations. Our work
provides an analytical framework with several statistical tools to analyze,
evaluate and further improve the symbolic dynamics for knowledge discovery in
time series.","cs.IT,cs.LG,math.IT"
"Economy Statistical Recurrent Units For Inferring Nonlinear Granger Causality. Granger causality is a widely-used criterion for analyzing interactions in
large-scale networks. As most physical interactions are inherently nonlinear,
we consider the problem of inferring the existence of pairwise Granger
causality between nonlinearly interacting stochastic processes from their time
series measurements. Our proposed approach relies on modeling the embedded
nonlinearities in the measurements using a component-wise time series
prediction model based on Statistical Recurrent Units (SRUs). We make a case
that the network topology of Granger causal relations is directly inferrable
from a structured sparse estimate of the internal parameters of the SRU
networks trained to predict the processes$'$ time series measurements. We
propose a variant of SRU, called economy-SRU, which, by design has considerably
fewer trainable parameters, and therefore less prone to overfitting. The
economy-SRU computes a low-dimensional sketch of its high-dimensional hidden
state in the form of random projections to generate the feedback for its
recurrent processing. Additionally, the internal weight parameters of the
economy-SRU are strategically regularized in a group-wise manner to facilitate
the proposed network in extracting meaningful predictive features that are
highly time-localized to mimic real-world causal events. Extensive experiments
are carried out to demonstrate that the proposed economy-SRU based time series
prediction model outperforms the MLP, LSTM and attention-gated CNN-based time
series models considered previously for inferring Granger causality.","cs.LG,eess.SP,stat.AP,stat.ME,stat.ML"
"FMix: Enhancing Mixed Sample Data Augmentation. Mixed Sample Data Augmentation (MSDA) has received increasing attention in
recent years, with many successful variants such as MixUp and CutMix. By
studying the mutual information between the function learned by a VAE on the
original data and on the augmented data we show that MixUp distorts learned
functions in a way that CutMix does not. We further demonstrate this by showing
that MixUp acts as a form of adversarial training, increasing robustness to
attacks such as Deep Fool and Uniform Noise which produce examples similar to
those generated by MixUp. We argue that this distortion prevents models from
learning about sample specific features in the data, aiding generalisation
performance. In contrast, we suggest that CutMix works more like a traditional
augmentation, improving performance by preventing memorisation without
distorting the data distribution. However, we argue that an MSDA which builds
on CutMix to include masks of arbitrary shape, rather than just square, could
further prevent memorisation whilst preserving the data distribution in the
same way. To this end, we propose FMix, an MSDA that uses random binary masks
obtained by applying a threshold to low frequency images sampled from Fourier
space. These random masks can take on a wide range of shapes and can be
generated for use with one, two, and three dimensional data. FMix improves
performance over MixUp and CutMix, without an increase in training time, for a
number of models across a range of data sets and problem settings, obtaining a
new single model state-of-the-art result on CIFAR-10 without external data.
Finally, we show that a consequence of the difference between interpolating
MSDA such as MixUp and masking MSDA such as FMix is that the two can be
combined to improve performance even further. Code for all experiments is
provided at https://github.com/ecs-vlc/FMix .","cs.CV,cs.IT,cs.LG,math.IT,stat.ML"
"Semi-Supervised Learning on Graphs with Feature-Augmented Graph Basis Functions. For semi-supervised learning on graphs, we study how initial kernels in a
supervised learning regime can be augmented with additional information from
known priors or from unsupervised learning outputs. These augmented kernels are
constructed in a simple update scheme based on the Schur-Hadamard product of
the kernel with additional feature kernels. As generators of the positive
definite kernels we will focus on graph basis functions (GBF) that allow to
include geometric information of the graph via the graph Fourier transform.
Using a regularized least squares (RLS) approach for machine learning, we will
test the derived augmented kernels for the classification of data on graphs.","cs.LG,cs.NA,eess.SP,math.NA"
"Learning Omni-frequency Region-adaptive Representations for Real Image Super-Resolution. Traditional single image super-resolution (SISR) methods that focus on
solving single and uniform degradation (i.e., bicubic down-sampling), typically
suffer from poor performance when applied into real-world low-resolution (LR)
images due to the complicated realistic degradations. The key to solving this
more challenging real image super-resolution (RealSR) problem lies in learning
feature representations that are both informative and content-aware. In this
paper, we propose an Omni-frequency Region-adaptive Network (ORNet) to address
both challenges, here we call features of all low, middle and high frequencies
omni-frequency features. Specifically, we start from the frequency perspective
and design a Frequency Decomposition (FD) module to separate different
frequency components to comprehensively compensate the information lost for
real LR image. Then, considering the different regions of real LR image have
different frequency information lost, we further design a Region-adaptive
Frequency Aggregation (RFA) module by leveraging dynamic convolution and
spatial attention to adaptively restore frequency components for different
regions. The extensive experiments endorse the effective, and scenario-agnostic
nature of our OR-Net for RealSR.","cs.CV,eess.IV"
"Coordinate Descent for MCP/SCAD Penalized Least Squares Converges Linearly. Recovering sparse signals from observed data is an important topic in
signal/imaging processing, statistics and machine learning. Nonconvex penalized
least squares have been attracted a lot of attentions since they enjoy nice
statistical properties. Computationally, coordinate descent (CD) is a workhorse
for minimizing the nonconvex penalized least squares criterion due to its
simplicity and scalability. In this work, we prove the linear convergence rate
to CD for solving MCP/SCAD penalized least squares problems.","cs.LG,stat.CO,stat.ML"
"Margin Maximization as Lossless Maximal Compression. The ultimate goal of a supervised learning algorithm is to produce models
constructed on the training data that can generalize well to new examples. In
classification, functional margin maximization -- correctly classifying as many
training examples as possible with maximal confidence --has been known to
construct models with good generalization guarantees. This work gives an
information-theoretic interpretation of a margin maximizing model on a
noiseless training dataset as one that achieves lossless maximal compression of
said dataset -- i.e. extracts from the features all the useful information for
predicting the label and no more. The connection offers new insights on
generalization in supervised machine learning, showing margin maximization as a
special case (that of classification) of a more general principle and explains
the success and potential limitations of popular learning algorithms like
gradient boosting. We support our observations with theoretical arguments and
empirical evidence and identify interesting directions for future work.","cs.IT,cs.LG,math.IT,stat.ML"
"Look Before you Speak: Visually Contextualized Utterances. While most conversational AI systems focus on textual dialogue only,
conditioning utterances on visual context (when it's available) can lead to
more realistic conversations. Unfortunately, a major challenge for
incorporating visual context into conversational dialogue is the lack of
large-scale labeled datasets. We provide a solution in the form of a new
visually conditioned Future Utterance Prediction task. Our task involves
predicting the next utterance in a video, using both visual frames and
transcribed speech as context. By exploiting the large number of instructional
videos online, we train a model to solve this task at scale, without the need
for manual annotations. Leveraging recent advances in multimodal learning, our
model consists of a novel co-attentional multimodal video transformer, and when
trained on both textual and visual context, outperforms baselines that use
textual inputs alone. Further, we demonstrate that our model trained for this
task on unlabelled videos achieves state-of-the-art performance on a number of
downstream VideoQA benchmarks such as MSRVTT-QA, MSVD-QA, ActivityNet-QA and
How2QA.","cs.CV,cs.HC"
"Hybrid Reasoning Network for Video-based Commonsense Captioning. The task of video-based commonsense captioning aims to generate event-wise
captions and meanwhile provide multiple commonsense descriptions (e.g.,
attribute, effect and intention) about the underlying event in the video. Prior
works explore the commonsense captions by using separate networks for different
commonsense types, which is time-consuming and lacks mining the interaction of
different commonsense. In this paper, we propose a Hybrid Reasoning Network
(HybridNet) to endow the neural networks with the capability of semantic-level
reasoning and word-level reasoning. Firstly, we develop multi-commonsense
learning for semantic-level reasoning by jointly training different commonsense
types in a unified network, which encourages the interaction between the clues
of multiple commonsense descriptions, event-wise captions and videos. Then,
there are two steps to achieve the word-level reasoning: (1) a memory module
records the history predicted sequence from the previous generation processes;
(2) a memory-routed multi-head attention (MMHA) module updates the word-level
attention maps by incorporating the history information from the memory module
into the transformer decoder for word-level reasoning. Moreover, the multimodal
features are used to make full use of diverse knowledge for commonsense
reasoning. Experiments and abundant analysis on the large-scale
Video-to-Commonsense benchmark show that our HybridNet achieves
state-of-the-art performance compared with other methods.","68T07,cs.CL,cs.CV"
"Fractal Gaussian Networks: A sparse random graph model based on Gaussian Multiplicative Chaos. We propose a novel stochastic network model, called Fractal Gaussian Network
(FGN), that embodies well-defined and analytically tractable fractal
structures. Such fractal structures have been empirically observed in diverse
applications. FGNs interpolate continuously between the popular purely random
geometric graphs (a.k.a. the Poisson Boolean network), and random graphs with
increasingly fractal behavior. In fact, they form a parametric family of sparse
random geometric graphs that are parametrized by a fractality parameter $\nu$
which governs the strength of the fractal structure. FGNs are driven by the
latent spatial geometry of Gaussian Multiplicative Chaos (GMC), a canonical
model of fractality in its own right. We asymptotically characterize the
expected number of edges and triangle in FGNs. We then examine the natural
question of detecting the presence of fractality and the problem of parameter
estimation based on observed network data, in addition to fundamental
properties of the FGN as a random graph model. We also explore fractality in
community structures by unveiling a natural stochastic block model in the
setting of FGNs.","cond-mat.dis-nn,cs.LG,math.PR,math.ST,stat.ML,stat.TH"
"Deeply Self-Supervised Contour Embedded Neural Network Applied to Liver Segmentation. Objective: Herein, a neural network-based liver segmentation algorithm is
proposed, and its performance was evaluated using abdominal computed tomography
(CT) images. Methods: A fully convolutional network was developed to overcome
the volumetric image segmentation problem. To guide a neural network to
accurately delineate a target liver object, the network was deeply supervised
by applying the adaptive self-supervision scheme to derive the essential
contour, which acted as a complement with the global shape. The discriminative
contour, shape, and deep features were internally merged for the segmentation
results. Results and Conclusion: 160 abdominal CT images were used for training
and validation. The quantitative evaluation of the proposed network was
performed through an eight-fold cross-validation. The result showed that the
method, which uses the contour feature, segmented the liver more accurately
than the state-of-the-art with a 2.13% improvement in the dice score.
Significance: In this study, a new framework was introduced to guide a neural
network and learn complementary contour features. The proposed neural network
demonstrates that the guided contour features can significantly improve the
performance of the segmentation task.","68U10,cs.CV"
"Identification of Latent Variables From Graphical Model Residuals. Graph-based causal discovery methods aim to capture conditional
independencies consistent with the observed data and differentiate causal
relationships from indirect or induced ones. Successful construction of
graphical models of data depends on the assumption of causal sufficiency: that
is, that all confounding variables are measured. When this assumption is not
met, learned graphical structures may become arbitrarily incorrect and effects
implied by such models may be wrongly attributed, carry the wrong magnitude, or
mis-represent direction of correlation. Wide application of graphical models to
increasingly less curated ""big data"" draws renewed attention to the unobserved
confounder problem.
  We present a novel method that aims to control for the latent space when
estimating a DAG by iteratively deriving proxies for the latent space from the
residuals of the inferred model. Under mild assumptions, our method improves
structural inference of Gaussian graphical models and enhances identifiability
of the causal effect. In addition, when the model is being used to predict
outcomes, it un-confounds the coefficients on the parents of the outcomes and
leads to improved predictive performance when out-of-sample regime is very
different from the training data. We show that any improvement of prediction of
an outcome is intrinsically capped and cannot rise beyond a certain limit as
compared to the confounded model. We extend our methodology beyond GGMs to
ordinal variables and nonlinear cases. Our R package provides both PCA and
autoencoder implementations of the methodology, suitable for GGMs with some
guarantees and for better performance in general cases but without such
guarantees.","cs.LG,q-bio.QM,stat.ML"
"Active Learning for Undirected Graphical Model Selection. This paper studies graphical model selection, i.e., the problem of estimating
a graph of statistical relationships among a collection of random variables.
Conventional graphical model selection algorithms are passive, i.e., they
require all the measurements to have been collected before processing begins.
We propose an active learning algorithm that uses junction tree representations
to adapt future measurements based on the information gathered from prior
measurements. We prove that, under certain conditions, our active learning
algorithm requires fewer scalar measurements than any passive algorithm to
reliably estimate a graph. A range of numerical results validate our theory and
demonstrates the benefits of active learning.","cs.IT,math.IT,math.ST,stat.ML,stat.TH"
"Deep Neural Networks to Recover Unknown Physical Parameters from Oscillating Time Series. Deep neural networks (DNNs) are widely used in pattern-recognition tasks for
which a human comprehensible, quantitative description of the data-generating
process, e.g., in the form of equations, cannot be achieved. While doing so,
DNNs often produce an abstract (entangled and non-interpretable) representation
of the data-generating process. This is one of the reasons why DNNs are not
extensively used in physics-signal processing: physicists generally require
their analyses to yield quantitative information about the studied systems. In
this article we use DNNs to disentangle components of oscillating time series,
and recover meaningful information. We show that, because DNNs can find useful
abstract feature representations, they can be used when prior knowledge about
the signal-generating process exists, but is not complete, as it is
particularly the case in ""new-physics"" searches. To this aim, we train our DNN
on synthetic oscillating time series to perform two tasks: a regression of the
signal latent parameters and signal denoising by an Autoencoder-like
architecture. We show that the regression and denoising performance is similar
to those of least-square curve fittings (LS-fit) with true latent parameters'
initial guesses, in spite of the DNN needing no initial guesses at all. We then
explore applications in which we believe our architecture could prove useful
for time-series processing in physics, when prior knowledge is incomplete. As
an example, we employ DNNs as a tool to inform LS-fits when initial guesses are
unknown. We show that the regression can be performed on some latent
parameters, while ignoring the existence of others. Because the Autoencoder
needs no prior information about the physical model, the remaining unknown
latent parameters can still be captured, thus making use of partial prior
knowledge, while leaving space for data exploration and discoveries.","cs.LG,cs.NA,math.NA,physics.data-an"
"Prescribed Generative Adversarial Networks. Generative adversarial networks (GANs) are a powerful approach to
unsupervised learning. They have achieved state-of-the-art performance in the
image domain. However, GANs are limited in two ways. They often learn
distributions with low support---a phenomenon known as mode collapse---and they
do not guarantee the existence of a probability density, which makes evaluating
generalization using predictive log-likelihood impossible. In this paper, we
develop the prescribed GAN (PresGAN) to address these shortcomings. PresGANs
add noise to the output of a density network and optimize an
entropy-regularized adversarial loss. The added noise renders tractable
approximations of the predictive log-likelihood and stabilizes the training
procedure. The entropy regularizer encourages PresGANs to capture all the modes
of the data distribution. Fitting PresGANs involves computing the intractable
gradients of the entropy regularization term; PresGANs sidestep this
intractability using unbiased stochastic estimates. We evaluate PresGANs on
several datasets and found they mitigate mode collapse and generate samples
with high perceptual quality. We further found that PresGANs reduce the gap in
performance in terms of predictive log-likelihood between traditional GANs and
variational autoencoders (VAEs).","cs.LG,stat.ME,stat.ML"
"Noise-tolerant Audio-visual Online Person Verification using an Attention-based Neural Network Fusion. In this paper, we present a multi-modal online person verification system
using both speech and visual signals. Inspired by neuroscientific findings on
the association of voice and face, we propose an attention-based end-to-end
neural network that learns multi-sensory associations for the task of person
verification. The attention mechanism in our proposed network learns to
conditionally select a salient modality between speech and facial
representations that provides a balance between complementary inputs. By virtue
of this capability, the network is robust to missing or corrupted data from
either modality. In the VoxCeleb2 dataset, we show that our method performs
favorably against competing multi-modal methods. Even for extreme cases of
large corruption or an entirely missing modality, our method demonstrates
robustness over other unimodal methods.","cs.CV,eess.AS"
"Towards Ophthalmologist Level Accurate Deep Learning System for OCT Screening and Diagnosis. In this work, we propose an advanced AI based grading system for OCT images.
The proposed system is a very deep fully convolutional attentive classification
network trained with end to end advanced transfer learning with online random
augmentation. It uses quasi random augmentation that outputs confidence values
for diseases prevalence during inference. Its a fully automated retinal OCT
analysis AI system capable of pathological lesions understanding without any
offline preprocessing/postprocessing step or manual feature extraction. We
present a state of the art performance on the publicly available Mendeley OCT
dataset.","68T45,cs.AI,cs.CV"
"Accelerating COVID-19 Differential Diagnosis with Explainable Ultrasound Image Analysis. Controlling the COVID-19 pandemic largely hinges upon the existence of fast,
safe, and highly-available diagnostic tools. Ultrasound, in contrast to CT or
X-Ray, has many practical advantages and can serve as a globally-applicable
first-line examination technique. We provide the largest publicly available
lung ultrasound (US) dataset for COVID-19 consisting of 106 videos from three
classes (COVID-19, bacterial pneumonia, and healthy controls); curated and
approved by medical experts. On this dataset, we perform an in-depth study of
the value of deep learning methods for differential diagnosis of COVID-19. We
propose a frame-based convolutional neural network that correctly classifies
COVID-19 US videos with a sensitivity of 0.98+-0.04 and a specificity of
0.91+-08 (frame-based sensitivity 0.93+-0.05, specificity 0.87+-0.07). We
further employ class activation maps for the spatio-temporal localization of
pulmonary biomarkers, which we subsequently validate for human-in-the-loop
scenarios in a blindfolded study with medical experts. Aiming for scalability
and robustness, we perform ablation studies comparing mobile-friendly, frame-
and video-based architectures and show reliability of the best model by
aleatoric and epistemic uncertainty estimates. We hope to pave the road for a
community effort toward an accessible, efficient and interpretable screening
method and we have started to work on a clinical validation of the proposed
method. Data and code are publicly available.","cs.CV,cs.DB,cs.DL,cs.LG,eess.IV"
"Characterizing and comparing external measures for the assessment of cluster analysis and community detection. In the context of cluster analysis and graph partitioning, many external
evaluation measures have been proposed in the literature to compare two
partitions of the same set. This makes the task of selecting the most
appropriate measure for a given situation a challenge for the end user.
However, this issue is overlooked in the literature. Researchers tend to follow
tradition and use the standard measures of their field, although they often
became standard only because previous researchers started consistently using
them. In this work, we propose a new empirical evaluation framework to solve
this issue, and help the end user selecting an appropriate measure for their
application. For a collection of candidate measures, it first consists in
describing their behavior by computing them for a generated dataset of
partitions, obtained by applying a set of predefined parametric partition
transformations. Second, our framework performs a regression analysis to
characterize the measures in terms of how they are affected by these parameters
and transformations. This allows both describing and comparing the measures.
Our approach is not tied to any specific measure or application, so it can be
applied to any situation. We illustrate its relevance by applying it to a
selection of standard measures, and show how it can be put in practice through
two concrete use cases.","cs.LG,physics.data-an"
"Ain't Nobody Got Time For Coding: Structure-Aware Program Synthesis From Natural Language. Program synthesis from natural language (NL) is practical for humans and,
once technically feasible, would significantly facilitate software development
and revolutionize end-user programming. We present SAPS, an end-to-end neural
network capable of mapping relatively complex, multi-sentence NL specifications
to snippets of executable code. The proposed architecture relies exclusively on
neural components, and is trained on abstract syntax trees, combined with a
pretrained word embedding and a bi-directional multi-layer LSTM for processing
of word sequences. The decoder features a doubly-recurrent LSTM, for which we
propose novel signal propagation schemes and soft attention mechanism. When
applied to a large dataset of problems proposed in a previous study, SAPS
performs on par with or better than the method proposed there, producing
correct programs in over 92% of cases. In contrast to other methods, it does
not require post-processing of the resulting programs, and uses a
fixed-dimensional latent representation as the only interface between the NL
analyzer and the source code generator.","cs.AI,cs.LG,cs.PL,stat.ML"
"Estimates on Learning Rates for Multi-Penalty Distribution Regression. This paper is concerned with functional learning by utilizing two-stage
sampled distribution regression. We study a multi-penalty regularization
algorithm for distribution regression under the framework of learning theory.
The algorithm aims at regressing to real valued outputs from probability
measures. The theoretical analysis on distribution regression is far from
maturity and quite challenging, since only second stage samples are observable
in practical setting. In the algorithm, to transform information from samples,
we embed the distributions to a reproducing kernel Hilbert space
$\mathcal{H}_K$ associated with Mercer kernel $K$ via mean embedding technique.
The main contribution of the paper is to present a novel multi-penalty
regularization algorithm to capture more features of distribution regression
and derive optimal learning rates for the algorithm. The work also derives
learning rates for distribution regression in the nonstandard setting
$f_{\rho}\notin\mathcal{H}_K$, which is not explored in existing literature.
Moreover, we propose a distribution regression-based distributed learning
algorithm to face large-scale data or information challenge. The optimal
learning rates are derived for the distributed learning algorithm. By providing
new algorithms and showing their learning rates, we improve the existing work
in different aspects in the literature.","cs.LG,math.ST,stat.ML,stat.TH"
"Attention-based Image Upsampling. Convolutional layers are an integral part of many deep neural network
solutions in computer vision. Recent work shows that replacing the standard
convolution operation with mechanisms based on self-attention leads to improved
performance on image classification and object detection tasks. In this work,
we show how attention mechanisms can be used to replace another canonical
operation: strided transposed convolution. We term our novel attention-based
operation attention-based upsampling since it increases/upsamples the spatial
dimensions of the feature maps. Through experiments on single image
super-resolution and joint-image upsampling tasks, we show that attention-based
upsampling consistently outperforms traditional upsampling methods based on
strided transposed convolution or based on adaptive filters while using fewer
parameters. We show that the inherent flexibility of the attention mechanism,
which allows it to use separate sources for calculating the attention
coefficients and the attention targets, makes attention-based upsampling a
natural choice when fusing information from multiple image modalities.","cs.CV,cs.LG"
"Symbolic Regression Methods for Reinforcement Learning. Reinforcement learning algorithms can be used to optimally solve dynamic
decision-making and control problems. With continuous-valued state and input
variables, reinforcement learning algorithms must rely on function
approximators to represent the value function and policy mappings. Commonly
used numerical approximators, such as neural networks or basis function
expansions, have two main drawbacks: they are black-box models offering no
insight in the mappings learned, and they require significant trial and error
tuning of their meta-parameters. In this paper, we propose a new approach to
constructing smooth value functions by means of symbolic regression. We
introduce three off-line methods for finding value functions based on a state
transition model: symbolic value iteration, symbolic policy iteration, and a
direct solution of the Bellman equation. The methods are illustrated on four
nonlinear control problems: velocity control under friction, one-link and
two-link pendulum swing-up, and magnetic manipulation. The results show that
the value functions not only yield well-performing policies, but also are
compact, human-readable and mathematically tractable. This makes them
potentially suitable for further analysis of the closed-loop system. A
comparison with alternative approaches using neural networks shows that our
method constructs well-performing value functions with substantially fewer
parameters.","cs.LG,cs.NE,cs.SY,stat.ML"
"SAL: Sign Agnostic Learning of Shapes from Raw Data. Recently, neural networks have been used as implicit representations for
surface reconstruction, modelling, learning, and generation. So far, training
neural networks to be implicit representations of surfaces required training
data sampled from a ground-truth signed implicit functions such as signed
distance or occupancy functions, which are notoriously hard to compute.
  In this paper we introduce Sign Agnostic Learning (SAL), a deep learning
approach for learning implicit shape representations directly from raw,
unsigned geometric data, such as point clouds and triangle soups.
  We have tested SAL on the challenging problem of surface reconstruction from
an un-oriented point cloud, as well as end-to-end human shape space learning
directly from raw scans dataset, and achieved state of the art reconstructions
compared to current approaches. We believe SAL opens the door to many geometric
deep learning applications with real-world data, alleviating the usual
painstaking, often manual pre-process.","cs.CV,cs.GR,cs.LG"
"COPT: Coordinated Optimal Transport for Graph Sketching. We introduce COPT, a novel distance metric between graphs defined via an
optimization routine, computing a coordinated pair of optimal transport maps
simultaneously. This gives an unsupervised way to learn general-purpose graph
representation, applicable to both graph sketching and graph comparison. COPT
involves simultaneously optimizing dual transport plans, one between the
vertices of two graphs, and another between graph signal probability
distributions. We show theoretically that our method preserves important global
structural information on graphs, in particular spectral information, and
analyze connections to existing studies. Empirically, COPT outperforms state of
the art methods in graph classification on both synthetic and real datasets.","cs.DS,cs.LG,stat.ML"
"Adversarial Domain Adaptation for Stable Brain-Machine Interfaces. Brain-Machine Interfaces (BMIs) have recently emerged as a clinically viable
option to restore voluntary movements after paralysis. These devices are based
on the ability to extract information about movement intent from neural signals
recorded using multi-electrode arrays chronically implanted in the motor
cortices of the brain. However, the inherent loss and turnover of recorded
neurons requires repeated recalibrations of the interface, which can
potentially alter the day-to-day user experience. The resulting need for
continued user adaptation interferes with the natural, subconscious use of the
BMI. Here, we introduce a new computational approach that decodes movement
intent from a low-dimensional latent representation of the neural data. We
implement various domain adaptation methods to stabilize the interface over
significantly long times. This includes Canonical Correlation Analysis used to
align the latent variables across days; this method requires prior
point-to-point correspondence of the time series across domains. Alternatively,
we match the empirical probability distributions of the latent variables across
days through the minimization of their Kullback-Leibler divergence. These two
methods provide a significant and comparable improvement in the performance of
the interface. However, implementation of an Adversarial Domain Adaptation
Network trained to match the empirical probability distribution of the
residuals of the reconstructed neural signals outperforms the two methods based
on latent variables, while requiring remarkably few data points to solve the
domain adaptation problem.","cs.LG,q-bio.NC,stat.ML"
"Options as responses: Grounding behavioural hierarchies in multi-agent RL. This paper investigates generalisation in multi-agent games, where the
generality of the agent can be evaluated by playing against opponents it hasn't
seen during training. We propose two new games with concealed information and
complex, non-transitive reward structure (think rock/paper/scissors). It turns
out that most current deep reinforcement learning methods fail to efficiently
explore the strategy space, thus learning policies that generalise poorly to
unseen opponents. We then propose a novel hierarchical agent architecture,
where the hierarchy is grounded in the game-theoretic structure of the game --
the top level chooses strategic responses to opponents, while the low level
implements them into policy over primitive actions. This grounding facilitates
credit assignment across the levels of hierarchy. Our experiments show that the
proposed hierarchical agent is capable of generalisation to unseen opponents,
while conventional baselines fail to generalise whatsoever.","cs.AI,cs.LG,cs.MA,cs.NE,stat.ML"
"A Gentle Introduction to Conformal Prediction and Distribution-Free Uncertainty Quantification. Black-box machine learning learning methods are now routinely used in
high-risk settings, like medical diagnostics, which demand uncertainty
quantification to avoid consequential model failures. Distribution-free
uncertainty quantification (distribution-free UQ) is a user-friendly paradigm
for creating statistically rigorous confidence intervals/sets for such
predictions. Critically, the intervals/sets are valid without distributional
assumptions or model assumptions, with explicit guarantees with finitely many
datapoints. Moreover, they adapt to the difficulty of the input; when the input
example is difficult, the uncertainty intervals/sets are large, signaling that
the model might be wrong. Without much work, one can use distribution-free
methods on any underlying algorithm, such as a neural network, to produce
confidence sets guaranteed to contain the ground truth with a user-specified
probability, such as 90%. Indeed, the methods are easy-to-understand and
general, applying to many modern prediction problems arising in the fields of
computer vision, natural language processing, deep reinforcement learning, and
so on. This hands-on introduction is aimed at a reader interested in the
practical implementation of distribution-free UQ, including conformal
prediction and related methods, who is not necessarily a statistician. We will
include many explanatory illustrations, examples, and code samples in Python,
with PyTorch syntax. The goal is to provide the reader a working understanding
of distribution-free UQ, allowing them to put confidence intervals on their
algorithms, with one self-contained document.","cs.AI,cs.LG,math.ST,stat.ME,stat.ML,stat.TH"
"Structured Neural Summarization. Summarization of long sequences into a concise statement is a core problem in
natural language processing, requiring non-trivial understanding of the input.
Based on the promising results of graph neural networks on highly structured
data, we develop a framework to extend existing sequence encoders with a graph
component that can reason about long-distance relationships in weakly
structured data such as text. In an extensive evaluation, we show that the
resulting hybrid sequence-graph models outperform both pure sequence models as
well as pure graph models on a range of summarization tasks.","cs.CL,cs.LG,cs.SE,stat.ML"
"Replacement AutoEncoder: A Privacy-Preserving Algorithm for Sensory Data Analysis. An increasing number of sensors on mobile, Internet of things (IoT), and
wearable devices generate time-series measurements of physical activities.
Though access to the sensory data is critical to the success of many beneficial
applications such as health monitoring or activity recognition, a wide range of
potentially sensitive information about the individuals can also be discovered
through access to sensory data and this cannot easily be protected using
traditional privacy approaches.
  In this paper, we propose a privacy-preserving sensing framework for managing
access to time-series data in order to provide utility while protecting
individuals' privacy. We introduce Replacement AutoEncoder, a novel algorithm
which learns how to transform discriminative features of data that correspond
to sensitive inferences, into some features that have been more observed in
non-sensitive inferences, to protect users' privacy. This efficiency is
achieved by defining a user-customized objective function for deep
autoencoders. Our replacement method will not only eliminate the possibility of
recognizing sensitive inferences, it also eliminates the possibility of
detecting the occurrence of them. That is the main weakness of other approaches
such as filtering or randomization. We evaluate the efficacy of the algorithm
with an activity recognition task in a multi-sensing environment using
extensive experiments on three benchmark datasets. We show that it can retain
the recognition accuracy of state-of-the-art techniques while simultaneously
preserving the privacy of sensitive information. Finally, we utilize the GANs
for detecting the occurrence of replacement, after releasing data, and show
that this can be done only if the adversarial network is trained on the users'
original data.","68T05,I.2.6,cs.LG,stat.ML"
"Estimating time-varying networks. Stochastic networks are a plausible representation of the relational
information among entities in dynamic systems such as living cells or social
communities. While there is a rich literature in estimating a static or
temporally invariant network from observation data, little has been done toward
estimating time-varying networks from time series of entity attributes. In this
paper we present two new machine learning methods for estimating time-varying
networks, which both build on a temporally smoothed $l_1$-regularized logistic
regression formalism that can be cast as a standard convex-optimization problem
and solved efficiently using generic solvers scalable to large networks. We
report promising results on recovering simulated time-varying networks. For
real data sets, we reverse engineer the latent sequence of temporally rewiring
political networks between Senators from the US Senate voting records and the
latent evolving regulatory networks underlying 588 genes across the life cycle
of Drosophila melanogaster from the microarray time course.","q-bio.MN,q-bio.QM,stat.AP,stat.ME,stat.ML"
"Semantic Labeling Using a Deep Contextualized Language Model. Generating schema labels automatically for column values of data tables has
many data science applications such as schema matching, and data discovery and
linking. For example, automatically extracted tables with missing headers can
be filled by the predicted schema labels which significantly minimizes human
effort. Furthermore, the predicted labels can reduce the impact of inconsistent
names across multiple data tables. Understanding the connection between column
values and contextual information is an important yet neglected aspect as
previously proposed methods treat each column independently. In this paper, we
propose a context-aware semantic labeling method using both the column values
and context. Our new method is based on a new setting for semantic labeling,
where we sequentially predict labels for an input table with missing headers.
We incorporate both the values and context of each data column using the
pre-trained contextualized language model, BERT, that has achieved significant
improvements in multiple natural language processing tasks. To our knowledge,
we are the first to successfully apply BERT to solve the semantic labeling
task. We evaluate our approach using two real-world datasets from different
domains, and we demonstrate substantial improvements in terms of evaluation
metrics over state-of-the-art feature-based methods.","cs.DB,cs.IR,cs.LG"
"Multi-Source Data Fusion for Cyberattack Detection in Power Systems. Cyberattacks can cause a severe impact on power systems unless detected
early. However, accurate and timely detection in critical infrastructure
systems presents challenges, e.g., due to zero-day vulnerability exploitations
and the cyber-physical nature of the system coupled with the need for high
reliability and resilience of the physical system. Conventional rule-based and
anomaly-based intrusion detection system (IDS) tools are insufficient for
detecting zero-day cyber intrusions in the industrial control system (ICS)
networks. Hence, in this work, we show that fusing information from multiple
data sources can help identify cyber-induced incidents and reduce false
positives. Specifically, we present how to recognize and address the barriers
that can prevent the accurate use of multiple data sources for fusion-based
detection. We perform multi-source data fusion for training IDS in a
cyber-physical power system testbed where we collect cyber and physical side
data from multiple sensors emulating real-world data sources that would be
found in a utility and synthesizes these into features for algorithms to detect
intrusions. Results are presented using the proposed data fusion application to
infer False Data and Command injection-based Man-in- The-Middle (MiTM) attacks.
Post collection, the data fusion application uses time-synchronized merge and
extracts features followed by pre-processing such as imputation and encoding
before training supervised, semi-supervised, and unsupervised learning models
to evaluate the performance of the IDS. A major finding is the improvement of
detection accuracy by fusion of features from cyber, security, and physical
domains. Additionally, we observed the co-training technique performs at par
with supervised learning methods when fed with our features.","cs.LG,cs.SY,eess.SY"
"Heterogeneous Deep Graph Infomax. Graph representation learning is to learn universal node representations that
preserve both node attributes and structural information. The derived node
representations can be used to serve various downstream tasks, such as node
classification and node clustering. When a graph is heterogeneous, the problem
becomes more challenging than the homogeneous graph node learning problem.
Inspired by the emerging information theoretic-based learning algorithm, in
this paper we propose an unsupervised graph neural network Heterogeneous Deep
Graph Infomax (HDGI) for heterogeneous graph representation learning. We use
the meta-path structure to analyze the connections involving semantics in
heterogeneous graphs and utilize graph convolution module and semantic-level
attention mechanism to capture local representations. By maximizing
local-global mutual information, HDGI effectively learns high-level node
representations that can be utilized in downstream graph-related tasks.
Experiment results show that HDGI remarkably outperforms state-of-the-art
unsupervised graph representation learning methods on both classification and
clustering tasks. By feeding the learned representations into a parametric
model, such as logistic regression, we even achieve comparable performance in
node classification tasks when comparing with state-of-the-art supervised
end-to-end GNN models.","cs.LG,cs.SI,stat.ML"
"Joint learning of multiple Granger causal networks via non-convex regularizations: Inference of group-level brain connectivity. This paper considers joint learning of multiple sparse Granger graphical
models to discover underlying common and differential Granger causality (GC)
structures across multiple time series. This can be applied to drawing
group-level brain connectivity inferences from a homogeneous group of subjects
or discovering network differences among groups of signals collected under
heterogeneous conditions. By recognizing that the GC of a single multivariate
time series can be characterized by common zeros of vector autoregressive (VAR)
lag coefficients, a group sparse prior is included in joint regularized
least-squares estimations of multiple VAR models. Group-norm regularizations
based on group- and fused-lasso penalties encourage a decomposition of multiple
networks into a common GC structure, with other remaining parts defined in
individual-specific networks. Prior information about sparseness and sparsity
patterns of desired GC networks are incorporated as relative weights, while a
non-convex group norm in the penalty is proposed to enhance the accuracy of
network estimation in low-sample settings. Extensive numerical results on
simulations illustrated our method's improvements over existing sparse
estimation approaches on GC network sparsity recovery. Our methods were also
applied to available resting-state fMRI time series from the ADHD-200 data sets
to learn the differences of causality mechanisms, called effective brain
connectivity, between adolescents with ADHD and typically developing children.
Our analysis revealed that parts of the causality differences between the two
groups often resided in the orbitofrontal region and areas associated with the
limbic system, which agreed with clinical findings and data-driven results in
previous studies.","cs.LG,eess.SP,q-bio.NC"
"Heterogeneous Graph Transformer. Recent years have witnessed the emerging success of graph neural networks
(GNNs) for modeling structured data. However, most GNNs are designed for
homogeneous graphs, in which all nodes and edges belong to the same types,
making them infeasible to represent heterogeneous structures. In this paper, we
present the Heterogeneous Graph Transformer (HGT) architecture for modeling
Web-scale heterogeneous graphs. To model heterogeneity, we design node- and
edge-type dependent parameters to characterize the heterogeneous attention over
each edge, empowering HGT to maintain dedicated representations for different
types of nodes and edges. To handle dynamic heterogeneous graphs, we introduce
the relative temporal encoding technique into HGT, which is able to capture the
dynamic structural dependency with arbitrary durations. To handle Web-scale
graph data, we design the heterogeneous mini-batch graph sampling
algorithm---HGSampling---for efficient and scalable training. Extensive
experiments on the Open Academic Graph of 179 million nodes and 2 billion edges
show that the proposed HGT model consistently outperforms all the
state-of-the-art GNN baselines by 9%--21% on various downstream tasks.","cs.LG,cs.SI,stat.ML"
"TreeCaps: Tree-Structured Capsule Networks for Program Source Code Processing. Program comprehension is a fundamental task in software development and
maintenance processes. Software developers often need to understand a large
amount of existing code before they can develop new features or fix bugs in
existing programs. Being able to process programming language code
automatically and provide summaries of code functionality accurately can
significantly help developers to reduce time spent in code navigation and
understanding, and thus increase productivity. Different from natural language
articles, source code in programming languages often follows rigid syntactical
structures and there can exist dependencies among code elements that are
located far away from each other through complex control flows and data flows.
Existing studies on tree-based convolutional neural networks (TBCNN) and gated
graph neural networks (GGNN) are not able to capture essential semantic
dependencies among code elements accurately. In this paper, we propose novel
tree-based capsule networks (TreeCaps) and relevant techniques for processing
program code in an automated way that encodes code syntactical structures and
captures code dependencies more accurately. Based on evaluation on programs
written in different programming languages, we show that our TreeCaps-based
approach can outperform other approaches in classifying the functionalities of
many programs.","cs.LG,cs.SE,stat.ML"
"Online learning of both state and dynamics using ensemble Kalman filters. The reconstruction of the dynamics of an observed physical system as a
surrogate model has been brought to the fore by recent advances in machine
learning. To deal with partial and noisy observations in that endeavor, machine
learning representations of the surrogate model can be used within a Bayesian
data assimilation framework. However, these approaches require to consider long
time series of observational data, meant to be assimilated all together. This
paper investigates the possibility to learn both the dynamics and the state
online, i.e. to update their estimates at any time, in particular when new
observations are acquired. The estimation is based on the ensemble Kalman
filter (EnKF) family of algorithms using a rather simple representation for the
surrogate model and state augmentation. We consider the implication of learning
dynamics online through (i) a global EnKF, (i) a local EnKF and (iii) an
iterative EnKF and we discuss in each case issues and algorithmic solutions. We
then demonstrate numerically the efficiency and assess the accuracy of these
methods using one-dimensional, one-scale and two-scale chaotic Lorenz models.","49M41,62M20,86-08,cs.LG,nlin.CD,physics.ao-ph,physics.data-an,stat.ML"
"Multivariate Gaussian Variational Inference by Natural Gradient Descent. This short note reviews so-called Natural Gradient Descent (NGD) for
multivariate Gaussians. The Fisher Information Matrix (FIM) is derived for
several different parameterizations of Gaussians. Careful attention is paid to
the symmetric nature of the covariance matrix when calculating derivatives. We
show that there are some advantages to choosing a parameterization comprising
the mean and inverse covariance matrix and provide a simple NGD update that
accounts for the symmetric (and sparse) nature of the inverse covariance
matrix.","cs.LG,cs.RO,math.ST,stat.ML,stat.TH"
"Explaining Neural Scaling Laws. The test loss of well-trained neural networks often follows precise power-law
scaling relations with either the size of the training dataset or the number of
parameters in the network. We propose a theory that explains and connects these
scaling laws. We identify variance-limited and resolution-limited scaling
behavior for both dataset and model size, for a total of four scaling regimes.
The variance-limited scaling follows simply from the existence of a
well-behaved infinite data or infinite width limit, while the
resolution-limited regime can be explained by positing that models are
effectively resolving a smooth data manifold. In the large width limit, this
can be equivalently obtained from the spectrum of certain kernels, and we
present evidence that large width and large dataset resolution-limited scaling
exponents are related by a duality. We exhibit all four scaling regimes in the
controlled setting of large random feature and pretrained models and test the
predictions empirically on a range of standard architectures and datasets. We
also observe several empirical relationships between datasets and scaling
exponents: super-classing image tasks does not change exponents, while changing
input distribution (via changing datasets or adding noise) has a strong effect.
We further explore the effect of architecture aspect ratio on scaling
exponents.","cond-mat.dis-nn,cs.LG,stat.ML"
"Integrating Physiological Time Series and Clinical Notes with Deep Learning for Improved ICU Mortality Prediction. Intensive Care Unit Electronic Health Records (ICU EHRs) store multimodal
data about patients including clinical notes, sparse and irregularly sampled
physiological time series, lab results, and more. To date, most methods
designed to learn predictive models from ICU EHR data have focused on a single
modality. In this paper, we leverage the recently proposed
interpolation-prediction deep learning architecture(Shukla and Marlin 2019) as
a basis for exploring how physiological time series data and clinical notes can
be integrated into a unified mortality prediction model. We study both early
and late fusion approaches and demonstrate how the relative predictive value of
clinical text and physiological data change over time. Our results show that a
late fusion approach can provide a statistically significant improvement in
mortality prediction performance over using individual modalities in isolation.","cs.CY,cs.LG,stat.ML"
"Robust Learning-Based Control via Bootstrapped Multiplicative Noise. Despite decades of research and recent progress in adaptive control and
reinforcement learning, there remains a fundamental lack of understanding in
designing controllers that provide robustness to inherent non-asymptotic
uncertainties arising from models estimated with finite, noisy data. We propose
a robust adaptive control algorithm that explicitly incorporates such
non-asymptotic uncertainties into the control design. The algorithm has three
components: (1) a least-squares nominal model estimator; (2) a bootstrap
resampling method that quantifies non-asymptotic variance of the nominal model
estimate; and (3) a non-conventional robust control design method using an
optimal linear quadratic regulator (LQR) with multiplicative noise. A key
advantage of the proposed approach is that the system identification and robust
control design procedures both use stochastic uncertainty representations, so
that the actual inherent statistical estimation uncertainty directly aligns
with the uncertainty the robust controller is being designed against. We show
through numerical experiments that the proposed robust adaptive controller can
significantly outperform the certainty equivalent controller on both expected
regret and measures of regret risk.","cs.LG,cs.SY,eess.SY,math.DS,math.OC,stat.ML"
"Policy Gradient using Weak Derivatives for Reinforcement Learning. This paper considers policy search in continuous state-action reinforcement
learning problems. Typically, one computes search directions using a classic
expression for the policy gradient called the Policy Gradient Theorem, which
decomposes the gradient of the value function into two factors: the score
function and the Q-function. This paper presents four results:(i) an
alternative policy gradient theorem using weak (measure-valued) derivatives
instead of score-function is established; (ii) the stochastic gradient
estimates thus derived are shown to be unbiased and to yield algorithms that
converge almost surely to stationary points of the non-convex value function of
the reinforcement learning problem; (iii) the sample complexity of the
algorithm is derived and is shown to be $O(1/\sqrt(k))$; (iv) finally, the
expected variance of the gradient estimates obtained using weak derivatives is
shown to be lower than those obtained using the popular score-function
approach. Experiments on OpenAI gym pendulum environment show superior
performance of the proposed algorithm.","cs.LG,cs.MA,cs.SY,eess.SY,math.OC,stat.ML"
"Signal-domain representation of symbolic music for learning embedding spaces. A key aspect of machine learning models lies in their ability to learn
efficient intermediate features. However, the input representation plays a
crucial role in this process, and polyphonic musical scores remain a
particularly complex type of information. In this paper, we introduce a novel
representation of symbolic music data, which transforms a polyphonic score into
a continuous signal. We evaluate the ability to learn meaningful features from
this representation from a musical point of view. Hence, we introduce an
evaluation method relying on principled generation of synthetic data. Finally,
to test our proposed representation we conduct an extensive benchmark against
recent polyphonic symbolic representations. We show that our signal-like
representation leads to better reconstruction and disentangled features. This
improvement is reflected in the metric properties and in the generation ability
of the space learned from our signal-like representation according to music
theory properties.","cs.LG,cs.SD,eess.AS"
"Taming Pretrained Transformers for Extreme Multi-label Text Classification. We consider the extreme multi-label text classification (XMC) problem: given
an input text, return the most relevant labels from a large label collection.
For example, the input text could be a product description on Amazon.com and
the labels could be product categories. XMC is an important yet challenging
problem in the NLP community. Recently, deep pretrained transformer models have
achieved state-of-the-art performance on many NLP tasks including sentence
classification, albeit with small label sets. However, naively applying deep
transformer models to the XMC problem leads to sub-optimal performance due to
the large output space and the label sparsity issue. In this paper, we propose
X-Transformer, the first scalable approach to fine-tuning deep transformer
models for the XMC problem. The proposed method achieves new state-of-the-art
results on four XMC benchmark datasets. In particular, on a Wiki dataset with
around 0.5 million labels, the prec@1 of X-Transformer is 77.28%, a substantial
improvement over state-of-the-art XMC approaches Parabel (linear) and
AttentionXML (neural), which achieve 68.70% and 76.95% precision@1,
respectively. We further apply X-Transformer to a product2query dataset from
Amazon and gained 10.7% relative improvement on prec@1 over Parabel.","cs.AI,cs.IR,cs.LG,stat.ML"
"Mid-Level Visual Representations Improve Generalization and Sample Efficiency for Learning Visuomotor Policies. How much does having visual priors about the world (e.g. the fact that the
world is 3D) assist in learning to perform downstream motor tasks (e.g.
delivering a package)? We study this question by integrating a generic
perceptual skill set (e.g. a distance estimator, an edge detector, etc.) within
a reinforcement learning framework--see Figure 1. This skill set (hereafter
mid-level perception) provides the policy with a more processed state of the
world compared to raw images.
  We find that using a mid-level perception confers significant advantages over
training end-to-end from scratch (i.e. not leveraging priors) in
navigation-oriented tasks. Agents are able to generalize to situations where
the from-scratch approach fails and training becomes significantly more sample
efficient. However, we show that realizing these gains requires careful
selection of the mid-level perceptual skills. Therefore, we refine our findings
into an efficient max-coverage feature set that can be adopted in lieu of raw
images. We perform our study in completely separate buildings for training and
testing and compare against visually blind baseline policies and
state-of-the-art feature learning methods.","cs.AI,cs.CV,cs.LG,cs.NE,cs.RO"
"The Logic of Graph Neural Networks. Graph neural networks (GNNs) are deep learning architectures for machine
learning problems on graphs. It has recently been shown that the expressiveness
of GNNs can be characterised precisely by the combinatorial Weisfeiler-Leman
algorithms and by finite variable counting logics. The correspondence has even
led to new, higher-order GNNs corresponding to the WL algorithm in higher
dimensions.
  The purpose of this paper is to explain these descriptive characterisations
of GNNs.","cs.AI,cs.LG,cs.LO"
"ModelDiff: Testing-Based DNN Similarity Comparison for Model Reuse Detection. The knowledge of a deep learning model may be transferred to a student model,
leading to intellectual property infringement or vulnerability propagation.
Detecting such knowledge reuse is nontrivial because the suspect models may not
be white-box accessible and/or may serve different tasks. In this paper, we
propose ModelDiff, a testing-based approach to deep learning model similarity
comparison. Instead of directly comparing the weights, activations, or outputs
of two models, we compare their behavioral patterns on the same set of test
inputs. Specifically, the behavioral pattern of a model is represented as a
decision distance vector (DDV), in which each element is the distance between
the model's reactions to a pair of inputs. The knowledge similarity between two
models is measured with the cosine similarity between their DDVs. To evaluate
ModelDiff, we created a benchmark that contains 144 pairs of models that cover
most popular model reuse methods, including transfer learning, model
compression, and model stealing. Our method achieved 91.7% correctness on the
benchmark, which demonstrates the effectiveness of using ModelDiff for model
reuse detection. A study on mobile deep learning apps has shown the feasibility
of ModelDiff on real-world models.","cs.AI,cs.LG,cs.SE"
"Learning Aesthetic Layouts via Visual Guidance. We explore computational approaches for visual guidance to aid in creating
aesthetically pleasing art and graphic design. Our work complements and builds
on previous work that developed models for how humans look at images. Our
approach comprises three steps. First, we collected a dataset of art
masterpieces and labeled the visual fixations with state-of-art vision models.
Second, we clustered the visual guidance templates of the art masterpieces with
unsupervised learning. Third, we developed a pipeline using generative
adversarial networks to learn the principles of visual guidance and that can
produce aesthetically pleasing layouts. We show that the aesthetic visual
guidance principles can be learned and integrated into a high-dimensional model
and can be queried by the features of graphic elements. We evaluate our
approach by generating layouts on various drawings and graphic designs.
Moreover, our model considers the color and structure of graphic elements when
generating layouts. Consequently, we believe our tool, which generates multiple
aesthetic layout options in seconds, can help artists create beautiful art and
graphic designs.","cs.CV,cs.GR,cs.MM"
"Task-Agnostic Online Reinforcement Learning with an Infinite Mixture of Gaussian Processes. Continuously learning to solve unseen tasks with limited experience has been
extensively pursued in meta-learning and continual learning, but with
restricted assumptions such as accessible task distributions, independently and
identically distributed tasks, and clear task delineations. However, real-world
physical tasks frequently violate these assumptions, resulting in performance
degradation. This paper proposes a continual online model-based reinforcement
learning approach that does not require pre-training to solve task-agnostic
problems with unknown task boundaries. We maintain a mixture of experts to
handle nonstationarity, and represent each different type of dynamics with a
Gaussian Process to efficiently leverage collected data and expressively model
uncertainty. We propose a transition prior to account for the temporal
dependencies in streaming data and update the mixture online via sequential
variational inference. Our approach reliably handles the task distribution
shift by generating new models for never-before-seen dynamics and reusing old
models for previously seen dynamics. In experiments, our approach outperforms
alternative methods in non-stationary tasks, including classic control with
changing dynamics and decision making in different driving scenarios.","cs.AI,cs.LG,cs.RO,stat.ML"
"Flexible dual-branched message passing neural network for quantum mechanical property prediction with molecular conformation. A molecule is a complex of heterogeneous components, and the spatial
arrangements of these components determine the whole molecular properties and
characteristics. With the advent of deep learning in computational chemistry,
several studies have focused on how to predict molecular properties based on
molecular configurations. Message passing neural network provides an effective
framework for capturing molecular geometric features with the perspective of a
molecule as a graph. However, most of these studies assumed that all
heterogeneous molecular features, such as atomic charge, bond length, or other
geometric features always contribute equivalently to the target prediction,
regardless of the task type. In this study, we propose a dual-branched neural
network for molecular property prediction based on message-passing framework.
Our model learns heterogeneous molecular features with different scales, which
are trained flexibly according to each prediction target. In addition, we
introduce a discrete branch to learn single atom features without local
aggregation, apart from message-passing steps. We verify that this novel
structure can improve the model performance with faster convergence in most
targets. The proposed model outperforms other recent models with sparser
representations. Our experimental results indicate that in the chemical
property prediction tasks, the diverse chemical nature of targets should be
carefully considered for both model performance and generalizability.","cs.LG,physics.chem-ph"
"Therapeutics Data Commons: Machine Learning Datasets and Tasks for Drug Discovery and Development. Therapeutics machine learning is an emerging field with incredible
opportunities for innovatiaon and impact. However, advancement in this field
requires formulation of meaningful learning tasks and careful curation of
datasets. Here, we introduce Therapeutics Data Commons (TDC), the first
unifying platform to systematically access and evaluate machine learning across
the entire range of therapeutics. To date, TDC includes 66 AI-ready datasets
spread across 22 learning tasks and spanning the discovery and development of
safe and effective medicines. TDC also provides an ecosystem of tools and
community resources, including 33 data functions and types of meaningful data
splits, 23 strategies for systematic model evaluation, 17 molecule generation
oracles, and 29 public leaderboards. All resources are integrated and
accessible via an open Python library. We carry out extensive experiments on
selected datasets, demonstrating that even the strongest algorithms fall short
of solving key therapeutics challenges, including real dataset distributional
shifts, multi-scale modeling of heterogeneous data, and robust generalization
to novel data points. We envision that TDC can facilitate algorithmic and
scientific advances and considerably accelerate machine-learning model
development, validation and transition into biomedical and clinical
implementation. TDC is an open-science initiative available at
https://tdcommons.ai.","cs.CY,cs.LG,q-bio.BM,q-bio.QM"
"SparseCodePicking: feature extraction in mass spectrometry using sparse coding algorithms. Mass spectrometry (MS) is an important technique for chemical profiling which
calculates for a sample a high dimensional histogram-like spectrum. A crucial
step of MS data processing is the peak picking which selects peaks containing
information about molecules with high concentrations which are of interest in
an MS investigation. We present a new procedure of the peak picking based on a
sparse coding algorithm. Given a set of spectra of different classes, i.e. with
different positions and heights of the peaks, this procedure can extract peaks
by means of unsupervised learning. Instead of an $l_1$-regularization penalty
term used in the original sparse coding algorithm we propose using an
elastic-net penalty term for better regularization. The evaluation is done by
means of simulation. We show that for a large region of parameters the proposed
peak picking method based on the sparse coding features outperforms a mean
spectrum-based method. Moreover, we demonstrate the procedure applying it to
two real-life datasets.","physics.med-ph,stat.AP,stat.ME,stat.ML"
"Not All Adversarial Examples Require a Complex Defense: Identifying Over-optimized Adversarial Examples with IQR-based Logit Thresholding. Detecting adversarial examples currently stands as one of the biggest
challenges in the field of deep learning. Adversarial attacks, which produce
adversarial examples, increase the prediction likelihood of a target class for
a particular data point. During this process, the adversarial example can be
further optimized, even when it has already been wrongly classified with 100%
confidence, thus making the adversarial example even more difficult to detect.
For this kind of adversarial examples, which we refer to as over-optimized
adversarial examples, we discovered that the logits of the model provide solid
clues on whether the data point at hand is adversarial or genuine. In this
context, we first discuss the masking effect of the softmax function for the
prediction made and explain why the logits of the model are more useful in
detecting over-optimized adversarial examples. To identify this type of
adversarial examples in practice, we propose a non-parametric and
computationally efficient method which relies on interquartile range, with this
method becoming more effective as the image resolution increases. We support
our observations throughout the paper with detailed experiments for different
datasets (MNIST, CIFAR-10, and ImageNet) and several architectures.","cs.CR,cs.CV,cs.LG,stat.ML"
"Continuous Meta-Learning without Tasks. Meta-learning is a promising strategy for learning to efficiently learn
within new tasks, using data gathered from a distribution of tasks. However,
the meta-learning literature thus far has focused on the task segmented
setting, where at train-time, offline data is assumed to be split according to
the underlying task, and at test-time, the algorithms are optimized to learn in
a single task. In this work, we enable the application of generic meta-learning
algorithms to settings where this task segmentation is unavailable, such as
continual online learning with a time-varying task. We present meta-learning
via online changepoint analysis (MOCA), an approach which augments a
meta-learning algorithm with a differentiable Bayesian changepoint detection
scheme. The framework allows both training and testing directly on time series
data without segmenting it into discrete tasks. We demonstrate the utility of
this approach on a nonlinear meta-regression benchmark as well as two
meta-image-classification benchmarks.","cs.CV,cs.LG,cs.NE,stat.ML"
"Complete Endomorphisms in Computer Vision. Correspondences between k-tuples of points are key in multiple view geometry
and motion analysis. Regular transformations are posed by homographies between
two projective planes that serves as structural models for images. Such
transformations can not include degenerate situations. Fundamental or essential
matrices expand homographies with structural information by using degenerate
bilinear maps. The projectivization of the endomorphisms of a three-dimensional
vector space includes all of them. Hence, they are able to explain a wider
range of eventually degenerate transformations between arbitrary pairs of
views. To include these degenerate situations, this paper introduces a
completion of bilinear maps between spaces given by an equivariant
compactification of regular transformations. This completion is extensible to
the varieties of fundamental and essential matrices, where most methods based
on regular transformations fail. The construction of complete endomorphisms
manages degenerate projection maps using a simultaneous action on source and
target spaces. In such way, this mathematical construction provides a robust
framework to relate corresponding views in multiple view geometry.","cs.CV,cs.RO"
"Medical Image Super-Resolution Using a Generative Adversarial Network. During the growing popularity of electronic medical records, electronic
medical record (EMR) data has exploded increasingly. It is very meaningful to
retrieve high quality EMR in mass data. In this paper, an EMR value network
with retrieval function is constructed by taking stroke disease as the research
object. It mainly includes: 1) It establishes the electronic medical record
database and corresponding stroke knowledge graph. 2) The strategy of
similarity measurement is included three parts(patients' chief complaint,
pathology results and medical images). Patients' chief complaints are text
data, mainly describing patients' symptoms and expressed in words or phrases,
and patients' chief complaints are input in independent tick of various
symptoms. The data of the pathology results is a structured and digitized
expression, so the input method is the same as the patient's chief complaint;
Image similarity adopts content-based image retrieval(CBIR) technology. 3) The
analytic hierarchy process (AHP) is used to establish the weights of the three
types of data and then synthesize them into an indicator. The accuracy rate of
similarity in top 5 was more than 85\% based on EMR database with more 200
stroke records using leave-one-out method. It will be the good tool for
assistant diagnosis and doctor training, as good quality records are colleted
into the databases, like Doctor Watson, in the future.","cs.CV,cs.NA,math.NA"
"Fast block-coordinate Frank-Wolfe algorithm for semi-relaxed optimal transport. Optimal transport (OT), which provides a distance between two probability
distributions by considering their spatial locations, has been applied to
widely diverse applications. Computing an OT problem requires solution of
linear programming with tight mass-conservation constraints. This requirement
hinders its application to large-scale problems. To alleviate this issue, the
recently proposed relaxed-OT approach uses a faster algorithm by relaxing such
constraints. Its effectiveness for practical applications has been
demonstrated. Nevertheless, it still exhibits slow convergence. To this end,
addressing a convex semi-relaxed OT, we propose a fast block-coordinate
Frank-Wolfe (BCFW) algorithm, which gives sparse solutions. Specifically, we
provide their upper bounds of the worst convergence iterations, and equivalence
between the linearization duality gap and the Lagrangian duality gap. Three
fast variants of the proposed BCFW are also proposed. Numerical evaluations in
color transfer problem demonstrate that the proposed algorithms outperform
state-of-the-art algorithms across different settings.","cs.LG,math.OC"
"Censored Exploration and the Dark Pool Problem. We introduce and analyze a natural algorithm for multi-venue exploration from
censored data, which is motivated by the Dark Pool Problem of modern
quantitative finance. We prove that our algorithm converges in polynomial time
to a near-optimal allocation policy; prior results for similar problems in
stochastic inventory control guaranteed only asymptotic convergence and
examined variants in which each venue could be treated independently. Our
analysis bears a strong resemblance to that of efficient exploration/
exploitation schemes in the reinforcement learning literature. We describe an
extensive experimental evaluation of our algorithm on the Dark Pool Problem
using real trading data.","cs.GT,cs.LG"
"Causal Inference via Kernel Deviance Measures. Discovering the causal structure among a set of variables is a fundamental
problem in many areas of science. In this paper, we propose Kernel Conditional
Deviance for Causal Inference (KCDC) a fully nonparametric causal discovery
method based on purely observational data. From a novel interpretation of the
notion of asymmetry between cause and effect, we derive a corresponding
asymmetry measure using the framework of reproducing kernel Hilbert spaces.
Based on this, we propose three decision rules for causal discovery. We
demonstrate the wide applicability of our method across a range of diverse
synthetic datasets. Furthermore, we test our method on real-world time series
data and the real-world benchmark dataset Tubingen Cause-Effect Pairs where we
outperform existing state-of-the-art methods.","cs.LG,stat.ME,stat.ML"
"Attention Based Semantic Segmentation on UAV Dataset for Natural Disaster Damage Assessment. The detrimental impacts of climate change include stronger and more
destructive hurricanes happening all over the world. Identifying different
damaged structures of an area including buildings and roads are vital since it
helps the rescue team to plan their efforts to minimize the damage caused by a
natural disaster. Semantic segmentation helps to identify different parts of an
image. We implement a novel self-attention based semantic segmentation model on
a high resolution UAV dataset and attain Mean IoU score of around 88% on the
test set. The result inspires to use self-attention schemes in natural disaster
damage assessment which will save human lives and reduce economic losses.","68T45,I.4.6,cs.CV"
"Understanding Bandits with Graph Feedback. The bandit problem with graph feedback, proposed in [Mannor and Shamir,
NeurIPS 2011], is modeled by a directed graph $G=(V,E)$ where $V$ is the
collection of bandit arms, and once an arm is triggered, all its incident arms
are observed. A fundamental question is how the structure of the graph affects
the min-max regret. We propose the notions of the fractional weak domination
number $\delta^*$ and the $k$-packing independence number capturing upper bound
and lower bound for the regret respectively. We show that the two notions are
inherently connected via aligning them with the linear program of the weakly
dominating set and its dual -- the fractional vertex packing set respectively.
Based on this connection, we utilize the strong duality theorem to prove a
general regret upper bound $O\left(\left( \delta^*\log
|V|\right)^{\frac{1}{3}}T^{\frac{2}{3}}\right)$ and a lower bound
$\Omega\left(\left(\delta^*/\alpha\right)^{\frac{1}{3}}T^{\frac{2}{3}}\right)$
where $\alpha$ is the integrality gap of the dual linear program. Therefore,
our bounds are tight up to a $\left(\log |V|\right)^{\frac{1}{3}}$ factor on
graphs with bounded integrality gap for the vertex packing problem including
trees and graphs with bounded degree. Moreover, we show that for several
special families of graphs, we can get rid of the $\left(\log
|V|\right)^{\frac{1}{3}}$ factor and establish optimal regret.","cs.DS,cs.LG,stat.ML"
"Model reduction for the material point method via learning the deformation map and its spatial-temporal gradients. This work proposes a model-reduction approach for the material point method
on nonlinear manifolds. The technique approximates the $\textit{kinematics}$ by
approximating the deformation map in a manner that restricts deformation
trajectories to reside on a low-dimensional manifold expressed from the
extrinsic view via a parameterization function. By explicitly approximating the
deformation map and its spatial-temporal gradients, the deformation gradient
and the velocity can be computed simply by differentiating the associated
parameterization function. Unlike classical model reduction techniques that
build a subspace for a finite number of degrees of freedom, the proposed method
approximates the entire deformation map with infinite degrees of freedom.
Therefore, the technique supports resolution changes in the reduced simulation,
attaining the challenging task of zero-shot super-resolution by generating
material points unseen in the training data. The ability to generate material
points also allows for adaptive quadrature rules for stress update. A family of
projection methods is devised to generate $\textit{dynamics}$, i.e., at every
time step, the methods perform three steps: (1) generate quadratures in the
full space from the reduced space, (2) compute position and velocity updates in
the full space, and (3) perform a least-squares projection of the updated
position and velocity onto the low-dimensional manifold and its tangent space.
Computational speedup is achieved via hyper-reduction, i.e., only a subset of
the original material points are needed for dynamics update. Large-scale
numerical examples with millions of material points illustrate the method's
ability to gain an order-of-magnitude computational-cost saving -- indeed
$\textit{real-time simulations}$ in some cases -- with negligible errors.","cs.CE,cs.GR,cs.LG,cs.NA,math.NA"
"Forecasting with time series imaging. Feature-based time series representations have attracted substantial
attention in a wide range of time series analysis methods. Recently, the use of
time series features for forecast model averaging has been an emerging research
focus in the forecasting community. Nonetheless, most of the existing
approaches depend on the manual choice of an appropriate set of features.
Exploiting machine learning methods to extract features from time series
automatically becomes crucial in state-of-the-art time series analysis. In this
paper, we introduce an automated approach to extract time series features based
on time series imaging. We first transform time series into recurrence plots,
from which local features can be extracted using computer vision algorithms.
The extracted features are used for forecast model averaging. Our experiments
show that forecasting based on automatically extracted features, with less
human intervention and a more comprehensive view of the raw time series data,
yields highly comparable performances with the best methods in the largest
forecasting competition dataset (M4) and outperforms the top methods in the
Tourism forecasting competition dataset.","cs.CV,cs.LG,stat.CO,stat.ML"
"Enforcing constraints for interpolation and extrapolation in Generative Adversarial Networks. We suggest ways to enforce given constraints in the output of a Generative
Adversarial Network (GAN) generator both for interpolation and extrapolation
(prediction). For the case of dynamical systems, given a time series, we wish
to train GAN generators that can be used to predict trajectories starting from
a given initial condition. In this setting, the constraints can be in algebraic
and/or differential form. Even though we are predominantly interested in the
case of extrapolation, we will see that the tasks of interpolation and
extrapolation are related. However, they need to be treated differently.
  For the case of interpolation, the incorporation of constraints is built into
the training of the GAN. The incorporation of the constraints respects the
primary game-theoretic setup of a GAN so it can be combined with existing
algorithms. However, it can exacerbate the problem of instability during
training that is well-known for GANs. We suggest adding small noise to the
constraints as a simple remedy that has performed well in our numerical
experiments.
  The case of extrapolation (prediction) is more involved. During training, the
GAN generator learns to interpolate a noisy version of the data and we enforce
the constraints. This approach has connections with model reduction that we can
utilize to improve the efficiency and accuracy of the training. Depending on
the form of the constraints, we may enforce them also during prediction through
a projection step. We provide examples of linear and nonlinear systems of
differential equations to illustrate the various constructions.","37M10,62M45,65L05,68Q32,68T05,cs.LG,stat.ML"
"A Principled Approach to Failure Analysis and Model Repairment: Demonstration in Medical Imaging. Machine learning models commonly exhibit unexpected failures post-deployment
due to either data shifts or uncommon situations in the training environment.
Domain experts typically go through the tedious process of inspecting the
failure cases manually, identifying failure modes and then attempting to fix
the model. In this work, we aim to standardise and bring principles to this
process through answering two critical questions: (i) how do we know that we
have identified meaningful and distinct failure types?; (ii) how can we
validate that a model has, indeed, been repaired? We suggest that the quality
of the identified failure types can be validated through measuring the intra-
and inter-type generalisation after fine-tuning and introduce metrics to
compare different subtyping methods. Furthermore, we argue that a model can be
considered repaired if it achieves high accuracy on the failure types while
retaining performance on the previously correct data. We combine these two
ideas into a principled framework for evaluating the quality of both the
identified failure subtypes and model repairment. We evaluate its utility on a
classification and an object detection tasks. Our code is available at
https://github.com/Rokken-lab6/Failure-Analysis-and-Model-Repairment","cs.AI,cs.CV,cs.LG,eess.IV"
"Controlling for Confounders in Multimodal Emotion Classification via Adversarial Learning. Various psychological factors affect how individuals express emotions. Yet,
when we collect data intended for use in building emotion recognition systems,
we often try to do so by creating paradigms that are designed just with a focus
on eliciting emotional behavior. Algorithms trained with these types of data
are unlikely to function outside of controlled environments because our
emotions naturally change as a function of these other factors. In this work,
we study how the multimodal expressions of emotion change when an individual is
under varying levels of stress. We hypothesize that stress produces modulations
that can hide the true underlying emotions of individuals and that we can make
emotion recognition algorithms more generalizable by controlling for variations
in stress. To this end, we use adversarial networks to decorrelate stress
modulations from emotion representations. We study how stress alters acoustic
and lexical emotional predictions, paying special attention to how modulations
due to stress affect the transferability of learned emotion recognition models
across domains. Our results show that stress is indeed encoded in trained
emotion classifiers and that this encoding varies across levels of emotions and
across the lexical and acoustic modalities. Our results also show that emotion
recognition models that control for stress during training have better
generalizability when applied to new domains, compared to models that do not
control for stress during training. We conclude that is is necessary to
consider the effect of extraneous psychological factors when building and
testing emotion recognition models.","cs.CL,cs.LG,cs.SD,eess.AS,stat.ML"
"Iterative graph cuts for image segmentation with a nonlinear statistical shape prior. Shape-based regularization has proven to be a useful method for delineating
objects within noisy images where one has prior knowledge of the shape of the
targeted object. When a collection of possible shapes is available, the
specification of a shape prior using kernel density estimation is a natural
technique. Unfortunately, energy functionals arising from kernel density
estimation are of a form that makes them impossible to directly minimize using
efficient optimization algorithms such as graph cuts. Our main contribution is
to show how one may recast the energy functional into a form that is
minimizable iteratively and efficiently using graph cuts.","cs.CV,math.OC,physics.data-an,q-bio.QM,stat.AP"
"Unsupervised Features Learning for Sampled Vector Fields. In this paper we introduce a new approach to computing hidden features of
sampled vector fields. The basic idea is to convert the vector field data to a
graph structure and use tools designed for automatic, unsupervised analysis of
graphs. Using a few data sets we show that the collected features of the vector
fields are correlated with the dynamics known for analytic models which
generates the data. In particular the method may be useful in analysis of data
sets where the analytic model is poorly understood or not known.","cs.LG,math.AT,math.DS"
"Scene Memory Transformer for Embodied Agents in Long-Horizon Tasks. Many robotic applications require the agent to perform long-horizon tasks in
partially observable environments. In such applications, decision making at any
step can depend on observations received far in the past. Hence, being able to
properly memorize and utilize the long-term history is crucial. In this work,
we propose a novel memory-based policy, named Scene Memory Transformer (SMT).
The proposed policy embeds and adds each observation to a memory and uses the
attention mechanism to exploit spatio-temporal dependencies. This model is
generic and can be efficiently trained with reinforcement learning over long
episodes. On a range of visual navigation tasks, SMT demonstrates superior
performance to existing reactive and memory-based policies by a margin.","cs.CV,cs.LG,cs.RO,stat.ML"
"Auto-Encoding Molecular Conformations. In this work we introduce an Autoencoder for molecular conformations. Our
proposed model converts the discrete spatial arrangements of atoms in a given
molecular graph (conformation) into and from a continuous fixed-sized latent
representation. We demonstrate that in this latent representation, similar
conformations cluster together while distinct conformations split apart.
Moreover, by training a probabilistic model on a large dataset of molecular
conformations, we demonstrate how our model can be used to generate diverse
sets of energetically favorable conformations for a given molecule. Finally, we
show that the continuous representation allows us to utilize optimization
methods to find molecules that have conformations with favourable spatial
properties.","cs.LG,physics.chem-ph,q-bio.QM"
"Primal-Dual $$ Learning: Sample Complexity and Sublinear Run Time for Ergodic Markov Decision Problems. Consider the problem of approximating the optimal policy of a Markov decision
process (MDP) by sampling state transitions. In contrast to existing
reinforcement learning methods that are based on successive approximations to
the nonlinear Bellman equation, we propose a Primal-Dual $\pi$ Learning method
in light of the linear duality between the value and policy. The $\pi$ learning
method is model-free and makes primal-dual updates to the policy and value
vectors as new data are revealed. For infinite-horizon undiscounted Markov
decision process with finite state space $S$ and finite action space $A$, the
$\pi$ learning method finds an $\epsilon$-optimal policy using the following
number of sample transitions $$ \tilde{O}( \frac{(\tau\cdot t^*_{mix})^2 |S|
|A| }{\epsilon^2} ),$$ where $t^*_{mix}$ is an upper bound of mixing times
across all policies and $\tau$ is a parameter characterizing the range of
stationary distributions across policies. The $\pi$ learning method also
applies to the computational problem of MDP where the transition probabilities
and rewards are explicitly given as the input. In the case where each state
transition can be sampled in $\tilde{O}(1)$ time, the $\pi$ learning method
gives a sublinear-time algorithm for solving the averaged-reward MDP.","cs.CC,cs.LG,math.OC"
"ATOM3D: Tasks On Molecules in Three Dimensions. Computational methods that operate on three-dimensional molecular structure
have the potential to solve important questions in biology and chemistry. In
particular, deep neural networks have gained significant attention, but their
widespread adoption in the biomolecular domain has been limited by a lack of
either systematic performance benchmarks or a unified toolkit for interacting
with molecular data. To address this, we present ATOM3D, a collection of both
novel and existing benchmark datasets spanning several key classes of
biomolecules. We implement several classes of three-dimensional molecular
learning methods for each of these tasks and show that they consistently
improve performance relative to methods based on one- and two-dimensional
representations. The specific choice of architecture proves to be critical for
performance, with three-dimensional convolutional networks excelling at tasks
involving complex geometries, graph networks performing well on systems
requiring detailed positional information, and the more recently developed
equivariant networks showing significant promise. Our results indicate that
many molecular problems stand to gain from three-dimensional molecular
learning, and that there is potential for improvement on many tasks which
remain underexplored. To lower the barrier to entry and facilitate further
developments in the field, we also provide a comprehensive suite of tools for
dataset processing, model training, and evaluation in our open-source atom3d
Python package. All datasets are available for download from
https://www.atom3d.ai .","cs.LG,physics.bio-ph,physics.comp-ph,q-bio.BM"
"BSNet: Bi-Similarity Network for Few-shot Fine-grained Image Classification. Few-shot learning for fine-grained image classification has gained recent
attention in computer vision. Among the approaches for few-shot learning, due
to the simplicity and effectiveness, metric-based methods are favorably
state-of-the-art on many tasks. Most of the metric-based methods assume a
single similarity measure and thus obtain a single feature space. However, if
samples can simultaneously be well classified via two distinct similarity
measures, the samples within a class can distribute more compactly in a smaller
feature space, producing more discriminative feature maps. Motivated by this,
we propose a so-called \textit{Bi-Similarity Network} (\textit{BSNet}) that
consists of a single embedding module and a bi-similarity module of two
similarity measures. After the support images and the query images pass through
the convolution-based embedding module, the bi-similarity module learns feature
maps according to two similarity measures of diverse characteristics. In this
way, the model is enabled to learn more discriminative and less
similarity-biased features from few shots of fine-grained images, such that the
model generalization ability can be significantly improved. Through extensive
experiments by slightly modifying established metric/similarity based networks,
we show that the proposed approach produces a substantial improvement on
several fine-grained image benchmark datasets. Codes are available at:
https://github.com/spraise/BSNet","68U10,I.4,cs.CV"
"Complex Query Answering with Neural Link Predictors. Neural link predictors are immensely useful for identifying missing edges in
large scale Knowledge Graphs. However, it is still not clear how to use these
models for answering more complex queries that arise in a number of domains,
such as queries using logical conjunctions ($\land$), disjunctions ($\lor$) and
existential quantifiers ($\exists$), while accounting for missing edges. In
this work, we propose a framework for efficiently answering complex queries on
incomplete Knowledge Graphs. We translate each query into an end-to-end
differentiable objective, where the truth value of each atom is computed by a
pre-trained neural link predictor. We then analyse two solutions to the
optimisation problem, including gradient-based and combinatorial search. In our
experiments, the proposed approach produces more accurate results than
state-of-the-art methods -- black-box neural models trained on millions of
generated queries -- without the need of training on a large and diverse set of
complex queries. Using orders of magnitude less training data, we obtain
relative improvements ranging from 8% up to 40% in Hits@3 across different
knowledge graphs containing factual information. Finally, we demonstrate that
it is possible to explain the outcome of our model in terms of the intermediate
solutions identified for each of the complex query atoms. All our source code
and datasets are available online, at https://github.com/uclnlp/cqd.","cs.AI,cs.LG,cs.LO,cs.NE"
"FALDOI: A new minimization strategy for large displacement variational optical flow. We propose a large displacement optical flow method that introduces a new
strategy to compute a good local minimum of any optical flow energy functional.
The method requires a given set of discrete matches, which can be extremely
sparse, and an energy functional which locally guides the interpolation from
those matches. In particular, the matches are used to guide a structured
coordinate-descent of the energy functional around these keypoints. It results
in a two-step minimization method at the finest scale which is very robust to
the inevitable outliers of the sparse matcher and able to capture large
displacements of small objects. Its benefits over other variational methods
that also rely on a set of sparse matches are its robustness against very few
matches, high levels of noise and outliers. We validate our proposal using
several optical flow variational models. The results consistently outperform
the coarse-to-fine approaches and achieve good qualitative and quantitative
performance on the standard optical flow benchmarks.","49M29,65K10,68U10,cs.CV"
"RDP-GAN: A Rnyi-Differential Privacy based Generative Adversarial Network. Generative adversarial network (GAN) has attracted increasing attention
recently owing to its impressive ability to generate realistic samples with
high privacy protection. Without directly interactive with training examples,
the generative model can be fully used to estimate the underlying distribution
of an original dataset while the discriminative model can examine the quality
of the generated samples by comparing the label values with the training
examples. However, when GANs are applied on sensitive or private training
examples, such as medical or financial records, it is still probable to divulge
individuals' sensitive and private information. To mitigate this information
leakage and construct a private GAN, in this work we propose a
R\'enyi-differentially private-GAN (RDP-GAN), which achieves differential
privacy (DP) in a GAN by carefully adding random noises on the value of the
loss function during training. Moreover, we derive the analytical results of
the total privacy loss under the subsampling method and cumulated iterations,
which show its effectiveness on the privacy budget allocation. In addition, in
order to mitigate the negative impact brought by the injecting noise, we
enhance the proposed algorithm by adding an adaptive noise tuning step, which
will change the volume of added noise according to the testing accuracy.
Through extensive experimental results, we verify that the proposed algorithm
can achieve a better privacy level while producing high-quality samples
compared with a benchmark DP-GAN scheme based on noise perturbation on training
gradients.","cs.CR,cs.LG,cs.NI,stat.ML"
"Training Generative Adversarial Networks via stochastic Nash games. Generative adversarial networks (GANs) are a class of generative models with
two antagonistic neural networks: a generator and a discriminator. These two
neural networks compete against each other through an adversarial process that
can be modeled as a stochastic Nash equilibrium problem. Since the associated
training process is challenging, it is fundamental to design reliable
algorithms to compute an equilibrium. In this paper, we propose a stochastic
relaxed forward-backward (SRFB) algorithm for GANs and we show convergence to
an exact solution when an increasing number of data is available. We also show
convergence of an averaged variant of the SRFB algorithm to a neighborhood of
the solution when only few samples are available. In both cases, convergence is
guaranteed when the pseudogradient mapping of the game is monotone. This
assumption is among the weakest known in the literature. Moreover, we apply our
algorithm to the image generation problem.","cs.GT,cs.LG,math.OC"
"Early Fusion for Goal Directed Robotic Vision. Building perceptual systems for robotics which perform well under tight
computational budgets requires novel architectures which rethink the
traditional computer vision pipeline. Modern vision architectures require the
agent to build a summary representation of the entire scene, even if most of
the input is irrelevant to the agent's current goal. In this work, we flip this
paradigm, by introducing EarlyFusion vision models that condition on a goal to
build custom representations for downstream tasks. We show that these goal
specific representations can be learned more quickly, are substantially more
parameter efficient, and more robust than existing attention mechanisms in our
domain. We demonstrate the effectiveness of these methods on a simulated
robotic item retrieval problem that is trained in a fully end-to-end manner via
imitation learning.","cs.CV,cs.RO"
"Applying recent advances in Visual Question Answering to Record Linkage. Multi-modal Record Linkage is the process of matching multi-modal records
from multiple sources that represent the same entity. This field has not been
explored in research and we propose two solutions based on Deep Learning
architectures that are inspired by recent work in Visual Question Answering.
The neural networks we propose use two different fusion modules, the Recurrent
Neural Network + Convolutional Neural Network fusion module and the Stacked
Attention Network fusion module, that jointly combine the visual and the
textual data of the records. The output of these fusion models is the input of
a Siamese Neural Network that computes the similarity of the records. Using
data from the Avito Duplicate Advertisements Detection dataset, we train these
solutions and from the experiments, we concluded that the Recurrent Neural
Network + Convolutional Neural Network fusion module outperforms a simple model
that uses hand-crafted features. We also find that the Recurrent Neural Network
+ Convolutional Neural Network fusion module classifies dissimilar
advertisements as similar more frequently if their average description is
bigger than 40 words. We conclude that the reason for this is that the longer
advertisements have a different distribution then the shorter advertisements
who are more prevalent in the dataset. In the end, we also conclude that
further research needs to be done with the Stacked Attention Network, to
further explore the effects of the visual data on the performance of the fusion
modules.","cs.AI,cs.DB,cs.LG,stat.ML"
"CDPA: Common and Distinctive Pattern Analysis between High-dimensional Datasets. A representative model in integrative analysis of two high-dimensional
correlated datasets is to decompose each data matrix into a low-rank common
matrix generated by latent factors shared across datasets, a low-rank
distinctive matrix corresponding to each dataset, and an additive noise matrix.
Existing decomposition methods claim that their common matrices capture the
common pattern of the two datasets. However, their so-called common pattern
only denotes the common latent factors but ignores the common pattern between
the two coefficient matrices of these common latent factors. We propose a new
unsupervised learning method, called the common and distinctive pattern
analysis (CDPA), which appropriately defines the two types of data patterns by
further incorporating the common and distinctive patterns of the coefficient
matrices. A consistent estimation approach is developed for high-dimensional
settings, and shows reasonably good finite-sample performance in simulations.
Our simulation studies and real data analysis corroborate that the proposed
CDPA can provide better characterization of common and distinctive patterns and
thereby benefit data mining.","cs.LG,stat.ML"
"SSGAN: Secure Steganography Based on Generative Adversarial Networks. In this paper, a novel strategy of Secure Steganograpy based on Generative
Adversarial Networks is proposed to generate suitable and secure covers for
steganography. The proposed architecture has one generative network, and two
discriminative networks. The generative network mainly evaluates the visual
quality of the generated images for steganography, and the discriminative
networks are utilized to assess their suitableness for information hiding.
Different from the existing work which adopts Deep Convolutional Generative
Adversarial Networks, we utilize another form of generative adversarial
networks. By using this new form of generative adversarial networks,
significant improvements are made on the convergence speed, the training
stability and the image quality. Furthermore, a sophisticated steganalysis
network is reconstructed for the discriminative network, and the network can
better evaluate the performance of the generated images. Numerous experiments
are conducted on the publicly available datasets to demonstrate the
effectiveness and robustness of the proposed method.","cs.CV,cs.MM"
"Sample-Efficient Reinforcement Learning for Linearly-Parameterized MDPs with a Generative Model. The curse of dimensionality is a widely known issue in reinforcement learning
(RL). In the tabular setting where the state space $\mathcal{S}$ and the action
space $\mathcal{A}$ are both finite, to obtain a nearly optimal policy with
sampling access to a generative model, the minimax optimal sample complexity
scales linearly with $|\mathcal{S}|\times|\mathcal{A}|$, which can be
prohibitively large when $\mathcal{S}$ or $\mathcal{A}$ is large. This paper
considers a Markov decision process (MDP) that admits a set of state-action
features, which can linearly express (or approximate) its probability
transition kernel. We show that a model-based approach (resp.$~$Q-learning)
provably learns an $\varepsilon$-optimal policy (resp.$~$Q-function) with high
probability as soon as the sample size exceeds the order of
$\frac{K}{(1-\gamma)^{3}\varepsilon^{2}}$
(resp.$~$$\frac{K}{(1-\gamma)^{4}\varepsilon^{2}}$), up to some logarithmic
factor. Here $K$ is the feature dimension and $\gamma\in(0,1)$ is the discount
factor of the MDP. Both sample complexity bounds are provably tight, and our
result for the model-based approach matches the minimax lower bound. Our
results show that for arbitrarily large-scale MDP, both the model-based
approach and Q-learning are sample-efficient when $K$ is relatively small, and
hence the title of this paper.","cs.IT,cs.LG,math.IT,math.OC,math.ST,stat.ML,stat.TH"
"Chow-Liu++: Optimal Prediction-Centric Learning of Tree Ising Models. We consider the problem of learning a tree-structured Ising model from data,
such that subsequent predictions computed using the model are accurate.
Concretely, we aim to learn a model such that posteriors $P(X_i|X_S)$ for small
sets of variables $S$ are accurate. Since its introduction more than 50 years
ago, the Chow-Liu algorithm, which efficiently computes the maximum likelihood
tree, has been the benchmark algorithm for learning tree-structured graphical
models. A bound on the sample complexity of the Chow-Liu algorithm with respect
to the prediction-centric local total variation loss was shown in [BK19]. While
those results demonstrated that it is possible to learn a useful model even
when recovering the true underlying graph is impossible, their bound depends on
the maximum strength of interactions and thus does not achieve the
information-theoretic optimum. In this paper, we introduce a new algorithm that
carefully combines elements of the Chow-Liu algorithm with tree metric
reconstruction methods to efficiently and optimally learn tree Ising models
under a prediction-centric loss. Our algorithm is robust to model
misspecification and adversarial corruptions. In contrast, we show that the
celebrated Chow-Liu algorithm can be arbitrarily suboptimal.","cs.DS,cs.IT,cs.LG,math.IT,math.ST,stat.TH"
"Transfer Learning Using Logistic Regression in Credit Scoring. The credit scoring risk management is a fast growing field due to consumer's
credit requests. Credit requests, of new and existing customers, are often
evaluated by classical discrimination rules based on customers information.
However, these kinds of strategies have serious limits and don't take into
account the characteristics difference between current customers and the future
ones. The aim of this paper is to measure credit worthiness for non customers
borrowers and to model potential risk given a heterogeneous population formed
by borrowers customers of the bank and others who are not. We hold on previous
works done in generalized gaussian discrimination and transpose them into the
logistic model to bring out efficient discrimination rules for non customers'
subpopulation.
  Therefore we obtain several simple models of connection between parameters of
both logistic models associated respectively to the two subpopulations. The
German credit data set is selected to experiment and to compare these models.
Experimental results show that the use of links between the two subpopulations
improve the classification accuracy for the new loan applicants.","cs.CE,cs.LG"
"Deep reinforcement learning from human preferences. For sophisticated reinforcement learning (RL) systems to interact usefully
with real-world environments, we need to communicate complex goals to these
systems. In this work, we explore goals defined in terms of (non-expert) human
preferences between pairs of trajectory segments. We show that this approach
can effectively solve complex RL tasks without access to the reward function,
including Atari games and simulated robot locomotion, while providing feedback
on less than one percent of our agent's interactions with the environment. This
reduces the cost of human oversight far enough that it can be practically
applied to state-of-the-art RL systems. To demonstrate the flexibility of our
approach, we show that we can successfully train complex novel behaviors with
about an hour of human time. These behaviors and environments are considerably
more complex than any that have been previously learned from human feedback.","cs.AI,cs.HC,cs.LG,stat.ML"
"Learning the Morphology of Brain Signals Using Alpha-Stable Convolutional Sparse Coding. Neural time-series data contain a wide variety of prototypical signal
waveforms (atoms) that are of significant importance in clinical and cognitive
research. One of the goals for analyzing such data is hence to extract such
'shift-invariant' atoms. Even though some success has been reported with
existing algorithms, they are limited in applicability due to their heuristic
nature. Moreover, they are often vulnerable to artifacts and impulsive noise,
which are typically present in raw neural recordings. In this study, we address
these issues and propose a novel probabilistic convolutional sparse coding
(CSC) model for learning shift-invariant atoms from raw neural signals
containing potentially severe artifacts. In the core of our model, which we
call $\alpha$CSC, lies a family of heavy-tailed distributions called
$\alpha$-stable distributions. We develop a novel, computationally efficient
Monte Carlo expectation-maximization algorithm for inference. The maximization
step boils down to a weighted CSC problem, for which we develop a
computationally efficient optimization algorithm. Our results show that the
proposed algorithm achieves state-of-the-art convergence speeds. Besides,
$\alpha$CSC is significantly more robust to artifacts when compared to three
competing algorithms: it can extract spike bursts, oscillations, and even
reveal more subtle phenomena such as cross-frequency coupling when applied to
noisy neural time series.","q-bio.NC,stat.AP,stat.ML"
"Locality-Sensitive Hashing for f-Divergences: Mutual Information Loss and Beyond. Computing approximate nearest neighbors in high dimensional spaces is a
central problem in large-scale data mining with a wide range of applications in
machine learning and data science. A popular and effective technique in
computing nearest neighbors approximately is the locality-sensitive hashing
(LSH) scheme. In this paper, we aim to develop LSH schemes for distance
functions that measure the distance between two probability distributions,
particularly for f-divergences as well as a generalization to capture mutual
information loss. First, we provide a general framework to design LHS schemes
for f-divergence distance functions and develop LSH schemes for the generalized
Jensen-Shannon divergence and triangular discrimination in this framework. We
show a two-sided approximation result for approximation of the generalized
Jensen-Shannon divergence by the Hellinger distance, which may be of
independent interest. Next, we show a general method of reducing the problem of
designing an LSH scheme for a Krein kernel (which can be expressed as the
difference of two positive definite kernels) to the problem of maximum inner
product search. We exemplify this method by applying it to the mutual
information loss, due to its several important applications such as model
compression.","cs.DB,cs.DS,cs.LG,stat.ML"
"Poisson Learning: Graph Based Semi-Supervised Learning At Very Low Label Rates. We propose a new framework, called Poisson learning, for graph based
semi-supervised learning at very low label rates. Poisson learning is motivated
by the need to address the degeneracy of Laplacian semi-supervised learning in
this regime. The method replaces the assignment of label values at training
points with the placement of sources and sinks, and solves the resulting
Poisson equation on the graph. The outcomes are provably more stable and
informative than those of Laplacian learning. Poisson learning is efficient and
simple to implement, and we present numerical experiments showing the method is
superior to other recent approaches to semi-supervised learning at low label
rates on MNIST, FashionMNIST, and Cifar-10. We also propose a graph-cut
enhancement of Poisson learning, called Poisson MBO, that gives higher accuracy
and can incorporate prior knowledge of relative class sizes.","05C81,35J08,35J15,35R02,68T05,I.2.6; G.1.8; G.2.2; I.5.3; I.4.0,cs.CV,cs.LG,cs.NA,math.AP,math.NA,stat.ML"
"Model reduction for the material point method via learning the deformation map and its spatial-temporal gradients. This work proposes a model-reduction approach for the material point method
on nonlinear manifolds. The technique approximates the $\textit{kinematics}$ by
approximating the deformation map in a manner that restricts deformation
trajectories to reside on a low-dimensional manifold expressed from the
extrinsic view via a parameterization function. By explicitly approximating the
deformation map and its spatial-temporal gradients, the deformation gradient
and the velocity can be computed simply by differentiating the associated
parameterization function. Unlike classical model reduction techniques that
build a subspace for a finite number of degrees of freedom, the proposed method
approximates the entire deformation map with infinite degrees of freedom.
Therefore, the technique supports resolution changes in the reduced simulation,
attaining the challenging task of zero-shot super-resolution by generating
material points unseen in the training data. The ability to generate material
points also allows for adaptive quadrature rules for stress update. A family of
projection methods is devised to generate $\textit{dynamics}$, i.e., at every
time step, the methods perform three steps: (1) generate quadratures in the
full space from the reduced space, (2) compute position and velocity updates in
the full space, and (3) perform a least-squares projection of the updated
position and velocity onto the low-dimensional manifold and its tangent space.
Computational speedup is achieved via hyper-reduction, i.e., only a subset of
the original material points are needed for dynamics update. Large-scale
numerical examples with millions of material points illustrate the method's
ability to gain an order-of-magnitude computational-cost saving -- indeed
$\textit{real-time simulations}$ in some cases -- with negligible errors.","cs.CE,cs.GR,cs.LG,cs.NA,math.NA"
"Topology Distillation for Recommender System. Recommender Systems (RS) have employed knowledge distillation which is a
model compression technique training a compact student model with the knowledge
transferred from a pre-trained large teacher model. Recent work has shown that
transferring knowledge from the teacher's intermediate layer significantly
improves the recommendation quality of the student. However, they transfer the
knowledge of individual representation point-wise and thus have a limitation in
that primary information of RS lies in the relations in the representation
space. This paper proposes a new topology distillation approach that guides the
student by transferring the topological structure built upon the relations in
the teacher space. We first observe that simply making the student learn the
whole topological structure is not always effective and even degrades the
student's performance. We demonstrate that because the capacity of the student
is highly limited compared to that of the teacher, learning the whole
topological structure is daunting for the student. To address this issue, we
propose a novel method named Hierarchical Topology Distillation (HTD) which
distills the topology hierarchically to cope with the large capacity gap. Our
extensive experiments on real-world datasets show that the proposed method
significantly outperforms the state-of-the-art competitors. We also provide
in-depth analyses to ascertain the benefit of distilling the topology for RS.","cs.IR,cs.LG"
"Picket: Guarding Against Corrupted Data in Tabular Data during Learning and Inference. Data corruption is an impediment to modern machine learning deployments.
Corrupted data can severely bias the learned model and can also lead to invalid
inferences. We present, Picket, a simple framework to safeguard against data
corruptions during both training and deployment of machine learning models over
tabular data. For the training stage, Picket identifies and removes corrupted
data points from the training data to avoid obtaining a biased model. For the
deployment stage, Picket flags, in an online manner, corrupted query points to
a trained machine learning model that due to noise will result in incorrect
predictions. To detect corrupted data, Picket uses a self-supervised deep
learning model for mixed-type tabular data, which we call PicketNet. To
minimize the burden of deployment, learning a PicketNet model does not require
any human-labeled data. Picket is designed as a plugin that can increase the
robustness of any machine learning pipeline. We evaluate Picket on a diverse
array of real-world data considering different corruption models that include
systematic and adversarial noise during both training and testing. We show that
Picket consistently safeguards against corrupted data during both training and
deployment of various models ranging from SVMs to neural networks, beating a
diverse array of competing methods that span from data quality validation
models to robust outlier-detection models.","68-04,68T05,68U35,H.2.8,cs.LG,stat.ML"
"Systematic Analysis of Image Generation using GANs. Generative Adversarial Networks have been crucial in the developments made in
unsupervised learning in recent times. Exemplars of image synthesis from text
or other images, these networks have shown remarkable improvements over
conventional methods in terms of performance. Trained on the adversarial
training philosophy, these networks aim to estimate the potential distribution
from the real data and then use this as input to generate the synthetic data.
Based on this fundamental principle, several frameworks can be generated that
are paragon implementations in several real-life applications such as art
synthesis, generation of high resolution outputs and synthesis of images from
human drawn sketches, to name a few. While theoretically GANs present better
results and prove to be an improvement over conventional methods in many
factors, the implementation of these frameworks for dedicated applications
remains a challenge. This study explores and presents a taxonomy of these
frameworks and their use in various image to image synthesis and text to image
synthesis applications. The basic GANs, as well as a variety of different niche
frameworks, are critically analyzed. The advantages of GANs for image
generation over conventional methods as well their disadvantages amongst other
frameworks are presented. The future applications of GANs in industries such as
healthcare, art and entertainment are also discussed.","cs.CV,cs.LG,eess.IV,stat.ML"
"Joint Characterization of Multiscale Information in High Dimensional Data. High dimensional data can contain multiple scales of variance. Analysis tools
that preferentially operate at one scale can be ineffective at capturing all
the information present in this cross-scale complexity. We propose a multiscale
joint characterization approach designed to exploit synergies between global
and local approaches to dimensionality reduction. We illustrate this approach
using Principal Components Analysis (PCA) to characterize global variance
structure and t-stochastic neighbor embedding (t-sne) to characterize local
variance structure. Using both synthetic images and real-world imaging
spectroscopy data, we show that joint characterization is capable of detecting
and isolating signals which are not evident from either PCA or t-sne alone.
Broadly, t-sne is effective at rendering a randomly oriented low-dimensional
map of local clusters, and PCA renders this map interpretable by providing
global, physically meaningful structure. This approach is illustrated using
imaging spectroscopy data, and may prove particularly useful for other
geospatial data given robust local variance structure due to spatial
autocorrelation and physical interpretability of global variance structure due
to spectral properties of Earth surface materials. However, the fundamental
premise could easily be extended to other high dimensional datasets, including
image time series and non-image data.","physics.data-an,physics.geo-ph,stat.ML"
"Long-Range Modeling of Source Code Files with eWASH: Extended Window Access by Syntax Hierarchy. Statistical language modeling and translation with transformers have found
many successful applications in program understanding and generation tasks,
setting high benchmarks for tools in modern software development environments.
The finite context window of these neural models means, however, that they will
be unable to leverage the entire relevant context of large files and packages
for any given task. While there are many efforts to extend the context window,
we introduce an architecture-independent approach for leveraging the syntactic
hierarchies of source code for incorporating entire file-level context into a
fixed-length window. Using concrete syntax trees of each source file we extract
syntactic hierarchies and integrate them into context window by selectively
removing from view more specific, less relevant scopes for a given task. We
evaluate this approach on code generation tasks and joint translation of
natural language and source code in Python programming language, achieving a
new state-of-the-art in code completion and summarization for Python in the
CodeXGLUE benchmark. We also introduce new CodeXGLUE benchmarks for
user-experience-motivated tasks: code completion with normalized literals,
method body completion/code summarization conditioned on file-level context.","cs.LG,cs.SE"
"UI-Net: Interactive Artificial Neural Networks for Iterative Image Segmentation Based on a User Model. For complex segmentation tasks, fully automatic systems are inherently
limited in their achievable accuracy for extracting relevant objects.
Especially in cases where only few data sets need to be processed for a highly
accurate result, semi-automatic segmentation techniques exhibit a clear benefit
for the user. One area of application is medical image processing during an
intervention for a single patient. We propose a learning-based cooperative
segmentation approach which includes the computing entity as well as the user
into the task. Our system builds upon a state-of-the-art fully convolutional
artificial neural network (FCN) as well as an active user model for training.
During the segmentation process, a user of the trained system can iteratively
add additional hints in form of pictorial scribbles as seed points into the FCN
system to achieve an interactive and precise segmentation result. The
segmentation quality of interactive FCNs is evaluated. Iterative FCN approaches
can yield superior results compared to networks without the user input channel
component, due to a consistent improvement in segmentation quality after each
interaction.","68T05,68T45,I.2.6; I.4.6; I.5.5,cs.AI,cs.CV,cs.LG,cs.NE"
"Multiple Metric Learning for Structured Data. We address the problem of merging graph and feature-space information while
learning a metric from structured data. Existing algorithms tackle the problem
in an asymmetric way, by either extracting vectorized summaries of the graph
structure or adding hard constraints to feature-space algorithms. Following a
different path, we define a metric regression scheme where we train
metric-constrained linear combinations of dissimilarity matrices. The idea is
that the input matrices can be pre-computed dissimilarity measures obtained
from any kind of available data (e.g. node attributes or edge structure). As
the model inputs are distance measures, we do not need to assume the existence
of any underlying feature space. Main challenge is that metric constraints
(especially positive-definiteness and sub-additivity), are not automatically
respected if, for example, the coefficients of the linear combination are
allowed to be negative. Both positive and sub-additive constraints are linear
inequalities, but the computational complexity of imposing them scales as
O(D3), where D is the size of the input matrices (i.e. the size of the data
set). This becomes quickly prohibitive, even when D is relatively small. We
propose a new graph-based technique for optimizing under such constraints and
show that, in some cases, our approach may reduce the original computational
complexity of the optimization process by one order of magnitude. Contrarily to
existing methods, our scheme applies to any (possibly non-convex)
metric-constrained objective function.","cs.LG,math.MG,stat.AP,stat.ML"
"Optimal oracle inequalities for solving projected fixed-point equations. Linear fixed point equations in Hilbert spaces arise in a variety of
settings, including reinforcement learning, and computational methods for
solving differential and integral equations. We study methods that use a
collection of random observations to compute approximate solutions by searching
over a known low-dimensional subspace of the Hilbert space. First, we prove an
instance-dependent upper bound on the mean-squared error for a linear
stochastic approximation scheme that exploits Polyak--Ruppert averaging. This
bound consists of two terms: an approximation error term with an
instance-dependent approximation factor, and a statistical error term that
captures the instance-specific complexity of the noise when projected onto the
low-dimensional subspace. Using information theoretic methods, we also
establish lower bounds showing that both of these terms cannot be improved,
again in an instance-dependent sense. A concrete consequence of our
characterization is that the optimal approximation factor in this problem can
be much larger than a universal constant. We show how our results precisely
characterize the error of a class of temporal difference learning methods for
the policy evaluation problem with linear function approximation, establishing
their optimality.","cs.LG,math.OC,math.ST,stat.ML,stat.TH"
"Fast image segmentation and restoration using parametric curve evolution with junctions and topology changes. Curve evolution schemes for image segmentation based on a region based
contour model allowing for junctions, vector-valued images and topology changes
are introduced. Together with an a posteriori denoising in the segmented
homogeneous regions this leads to a fast and efficient method for image
segmentation and restoration. An uneven spread of mesh points is avoided by
using the tangential degrees of freedom. Several numerical simulations on
artificial test problems and on real images illustrate the performance of the
method.","35K55,49Q10,65K10,68U10,94A08,cs.CV,math.AP,math.NA"
"3D tissue reconstruction with Kinect to evaluate neck lymphedema. Lymphedema is a condition of localized tissue swelling caused by a damaged
lymphatic system. Therapy to these tissues is applied manually. Some of the
methods are lymph drainage, compression therapy or bandaging. However, the
therapy methods are still insufficiently evaluated. Especially, because of not
having a reliable method to measure the change of such a soft and flexible
tissue. In this research, our goal has been providing a 3d computer vision
based method to measure the changes of the neck tissues. To do so, we used
Kinect as a depth sensor and built our algorithms for the point cloud data
acquired from this sensor. The resulting 3D models of the patient necks are
used for comparing the models in time and measuring the volumetric changes
accurately. Our discussions with the medical doctors validate that, when used
in practice this approach would be able to give better indication on which
therapy method is helping and how the tissue is changing in time.","68T45,cs.CG,cs.CV"
"Are scene graphs good enough to improve Image Captioning?. Many top-performing image captioning models rely solely on object features
computed with an object detection model to generate image descriptions.
However, recent studies propose to directly use scene graphs to introduce
information about object relations into captioning, hoping to better describe
interactions between objects. In this work, we thoroughly investigate the use
of scene graphs in image captioning. We empirically study whether using
additional scene graph encoders can lead to better image descriptions and
propose a conditional graph attention network (C-GAT), where the image
captioning decoder state is used to condition the graph updates. Finally, we
determine to what extent noise in the predicted scene graphs influence caption
quality. Overall, we find no significant difference between models that use
scene graph features and models that only use object detection features across
different captioning metrics, which suggests that existing scene graph
generation models are still too noisy to be useful in image captioning.
Moreover, although the quality of predicted scene graphs is very low in
general, when using high quality scene graphs we obtain gains of up to 3.3
CIDEr compared to a strong Bottom-Up Top-Down baseline. We open source code to
reproduce all our experiments in
https://github.com/iacercalixto/butd-image-captioning.","68T45,68T50,I.2.7; I.2.10,cs.CL,cs.CV"
"Jane Jacobs in the Sky: Predicting Urban Vitality with Open Satellite Data. The presence of people in an urban area throughout the day -- often called
'urban vitality' -- is one of the qualities world-class cities aspire to the
most, yet it is one of the hardest to achieve. Back in the 1970s, Jane Jacobs
theorized urban vitality and found that there are four conditions required for
the promotion of life in cities: diversity of land use, small block sizes, the
mix of economic activities, and concentration of people. To build proxies for
those four conditions and ultimately test Jane Jacobs's theory at scale,
researchers have had to collect both private and public data from a variety of
sources, and that took decades. Here we propose the use of one single source of
data, which happens to be publicly available: Sentinel-2 satellite imagery. In
particular, since the first two conditions (diversity of land use and small
block sizes) are visible to the naked eye from satellite imagery, we tested
whether we could automatically extract them with a state-of-the-art
deep-learning framework and whether, in the end, the extracted features could
predict vitality. In six Italian cities for which we had call data records, we
found that our framework is able to explain on average 55% of the variance in
urban vitality extracted from those records.","cs.CV,cs.CY,eess.IV"
"Fleet Control using Coregionalized Gaussian Process Policy Iteration. In many settings, as for example wind farms, multiple machines are
instantiated to perform the same task, which is called a fleet. The recent
advances with respect to the Internet of Things allow control devices and/or
machines to connect through cloud-based architectures in order to share
information about their status and environment. Such an infrastructure allows
seamless data sharing between fleet members, which could greatly improve the
sample-efficiency of reinforcement learning techniques. However in practice,
these machines, while almost identical in design, have small discrepancies due
to production errors or degradation, preventing control algorithms to simply
aggregate and employ all fleet data. We propose a novel reinforcement learning
method that learns to transfer knowledge between similar fleet members and
creates member-specific dynamics models for control. Our algorithm uses
Gaussian processes to establish cross-member covariances. This is significantly
different from standard transfer learning methods, as the focus is not on
sharing information over tasks, but rather over system specifications. We
demonstrate our approach on two benchmarks and a realistic wind farm setting.
Our method significantly outperforms two baseline approaches, namely individual
learning and joint learning where all samples are aggregated, in terms of the
median and variance of the results.","cs.AI,cs.LG,cs.SY,eess.SY,stat.ML"
"Distortion-aware Monocular Depth Estimation for Omnidirectional Images. A main challenge for tasks on panorama lies in the distortion of objects
among images. In this work, we propose a Distortion-Aware Monocular
Omnidirectional (DAMO) dense depth estimation network to address this challenge
on indoor panoramas with two steps. First, we introduce a distortion-aware
module to extract calibrated semantic features from omnidirectional images.
Specifically, we exploit deformable convolution to adjust its sampling grids to
geometric variations of distorted objects on panoramas and then utilize a strip
pooling module to sample against horizontal distortion introduced by inverse
gnomonic projection. Second, we further introduce a plug-and-play
spherical-aware weight matrix for our objective function to handle the uneven
distribution of areas projected from a sphere. Experiments on the 360D dataset
show that the proposed method can effectively extract semantic features from
distorted panoramas and alleviate the supervision bias caused by distortion. It
achieves state-of-the-art performance on the 360D dataset with high efficiency.","cs.CV,cs.LG,eess.IV"
"A New Distributed Method for Training Generative Adversarial Networks. Generative adversarial networks (GANs) are emerging machine learning models
for generating synthesized data similar to real data by jointly training a
generator and a discriminator. In many applications, data and computational
resources are distributed over many devices, so centralized computation with
all data in one location is infeasible due to privacy and/or communication
constraints. This paper proposes a new framework for training GANs in a
distributed fashion: Each device computes a local discriminator using local
data; a single server aggregates their results and computes a global GAN.
Specifically, in each iteration, the server sends the global GAN to the
devices, which then update their local discriminators; the devices send their
results to the server, which then computes their average as the global
discriminator and updates the global generator accordingly. Two different
update schedules are designed with different levels of parallelism between the
devices and the server. Numerical results obtained using three popular datasets
demonstrate that the proposed framework can outperform a state-of-the-art
framework in terms of convergence speed.","cs.DC,cs.IT,cs.LG,cs.NI,math.IT"
"Non-technical Loss Detection with Statistical Profile Images Based on Semi-supervised Learning. In order to keep track of the operational state of power grid, the world's
largest sensor systems, smart grid, was built by deploying hundreds of millions
of smart meters. Such system makes it possible to discover and make quick
response to any hidden threat to the entire power grid. Non-technical losses
(NTLs) have always been a major concern for its consequent security risks as
well as immeasurable revenue loss. However, various causes of NTL may have
different characteristics reflected in the data. Accurately capturing these
anomalies faced with such large scale of collected data records is rather
tricky as a result. In this paper, we proposed a new methodology of detecting
abnormal electricity consumptions. We did a transformation of the collected
time-series data which turns it into an image representation that could well
reflect users' relatively long term consumption behaviors. Inspired by the
excellent neural network architecture used for objective detection in computer
vision domain, we designed our deep learning model that takes the transformed
images as input and yields joint featured inferred from the multiple aspects
the input provides. Considering the limited labeled samples, especially the
abnormal ones, we used our model in a semi-supervised fashion that is brought
out in recent years. The model is tested on samples which are verified by
on-field inspections and our method showed significant improvement.","cs.LG,stat.ML"
"Deep Learning for Molecular Graphs with Tiered Graph Autoencoders and Graph Prediction. Tiered graph autoencoders provide the architecture and mechanisms for
learning tiered latent representations and latent spaces for molecular graphs
that explicitly represent and utilize groups (e.g., functional groups). This
enables the utilization and exploration of tiered molecular latent spaces,
either individually - the node (atom) tier, the group tier, or the graph
(molecule) tier - or jointly, as well as navigation across the tiers. In this
paper, we discuss the use of tiered graph autoencoders together with graph
prediction for molecular graphs. We show features of molecular graphs used, and
groups in molecular graphs identified for some sample molecules. We briefly
review graph prediction and the QM9 dataset for background information, and
discuss the use of tiered graph embeddings for graph prediction, particularly
weighted group pooling. We find that functional groups and ring groups
effectively capture and represent the chemical essence of molecular graphs
(structures). Further, tiered graph autoencoders and graph prediction together
provide effective, efficient and interpretable deep learning for molecular
graphs, with the former providing unsupervised, transferable learning and the
latter providing supervised, task-optimized learning.","cs.LG,q-bio.BM"
"Learning 2D Temporal Adjacent Networks for Moment Localization with Natural Language. We address the problem of retrieving a specific moment from an untrimmed
video by a query sentence. This is a challenging problem because a target
moment may take place in relations to other temporal moments in the untrimmed
video. Existing methods cannot tackle this challenge well since they consider
temporal moments individually and neglect the temporal dependencies. In this
paper, we model the temporal relations between video moments by a
two-dimensional map, where one dimension indicates the starting time of a
moment and the other indicates the end time. This 2D temporal map can cover
diverse video moments with different lengths, while representing their adjacent
relations. Based on the 2D map, we propose a Temporal Adjacent Network
(2D-TAN), a single-shot framework for moment localization. It is capable of
encoding the adjacent temporal relation, while learning discriminative features
for matching video moments with referring expressions. We evaluate the proposed
2D-TAN on three challenging benchmarks, i.e., Charades-STA, ActivityNet
Captions, and TACoS, where our 2D-TAN outperforms the state-of-the-art.","cs.CV,cs.IR,cs.MM"
"Rethinking ResNets: Improved Stacking Strategies With High Order Schemes. Various deep neural network architectures (DNNs) maintain massive vital
records in computer vision. While drawing attention worldwide, the design of
the overall structure lacks general guidance. Based on the relationship between
DNN design and numerical differential equations, we performed a fair comparison
of the residual design with higher-order perspectives. We show that the widely
used DNN design strategy, constantly stacking a small design (usually 2-3
layers), could be easily improved, supported by solid theoretical knowledge and
with no extra parameters needed. We reorganise the residual design in
higher-order ways, which is inspired by the observation that many effective
networks can be interpreted as different numerical discretisations of
differential equations. The design of ResNet follows a relatively simple
scheme, which is Euler forward; however, the situation becomes complicated
rapidly while stacking. We suppose that stacked ResNet is somehow equalled to a
higher-order scheme; then, the current method of forwarding propagation might
be relatively weak compared with a typical high-order method such as
Runge-Kutta. We propose HO-ResNet to verify the hypothesis of widely used CV
benchmarks with sufficient experiments. Stable and noticeable increases in
performance are observed, and convergence and robustness are also improved. Our
stacking strategy improved ResNet-30 by 2.15 per cent and ResNet-58 by 2.35 per
cent on CIFAR-10, with the same settings and parameters. The proposed strategy
is fundamental and theoretical and can therefore be applied to any network as a
general guideline.","68T07,cs.CV,cs.LG"
"Color Image Segmentation using Adaptive Particle Swarm Optimization and Fuzzy C-means. Segmentation partitions an image into different regions containing pixels
with similar attributes. A standard non-contextual variant of Fuzzy C-means
clustering algorithm (FCM), considering its simplicity is generally used in
image segmentation. Using FCM has its disadvantages like it is dependent on the
initial guess of the number of clusters and highly sensitive to noise.
Satisfactory visual segments cannot be obtained using FCM. Particle Swarm
Optimization (PSO) belongs to the class of evolutionary algorithms and has good
convergence speed and fewer parameters compared to Genetic Algorithms (GAs). An
optimized version of PSO can be combined with FCM to act as a proper
initializer for the algorithm thereby reducing its sensitivity to initial
guess. A hybrid PSO algorithm named Adaptive Particle Swarm Optimization (APSO)
which improves in the calculation of various hyper parameters like inertia
weight, learning factors over standard PSO, using insights from swarm
behaviour, leading to improvement in cluster quality can be used. This paper
presents a new image segmentation algorithm called Adaptive Particle Swarm
Optimization and Fuzzy C-means Clustering Algorithm (APSOF), which is based on
Adaptive Particle Swarm Optimization (APSO) and Fuzzy C-means clustering.
Experimental results show that APSOF algorithm has edge over FCM in correctly
identifying the optimum cluster centers, there by leading to accurate
classification of the image pixels. Hence, APSOF algorithm has superior
performance in comparison with classic Particle Swarm Optimization (PSO) and
Fuzzy C-means clustering algorithm (FCM) for image segmentation.","68U10,I.4.6,cs.CV"
"A non-autonomous equation discovery method for time signal classification. Certain neural network architectures, in the infinite-layer limit, lead to
systems of nonlinear differential equations. Motivated by this idea, we develop
a framework for analyzing time signals based on non-autonomous dynamical
equations. We view the time signal as a forcing function for a dynamical system
that governs a time-evolving hidden variable. As in equation discovery, the
dynamical system is represented using a dictionary of functions and the
coefficients are learned from data. This framework is applied to the time
signal classification problem. We show how gradients can be efficiently
computed using the adjoint method, and we apply methods from dynamical systems
to establish stability of the classifier. Through a variety of experiments, on
both synthetic and real datasets, we show that the proposed method uses orders
of magnitude fewer parameters than competing methods, while achieving
comparable accuracy. We created the synthetic datasets using dynamical systems
of increasing complexity; though the ground truth vector fields are often
polynomials, we find consistently that a Fourier dictionary yields the best
results. We also demonstrate how the proposed method yields graphical
interpretability in the form of phase portraits.","34H05,62L10,68T07,cs.LG,math.DS,math.OC,stat.ML"
"Ising Model Selection Using $\ell_{1}$-Regularized Linear Regression. We theoretically investigate the performance of $\ell_{1}$-regularized linear
regression ($\ell_1$-LinR) for the problem of Ising model selection using the
replica method from statistical mechanics. The regular random graph is
considered under paramagnetic assumption. Our results show that despite model
misspecification, the $\ell_1$-LinR estimator can successfully recover the
graph structure of the Ising model with $N$ variables using
$M=\mathcal{O}\left(\log N\right)$ samples, which is of the same order as that
of $\ell_{1}$-regularized logistic regression. Moreover, we provide a
computationally efficient method to accurately predict the non-asymptotic
performance of the $\ell_1$-LinR estimator with moderate $M$ and $N$.
Simulations show an excellent agreement between theoretical predictions and
experimental results, which supports our findings.","cond-mat.dis-nn,cs.AI,cs.LG"
"Learning in Implicit Generative Models. Generative adversarial networks (GANs) provide an algorithmic framework for
constructing generative models with several appealing properties: they do not
require a likelihood function to be specified, only a generating procedure;
they provide samples that are sharp and compelling; and they allow us to
harness our knowledge of building highly accurate neural network classifiers.
Here, we develop our understanding of GANs with the aim of forming a rich view
of this growing area of machine learning---to build connections to the diverse
set of statistical thinking on this topic, of which much can be gained by a
mutual exchange of ideas. We frame GANs within the wider landscape of
algorithms for learning in implicit generative models--models that only specify
a stochastic procedure with which to generate data--and relate these ideas to
modelling problems in related fields, such as econometrics and approximate
Bayesian computation. We develop likelihood-free inference methods and
highlight hypothesis testing as a principle for learning in implicit generative
models, using which we are able to derive the objective function used by GANs,
and many other related objectives. The testing viewpoint directs our focus to
the general problem of density ratio estimation. There are four approaches for
density ratio estimation, one of which is a solution using classifiers to
distinguish real from generated data. Other approaches such as divergence
minimisation and moment matching have also been explored in the GAN literature,
and we synthesise these views to form an understanding in terms of the
relationships between them and the wider literature, highlighting avenues for
future exploration and cross-pollination.","cs.LG,stat.CO,stat.ML"
"UV-Net: Learning from Boundary Representations. We introduce UV-Net, a novel neural network architecture and representation
designed to operate directly on Boundary representation (B-rep) data from 3D
CAD models. The B-rep format is widely used in the design, simulation and
manufacturing industries to enable sophisticated and precise CAD modeling
operations. However, B-rep data presents some unique challenges when used with
modern machine learning due to the complexity of the data structure and its
support for both continuous non-Euclidean geometric entities and discrete
topological entities. In this paper, we propose a unified representation for
B-rep data that exploits the U and V parameter domain of curves and surfaces to
model geometry, and an adjacency graph to explicitly model topology. This leads
to a unique and efficient network architecture, UV-Net, that couples image and
graph convolutional neural networks in a compute and memory-efficient manner.
To aid in future research we present a synthetic labelled B-rep dataset,
SolidLetters, derived from human designed fonts with variations in both
geometry and topology. Finally we demonstrate that UV-Net can generalize to
supervised and unsupervised tasks on five datasets, while outperforming
alternate 3D shape representations such as point clouds, voxels, and meshes.","68T07,68T10,I.5.1; I.3.5,cs.CV,cs.LG"
"Bayesian semi-supervised learning for uncertainty-calibrated prediction of molecular properties and active learning. Predicting bioactivity and physical properties of small molecules is a
central challenge in drug discovery. Deep learning is becoming the method of
choice but studies to date focus on mean accuracy as the main metric. However,
to replace costly and mission-critical experiments by models, a high mean
accuracy is not enough: Outliers can derail a discovery campaign, thus models
need reliably predict when it will fail, even when the training data is biased;
experiments are expensive, thus models need to be data-efficient and suggest
informative training sets using active learning. We show that uncertainty
quantification and active learning can be achieved by Bayesian semi-supervised
graph convolutional neural networks. The Bayesian approach estimates
uncertainty in a statistically principled way through sampling from the
posterior distribution. Semi-supervised learning disentangles representation
learning and regression, keeping uncertainty estimates accurate in the low data
limit and allowing the model to start active learning from a small initial pool
of training data. Our study highlights the promise of Bayesian deep learning
for chemistry.","cs.LG,physics.chem-ph,stat.ML"
"Sparsifying the Update Step in Graph Neural Networks. Message-Passing Neural Networks (MPNNs), the most prominent Graph Neural
Network (GNN) framework, celebrate much success in the analysis of
graph-structured data. Concurrently, the sparsification of Neural Network
models attracts a great amount of academic and industrial interest. In this
paper, we conduct a structured study of the effect of sparsification on the
trainable part of MPNNs known as the Update step. To this end, we design a
series of models to successively sparsify the linear transform in the Update
step. Specifically, we propose the ExpanderGNN model with a tuneable
sparsification rate and the Activation-Only GNN, which has no linear transform
in the Update step. In agreement with a growing trend in the literature, the
sparsification paradigm is changed by initialising sparse neural network
architectures rather than expensively sparsifying already trained
architectures. Our novel benchmark models enable a better understanding of the
influence of the Update step on model performance and outperform existing
simplified benchmark models such as the Simple Graph Convolution. The
ExpanderGNNs, and in some cases the Activation-Only models, achieve performance
on par with their vanilla counterparts on several downstream tasks while
containing significantly fewer trainable parameters. In experiments with
matching parameter numbers, our benchmark models outperform the
state-of-the-art GNN models. Our code is publicly available at:
https://github.com/ChangminWu/ExpanderGNN.","cs.AI,cs.LG,cs.SI"
"Graphical Models: An Extension to Random Graphs, Trees, and Other Objects. In this work, we consider an extension of graphical models to random graphs,
trees, and other objects. To do this, many fundamental concepts for
multivariate random variables (e.g., marginal variables, Gibbs distribution,
Markov properties) must be extended to other mathematical objects; it turns out
that this extension is possible, as we will discuss, if we have a consistent,
complete system of projections on a given object. Each projection defines a
marginal random variable, allowing one to specify independence assumptions
between them. Furthermore, these independencies can be specified in terms of a
small subset of these marginal variables (which we call the atomic variables),
allowing the compact representation of independencies by a directed graph.
Projections also define factors, functions on the projected object space, and
hence a projection family defines a set of possible factorizations for a
distribution; these can be compactly represented by an undirected graph.
  The invariances used in graphical models are essential for learning
distributions, not just on multivariate random variables, but also on other
objects. When they are applied to random graphs and random trees, the result is
a general class of models that is applicable to a broad range of problems,
including those in which the graphs and trees have complicated edge structures.
These models need not be conditioned on a fixed number of vertices, as is often
the case in the literature for random graphs, and can be used for problems in
which attributes are associated with vertices and edges. For graphs,
applications include the modeling of molecules, neural networks, and relational
real-world scenes; for trees, applications include the modeling of infectious
diseases, cell fusion, the structure of language, and the structure of objects
in visual scenes. Many classic models are particular instances of this
framework.","cs.AI,cs.SI,stat.ML"
"Tightening Exploration in Upper Confidence Reinforcement Learning. The upper confidence reinforcement learning (UCRL2) algorithm introduced in
(Jaksch et al., 2010) is a popular method to perform regret minimization in
unknown discrete Markov Decision Processes under the average-reward criterion.
Despite its nice and generic theoretical regret guarantees, this algorithm and
its variants have remained until now mostly theoretical as numerical
experiments in simple environments exhibit long burn-in phases before the
learning takes place. In pursuit of practical efficiency, we present UCRL3,
following the lines of UCRL2, but with two key modifications: First, it uses
state-of-the-art time-uniform concentration inequalities to compute confidence
sets on the reward and (component-wise) transition distributions for each
state-action pair. Furthermore, to tighten exploration, it uses an adaptive
computation of the support of each transition distribution, which in turn
enables us to revisit the extended value iteration procedure of UCRL2 to
optimize over distributions with reduced support by disregarding low
probability transitions, while still ensuring near-optimism. We demonstrate,
through numerical experiments in standard environments, that reducing
exploration this way yields a substantial numerical improvement compared to
UCRL2 and its variants. On the theoretical side, these key modifications enable
us to derive a regret bound for UCRL3 improving on UCRL2, that for the first
time makes appear notions of local diameter and local effective support, thanks
to variance-aware concentration bounds.","cs.LG,cs.SY,eess.SY,stat.ML"
"What's in the box? Explaining the black-box model through an evaluation of its interpretable features. Algorithms are powerful and necessary tools behind a large part of the
information we use every day. However, they may introduce new sources of bias,
discrimination and other unfair practices that affect people who are unaware of
it. Greater algorithm transparency is indispensable to provide more credible
and reliable services. Moreover, requiring developers to design transparent
algorithm-driven applications allows them to keep the model accessible and
human understandable, increasing the trust of end users. In this paper we
present EBAnO, a new engine able to produce prediction-local explanations for a
black-box model exploiting interpretable feature perturbations. EBAnO exploits
the hypercolumns representation together with the cluster analysis to identify
a set of interpretable features of images. Furthermore two indices have been
proposed to measure the influence of input features on the final prediction
made by a CNN model. EBAnO has been preliminarily tested on a set of
heterogeneous images. The results highlight the effectiveness of EBAnO in
explaining the CNN classification through the evaluation of interpretable
features influence.","68T30,68U10,I.2,cs.AI,cs.CV,cs.LG"
"Adaptive Neural Message Passing for Inductive Learning on Hypergraphs. Graphs are the most ubiquitous data structures for representing relational
datasets and performing inferences in them. They model, however, only pairwise
relations between nodes and are not designed for encoding the higher-order
relations. This drawback is mitigated by hypergraphs, in which an edge can
connect an arbitrary number of nodes. Most hypergraph learning approaches
convert the hypergraph structure to that of a graph and then deploy existing
geometric deep learning methods. This transformation leads to information loss,
and sub-optimal exploitation of the hypergraph's expressive power. We present
HyperMSG, a novel hypergraph learning framework that uses a modular two-level
neural message passing strategy to accurately and efficiently propagate
information within each hyperedge and across the hyperedges. HyperMSG adapts to
the data and task by learning an attention weight associated with each node's
degree centrality. Such a mechanism quantifies both local and global importance
of a node, capturing the structural properties of a hypergraph. HyperMSG is
inductive, allowing inference on previously unseen nodes. Further, it is robust
and outperforms state-of-the-art hypergraph learning methods on a wide range of
tasks and datasets. Finally, we demonstrate the effectiveness of HyperMSG in
learning multimodal relations through detailed experimentation on a challenging
multimedia dataset.","cs.LG,cs.MM"
"GAN-powered Deep Distributional Reinforcement Learning for Resource Management in Network Slicing. Network slicing is a key technology in 5G communications system. Its purpose
is to dynamically and efficiently allocate resources for diversified services
with distinct requirements over a common underlying physical infrastructure.
Therein, demand-aware resource allocation is of significant importance to
network slicing. In this paper, we consider a scenario that contains several
slices in a radio access network with base stations that share the same
physical resources (e.g., bandwidth or slots). We leverage deep reinforcement
learning (DRL) to solve this problem by considering the varying service demands
as the environment state and the allocated resources as the environment action.
In order to reduce the effects of the annoying randomness and noise embedded in
the received service level agreement (SLA) satisfaction ratio (SSR) and
spectrum efficiency (SE), we primarily propose generative adversarial
network-powered deep distributional Q network (GAN-DDQN) to learn the
action-value distribution driven by minimizing the discrepancy between the
estimated action-value distribution and the target action-value distribution.
We put forward a reward-clipping mechanism to stabilize GAN-DDQN training
against the effects of widely-spanning utility values. Moreover, we further
develop Dueling GAN-DDQN, which uses a specially designed dueling generator, to
learn the action-value distribution by estimating the state-value distribution
and the action advantage function. Finally, we verify the performance of the
proposed GAN-DDQN and Dueling GAN-DDQN algorithms through extensive
simulations.","cs.AI,cs.LG,cs.NI,stat.ML"
"Fashion Editing with Adversarial Parsing Learning. Interactive fashion image manipulation, which enables users to edit images
with sketches and color strokes, is an interesting research problem with great
application value. Existing works often treat it as a general inpainting task
and do not fully leverage the semantic structural information in fashion
images. Moreover, they directly utilize conventional convolution and
normalization layers to restore the incomplete image, which tends to wash away
the sketch and color information. In this paper, we propose a novel Fashion
Editing Generative Adversarial Network (FE-GAN), which is capable of
manipulating fashion images by free-form sketches and sparse color strokes.
FE-GAN consists of two modules: 1) a free-form parsing network that learns to
control the human parsing generation by manipulating sketch and color; 2) a
parsing-aware inpainting network that renders detailed textures with semantic
guidance from the human parsing map. A new attention normalization layer is
further applied at multiple scales in the decoder of the inpainting network to
enhance the quality of the synthesized image. Extensive experiments on
high-resolution fashion image datasets demonstrate that the proposed method
significantly outperforms the state-of-the-art methods on image manipulation.","cs.CV,eess.IV"
"Knowledge extraction from the learning of sequences in a long short term memory (LSTM) architecture. We introduce a general method to extract knowledge from a recurrent neural
network (Long Short Term Memory) that has learnt to detect if a given input
sequence is valid or not, according to an unknown generative automaton. Based
on the clustering of the hidden states, we explain how to build and validate an
automaton that corresponds to the underlying (unknown) automaton, and allows to
predict if a given sequence is valid or not. The method is illustrated on
artificial grammars (Reber's grammar variations) as well as on a real use-case
whose underlying grammar is unknown.","cs.LG,cs.NE,stat.ML"
"Prescribed Generative Adversarial Networks. Generative adversarial networks (GANs) are a powerful approach to
unsupervised learning. They have achieved state-of-the-art performance in the
image domain. However, GANs are limited in two ways. They often learn
distributions with low support---a phenomenon known as mode collapse---and they
do not guarantee the existence of a probability density, which makes evaluating
generalization using predictive log-likelihood impossible. In this paper, we
develop the prescribed GAN (PresGAN) to address these shortcomings. PresGANs
add noise to the output of a density network and optimize an
entropy-regularized adversarial loss. The added noise renders tractable
approximations of the predictive log-likelihood and stabilizes the training
procedure. The entropy regularizer encourages PresGANs to capture all the modes
of the data distribution. Fitting PresGANs involves computing the intractable
gradients of the entropy regularization term; PresGANs sidestep this
intractability using unbiased stochastic estimates. We evaluate PresGANs on
several datasets and found they mitigate mode collapse and generate samples
with high perceptual quality. We further found that PresGANs reduce the gap in
performance in terms of predictive log-likelihood between traditional GANs and
variational autoencoders (VAEs).","cs.LG,stat.ME,stat.ML"
"Credit Assignment with Meta-Policy Gradient for Multi-Agent Reinforcement Learning. Reward decomposition is a critical problem in centralized training with
decentralized execution~(CTDE) paradigm for multi-agent reinforcement learning.
To take full advantage of global information, which exploits the states from
all agents and the related environment for decomposing Q values into individual
credits, we propose a general meta-learning-based Mixing Network with Meta
Policy Gradient~(MNMPG) framework to distill the global hierarchy for delicate
reward decomposition. The excitation signal for learning global hierarchy is
deduced from the episode reward difference between before and after ""exercise
updates"" through the utility network. Our method is generally applicable to the
CTDE method using a monotonic mixing network. Experiments on the StarCraft II
micromanagement benchmark demonstrate that our method just with a simple
utility network is able to outperform the current state-of-the-art MARL
algorithms on 4 of 5 super hard scenarios. Better performance can be further
achieved when combined with a role-based utility network.","cs.AI,cs.LG,cs.MA"
"Inference in Graphical Models via Semidefinite Programming Hierarchies. Maximum A posteriori Probability (MAP) inference in graphical models amounts
to solving a graph-structured combinatorial optimization problem. Popular
inference algorithms such as belief propagation (BP) and generalized belief
propagation (GBP) are intimately related to linear programming (LP) relaxation
within the Sherali-Adams hierarchy. Despite the popularity of these algorithms,
it is well understood that the Sum-of-Squares (SOS) hierarchy based on
semidefinite programming (SDP) can provide superior guarantees. Unfortunately,
SOS relaxations for a graph with $n$ vertices require solving an SDP with
$n^{\Theta(d)}$ variables where $d$ is the degree in the hierarchy. In
practice, for $d\ge 4$, this approach does not scale beyond a few tens of
variables. In this paper, we propose binary SDP relaxations for MAP inference
using the SOS hierarchy with two innovations focused on computational
efficiency. Firstly, in analogy to BP and its variants, we only introduce
decision variables corresponding to contiguous regions in the graphical model.
Secondly, we solve the resulting SDP using a non-convex Burer-Monteiro style
method, and develop a sequential rounding procedure. We demonstrate that the
resulting algorithm can solve problems with tens of thousands of variables
within minutes, and outperforms BP and GBP on practical problems such as image
denoising and Ising spin glasses. Finally, for specific graph types, we
establish a sufficient condition for the tightness of the proposed partial SOS
relaxation.","cs.DS,stat.ML"
"Estimating Blink Probability for Highlight Detection in Figure Skating Videos. Highlight detection in sports videos has a broad viewership and huge
commercial potential. It is thus imperative to detect highlight scenes more
suitably for human interest with high temporal accuracy. Since people
instinctively suppress blinks during attention-grabbing events and
synchronously generate blinks at attention break points in videos, the
instantaneous blink rate can be utilized as a highly accurate temporal
indicator of human interest. Therefore, in this study, we propose a novel,
automatic highlight detection method based on the blink rate. The method trains
a one-dimensional convolution network (1D-CNN) to assess blink rates at each
video frame from the spatio-temporal pose features of figure skating videos.
Experiments show that the method successfully estimates the blink rate in 94%
of the video clips and predicts the temporal change in the blink rate around a
jump event with high accuracy. Moreover, the method detects not only the
representative athletic action, but also the distinctive artistic expression of
figure skating performance as key frames. This suggests that the
blink-rate-based supervised learning approach enables high-accuracy highlight
detection that more closely matches human sensibility.","cs.CV,cs.MM"
"Tackling the Unannotated: Scene Graph Generation with Bias-Reduced Models. Predicting a scene graph that captures visual entities and their interactions
in an image has been considered a crucial step towards full scene
comprehension. Recent scene graph generation (SGG) models have shown their
capability of capturing the most frequent relations among visual entities.
However, the state-of-the-art results are still far from satisfactory, e.g.
models can obtain 31% in overall recall R@100, whereas the likewise important
mean class-wise recall mR@100 is only around 8% on Visual Genome (VG). The
discrepancy between R and mR results urges to shift the focus from pursuing a
high R to a high mR with a still competitive R. We suspect that the observed
discrepancy stems from both the annotation bias and sparse annotations in VG,
in which many visual entity pairs are either not annotated at all or only with
a single relation when multiple ones could be valid. To address this particular
issue, we propose a novel SGG training scheme that capitalizes on self-learned
knowledge. It involves two relation classifiers, one offering a less biased
setting for the other to base on. The proposed scheme can be applied to most of
the existing SGG models and is straightforward to implement. We observe
significant relative improvements in mR (between +6.6% and +20.4%) and
competitive or better R (between -2.4% and 0.3%) across all standard SGG tasks.","cs.CV,cs.LG"
"Causality based Feature Fusion for Brain Neuro-Developmental Analysis. Human brain development is a complex and dynamic process that is affected by
several factors such as genetics, sex hormones, and environmental changes. A
number of recent studies on brain development have examined functional
connectivity (FC) defined by the temporal correlation between time series of
different brain regions. We propose to add the directional flow of information
during brain maturation. To do so, we extract effective connectivity (EC)
through Granger causality (GC) for two different groups of subjects, i.e.,
children and young adults. The motivation is that the inclusion of causal
interaction may further discriminate brain connections between two age groups
and help to discover new connections between brain regions. The contributions
of this study are threefold. First, there has been a lack of attention to
EC-based feature extraction in the context of brain development. To this end,
we propose a new kernel-based GC (KGC) method to learn nonlinearity of complex
brain network, where a reduced Sine hyperbolic polynomial (RSP) neural network
was used as our proposed learner. Second, we used causality values as the
weight for the directional connectivity between brain regions. Our findings
indicated that the strength of connections was significantly higher in young
adults relative to children. In addition, our new EC-based feature outperformed
FC-based analysis from Philadelphia neurocohort (PNC) study with better
discrimination of the different age groups. Moreover, the fusion of these two
sets of features (FC + EC) improved brain age prediction accuracy by more than
4%, indicating that they should be used together for brain development studies.","cs.AI,cs.CV,q-bio.NC"
"Accelerated WGAN update strategy with loss change rate balancing. Optimizing the discriminator in Generative Adversarial Networks (GANs) to
completion in the inner training loop is computationally prohibitive, and on
finite datasets would result in overfitting. To address this, a common update
strategy is to alternate between k optimization steps for the discriminator D
and one optimization step for the generator G. This strategy is repeated in
various GAN algorithms where k is selected empirically. In this paper, we show
that this update strategy is not optimal in terms of accuracy and convergence
speed, and propose a new update strategy for Wasserstein GANs (WGAN) and other
GANs using the WGAN loss(e.g. WGAN-GP, Deblur GAN, and Super-resolution GAN).
The proposed update strategy is based on a loss change ratio comparison of G
and D. We demonstrate that the proposed strategy improves both convergence
speed and accuracy.","cs.CV,cs.LG,eess.IV"
"HySTER: A Hybrid Spatio-Temporal Event Reasoner. The task of Video Question Answering (VideoQA) consists in answering natural
language questions about a video and serves as a proxy to evaluate the
performance of a model in scene sequence understanding. Most methods designed
for VideoQA up-to-date are end-to-end deep learning architectures which
struggle at complex temporal and causal reasoning and provide limited
transparency in reasoning steps. We present the HySTER: a Hybrid
Spatio-Temporal Event Reasoner to reason over physical events in videos. Our
model leverages the strength of deep learning methods to extract information
from video frames with the reasoning capabilities and explainability of
symbolic artificial intelligence in an answer set programming framework. We
define a method based on general temporal, causal and physics rules which can
be transferred across tasks. We apply our model to the CLEVRER dataset and
demonstrate state-of-the-art results in question answering accuracy. This work
sets the foundations for the incorporation of inductive logic programming in
the field of VideoQA.","cs.AI,cs.CL,cs.CV,cs.LO"
"Color Image Segmentation using Adaptive Particle Swarm Optimization and Fuzzy C-means. Segmentation partitions an image into different regions containing pixels
with similar attributes. A standard non-contextual variant of Fuzzy C-means
clustering algorithm (FCM), considering its simplicity is generally used in
image segmentation. Using FCM has its disadvantages like it is dependent on the
initial guess of the number of clusters and highly sensitive to noise.
Satisfactory visual segments cannot be obtained using FCM. Particle Swarm
Optimization (PSO) belongs to the class of evolutionary algorithms and has good
convergence speed and fewer parameters compared to Genetic Algorithms (GAs). An
optimized version of PSO can be combined with FCM to act as a proper
initializer for the algorithm thereby reducing its sensitivity to initial
guess. A hybrid PSO algorithm named Adaptive Particle Swarm Optimization (APSO)
which improves in the calculation of various hyper parameters like inertia
weight, learning factors over standard PSO, using insights from swarm
behaviour, leading to improvement in cluster quality can be used. This paper
presents a new image segmentation algorithm called Adaptive Particle Swarm
Optimization and Fuzzy C-means Clustering Algorithm (APSOF), which is based on
Adaptive Particle Swarm Optimization (APSO) and Fuzzy C-means clustering.
Experimental results show that APSOF algorithm has edge over FCM in correctly
identifying the optimum cluster centers, there by leading to accurate
classification of the image pixels. Hence, APSOF algorithm has superior
performance in comparison with classic Particle Swarm Optimization (PSO) and
Fuzzy C-means clustering algorithm (FCM) for image segmentation.","68U10,I.4.6,cs.CV"
"Natural data structure extracted from neighborhood-similarity graphs. 'Big' high-dimensional data are commonly analyzed in low-dimensions, after
performing a dimensionality-reduction step that inherently distorts the data
structure. For the same purpose, clustering methods are also often used. These
methods also introduce a bias, either by starting from the assumption of a
particular geometric form of the clusters, or by using iterative schemes to
enhance cluster contours, with uncontrollable consequences. The goal of data
analysis should, however, be to encode and detect structural data features at
all scales and densities simultaneously, without assuming a parametric form of
data point distances, or modifying them. We propose a novel approach that
directly encodes data point neighborhood similarities as a sparse graph. Our
non-iterative framework permits a transparent interpretation of data, without
altering the original data dimension and metric. Several natural and synthetic
data applications demonstrate the efficacy of our novel approach.","cond-mat.dis-nn,cs.CV,cs.LG,stat.ML"
"Point-wise Map Recovery and Refinement from Functional Correspondence. Since their introduction in the shape analysis community, functional maps
have met with considerable success due to their ability to compactly represent
dense correspondences between deformable shapes, with applications ranging from
shape matching and image segmentation, to exploration of large shape
collections. Despite the numerous advantages of such representation, however,
the problem of converting a given functional map back to a point-to-point map
has received a surprisingly limited interest. In this paper we analyze the
general problem of point-wise map recovery from arbitrary functional maps. In
doing so, we rule out many of the assumptions required by the currently
established approach -- most notably, the limiting requirement of the input
shapes being nearly-isometric. We devise an efficient recovery process based on
a simple probabilistic model. Experiments confirm that this approach achieves
remarkable accuracy improvements in very challenging cases.","cs.CG,cs.CV"
"Autoformer: Decomposition Transformers with Auto-Correlation for Long-Term Series Forecasting. Extending the forecasting time is a critical demand for real applications,
such as extreme weather early warning and long-term energy consumption
planning. This paper studies the \textit{long-term forecasting} problem of time
series. Prior Transformer-based models adopt various self-attention mechanisms
to discover the long-range dependencies. However, intricate temporal patterns
of the long-term future prohibit the model from finding reliable dependencies.
Also, Transformers have to adopt the sparse versions of point-wise
self-attentions for long series efficiency, resulting in the information
utilization bottleneck. Towards these challenges, we propose Autoformer as a
novel decomposition architecture with an Auto-Correlation mechanism. We go
beyond the pre-processing convention of series decomposition and renovate it as
a basic inner block of deep models. This design empowers Autoformer with
progressive decomposition capacities for complex time series. Further, inspired
by the stochastic process theory, we design the Auto-Correlation mechanism
based on the series periodicity, which conducts the dependencies discovery and
representation aggregation at the sub-series level. Auto-Correlation
outperforms self-attention in both efficiency and accuracy. In long-term
forecasting, Autoformer yields state-of-the-art accuracy, with a 38% relative
improvement on six benchmarks, covering five practical applications: energy,
traffic, economics, weather and disease.","cs.AI,cs.LG"
"Attention Forcing for Sequence-to-sequence Model Training. Auto-regressive sequence-to-sequence models with attention mechanism have
achieved state-of-the-art performance in many tasks such as machine translation
and speech synthesis. These models can be difficult to train. The standard
approach, teacher forcing, guides a model with reference output history during
training. The problem is that the model is unlikely to recover from its
mistakes during inference, where the reference output is replaced by generated
output. Several approaches deal with this problem, largely by guiding the model
with generated output history. To make training stable, these approaches often
require a heuristic schedule or an auxiliary classifier. This paper introduces
attention forcing, which guides the model with generated output history and
reference attention. This approach can train the model to recover from its
mistakes, in a stable fashion, without the need for a schedule or a classifier.
In addition, it allows the model to generate output sequences aligned with the
references, which can be important for cascaded systems like many speech
synthesis systems. Experiments on speech synthesis show that attention forcing
yields significant performance gain. Experiments on machine translation show
that for tasks where various re-orderings of the output are valid, guiding the
model with generated output history is challenging, while guiding the model
with reference attention is beneficial.","I.2,cs.CL,cs.LG,eess.AS,stat.ML"
"Inferring community characteristics in labelled networks. Labelled networks form a very common and important class of data, naturally
appearing in numerous applications in science and engineering. A typical
inference goal is to determine how the vertex labels(or {\em features}) affect
the network's graph structure. A standard approach has been to partition the
network into blocks grouped by distinct values of the feature of interest. A
block-based random graph model -- typically a variant of the stochastic block
model -- is then used to test for evidence of asymmetric behaviour within these
feature-based communities. Nevertheless, the resulting communities often do not
produce a natural partition of the graph. In this work, we introduce a new
generative model, the feature-first block model (FFBM), which is more effective
at describing vertex-labelled undirected graphs and also facilitates the use of
richer queries on labelled networks. We develop a Bayesian framework for
inference with this model, and we present a method to efficiently sample from
the posterior distribution of the FFBM parameters. The FFBM's structure is kept
deliberately simple to retain easy interpretability of the parameter values. We
apply the proposed methods to a variety of network data to extract the most
important features along which the vertices are partitioned. The main
advantages of the proposed approach are that the whole feature-space is used
automatically, and features can be rank-ordered implicitly according to impact.
Any features that do not significantly impact the high-level structure can be
discarded to reduce the problem dimension. In cases where the vertex features
available do not readily explain the community structure in the resulting
network, the approach detects this and is protected against over-fitting.
Results on several real-world datasets illustrate the performance of the
proposed methods.","cs.LG,cs.SI,stat.AP"
"pAElla: Edge-AI based Real-Time Malware Detection in Data Centers. The increasing use of Internet-of-Things (IoT) devices for monitoring a wide
spectrum of applications, along with the challenges of ""big data"" streaming
support they often require for data analysis, is nowadays pushing for an
increased attention to the emerging edge computing paradigm. In particular,
smart approaches to manage and analyze data directly on the network edge, are
more and more investigated, and Artificial Intelligence (AI) powered edge
computing is envisaged to be a promising direction. In this paper, we focus on
Data Centers (DCs) and Supercomputers (SCs), where a new generation of
high-resolution monitoring systems is being deployed, opening new opportunities
for analysis like anomaly detection and security, but introducing new
challenges for handling the vast amount of data it produces. In detail, we
report on a novel lightweight and scalable approach to increase the security of
DCs/SCs, that involves AI-powered edge computing on high-resolution power
consumption. The method -- called pAElla -- targets real-time Malware Detection
(MD), it runs on an out-of-band IoT-based monitoring system for DCs/SCs, and
involves Power Spectral Density of power measurements, along with AutoEncoders.
Results are promising, with an F1-score close to 1, and a False Alarm and
Malware Miss rate close to 0%. We compare our method with State-of-the-Art MD
techniques and show that, in the context of DCs/SCs, pAElla can cover a wider
range of malware, significantly outperforming SoA approaches in terms of
accuracy. Moreover, we propose a methodology for online training suitable for
DCs/SCs in production, and release open dataset and code.","cs.CR,cs.LG,eess.SP,stat.ML"
"Panoptic Segmentation of Satellite Image Time Series with Convolutional Temporal Attention Networks. Unprecedented access to multi-temporal satellite imagery has opened new
perspectives for a variety of Earth observation tasks. Among them,
pixel-precise panoptic segmentation of agricultural parcels has major economic
and environmental implications. While researchers have explored this problem
for single images, we argue that the complex temporal patterns of crop
phenology are better addressed with temporal sequences of images. In this
paper, we present the first end-to-end, single-stage method for panoptic
segmentation of Satellite Image Time Series (SITS). This module can be combined
with our novel image sequence encoding network which relies on temporal
self-attention to extract rich and adaptive multi-scale spatio-temporal
features. We also introduce PASTIS, the first open-access SITS dataset with
panoptic annotations. We demonstrate the superiority of our encoder for
semantic segmentation against multiple competing architectures, and set up the
first state-of-the-art of panoptic segmentation of SITS. Our implementation and
PASTIS are publicly available.","68T07,68T45,I.4.6; I.2.6; J.2,cs.CV"
"Learning protein sequence embeddings using information from structure. Inferring the structural properties of a protein from its amino acid sequence
is a challenging yet important problem in biology. Structures are not known for
the vast majority of protein sequences, but structure is critical for
understanding function. Existing approaches for detecting structural similarity
between proteins from sequence are unable to recognize and exploit structural
patterns when sequences have diverged too far, limiting our ability to transfer
knowledge between structurally related proteins. We newly approach this problem
through the lens of representation learning. We introduce a framework that maps
any protein sequence to a sequence of vector embeddings --- one per amino acid
position --- that encode structural information. We train bidirectional long
short-term memory (LSTM) models on protein sequences with a two-part feedback
mechanism that incorporates information from (i) global structural similarity
between proteins and (ii) pairwise residue contact maps for individual
proteins. To enable learning from structural similarity information, we define
a novel similarity measure between arbitrary-length sequences of vector
embeddings based on a soft symmetric alignment (SSA) between them. Our method
is able to learn useful position-specific embeddings despite lacking direct
observations of position-level correspondence between sequences. We show
empirically that our multi-task framework outperforms other sequence-based
methods and even a top-performing structure-based alignment method when
predicting structural similarity, our goal. Finally, we demonstrate that our
learned embeddings can be transferred to other protein sequence problems,
improving the state-of-the-art in transmembrane domain prediction.","cs.LG,q-bio.BM,stat.ML"
"Sparsity Based Methods for Overparameterized Variational Problems. Two complementary approaches have been extensively used in signal and image
processing leading to novel results, the sparse representation methodology and
the variational strategy. Recently, a new sparsity based model has been
proposed, the cosparse analysis framework, which may potentially help in
bridging sparse approximation based methods to the traditional total-variation
minimization. Based on this, we introduce a sparsity based framework for
solving overparameterized variational problems. The latter has been used to
improve the estimation of optical flow and also for general denoising of
signals and images. However, the recovery of the space varying parameters
involved was not adequately addressed by traditional variational methods. We
first demonstrate the efficiency of the new framework for one dimensional
signals in recovering a piecewise linear and polynomial function. Then, we
illustrate how the new technique can be used for denoising and segmentation of
images.","35A15,47N10,49N45,65D18,65J22,65M20,68U10,94A12,cs.CV,stat.ML"
"Coverage Hole Detection for mmWave Networks: An Unsupervised Learning Approach. The utilization of millimeter-wave (mmWave) bands in 5G networks poses new
challenges to network planning. Vulnerability to blockages at mmWave bands can
cause coverage holes (CHs) in the radio environment, leading to radio link
failure when a user enters these CHs. Detection of the CHs carries critical
importance so that necessary remedies can be introduced to improve coverage. In
this letter, we propose a novel approach to identify the CHs in an unsupervised
fashion using a state-of-the-art manifold learning technique: uniform manifold
approximation and projection. The key idea is to preserve the
local-connectedness structure inherent in the collected unlabelled channel
samples, such that the CHs from the service area are detectable. Our results on
the DeepMIMO dataset scenario demonstrate that the proposed method can learn
the structure within the data samples and provide visual holes in the
low-dimensional embedding while preserving the CH boundaries. Once the CH
boundary is determined in the low-dimensional embedding, channel-based
localization techniques can be applied to these samples to obtain the
geographical boundaries of the CHs.","cs.IT,cs.LG,cs.NI,math.IT"
"Denoising Linear Models with Permuted Data. The multivariate linear regression model with shuffled data and additive
Gaussian noise arises in various correspondence estimation and matching
problems. Focusing on the denoising aspect of this problem, we provide a
characterization the minimax error rate that is sharp up to logarithmic
factors. We also analyze the performance of two versions of a computationally
efficient estimator, and establish their consistency for a large range of input
parameters. Finally, we provide an exact algorithm for the noiseless problem
and demonstrate its performance on an image point-cloud matching task. Our
analysis also extends to datasets with outliers.","cs.IT,math.IT,math.ST,stat.ML,stat.TH"
"Disrupting Deepfakes: Adversarial Attacks Against Conditional Image Translation Networks and Facial Manipulation Systems. Face modification systems using deep learning have become increasingly
powerful and accessible. Given images of a person's face, such systems can
generate new images of that same person under different expressions and poses.
Some systems can also modify targeted attributes such as hair color or age.
This type of manipulated images and video have been coined Deepfakes. In order
to prevent a malicious user from generating modified images of a person without
their consent we tackle the new problem of generating adversarial attacks
against such image translation systems, which disrupt the resulting output
image. We call this problem disrupting deepfakes. Most image translation
architectures are generative models conditioned on an attribute (e.g. put a
smile on this person's face). We are first to propose and successfully apply
(1) class transferable adversarial attacks that generalize to different
classes, which means that the attacker does not need to have knowledge about
the conditioning class, and (2) adversarial training for generative adversarial
networks (GANs) as a first step towards robust image translation networks.
Finally, in gray-box scenarios, blurring can mount a successful defense against
disruption. We present a spread-spectrum adversarial attack, which evades blur
defenses. Our open-source code can be found at
https://github.com/natanielruiz/disrupting-deepfakes.","cs.CR,cs.CV,cs.CY,cs.LG"
"Training GANs with Optimism. We address the issue of limit cycling behavior in training Generative
Adversarial Networks and propose the use of Optimistic Mirror Decent (OMD) for
training Wasserstein GANs. Recent theoretical results have shown that
optimistic mirror decent (OMD) can enjoy faster regret rates in the context of
zero-sum games. WGANs is exactly a context of solving a zero-sum game with
simultaneous no-regret dynamics. Moreover, we show that optimistic mirror
decent addresses the limit cycling problem in training WGANs. We formally show
that in the case of bi-linear zero-sum games the last iterate of OMD dynamics
converges to an equilibrium, in contrast to GD dynamics which are bound to
cycle. We also portray the huge qualitative difference between GD and OMD
dynamics with toy examples, even when GD is modified with many adaptations
proposed in the recent literature, such as gradient penalty or momentum. We
apply OMD WGAN training to a bioinformatics problem of generating DNA
sequences. We observe that models trained with OMD achieve consistently smaller
KL divergence with respect to the true underlying distribution, than models
trained with GD variants. Finally, we introduce a new algorithm, Optimistic
Adam, which is an optimistic variant of Adam. We apply it to WGAN training on
CIFAR10 and observe improved performance in terms of inception score as
compared to Adam.","cs.GT,cs.LG,stat.ML"
"Generative Multi-Adversarial Networks. Generative adversarial networks (GANs) are a framework for producing a
generative model by way of a two-player minimax game. In this paper, we propose
the \emph{Generative Multi-Adversarial Network} (GMAN), a framework that
extends GANs to multiple discriminators. In previous work, the successful
training of GANs requires modifying the minimax objective to accelerate
training early on. In contrast, GMAN can be reliably trained with the original,
untampered objective. We explore a number of design perspectives with the
discriminator role ranging from formidable adversary to forgiving teacher.
Image generation tasks comparing the proposed framework to standard GANs
demonstrate GMAN produces higher quality samples in a fraction of the
iterations when measured by a pairwise GAM-type metric.","cs.LG,cs.MA,cs.NE"
"SEAN: Image Synthesis with Semantic Region-Adaptive Normalization. We propose semantic region-adaptive normalization (SEAN), a simple but
effective building block for Generative Adversarial Networks conditioned on
segmentation masks that describe the semantic regions in the desired output
image. Using SEAN normalization, we can build a network architecture that can
control the style of each semantic region individually, e.g., we can specify
one style reference image per region. SEAN is better suited to encode,
transfer, and synthesize style than the best previous method in terms of
reconstruction quality, variability, and visual quality. We evaluate SEAN on
multiple datasets and report better quantitative metrics (e.g. FID, PSNR) than
the current state of the art. SEAN also pushes the frontier of interactive
image editing. We can interactively edit images by changing segmentation masks
or the style for any given region. We can also interpolate styles from two
reference images per region.","cs.CV,cs.GR,eess.IV"
"Adaptive Neural Message Passing for Inductive Learning on Hypergraphs. Graphs are the most ubiquitous data structures for representing relational
datasets and performing inferences in them. They model, however, only pairwise
relations between nodes and are not designed for encoding the higher-order
relations. This drawback is mitigated by hypergraphs, in which an edge can
connect an arbitrary number of nodes. Most hypergraph learning approaches
convert the hypergraph structure to that of a graph and then deploy existing
geometric deep learning methods. This transformation leads to information loss,
and sub-optimal exploitation of the hypergraph's expressive power. We present
HyperMSG, a novel hypergraph learning framework that uses a modular two-level
neural message passing strategy to accurately and efficiently propagate
information within each hyperedge and across the hyperedges. HyperMSG adapts to
the data and task by learning an attention weight associated with each node's
degree centrality. Such a mechanism quantifies both local and global importance
of a node, capturing the structural properties of a hypergraph. HyperMSG is
inductive, allowing inference on previously unseen nodes. Further, it is robust
and outperforms state-of-the-art hypergraph learning methods on a wide range of
tasks and datasets. Finally, we demonstrate the effectiveness of HyperMSG in
learning multimodal relations through detailed experimentation on a challenging
multimedia dataset.","cs.LG,cs.MM"
"Analyzing the tree-layer structure of Deep Forests. Random forests on the one hand, and neural networks on the other hand, have
met great success in the machine learning community for their predictive
performance. Combinations of both have been proposed in the literature, notably
leading to the so-called deep forests (DF) (Zhou \& Feng,2019). In this paper,
our aim is not to benchmark DF performances but to investigate instead their
underlying mechanisms. Additionally, DF architecture can be generally
simplified into more simple and computationally efficient shallow forest
networks. Despite some instability, the latter may outperform standard
predictive tree-based methods. We exhibit a theoretical framework in which a
shallow tree network is shown to enhance the performance of classical decision
trees. In such a setting, we provide tight theoretical lower and upper bounds
on its excess risk. These theoretical results show the interest of tree-network
architectures for well-structured data provided that the first layer, acting as
a data encoder, is rich enough.","cs.LG,math.ST,stat.TH"
"RobustPeriod: Time-Frequency Mining for Robust Multiple Periodicity Detection. Periodicity detection is a crucial step in time series tasks, including
monitoring and forecasting of metrics in many areas, such as IoT applications
and self-driving database management system. In many of these applications,
multiple periodic components exist and are often interlaced with each other.
Such dynamic and complicated periodic patterns make the accurate periodicity
detection difficult. In addition, other components in the time series, such as
trend, outliers and noises, also pose additional challenges for accurate
periodicity detection. In this paper, we propose a robust and general framework
for multiple periodicity detection. Our algorithm applies maximal overlap
discrete wavelet transform to transform the time series into multiple
temporal-frequency scales such that different periodic components can be
isolated. We rank them by wavelet variance, and then at each scale detect
single periodicity by our proposed Huber-periodogram and Huber-ACF robustly. We
rigorously prove the theoretical properties of Huber-periodogram and justify
the use of Fisher's test on Huber-periodogram for periodicity detection. To
further refine the detected periods, we compute unbiased autocorrelation
function based on Wiener-Khinchin theorem from Huber-periodogram for improved
robustness and efficiency. Experiments on synthetic and real-world datasets
show that our algorithm outperforms other popular ones for both single and
multiple periodicity detection.","cs.LG,eess.SP,stat.AP,stat.ML"
"Machine learning with quantum field theories. The precise equivalence between discretized Euclidean field theories and a
certain class of probabilistic graphical models, namely the mathematical
framework of Markov random fields, opens up the opportunity to investigate
machine learning from the perspective of quantum field theory. In this
contribution we will demonstrate, through the Hammersley-Clifford theorem, that
the $\phi^{4}$ scalar field theory on a square lattice satisfies the local
Markov property and can therefore be recast as a Markov random field. We will
then derive from the $\phi^{4}$ theory machine learning algorithms and neural
networks which can be viewed as generalizations of conventional neural network
architectures. Finally, we will conclude by presenting applications based on
the minimization of an asymmetric distance between the probability distribution
of the $\phi^{4}$ machine learning algorithms and target probability
distributions.","cs.LG,hep-lat,hep-th,math-ph,math.MP,math.PR"
"Performance Tuning Of J48 Algorithm For Prediction Of Soil Fertility. Data mining involves the systematic analysis of large data sets, and data
mining in agricultural soil datasets is exciting and modern research area. The
productive capacity of a soil depends on soil fertility. Achieving and
maintaining appropriate levels of soil fertility, is of utmost importance if
agricultural land is to remain capable of nourishing crop production. In this
research, Steps for building a predictive model of soil fertility have been
explained.
  This paper aims at predicting soil fertility class using decision tree
algorithms in data mining . Further, it focuses on performance tuning of J48
decision tree algorithm with the help of meta-techniques such as attribute
selection and boosting.","cs.DB,cs.LG,cs.PF,stat.ML"
"Conjugate Variables as a Resource in Signal and Image Processing. In this paper we develop a new technique to model joint distributions of
signals. Our technique is based on quantum mechanical conjugate variables. We
show that the transition probability of quantum states leads to a distance
function on the signals. This distance function obeys the triangle inequality
on all quantum states and becomes a metric on pure quantum states. Treating
signals as conjugate variables allows us to create a new approach to segment
them.
  Keywords: Quantum information, transition probability, Euclidean distance,
Fubini-study metric, Bhattacharyya coefficients, conjugate variable,
signal/sensor fusion, signal and image segmentation.","cs.CV,physics.data-an,quant-ph"
"Video Influencers: Unboxing the Mystique. Influencer marketing is being used increasingly as a tool to reach customers
because of the growing popularity of social media stars who primarily reach
their audience(s) via custom videos. Despite the rapid growth in influencer
marketing, there has been little research on the design and effectiveness of
influencer videos. Using publicly available data on YouTube influencer videos,
we implement novel interpretable deep learning architectures, supported by
transfer learning, to identify significant relationships between advertising
content in videos (across text, audio, and images) and video views, interaction
rates and sentiment. By avoiding ex-ante feature engineering and instead using
ex-post interpretation, our approach avoids making a trade-off between
interpretability and predictive ability. We filter out relationships that are
affected by confounding factors unassociated with an increase in attention to
video elements, thus facilitating the generation of plausible causal
relationships between video elements and marketing outcomes which can be tested
in the field. A key finding is that brand mentions in the first 30 seconds of a
video are on average associated with a significant increase in attention to the
brand but a significant decrease in sentiment expressed towards the video. We
illustrate the learnings from our approach for both influencers and brands.","cs.CL,cs.CV,cs.LG,cs.SD,eess.AS"
"Is There Tradeoff between Spatial and Temporal in Video Super-Resolution?. Recent advances of deep learning lead to great success of image and video
super-resolution (SR) methods that are based on convolutional neural networks
(CNN). For video SR, advanced algorithms have been proposed to exploit the
temporal correlation between low-resolution (LR) video frames, and/or to
super-resolve a frame with multiple LR frames. These methods pursue higher
quality of super-resolved frames, where the quality is usually measured frame
by frame in e.g. PSNR. However, frame-wise quality may not reveal the
consistency between frames. If an algorithm is applied to each frame
independently (which is the case of most previous methods), the algorithm may
cause temporal inconsistency, which can be observed as flickering. It is a
natural requirement to improve both frame-wise fidelity and between-frame
consistency, which are termed spatial quality and temporal quality,
respectively. Then we may ask, is a method optimized for spatial quality also
optimized for temporal quality? Can we optimize the two quality metrics
jointly?",cs.CV
"Incremental inference of collective graphical models. We consider incremental inference problems from aggregate data for collective
dynamics. In particular, we address the problem of estimating the aggregate
marginals of a Markov chain from noisy aggregate observations in an incremental
(online) fashion. We propose a sliding window Sinkhorn belief propagation
(SW-SBP) algorithm that utilizes a sliding window filter of the most recent
noisy aggregate observations along with encoded information from discarded
observations. Our algorithm is built upon the recently proposed multi-marginal
optimal transport based SBP algorithm that leverages standard belief
propagation and Sinkhorn algorithm to solve inference problems from aggregate
data. We demonstrate the performance of our algorithm on applications such as
inferring population flow from aggregate observations.","cs.IT,cs.LG,cs.SY,eess.SY,math.IT,math.OC,stat.ML"
"VidCEP: Complex Event Processing Framework to Detect Spatiotemporal Patterns in Video Streams. Video data is highly expressive and has traditionally been very difficult for
a machine to interpret. Querying event patterns from video streams is
challenging due to its unstructured representation. Middleware systems such as
Complex Event Processing (CEP) mine patterns from data streams and send
notifications to users in a timely fashion. Current CEP systems have inherent
limitations to query video streams due to their unstructured data model and
lack of expressive query language. In this work, we focus on a CEP framework
where users can define high-level expressive queries over videos to detect a
range of spatiotemporal event patterns. In this context, we propose: i) VidCEP,
an in-memory, on the fly, near real-time complex event matching framework for
video streams. The system uses a graph-based event representation for video
streams which enables the detection of high-level semantic concepts from video
using cascades of Deep Neural Network models, ii) a Video Event Query language
(VEQL) to express high-level user queries for video streams in CEP, iii) a
complex event matcher to detect spatiotemporal video event patterns by matching
expressive user queries over video data. The proposed approach detects
spatiotemporal video event patterns with an F-score ranging from 0.66 to 0.89.
VidCEP maintains near real-time performance with an average throughput of 70
frames per second for 5 parallel videos with sub-second matching latency.","cs.CV,cs.DB,cs.MM,eess.IV"
"Bounded Manifold Completion. Nonlinear dimensionality reduction or, equivalently, the approximation of
high-dimensional data using a low-dimensional nonlinear manifold is an active
area of research. In this paper, we will present a thematically different
approach to detect the existence of a low-dimensional manifold of a given
dimension that lies within a set of bounds derived from a given point cloud. A
matrix representing the appropriately defined distances on a low-dimensional
manifold is low-rank, and our method is based on current techniques for
recovering a partially observed matrix from a small set of fully observed
entries that can be implemented as a low-rank Matrix Completion (MC) problem.
MC methods are currently used to solve challenging real-world problems, such as
image inpainting and recommender systems, and we leverage extent efficient
optimization techniques that use a nuclear norm convex relaxation as a
surrogate for non-convex and discontinuous rank minimization. Our proposed
method provides several advantages over current nonlinear dimensionality
reduction techniques, with the two most important being theoretical guarantees
on the detection of low-dimensional embeddings and robustness to non-uniformity
in the sampling of the manifold. We validate the performance of this approach
using both a theoretical analysis as well as synthetic and real-world benchmark
datasets.","68T05,H.2.8; I.2.6,cs.CV,cs.LG,stat.ML"
"Leveraging the Self-Transition Probability of Ordinal Pattern Transition Graph for Transportation Mode Classification. The analysis of GPS trajectories is a well-studied problem in Urban Computing
and has been used to track people. Analyzing people mobility and identifying
the transportation mode used by them is essential for cities that want to
reduce traffic jams and travel time between their points, thus helping to
improve the quality of life of citizens. The trajectory data of a moving object
is represented by a discrete collection of points through time, i.e., a time
series. Regarding its interdisciplinary and broad scope of real-world
applications, it is evident the need of extracting knowledge from time series
data. Mining this type of data, however, faces several complexities due to its
unique properties. Different representations of data may overcome this. In this
work, we propose the use of a feature retained from the Ordinal Pattern
Transition Graph, called the probability of self-transition for transportation
mode classification. The proposed feature presents better accuracy results than
Permutation Entropy and Statistical Complexity, even when these two are
combined. This is the first work, to the best of our knowledge, that uses
Information Theory quantifiers to transportation mode classification, showing
that it is a feasible approach to this kind of problem.","cs.LG,eess.SP,stat.ML"
"Robust Factorization of Real-world Tensor Streams with Patterns, Missing Values, and Outliers. Consider multiple seasonal time series being collected in real-time, in the
form of a tensor stream. Real-world tensor streams often include missing
entries (e.g., due to network disconnection) and at the same time unexpected
outliers (e.g., due to system errors). Given such a real-world tensor stream,
how can we estimate missing entries and predict future evolution accurately in
real-time? In this work, we answer this question by introducing SOFIA, a robust
factorization method for real-world tensor streams. In a nutshell, SOFIA
smoothly and tightly integrates tensor factorization, outlier removal, and
temporal-pattern detection, which naturally reinforce each other. Moreover,
SOFIA integrates them in linear time, in an online manner, despite the presence
of missing entries. We experimentally show that SOFIA is (a) robust and
accurate: yielding up to 76% lower imputation error and 71% lower forecasting
error; (b) fast: up to 935X faster than the second-most accurate competitor;
and (c) scalable: scaling linearly with the number of new entries per time
step.","cs.DB,cs.LG"
"Vertical stratification of forest canopy for segmentation of under-story trees within small-footprint airborne LiDAR point clouds. Airborne LiDAR point cloud representing a forest contains 3D data, from which
vertical stand structure even of understory layers can be derived. This paper
presents a tree segmentation approach for multi-story stands that stratifies
the point cloud to canopy layers and segments individual tree crowns within
each layer using a digital surface model based tree segmentation method. The
novelty of the approach is the stratification procedure that separates the
point cloud to an overstory and multiple understory tree canopy layers by
analyzing vertical distributions of LiDAR points within overlapping locales.
The procedure does not make a priori assumptions about the shape and size of
the tree crowns and can, independent of the tree segmentation method, be
utilized to vertically stratify tree crowns of forest canopies. We applied the
proposed approach to the University of Kentucky Robinson Forest - a natural
deciduous forest with complex and highly variable terrain and vegetation
structure. The segmentation results showed that using the stratification
procedure strongly improved detecting understory trees (from 46% to 68%) at the
cost of introducing a fair number of over-segmented understory trees (increased
from 1% to 16%), while barely affecting the overall segmentation quality of
overstory trees. Results of vertical stratification of the canopy showed that
the point density of understory canopy layers were suboptimal for performing a
reasonable tree segmentation, suggesting that acquiring denser LiDAR point
clouds would allow more improvements in segmenting understory trees. As shown
by inspecting correlations of the results with forest structure, the
segmentation approach is applicable to a variety of forest types.","cs.CE,cs.CG,cs.CV"
"Fully Learnable Deep Wavelet Transform for Unsupervised Monitoring of High-Frequency Time Series. High-Frequency (HF) signal are ubiquitous in the industrial world and are of
great use for the monitoring of industrial assets. Most deep learning tools are
designed for inputs of fixed and/or very limited size and many successful
applications of deep learning to the industrial context use as inputs extracted
features, which is a manually and often arduously obtained compact
representation of the original signal. In this paper, we propose a fully
unsupervised deep learning framework that is able to extract meaningful and
sparse representation of raw HF signals. We embed in our architecture important
properties of the fast discrete wavelet transformation (FDWT) such as (1) the
cascade algorithm, (2) the quadrature mirror filter property that relates
together the wavelet, the scaling and transposed filter functions, and (3) the
coefficient denoising. Using deep learning, we make this architecture fully
learnable: both the wavelet bases and the wavelet coefficient denoising are
learnable. To achieve this objective, we introduce a new activation function
that performs a learnable hard-thresholding of the wavelet coefficients. With
our framework, the denoising FDWT becomes a fully learnable unsupervised tool
that does neither require any type of pre- nor post-processing, nor any prior
knowledge on wavelet transform. We demonstrate the benefit of embedding all
these properties on three machine-learning tasks performed on open source sound
datasets. We achieve results well above baseline and we perform an ablation
study of the impact of each property on the performance of the architecture.","cs.LG,cs.SD,eess.AS"
"Bootstrapping Statistical Inference for Off-Policy Evaluation. Bootstrapping provides a flexible and effective approach for assessing the
quality of batch reinforcement learning, yet its theoretical property is less
understood. In this paper, we study the use of bootstrapping in off-policy
evaluation (OPE), and in particular, we focus on the fitted Q-evaluation (FQE)
that is known to be minimax-optimal in the tabular and linear-model cases. We
propose a bootstrapping FQE method for inferring the distribution of the policy
evaluation error and show that this method is asymptotically efficient and
distributionally consistent for off-policy statistical inference. To overcome
the computation limit of bootstrapping, we further adapt a subsampling
procedure that improves the runtime by an order of magnitude. We numerically
evaluate the bootrapping method in classical RL environments for confidence
interval estimation, estimating the variance of off-policy evaluator, and
estimating the correlation between multiple off-policy evaluators.","cs.LG,math.ST,stat.ML,stat.TH"
"Algorithms for Learning Graphs in Financial Markets. In the past two decades, the field of applied finance has tremendously
benefited from graph theory. As a result, novel methods ranging from asset
network estimation to hierarchical asset selection and portfolio allocation are
now part of practitioners' toolboxes. In this paper, we investigate the
fundamental problem of learning undirected graphical models under Laplacian
structural constraints from the point of view of financial market times series
data. In particular, we present natural justifications, supported by empirical
evidence, for the usage of the Laplacian matrix as a model for the precision
matrix of financial assets, while also establishing a direct link that reveals
how Laplacian constraints are coupled to meaningful physical interpretations
related to the market index factor and to conditional correlations between
stocks. Those interpretations lead to a set of guidelines that practitioners
should be aware of when estimating graphs in financial markets. In addition, we
design numerical algorithms based on the alternating direction method of
multipliers to learn undirected, weighted graphs that take into account
stylized facts that are intrinsic to financial data such as heavy tails and
modularity. We illustrate how to leverage the learned graphs into practical
scenarios such as stock time series clustering and foreign exchange network
estimation. The proposed graph learning algorithms outperform the
state-of-the-art methods in an extensive set of practical experiments.
Furthermore, we obtain theoretical and empirical convergence results for the
proposed algorithms. Along with the developed methodologies for graph learning
in financial markets, we release an R package, called fingraph, accommodating
the code and data to obtain all the experimental results.","cs.LG,eess.SP,q-fin.ST"
"Identification of Latent Variables From Graphical Model Residuals. Graph-based causal discovery methods aim to capture conditional
independencies consistent with the observed data and differentiate causal
relationships from indirect or induced ones. Successful construction of
graphical models of data depends on the assumption of causal sufficiency: that
is, that all confounding variables are measured. When this assumption is not
met, learned graphical structures may become arbitrarily incorrect and effects
implied by such models may be wrongly attributed, carry the wrong magnitude, or
mis-represent direction of correlation. Wide application of graphical models to
increasingly less curated ""big data"" draws renewed attention to the unobserved
confounder problem.
  We present a novel method that aims to control for the latent space when
estimating a DAG by iteratively deriving proxies for the latent space from the
residuals of the inferred model. Under mild assumptions, our method improves
structural inference of Gaussian graphical models and enhances identifiability
of the causal effect. In addition, when the model is being used to predict
outcomes, it un-confounds the coefficients on the parents of the outcomes and
leads to improved predictive performance when out-of-sample regime is very
different from the training data. We show that any improvement of prediction of
an outcome is intrinsically capped and cannot rise beyond a certain limit as
compared to the confounded model. We extend our methodology beyond GGMs to
ordinal variables and nonlinear cases. Our R package provides both PCA and
autoencoder implementations of the methodology, suitable for GGMs with some
guarantees and for better performance in general cases but without such
guarantees.","cs.LG,q-bio.QM,stat.ML"
"An FPGA-based Parallel Architecture for Face Detection using Mixed Color Models. In this paper, a reliable method for detecting human faces in color images is
proposed. This system firstly detects skin color in YCgCr and YIQ color space,
then filters binary texture and the result is morphological processed, finally
converts skin tone to the preferred skin color configured by users in YIQ color
space. The real-time adjusting circuit is implemented and some of simulation
results are given out. Experimental results demonstrate that the method has
achieved high rates and low false positives, another advantage is its
simplicity and minor computational costs.","68U10,cs.CV"
"Expressive power of tensor-network factorizations for probabilistic modeling, with applications from hidden Markov models to quantum machine learning. Tensor-network techniques have enjoyed outstanding success in physics, and
have recently attracted attention in machine learning, both as a tool for the
formulation of new learning algorithms and for enhancing the mathematical
understanding of existing methods. Inspired by these developments, and the
natural correspondence between tensor networks and probabilistic graphical
models, we provide a rigorous analysis of the expressive power of various
tensor-network factorizations of discrete multivariate probability
distributions. These factorizations include non-negative tensor-trains/MPS,
which are in correspondence with hidden Markov models, and Born machines, which
are naturally related to local quantum circuits. When used to model probability
distributions, they exhibit tractable likelihoods and admit efficient learning
algorithms. Interestingly, we prove that there exist probability distributions
for which there are unbounded separations between the resource requirements of
some of these tensor-network factorizations. Particularly surprising is the
fact that using complex instead of real tensors can lead to an arbitrarily
large reduction in the number of parameters of the network. Additionally, we
introduce locally purified states (LPS), a new factorization inspired by
techniques for the simulation of quantum systems, with provably better
expressive power than all other representations considered. The ramifications
of this result are explored through numerical experiments. Our findings imply
that LPS should be considered over hidden Markov models, and furthermore
provide guidelines for the design of local quantum circuits for probabilistic
modeling.","cond-mat.str-el,cs.LG,math.OC,quant-ph,stat.ML"
"A Reinforcement Learning Environment for Polyhedral Optimizations. The polyhedral model allows a structured way of defining semantics-preserving
transformations to improve the performance of a large class of loops. Finding
profitable points in this space is a hard problem which is usually approached
by heuristics that generalize from domain-expert knowledge. Existing problem
formulations in state-of-the-art heuristics depend on the shape of particular
loops, making it hard to leverage generic and more powerful optimization
techniques from the machine learning domain. In this paper, we propose PolyGym,
a shape-agnostic formulation for the space of legal transformations in the
polyhedral model as a Markov Decision Process (MDP). Instead of using
transformations, the formulation is based on an abstract space of possible
schedules. In this formulation, states model partial schedules, which are
constructed by actions that are reusable across different loops. With a simple
heuristic to traverse the space, we demonstrate that our formulation is
powerful enough to match and outperform state-of-the-art heuristics. On the
Polybench benchmark suite, we found transformations that led to a speedup of
3.39x over LLVM O3, which is 1.83x better than the speedup achieved by ISL. Our
generic MDP formulation enables using reinforcement learning to learn
optimization policies over a wide range of loops. This also contributes to the
emerging field of machine learning in compilers, as it exposes a novel problem
formulation that can push the limits of existing methods.","cs.DC,cs.DM,cs.LG,cs.PF"
"FoldingZero: Protein Folding from Scratch in Hydrophobic-Polar Model. De novo protein structure prediction from amino acid sequence is one of the
most challenging problems in computational biology. As one of the extensively
explored mathematical models for protein folding, Hydrophobic-Polar (HP) model
enables thorough investigation of protein structure formation and evolution.
Although HP model discretizes the conformational space and simplifies the
folding energy function, it has been proven to be an NP-complete problem. In
this paper, we propose a novel protein folding framework FoldingZero,
self-folding a de novo protein 2D HP structure from scratch based on deep
reinforcement learning. FoldingZero features the coupled approach of a two-head
(policy and value heads) deep convolutional neural network (HPNet) and a
regularized Upper Confidence Bounds for Trees (R-UCT). It is trained solely by
a reinforcement learning algorithm, which improves HPNet and R-UCT iteratively
through iterative policy optimization. Without any supervision and domain
knowledge, FoldingZero not only achieves comparable results, but also learns
the latent folding knowledge to stabilize the structure. Without exponential
computation, FoldingZero shows promising potential to be adopted for real-world
protein properties prediction.","cs.AI,cs.LG,q-bio.BM"
"Attentional Graph Convolutional Networks for Knowledge Concept Recommendation in MOOCs in a Heterogeneous View. Massive open online courses are becoming a modish way for education, which
provides a large-scale and open-access learning opportunity for students to
grasp the knowledge. To attract students' interest, the recommendation system
is applied by MOOCs providers to recommend courses to students. However, as a
course usually consists of a number of video lectures, with each one covering
some specific knowledge concepts, directly recommending courses overlook
students'interest to some specific knowledge concepts. To fill this gap, in
this paper, we study the problem of knowledge concept recommendation. We
propose an end-to-end graph neural network-based approach
calledAttentionalHeterogeneous Graph Convolutional Deep Knowledge
Recommender(ACKRec) for knowledge concept recommendation in MOOCs. Like other
recommendation problems, it suffers from sparsity issues. To address this
issue, we leverage both content information and context information to learn
the representation of entities via graph convolution network. In addition to
students and knowledge concepts, we consider other types of entities (e.g.,
courses, videos, teachers) and construct a heterogeneous information network to
capture the corresponding fruitful semantic relationships among different types
of entities and incorporate them into the representation learning process.
Specifically, we use meta-path on the HIN to guide the propagation of students'
preferences. With the help of these meta-paths, the students' preference
distribution with respect to a candidate knowledge concept can be captured.
Furthermore, we propose an attention mechanism to adaptively fuse the context
information from different meta-paths, in order to capture the different
interests of different students. The promising experiment results show that the
proposedACKRecis able to effectively recommend knowledge concepts to students
pursuing online learning in MOOCs.","cs.CY,cs.IR,cs.LG,stat.ML"
"G5: A Universal GRAPH-BERT for Graph-to-Graph Transfer and Apocalypse Learning. The recent GRAPH-BERT model introduces a new approach to learning graph
representations merely based on the attention mechanism. GRAPH-BERT provides an
opportunity for transferring pre-trained models and learned graph
representations across different tasks within the same graph dataset. In this
paper, we will further investigate the graph-to-graph transfer of a universal
GRAPH-BERT for graph representation learning across different graph datasets,
and our proposed model is also referred to as the G5 for simplicity. Many
challenges exist in learning G5 to adapt the distinct input and output
configurations for each graph data source, as well as the information
distributions differences. G5 introduces a pluggable model architecture: (a)
each data source will be pre-processed with a unique input representation
learning component; (b) each output application task will also have a specific
functional component; and (c) all such diverse input and output components will
all be conjuncted with a universal GRAPH-BERT core component via an input size
unification layer and an output representation fusion layer, respectively.
  The G5 model removes the last obstacle for cross-graph representation
learning and transfer. For the graph sources with very sparse training data,
the G5 model pre-trained on other graphs can still be utilized for
representation learning with necessary fine-tuning. What's more, the
architecture of G5 also allows us to learn a supervised functional classifier
for data sources without any training data at all. Such a problem is also named
as the Apocalypse Learning task in this paper. Two different label reasoning
strategies, i.e., Cross-Source Classification Consistency Maximization (CCCM)
and Cross-Source Dynamic Routing (CDR), are introduced in this paper to address
the problem.","cs.LG,cs.NE,cs.SI,stat.ML"
"A Survey on Reproducibility by Evaluating Deep Reinforcement Learning Algorithms on Real-World Robots. As reinforcement learning (RL) achieves more success in solving complex
tasks, more care is needed to ensure that RL research is reproducible and that
algorithms herein can be compared easily and fairly with minimal bias. RL
results are, however, notoriously hard to reproduce due to the algorithms'
intrinsic variance, the environments' stochasticity, and numerous (potentially
unreported) hyper-parameters. In this work we investigate the many issues
leading to irreproducible research and how to manage those. We further show how
to utilise a rigorous and standardised evaluation approach for easing the
process of documentation, evaluation and fair comparison of different
algorithms, where we emphasise the importance of choosing the right measurement
metrics and conducting proper statistics on the results, for unbiased reporting
of the results.","cs.AI,cs.LG,cs.RO,stat.ML"
"Learning Graph Representation by Aggregating Subgraphs via Mutual Information Maximization. In this paper, we introduce a self-supervised learning method to enhance the
graph-level representations with the help of a set of subgraphs. For this
purpose, we propose a universal framework to generate subgraphs in an
auto-regressive way and then using these subgraphs to guide the learning of
graph representation by Graph Neural Networks. Under this framework, we can get
a comprehensive understanding of the graph structure in a learnable way. And to
fully capture enough information of original graphs, we design three
information aggregators: \textbf{attribute-conv}, \textbf{layer-conv} and
\textbf{subgraph-conv} to gather information from different aspects. And to
achieve efficient and effective contrastive learning, a Head-Tail contrastive
construction is proposed to provide abundant negative samples. Under all
proposed components which can be generalized to any Graph Neural Networks, in
the unsupervised case, we achieve new state-of-the-art results in several
benchmarks. We also evaluate our model on semi-supervised learning tasks and
make a fair comparison to state-of-the-art semi-supervised methods.","cs.IT,cs.LG,math.IT"
"Distinguishing noise from chaos: objective versus subjective criteria using Horizontal Visibility Graph. A recently proposed methodology called the Horizontal Visibility Graph (HVG)
[Luque {\it et al.}, Phys. Rev. E., 80, 046103 (2009)] that constitutes a
geometrical simplification of the well known Visibility Graph algorithm [Lacasa
{\it et al.\/}, Proc. Natl. Sci. U.S.A. 105, 4972 (2008)], has been used to
study the distinction between deterministic and stochastic components in time
series [L. Lacasa and R. Toral, Phys. Rev. E., 82, 036120 (2010)].
Specifically, the authors propose that the node degree distribution of these
processes follows an exponential functional of the form $P(\kappa)\sim
\exp(-\lambda~\kappa)$, in which $\kappa$ is the node degree and $\lambda$ is a
positive parameter able to distinguish between deterministic (chaotic) and
stochastic (uncorrelated and correlated) dynamics. In this work, we investigate
the characteristics of the node degree distributions constructed by using HVG,
for time series corresponding to $28$ chaotic maps and $3$ different stochastic
processes. We thoroughly study the methodology proposed by Lacasa and Toral
finding several cases for which their hypothesis is not valid. We propose a
methodology that uses the HVG together with Information Theory quantifiers. An
extensive and careful analysis of the node degree distributions obtained by
applying HVG allow us to conclude that the Fisher-Shannon information plane is
a remarkable tool able to graphically represent the different nature,
deterministic or stochastic, of the systems under study.","cs.IT,math.IT,nlin.CD,stat.ML"
"Interval Forecasting of Electricity Demand: A Novel Bivariate EMD-based Support Vector Regression Modeling Framework. Highly accurate interval forecasting of electricity demand is fundamental to
the success of reducing the risk when making power system planning and
operational decisions by providing a range rather than point estimation. In
this study, a novel modeling framework integrating bivariate empirical mode
decomposition (BEMD) and support vector regression (SVR), extended from the
well-established empirical mode decomposition (EMD) based time series modeling
framework in the energy demand forecasting literature, is proposed for interval
forecasting of electricity demand. The novelty of this study arises from the
employment of BEMD, a new extension of classical empirical model decomposition
(EMD) destined to handle bivariate time series treated as complex-valued time
series, as decomposition method instead of classical EMD only capable of
decomposing one-dimensional single-valued time series. This proposed modeling
framework is endowed with BEMD to decompose simultaneously both the lower and
upper bounds time series, constructed in forms of complex-valued time series,
of electricity demand on a monthly per hour basis, resulting in capturing the
potential interrelationship between lower and upper bounds. The proposed
modeling framework is justified with monthly interval-valued electricity demand
data per hour in Pennsylvania-New Jersey-Maryland Interconnection, indicating
it as a promising method for interval-valued electricity demand forecasting.","cs.LG,stat.AP"
"Symmetric Boolean Factor Analysis with Applications to InstaHide. In this work we examine the security of InstaHide, a recently proposed scheme
for distributed learning (Huang et al.). A number of recent works have given
reconstruction attacks for InstaHide in various regimes by leveraging an
intriguing connection to the following matrix factorization problem: given the
Gram matrix of a collection of m random k-sparse Boolean vectors in {0,1}^r,
recover the vectors (up to the trivial symmetries). Equivalently, this can be
thought of as a sparse, symmetric variant of the well-studied problem of
Boolean factor analysis, or as an average-case version of the classic problem
of recovering a k-uniform hypergraph from its line graph.
  As previous algorithms either required m to be exponentially large in k or
only applied to k = 2, they left open the question of whether InstaHide
possesses some form of ""fine-grained security"" against reconstruction attacks
for moderately large k. In this work, we answer this in the negative by giving
a simple O(m^{\omega + 1}) time algorithm for the above matrix factorization
problem. Our algorithm, based on tensor decomposition, only requires m to be at
least quasi-linear in r. We complement this result with a quasipolynomial-time
algorithm for a worst-case setting of the problem where the collection of
k-sparse vectors is chosen arbitrarily.","cs.CR,cs.DS,cs.LG,stat.ML"
"Symmetric Spaces for Graph Embeddings: A Finsler-Riemannian Approach. Learning faithful graph representations as sets of vertex embeddings has
become a fundamental intermediary step in a wide range of machine learning
applications. We propose the systematic use of symmetric spaces in
representation learning, a class encompassing many of the previously used
embedding targets. This enables us to introduce a new method, the use of
Finsler metrics integrated in a Riemannian optimization scheme, that better
adapts to dissimilar structures in the graph. We develop a tool to analyze the
embeddings and infer structural properties of the data sets. For
implementation, we choose Siegel spaces, a versatile family of symmetric
spaces. Our approach outperforms competitive baselines for graph reconstruction
tasks on various synthetic and real-world datasets. We further demonstrate its
applicability on two downstream tasks, recommender systems and node
classification.","I.2,cs.CG,cs.LG"
"Depth-Aware Arbitrary Style Transfer Using Instance Normalization. Style transfer is the process of rendering one image with some content in the
style of another image, representing the style. Recent studies of Liu et al.
(2017) show that traditional style transfer methods of Gatys et al. (2016) and
Johnson et al. (2016) fail to reproduce the depth of the content image, which
is critical for human perception. They suggest to preserve the depth map by
additional regularizer in the optimized loss function, forcing preservation of
the depth map. However these traditional methods are either computationally
inefficient or require training a separate neural network for each style. AdaIN
method of Huang et al. (2017) allows efficient transferring of arbitrary style
without training a separate model but is not able to reproduce the depth map of
the content image. We propose an extension to this method, allowing depth map
preservation by applying variable stylization strength. Qualitative analysis
and results of user evaluation study indicate that the proposed method provides
better stylizations, compared to the original AdaIN style transfer method.","68T45,I.4.9; I.4.10,cs.CV,eess.IV"
"Super-resolution of Time-series Labels for Bootstrapped Event Detection. Solving real-world problems, particularly with deep learning, relies on the
availability of abundant, quality data. In this paper we develop a novel
framework that maximises the utility of time-series datasets that contain only
small quantities of expertly-labelled data, larger quantities of weakly (or
coarsely) labelled data and a large volume of unlabelled data. This represents
scenarios commonly encountered in the real world, such as in crowd-sourcing
applications. In our work, we use a nested loop using a Kernel Density
Estimator (KDE) to super-resolve the abundant low-quality data labels, thereby
enabling effective training of a Convolutional Neural Network (CNN). We
demonstrate two key results: a) The KDE is able to super-resolve labels more
accurately, and with better calibrated probabilities, than well-established
classifiers acting as baselines; b) Our CNN, trained on super-resolved labels
from the KDE, achieves an improvement in F1 score of 22.1% over the next best
baseline system in our candidate problem domain.","cs.CV,cs.LG,cs.SD,eess.AS,stat.ML"
"Logically-Constrained Reinforcement Learning. We present the first model-free Reinforcement Learning (RL) algorithm to
synthesise policies for an unknown Markov Decision Process (MDP), such that a
linear time property is satisfied. The given temporal property is converted
into a Limit Deterministic Buchi Automaton (LDBA) and a robust reward function
is defined over the state-action pairs of the MDP according to the resulting
LDBA. With this reward function, the policy synthesis procedure is
""constrained"" by the given specification. These constraints guide the MDP
exploration so as to minimize the solution time by only considering the portion
of the MDP that is relevant to satisfaction of the LTL property. This improves
performance and scalability of the proposed method by avoiding an exhaustive
update over the whole state space while the efficiency of standard methods such
as dynamic programming is hindered by excessive memory requirements, caused by
the need to store a full-model in memory. Additionally, we show that the RL
procedure sets up a local value iteration method to efficiently calculate the
maximum probability of satisfying the given property, at any given state of the
MDP. We prove that our algorithm is guaranteed to find a policy whose traces
probabilistically satisfy the LTL property if such a policy exists, and
additionally we show that our method produces reasonable control policies even
when the LTL property cannot be satisfied. The performance of the algorithm is
evaluated via a set of numerical examples. We observe an improvement of one
order of magnitude in the number of iterations required for the synthesis
compared to existing approaches.","cs.LG,cs.LO"
"BayReL: Bayesian Relational Learning for Multi-omics Data Integration. High-throughput molecular profiling technologies have produced
high-dimensional multi-omics data, enabling systematic understanding of living
systems at the genome scale. Studying molecular interactions across different
data types helps reveal signal transduction mechanisms across different classes
of molecules. In this paper, we develop a novel Bayesian representation
learning method that infers the relational interactions across multi-omics data
types. Our method, Bayesian Relational Learning (BayReL) for multi-omics data
integration, takes advantage of a priori known relationships among the same
class of molecules, modeled as a graph at each corresponding view, to learn
view-specific latent variables as well as a multi-partite graph that encodes
the interactions across views. Our experiments on several real-world datasets
demonstrate enhanced performance of BayReL in inferring meaningful interactions
compared to existing baselines.","cs.LG,q-bio.MN,stat.AP,stat.ML"
"Decision tree heuristics can fail, even in the smoothed setting. Greedy decision tree learning heuristics are mainstays of machine learning
practice, but theoretical justification for their empirical success remains
elusive. In fact, it has long been known that there are simple target functions
for which they fail badly (Kearns and Mansour, STOC 1996).
  Recent work of Brutzkus, Daniely, and Malach (COLT 2020) considered the
smoothed analysis model as a possible avenue towards resolving this disconnect.
Within the smoothed setting and for targets $f$ that are $k$-juntas, they
showed that these heuristics successfully learn $f$ with depth-$k$ decision
tree hypotheses. They conjectured that the same guarantee holds more generally
for targets that are depth-$k$ decision trees.
  We provide a counterexample to this conjecture: we construct targets that are
depth-$k$ decision trees and show that even in the smoothed setting, these
heuristics build trees of depth $2^{\Omega(k)}$ before achieving high accuracy.
We also show that the guarantees of Brutzkus et al. cannot extend to the
agnostic setting: there are targets that are very close to $k$-juntas, for
which these heuristics build trees of depth $2^{\Omega(k)}$ before achieving
high accuracy.","cs.DS,cs.LG,stat.ML"
"A Systematic Assessment of Deep Learning Models for Molecule Generation. In recent years the scientific community has devoted much effort in the
development of deep learning models for the generation of new molecules with
desirable properties (i.e. drugs). This has produced many proposals in
literature. However, a systematic comparison among the different VAE methods is
still missing. For this reason, we propose an extensive testbed for the
evaluation of generative models for drug discovery, and we present the results
obtained by many of the models proposed in literature.","cs.LG,q-bio.QM"
"Reinforcement-Learning-Based Variational Quantum Circuits Optimization for Combinatorial Problems. Quantum computing exploits basic quantum phenomena such as state
superposition and entanglement to perform computations. The Quantum Approximate
Optimization Algorithm (QAOA) is arguably one of the leading quantum algorithms
that can outperform classical state-of-the-art methods in the near term. QAOA
is a hybrid quantum-classical algorithm that combines a parameterized quantum
state evolution with a classical optimization routine to approximately solve
combinatorial problems. The quality of the solution obtained by QAOA within a
fixed budget of calls to the quantum computer depends on the performance of the
classical optimization routine used to optimize the variational parameters. In
this work, we propose an approach based on reinforcement learning (RL) to train
a policy network that can be used to quickly find high-quality variational
parameters for unseen combinatorial problem instances. The RL agent is trained
on small problem instances which can be simulated on a classical computer, yet
the learned RL policy is generalizable and can be used to efficiently solve
larger instances. Extensive simulations using the IBM Qiskit Aer quantum
circuit simulator demonstrate that our trained RL policy can reduce the
optimality gap by a factor up to 8.61 compared with other off-the-shelf
optimizers tested.","cs.LG,quant-ph,stat.ML"
"A Nonlocal Denoising Algorithm for Manifold-Valued Images Using Second Order Statistics. Nonlocal patch-based methods, in particular the Bayes' approach of Lebrun,
Buades and Morel (2013), are considered as state-of-the-art methods for
denoising (color) images corrupted by white Gaussian noise of moderate
variance. This paper is the first attempt to generalize this technique to
manifold-valued images. Such images, for example images with phase or
directional entries or with values in the manifold of symmetric positive
definite matrices, are frequently encountered in real-world applications.
Generalizing the normal law to manifolds is not canonical and different
attempts have been considered. Here we focus on a straightforward intrinsic
model and discuss the relation to other approaches for specific manifolds. We
reinterpret the Bayesian approach of Lebrun et al. (2013) in terms of minimum
mean squared error estimation, which motivates our definition of a
corresponding estimator on the manifold. With this estimator at hand we present
a nonlocal patch-based method for the restoration of manifold-valued images.
Various proof of concept examples demonstrate the potential of the proposed
algorithm.","cs.CV,math.NA"
"Provably Convergent Policy Gradient Methods for Model-Agnostic Meta-Reinforcement Learning. We consider Model-Agnostic Meta-Learning (MAML) methods for Reinforcement
Learning (RL) problems where the goal is to find a policy using data from
several tasks represented by Markov Decision Processes (MDPs) that can be
updated by one step of stochastic policy gradient for the realized MDP. In
particular, using stochastic gradients in MAML update step is crucial for RL
problems since computation of exact gradients requires access to a large number
of possible trajectories. For this formulation, we propose a variant of the
MAML method, named Stochastic Gradient Meta-Reinforcement Learning (SG-MRL),
and study its convergence properties. We derive the iteration and sample
complexity of SG-MRL to find an $\epsilon$-first-order stationary point, which,
to the best of our knowledge, provides the first convergence guarantee for
model-agnostic meta-reinforcement learning algorithms. We further show how our
results extend to the case where more than one step of stochastic policy
gradient method is used in the update during the test time.","cs.LG,math.OC,stat.ML"
"Bayesian bandits: balancing the exploration-exploitation tradeoff via double sampling. Reinforcement learning studies how to balance exploration and exploitation in
real-world systems, optimizing interactions with the world while simultaneously
learning how the world operates. One general class of algorithms for such
learning is the multi-armed bandit setting. Randomized probability matching,
based upon the Thompson sampling approach introduced in the 1930s, has recently
been shown to perform well and to enjoy provable optimality properties. It
permits generative, interpretable modeling in a Bayesian setting, where prior
knowledge is incorporated, and the computed posteriors naturally capture the
full state of knowledge. In this work, we harness the information contained in
the Bayesian posterior and estimate its sufficient statistics via sampling. In
several application domains, for example in health and medicine, each
interaction with the world can be expensive and invasive, whereas drawing
samples from the model is relatively inexpensive. Exploiting this viewpoint, we
develop a double sampling technique driven by the uncertainty in the learning
process: it favors exploitation when certain about the properties of each arm,
exploring otherwise. The proposed algorithm does not make any distributional
assumption and it is applicable to complex reward distributions, as long as
Bayesian posterior updates are computable. Utilizing the estimated posterior
sufficient statistics, double sampling autonomously balances the
exploration-exploitation tradeoff to make better informed decisions. We
empirically show its reduced cumulative regret when compared to
state-of-the-art alternatives in representative bandit settings.","I.2.6,cs.LG,stat.CO,stat.ML"
"Structure and Performance of Fully Connected Neural Networks: Emerging Complex Network Properties. Understanding the behavior of Artificial Neural Networks is one of the main
topics in the field recently, as black-box approaches have become usual since
the widespread of deep learning. Such high-dimensional models may manifest
instabilities and weird properties that resemble complex systems. Therefore, we
propose Complex Network (CN) techniques to analyze the structure and
performance of fully connected neural networks. For that, we build a dataset
with 4 thousand models and their respective CN properties. They are employed in
a supervised classification setup considering four vision benchmarks. Each
neural network is approached as a weighted and undirected graph of neurons and
synapses, and centrality measures are computed after training. Results show
that these measures are highly related to the network classification
performance. We also propose the concept of Bag-Of-Neurons (BoN), a CN-based
approach for finding topological signatures linking similar neurons. Results
suggest that six neuronal types emerge in such networks, independently of the
target domain, and are distributed differently according to classification
accuracy. We also tackle specific CN properties related to performance, such as
higher subgraph centrality on lower-performing models. Our findings suggest
that CN properties play a critical role in the performance of fully connected
neural networks, with topological patterns emerging independently on a wide
range of models.","68T07,I.2.6; I.5.4; J.2,cs.AI,cs.CV,cs.LG,physics.app-ph,physics.comp-ph"
"A Complete Characterization of Projectivity for Statistical Relational Models. A generative probabilistic model for relational data consists of a family of
probability distributions for relational structures over domains of different
sizes. In most existing statistical relational learning (SRL) frameworks, these
models are not projective in the sense that the marginal of the distribution
for size-$n$ structures on induced sub-structures of size $k<n$ is equal to the
given distribution for size-$k$ structures. Projectivity is very beneficial in
that it directly enables lifted inference and statistically consistent learning
from sub-sampled relational structures. In earlier work some simple fragments
of SRL languages have been identified that represent projective models.
However, no complete characterization of, and representation framework for
projective models has been given. In this paper we fill this gap: exploiting
representation theorems for infinite exchangeable arrays we introduce a class
of directed graphical latent variable models that precisely correspond to the
class of projective relational models. As a by-product we also obtain a
characterization for when a given distribution over size-$k$ structures is the
statistical frequency distribution of size-$k$ sub-structures in much larger
size-$n$ structures. These results shed new light onto the old open problem of
how to apply Halpern et al.'s ""random worlds approach"" for probabilistic
inference to general relational signatures.","60G09,I.2.6,cs.AI,cs.LG,stat.ML"
"Asynchronous Convolutional Networks for Object Detection in Neuromorphic Cameras. Event-based cameras, also known as neuromorphic cameras, are bioinspired
sensors able to perceive changes in the scene at high frequency with low power
consumption. Becoming available only very recently, a limited amount of work
addresses object detection on these devices. In this paper we propose two
neural networks architectures for object detection: YOLE, which integrates the
events into surfaces and uses a frame-based model to process them, and fcYOLE,
an asynchronous event-based fully convolutional network which uses a novel and
general formalization of the convolutional and max pooling layers to exploit
the sparsity of camera events. We evaluate the algorithm with different
extensions of publicly available datasets and on a novel synthetic dataset.","68T45,cs.CV"
"Building a Foundation for Data-Driven, Interpretable, and Robust Policy Design using the AI Economist. Optimizing economic and public policy is critical to address socioeconomic
issues and trade-offs, e.g., improving equality, productivity, or wellness, and
poses a complex mechanism design problem. A policy designer needs to consider
multiple objectives, policy levers, and behavioral responses from strategic
actors who optimize for their individual objectives. Moreover, real-world
policies should be explainable and robust to simulation-to-reality gaps, e.g.,
due to calibration issues. Existing approaches are often limited to a narrow
set of policy levers or objectives that are hard to measure, do not yield
explicit optimal policies, or do not consider strategic behavior, for example.
Hence, it remains challenging to optimize policy in real-world scenarios. Here
we show that the AI Economist framework enables effective, flexible, and
interpretable policy design using two-level reinforcement learning (RL) and
data-driven simulations. We validate our framework on optimizing the stringency
of US state policies and Federal subsidies during a pandemic, e.g., COVID-19,
using a simulation fitted to real data. We find that log-linear policies
trained using RL significantly improve social welfare, based on both public
health and economic outcomes, compared to past outcomes. Their behavior can be
explained, e.g., well-performing policies respond strongly to changes in
recovery and vaccination rates. They are also robust to calibration errors,
e.g., infection rates that are over or underestimated. As of yet, real-world
policymaking has not seen adoption of machine learning methods at large,
including RL and AI-driven simulations. Our results show the potential of AI to
guide policy design and improve social welfare amidst the complexity of the
real world.","cs.AI,cs.LG,cs.MA,econ.EM,econ.GN,q-fin.EC"
"Data Engineering for the Analysis of Semiconductor Manufacturing Data. We have analyzed manufacturing data from several different semiconductor
manufacturing plants, using decision tree induction software called Q-YIELD.
The software generates rules for predicting when a given product should be
rejected. The rules are intended to help the process engineers improve the
yield of the product, by helping them to discover the causes of rejection.
Experience with Q-YIELD has taught us the importance of data engineering --
preprocessing the data to enable or facilitate decision tree induction. This
paper discusses some of the data engineering problems we have encountered with
semiconductor manufacturing data. The paper deals with two broad classes of
problems: engineering the features in a feature vector representation and
engineering the definition of the target concept (the classes). Manufacturing
process data present special problems for feature engineering, since the data
have multiple levels of granularity (detail, resolution). Engineering the
target concept is important, due to our focus on understanding the past, as
opposed to the more common focus in machine learning on predicting the future.","I.2.6; I.5.2; I.5.4; J.2,cs.CE,cs.CV,cs.LG"
"Densely connected neural networks for nonlinear regression. Densely connected convolutional networks (DenseNet) behave well in image
processing. However, for regression tasks, convolutional DenseNet may lose
essential information from independent input features. To tackle this issue, we
propose a novel DenseNet regression model where convolution and pooling layers
are replaced by fully connected layers and the original concatenation shortcuts
are maintained to reuse the feature. To investigate the effects of depth and
input dimension of proposed model, careful validations are performed by
extensive numerical simulation. The results give an optimal depth (19) and
recommend a limited input dimension (under 200). Furthermore, compared with the
baseline models including support vector regression, decision tree regression,
and residual regression, our proposed model with the optimal depth performs
best. Ultimately, DenseNet regression is applied to predict relative humidity,
and the outcome shows a high correlation (0.91) with observations, which
indicates that our model could advance environmental data analysis.","cs.LG,physics.ao-ph,physics.data-an,stat.ME"
"CURL: Co-trained Unsupervised Representation Learning for Image Classification. In this paper we propose a strategy for semi-supervised image classification
that leverages unsupervised representation learning and co-training. The
strategy, that is called CURL from Co-trained Unsupervised Representation
Learning, iteratively builds two classifiers on two different views of the
data. The two views correspond to different representations learned from both
labeled and unlabeled data and differ in the fusion scheme used to combine the
image features. To assess the performance of our proposal, we conducted several
experiments on widely used data sets for scene and object recognition. We
considered three scenarios (inductive, transductive and self-taught learning)
that differ in the strategy followed to exploit the unlabeled data. As image
features we considered a combination of GIST, PHOG, and LBP as well as features
extracted from a Convolutional Neural Network. Moreover, two embodiments of
CURL are investigated: one using Ensemble Projection as unsupervised
representation learning coupled with Logistic Regression, and one based on
LapSVM. The results show that CURL clearly outperforms other supervised and
semi-supervised learning methods in the state of the art.","I.2.6,cs.CV,cs.LG,stat.ML"
"Input Convex Neural Networks. This paper presents the input convex neural network architecture. These are
scalar-valued (potentially deep) neural networks with constraints on the
network parameters such that the output of the network is a convex function of
(some of) the inputs. The networks allow for efficient inference via
optimization over some inputs to the network given others, and can be applied
to settings including structured prediction, data imputation, reinforcement
learning, and others. In this paper we lay the basic groundwork for these
models, proposing methods for inference, optimization and learning, and analyze
their representational power. We show that many existing neural network
architectures can be made input-convex with a minor modification, and develop
specialized optimization algorithms tailored to this setting. Finally, we
highlight the performance of the methods on multi-label prediction, image
completion, and reinforcement learning problems, where we show improvement over
the existing state of the art in many cases.","cs.LG,math.OC"
"Better sampling in explanation methods can prevent dieselgate-like deception. Machine learning models are used in many sensitive areas where besides
predictive accuracy their comprehensibility is also important. Interpretability
of prediction models is necessary to determine their biases and causes of
errors, and is a necessary prerequisite for users' confidence. For complex
state-of-the-art black-box models post-hoc model-independent explanation
techniques are an established solution. Popular and effective techniques, such
as IME, LIME, and SHAP, use perturbation of instance features to explain
individual predictions. Recently, Slack et al. (2020) put their robustness into
question by showing that their outcomes can be manipulated due to poor
perturbation sampling employed. This weakness would allow dieselgate type
cheating of owners of sensitive models who could deceive inspection and hide
potentially unethical or illegal biases existing in their predictive models.
This could undermine public trust in machine learning models and give rise to
legal restrictions on their use.
  We show that better sampling in these explanation methods prevents malicious
manipulations. The proposed sampling uses data generators that learn the
training set distribution and generate new perturbation instances much more
similar to the training set. We show that the improved sampling increases the
robustness of the LIME and SHAP, while previously untested method IME is
already the most robust of all.","cs.AI,cs.CR,cs.LG"
"SmartPaste: Learning to Adapt Source Code. Deep Neural Networks have been shown to succeed at a range of natural
language tasks such as machine translation and text summarization. While tasks
on source code (ie, formal languages) have been considered recently, most work
in this area does not attempt to capitalize on the unique opportunities offered
by its known syntax and structure. In this work, we introduce SmartPaste, a
first task that requires to use such information. The task is a variant of the
program repair problem that requires to adapt a given (pasted) snippet of code
to surrounding, existing source code. As first solutions, we design a set of
deep neural models that learn to represent the context of each variable
location and variable usage in a data flow-sensitive way. Our evaluation
suggests that our models can learn to solve the SmartPaste task in many cases,
achieving 58.6% accuracy, while learning meaningful representation of variable
usages.","cs.LG,cs.SE"
"Deceiving End-to-End Deep Learning Malware Detectors using Adversarial Examples. In recent years, deep learning has shown performance breakthroughs in many
applications, such as image detection, image segmentation, pose estimation, and
speech recognition. However, this comes with a major concern: deep networks
have been found to be vulnerable to adversarial examples. Adversarial examples
are slightly modified inputs that are intentionally designed to cause a
misclassification by the model. In the domains of images and speech, the
modifications are so small that they are not seen or heard by humans, but
nevertheless greatly affect the classification of the model.
  Deep learning models have been successfully applied to malware detection. In
this domain, generating adversarial examples is not straightforward, as small
modifications to the bytes of the file could lead to significant changes in its
functionality and validity. We introduce a novel loss function for generating
adversarial examples specifically tailored for discrete input sets, such as
executable bytes. We modify malicious binaries so that they would be detected
as benign, while preserving their original functionality, by injecting a small
sequence of bytes (payload) in the binary file. We applied this approach to an
end-to-end convolutional deep learning malware detection model and show a high
rate of detection evasion. Moreover, we show that our generated payload is
robust enough to be transferable within different locations of the same file
and across different files, and that its entropy is low and similar to that of
benign data sections.","cs.CR,cs.LG"
"Permutation invariant networks to learn Wasserstein metrics. Understanding the space of probability measures on a metric space equipped
with a Wasserstein distance is one of the fundamental questions in mathematical
analysis. The Wasserstein metric has received a lot of attention in the machine
learning community especially for its principled way of comparing
distributions. In this work, we use a permutation invariant network to map
samples from probability measures into a low-dimensional space such that the
Euclidean distance between the encoded samples reflects the Wasserstein
distance between probability measures. We show that our network can generalize
to correctly compute distances between unseen densities. We also show that
these networks can learn the first and the second moments of probability
distributions.","cs.LG,math.PR,stat.ML"
"COUnty aggRegation mixup AuGmEntation (COURAGE) COVID-19 Prediction. The global spread of COVID-19, the disease caused by the novel coronavirus
SARS-CoV-2, has cast a significant threat to mankind. As the COVID-19 situation
continues to evolve, predicting localized disease severity is crucial for
advanced resource allocation. This paper proposes a method named COURAGE
(COUnty aggRegation mixup AuGmEntation) to generate a short-term prediction of
2-week-ahead COVID-19 related deaths for each county in the United States,
leveraging modern deep learning techniques. Specifically, our method adopts a
self-attention model from Natural Language Processing, known as the transformer
model, to capture both short-term and long-term dependencies within the time
series while enjoying computational efficiency. Our model fully utilizes
publicly available information of COVID-19 related confirmed cases, deaths,
community mobility trends and demographic information, and can produce
state-level prediction as an aggregation of the corresponding county-level
predictions. Our numerical experiments demonstrate that our model achieves the
state-of-the-art performance among the publicly available benchmark models.","cs.LG,stat.AP"
"Evaluation of Predictive Data Mining Algorithms in Erythemato-Squamous Disease Diagnosis. A lot of time is spent searching for the most performing data mining
algorithms applied in clinical diagnosis. The study set out to identify the
most performing predictive data mining algorithms applied in the diagnosis of
Erythemato-squamous diseases. The study used Naive Bayes, Multilayer Perceptron
and J48 decision tree induction to build predictive data mining models on 366
instances of Erythemato-squamous diseases datasets. Also, 10-fold
cross-validation and sets of performance metrics were used to evaluate the
baseline predictive performance of the classifiers. The comparative analysis
shows that the Naive Bayes performed best with accuracy of 97.4%, Multilayer
Perceptron came out second with accuracy of 96.6%, and J48 came out the worst
with accuracy of 93.5%. The evaluation of these classifiers on clinical
datasets, gave an insight into the predictive ability of different data mining
algorithms applicable in clinical diagnosis especially in the diagnosis of
Erythemato-squamous diseases.","cs.CE,cs.LG"
"Parameter Priors for Directed Acyclic Graphical Models and the Characterization of Several Probability Distributions. We develop simple methods for constructing parameter priors for model choice
among Directed Acyclic Graphical (DAG) models. In particular, we introduce
several assumptions that permit the construction of parameter priors for a
large number of DAG models from a small set of assessments. We then present a
method for directly computing the marginal likelihood of every DAG model given
a random sample with no missing observations. We apply this methodology to
Gaussian DAG models which consist of a recursive set of linear regression
models. We show that the only parameter prior for complete Gaussian DAG models
that satisfies our assumptions is the normal-Wishart distribution. Our analysis
is based on the following new characterization of the Wishart distribution: let
$W$ be an $n \times n$, $n \ge 3$, positive-definite symmetric matrix of random
variables and $f(W)$ be a pdf of $W$. Then, f$(W)$ is a Wishart distribution if
and only if $W_{11} - W_{12} W_{22}^{-1} W'_{12}$ is independent of
$\{W_{12},W_{22}\}$ for every block partitioning $W_{11},W_{12}, W'_{12},
W_{22}$ of $W$. Similar characterizations of the normal and normal-Wishart
distributions are provided as well.","I.2; G.3,cs.LG,math.ST,stat.ML,stat.TH"
"Option Encoder: A Framework for Discovering a Policy Basis in Reinforcement Learning. Option discovery and skill acquisition frameworks are integral to the
functioning of a Hierarchically organized Reinforcement learning agent.
However, such techniques often yield a large number of options or skills, which
can potentially be represented succinctly by filtering out any redundant
information. Such a reduction can reduce the required computation while also
improving the performance on a target task. In order to compress an array of
option policies, we attempt to find a policy basis that accurately captures the
set of all options. In this work, we propose Option Encoder, an auto-encoder
based framework with intelligently constrained weights, that helps discover a
collection of basis policies. The policy basis can be used as a proxy for the
original set of skills in a suitable hierarchically organized framework. We
demonstrate the efficacy of our method on a collection of grid-worlds and on
the high-dimensional Fetch-Reach robotic manipulation task by evaluating the
obtained policy basis on a set of downstream tasks.","cs.AI,cs.LG,stat.ML"
"Fast Global Convergence of Natural Policy Gradient Methods with Entropy Regularization. Natural policy gradient (NPG) methods are among the most widely used policy
optimization algorithms in contemporary reinforcement learning. This class of
methods is often applied in conjunction with entropy regularization -- an
algorithmic scheme that encourages exploration -- and is closely related to
soft policy iteration and trust region policy optimization. Despite the
empirical success, the theoretical underpinnings for NPG methods remain limited
even for the tabular setting. This paper develops $\textit{non-asymptotic}$
convergence guarantees for entropy-regularized NPG methods under softmax
parameterization, focusing on discounted Markov decision processes (MDPs).
Assuming access to exact policy evaluation, we demonstrate that the algorithm
converges linearly -- or even quadratically once it enters a local region
around the optimal policy -- when computing optimal value functions of the
regularized MDP. Moreover, the algorithm is provably stable vis-\`a-vis
inexactness of policy evaluation. Our convergence results accommodate a wide
range of learning rates, and shed light upon the role of entropy regularization
in enabling fast convergence.","cs.IT,cs.LG,math.IT,math.OC,stat.ML"
"AlignNet: A Unifying Approach to Audio-Visual Alignment. We present AlignNet, a model that synchronizes videos with reference audios
under non-uniform and irregular misalignments. AlignNet learns the end-to-end
dense correspondence between each frame of a video and an audio. Our method is
designed according to simple and well-established principles: attention,
pyramidal processing, warping, and affinity function. Together with the model,
we release a dancing dataset Dance50 for training and evaluation. Qualitative,
quantitative and subjective evaluation results on dance-music alignment and
speech-lip alignment demonstrate that our method far outperforms the
state-of-the-art methods. Project video and code are available at
https://jianrenw.github.io/AlignNet.","cs.CV,cs.LG,cs.MM,cs.SD,eess.AS"
"Sample-Efficient Reinforcement Learning through Transfer and Architectural Priors. Recent work in deep reinforcement learning has allowed algorithms to learn
complex tasks such as Atari 2600 games just from the reward provided by the
game, but these algorithms presently require millions of training steps in
order to learn, making them approximately five orders of magnitude slower than
humans. One reason for this is that humans build robust shared representations
that are applicable to collections of problems, making it much easier to
assimilate new variants. This paper first introduces the idea of
automatically-generated game sets to aid in transfer learning research, and
then demonstrates the utility of shared representations by showing that models
can substantially benefit from the incorporation of relevant architectural
priors. This technique affords a remarkable 50x positive transfer on a toy
problem-set.","68T05,cs.AI,cs.LG"
"Tensor Fields for Data Extraction from Chart Images: Bar Charts and Scatter Plots. Charts are an essential part of both graphicacy (graphical literacy), and
statistical literacy. As chart understanding has become increasingly relevant
in data science, automating chart analysis by processing raster images of the
charts has become a significant problem. Automated chart reading involves data
extraction and contextual understanding of the data from chart images. In this
paper, we perform the first step of determining the computational model of
chart images for data extraction for selected chart types, namely, bar charts,
and scatter plots. We demonstrate the use of positive semidefinite second-order
tensor fields as an effective model. We identify an appropriate tensor field as
the model and propose a methodology for the use of its degenerate point
extraction for data extraction from chart images. Our results show that tensor
voting is effective for data extraction from bar charts and scatter plots, and
histograms, as a special case of bar charts.","68U05,68U10,cs.CV,cs.GR,cs.NA,math.NA"
"Distributed Learning in Wireless Networks: Recent Progress and Future Challenges. The next-generation of wireless networks will enable many machine learning
(ML) tools and applications to efficiently analyze various types of data
collected by edge devices for inference, autonomy, and decision making
purposes. However, due to resource constraints, delay limitations, and privacy
challenges, edge devices cannot offload their entire collected datasets to a
cloud server for centrally training their ML models or inference purposes. To
overcome these challenges, distributed learning and inference techniques have
been proposed as a means to enable edge devices to collaboratively train ML
models without raw data exchanges, thus reducing the communication overhead and
latency as well as improving data privacy. However, deploying distributed
learning over wireless networks faces several challenges including the
uncertain wireless environment, limited wireless resources (e.g., transmit
power and radio spectrum), and hardware resources. This paper provides a
comprehensive study of how distributed learning can be efficiently and
effectively deployed over wireless edge networks. We present a detailed
overview of several emerging distributed learning paradigms, including
federated learning, federated distillation, distributed inference, and
multi-agent reinforcement learning. For each learning framework, we first
introduce the motivation for deploying it over wireless networks. Then, we
present a detailed literature review on the use of communication techniques for
its efficient deployment. We then introduce an illustrative example to show how
to optimize wireless networks to improve its performance. Finally, we introduce
future research opportunities. In a nutshell, this paper provides a holistic
set of guidelines on how to deploy a broad range of distributed learning
frameworks over real-world wireless communication networks.","cs.IT,cs.LG,math.IT"
"Pyramidal Gradient Matching for Optical Flow Estimation. Initializing optical flow field by either sparse descriptor matching or dense
patch matches has been proved to be particularly useful for capturing large
displacements. In this paper, we present a pyramidal gradient matching approach
that can provide dense matches for highly accurate and efficient optical flow
estimation. A novel contribution of our method is that image gradient is used
to describe image patches and proved to be able to produce robust matching.
Therefore, our method is more efficient than methods that adopt special
features (like SIFT) or patch distance metric. Moreover, we find that image
gradient is scalable for optical flow estimation, which means we can use
different levels of gradient feature (for example, full gradients or only
direction information of gradients) to obtain different complexity without
dramatic changes in accuracy. Another contribution is that we uncover the
secrets of limited PatchMatch through a thorough analysis and design a
pyramidal matching framework based these secrets. Our pyramidal matching
framework is aimed at robust gradient matching and effective to grow inliers
and reject outliers. In this framework, we present some special enhancements
for outlier filtering in gradient matching. By initializing EpicFlow with our
matches, experimental results show that our method is efficient and robust
(ranking 1st on both clean pass and final pass of MPI Sintel dataset among
published methods).",cs.CV
"Center-based 3D Object Detection and Tracking. Three-dimensional objects are commonly represented as 3D boxes in a
point-cloud. This representation mimics the well-studied image-based 2D
bounding-box detection but comes with additional challenges. Objects in a 3D
world do not follow any particular orientation, and box-based detectors have
difficulties enumerating all orientations or fitting an axis-aligned bounding
box to rotated objects. In this paper, we instead propose to represent, detect,
and track 3D objects as points. Our framework, CenterPoint, first detects
centers of objects using a keypoint detector and regresses to other attributes,
including 3D size, 3D orientation, and velocity. In a second stage, it refines
these estimates using additional point features on the object. In CenterPoint,
3D object tracking simplifies to greedy closest-point matching. The resulting
detection and tracking algorithm is simple, efficient, and effective.
CenterPoint achieved state-of-the-art performance on the nuScenes benchmark for
both 3D detection and tracking, with 65.5 NDS and 63.8 AMOTA for a single
model. On the Waymo Open Dataset, CenterPoint outperforms all previous single
model method by a large margin and ranks first among all Lidar-only
submissions. The code and pretrained models are available at
https://github.com/tianweiy/CenterPoint.",cs.CV
"Machine Learning for Naval Architecture, Ocean and Marine Engineering. Machine Learning (ML) based algorithms have found significant impact in many
fields of engineering and sciences, where datasets are available from
experiments and high fidelity numerical simulations. Those datasets are
generally utilized in a machine learning model to extract information about the
underlying physics and derive functional relationships mapping input variables
to target quantities of interest. Commonplace machine learning algorithms
utilized in Scientific Machine Learning (SciML) include neural networks,
regression trees, random forests, support vector machines, etc. The focus of
this article is to review the applications of ML in naval architecture, ocean,
and marine engineering problems; and identify priority directions of research.
We discuss the applications of machine learning algorithms for different
problems such as wave height prediction, calculation of wind loads on ships,
damage detection of offshore platforms, calculation of ship added resistance,
and various other applications in coastal and marine environments. The details
of the data sets including the source of data-sets utilized in the ML model
development are included. The features used as the inputs to the ML models are
presented in detail and finally, the methods employed in optimization of the ML
models were also discussed. Based on this comprehensive analysis we point out
future directions of research that may be fruitful for the application of ML to
the ocean and marine engineering problems.","cs.LG,physics.ao-ph,physics.data-an"
"Variational mean-field theory for training restricted Boltzmann machines with binary synapses. Unsupervised learning requiring only raw data is not only a fundamental
function of the cerebral cortex, but also a foundation for a next generation of
artificial neural networks. However, a unified theoretical framework to treat
sensory inputs, synapses and neural activity together is still lacking. The
computational obstacle originates from the discrete nature of synapses, and
complex interactions among these three essential elements of learning. Here, we
propose a variational mean-field theory in which the distribution of synaptic
weights is considered. The unsupervised learning can then be decomposed into
two intertwined steps: a maximization step is carried out as a gradient ascent
of the lower-bound on the data log-likelihood, in which the synaptic weight
distribution is determined by updating variational parameters, and an
expectation step is carried out as a message passing procedure on an equivalent
or dual neural network whose parameter is specified by the variational
parameters of the weight distribution. Therefore, our framework provides
insights on how data (or sensory inputs), synapses and neural activities
interact with each other to achieve the goal of extracting statistical
regularities in sensory inputs. This variational framework is verified in
restricted Boltzmann machines with planted synaptic weights and
handwritten-digits learning.","cond-mat.dis-nn,cs.LG,cs.NE,q-bio.NC,stat.ML"
"Scalable Factorized Hierarchical Variational Autoencoder Training. Deep generative models have achieved great success in unsupervised learning
with the ability to capture complex nonlinear relationships between latent
generating factors and observations. Among them, a factorized hierarchical
variational autoencoder (FHVAE) is a variational inference-based model that
formulates a hierarchical generative process for sequential data. Specifically,
an FHVAE model can learn disentangled and interpretable representations, which
have been proven useful for numerous speech applications, such as speaker
verification, robust speech recognition, and voice conversion. However, as we
will elaborate in this paper, the training algorithm proposed in the original
paper is not scalable to datasets of thousands of hours, which makes this model
less applicable on a larger scale. After identifying limitations in terms of
runtime, memory, and hyperparameter optimization, we propose a hierarchical
sampling training algorithm to address all three issues. Our proposed method is
evaluated comprehensively on a wide variety of datasets, ranging from 3 to
1,000 hours and involving different types of generating factors, such as
recording conditions and noise types. In addition, we also present a new
visualization method for qualitatively evaluating the performance with respect
to the interpretability and disentanglement. Models trained with our proposed
algorithm demonstrate the desired characteristics on all the datasets.","cs.CL,cs.LG,cs.SD,eess.AS,stat.ML"
"Complex-valued neural networks for machine learning on non-stationary physical data. Deep learning has become an area of interest in most scientific areas,
including physical sciences. Modern networks apply real-valued transformations
on the data. Particularly, convolutions in convolutional neural networks
discard phase information entirely. Many deterministic signals, such as seismic
data or electrical signals, contain significant information in the phase of the
signal. We explore complex-valued deep convolutional networks to leverage
non-linear feature maps. Seismic data commonly has a lowcut filter applied, to
attenuate noise from ocean waves and similar long wavelength contributions.
Discarding the phase information leads to low-frequency aliasing analogous to
the Nyquist-Shannon theorem for high frequencies. In non-stationary data, the
phase content can stabilize training and improve the generalizability of neural
networks. While it has been shown that phase content can be restored in deep
neural networks, we show how including phase information in feature maps
improves both training and inference from deterministic physical data.
Furthermore, we show that the reduction of parameters in a complex network
outperforms larger real-valued networks.","cs.CV,cs.LG,physics.comp-ph,physics.geo-ph,stat.ML"
"Multi-agent Reinforcement Learning for Decentralized Stable Matching. In the real world, people/entities usually find matches independently and
autonomously, such as finding jobs, partners, roommates, etc. It is possible
that this search for matches starts with no initial knowledge of the
environment. We propose the use of a multi-agent reinforcement learning (MARL)
paradigm for a spatially formulated decentralized two-sided matching market
with independent and autonomous agents. Having autonomous agents acting
independently makes our environment very dynamic and uncertain. Moreover,
agents lack the knowledge of preferences of other agents and have to explore
the environment and interact with other agents to discover their own
preferences through noisy rewards. We think such a setting better approximates
the real world and we study the usefulness of our MARL approach for it. Along
with conventional stable matching case where agents have strictly ordered
preferences, we check the applicability of our approach for stable matching
with incomplete lists and ties. We investigate our results for stability, level
of instability (for unstable results), and fairness. Our MARL approach mostly
yields stable and fair outcomes.","cs.AI,cs.LG,cs.MA"
"Graduate Employment Prediction with Bias. The failure of landing a job for college students could cause serious social
consequences such as drunkenness and suicide. In addition to academic
performance, unconscious biases can become one key obstacle for hunting jobs
for graduating students. Thus, it is necessary to understand these unconscious
biases so that we can help these students at an early stage with more
personalized intervention. In this paper, we develop a framework, i.e., MAYA
(Multi-mAjor emploYment stAtus) to predict students' employment status while
considering biases. The framework consists of four major components. Firstly,
we solve the heterogeneity of student courses by embedding academic performance
into a unified space. Then, we apply a generative adversarial network (GAN) to
overcome the class imbalance problem. Thirdly, we adopt Long Short-Term Memory
(LSTM) with a novel dropout mechanism to comprehensively capture sequential
information among semesters. Finally, we design a bias-based regularization to
capture the job market biases. We conduct extensive experiments on a
large-scale educational dataset and the results demonstrate the effectiveness
of our prediction framework.","cs.CY,cs.LG,stat.ML"
"A Convenient Infinite Dimensional Framework for Generative Adversarial Learning. In recent years, generative adversarial networks (GANs) have demonstrated
impressive experimental results while there are only a few works that foster
statistical learning theory for GANs. In this work, we propose an infinite
dimensional theoretical framework for generative adversarial learning. Assuming
the class of uniformly bounded $k$-times $\alpha$-H\""older differentiable and
uniformly positive densities, we show that the Rosenblatt transformation
induces an optimal generator, which is realizable in the hypothesis space of
$\alpha$-H\""older differentiable generators. With a consistent definition of
the hypothesis space of discriminators, we further show that in our framework
the Jensen-Shannon divergence between the distribution induced by the generator
from the adversarial learning procedure and the data generating distribution
converges to zero. Under sufficiently strict regularity assumptions on the
density of the data generating process, we also provide rates of convergence
based on concentration and chaining.","62G20,68T05,cs.LG,math.ST,stat.TH"
"A Literature Study of Embeddings on Source Code. Natural language processing has improved tremendously after the success of
word embedding techniques such as word2vec. Recently, the same idea has been
applied on source code with encouraging results. In this survey, we aim to
collect and discuss the usage of word embedding techniques on programs and
source code. The articles in this survey have been collected by asking authors
of related work and with an extensive search on Google Scholar. Each article is
categorized into five categories: 1. embedding of tokens 2. embedding of
functions or methods 3. embedding of sequences or sets of method calls 4.
embedding of binary code 5. other embeddings. We also provide links to
experimental data and show some remarkable visualization of code embeddings. In
summary, word embedding has been successfully applied on different
granularities of source code. With access to countless open-source
repositories, we see a great potential of applying other data-driven natural
language processing techniques on source code in the future.","cs.LG,cs.PL,cs.SE,stat.ML"
"AdaSpring: Context-adaptive and Runtime-evolutionary Deep Model Compression for Mobile Applications. There are many deep learning (e.g., DNN) powered mobile and wearable
applications today continuously and unobtrusively sensing the ambient
surroundings to enhance all aspects of human lives. To enable robust and
private mobile sensing, DNN tends to be deployed locally on the
resource-constrained mobile devices via model compression. The current practice
either hand-crafted DNN compression techniques, i.e., for optimizing
DNN-relative performance (e.g., parameter size), or on-demand DNN compression
methods, i.e., for optimizing hardware-dependent metrics (e.g., latency),
cannot be locally online because they require offline retraining to ensure
accuracy. Also, none of them have correlated their efforts with runtime
adaptive compression to consider the dynamic nature of the deployment context
of mobile applications. To address those challenges, we present AdaSpring, a
context-adaptive and self-evolutionary DNN compression framework. It enables
the runtime adaptive DNN compression locally online. Specifically, it presents
the ensemble training of a retraining-free and self-evolutionary network to
integrate multiple alternative DNN compression configurations (i.e., compressed
architectures and weights). It then introduces the runtime search strategy to
quickly search for the most suitable compression configurations and evolve the
corresponding weights. With evaluation on five tasks across three platforms and
a real-world case study, experiment outcomes show that AdaSpring obtains up to
3.1x latency reduction, 4.2 x energy efficiency improvement in DNNs, compared
to hand-crafted compression techniques, while only incurring <= 6.2ms
runtime-evolution latency.","cs.LG,cs.NI"
"A Group-Theoretic Framework for Data Augmentation. Data augmentation is a widely used trick when training deep neural networks:
in addition to the original data, properly transformed data are also added to
the training set. However, to the best of our knowledge, a clear mathematical
framework to explain the performance benefits of data augmentation is not
available. In this paper, we develop such a theoretical framework. We show data
augmentation is equivalent to an averaging operation over the orbits of a
certain group that keeps the data distribution approximately invariant. We
prove that it leads to variance reduction. We study empirical risk
minimization, and the examples of exponential families, linear regression, and
certain two-layer neural networks. We also discuss how data augmentation could
be used in problems with symmetry where other approaches are prevalent, such as
in cryo-electron microscopy (cryo-EM).","cs.LG,math.ST,stat.ML,stat.TH"
"Development of a Soft Actor Critic Deep Reinforcement Learning Approach for Harnessing Energy Flexibility in a Large Office Building. This research is concerned with the novel application and investigation of
`Soft Actor Critic' (SAC) based Deep Reinforcement Learning (DRL) to control
the cooling setpoint (and hence cooling loads) of a large commercial building
to harness energy flexibility. The research is motivated by the challenge
associated with the development and application of conventional model-based
control approaches at scale to the wider building stock. SAC is a model-free
DRL technique that is able to handle continuous action spaces and which has
seen limited application to real-life or high-fidelity simulation
implementations in the context of automated and intelligent control of building
energy systems. Such control techniques are seen as one possible solution to
supporting the operation of a smart, sustainable and future electrical grid.
This research tests the suitability of the SAC DRL technique through training
and deployment of the agent on an EnergyPlus based environment of the office
building. The SAC DRL was found to learn an optimal control policy that was
able to minimise energy costs by 9.7% compared to the default rule-based
control (RBC) scheme and was able to improve or maintain thermal comfort limits
over a test period of one week. The algorithm was shown to be robust to the
different hyperparameters and this optimal control policy was learnt through
the use of a minimal state space consisting of readily available variables. The
robustness of the algorithm was tested through investigation of the speed of
learning and ability to deploy to different seasons and climates. It was found
that the SAC DRL requires minimal training sample points and outperforms the
RBC after three months of operation and also without disruption to thermal
comfort during this period. The agent is transferable to other climates and
seasons although further retraining or hyperparameter tuning is recommended.","J.2,cs.LG,cs.SY,eess.SY"
"NeRV: Neural Reflectance and Visibility Fields for Relighting and View Synthesis. We present a method that takes as input a set of images of a scene
illuminated by unconstrained known lighting, and produces as output a 3D
representation that can be rendered from novel viewpoints under arbitrary
lighting conditions. Our method represents the scene as a continuous volumetric
function parameterized as MLPs whose inputs are a 3D location and whose outputs
are the following scene properties at that input location: volume density,
surface normal, material parameters, distance to the first surface intersection
in any direction, and visibility of the external environment in any direction.
Together, these allow us to render novel views of the object under arbitrary
lighting, including indirect illumination effects. The predicted visibility and
surface intersection fields are critical to our model's ability to simulate
direct and indirect illumination during training, because the brute-force
techniques used by prior work are intractable for lighting conditions outside
of controlled setups with a single light. Our method outperforms alternative
approaches for recovering relightable 3D scene representations, and performs
well in complex lighting settings that have posed a significant challenge to
prior work.","cs.CV,cs.GR"
"Towards mental time travel: a hierarchical memory for reinforcement learning agents. Reinforcement learning agents often forget details of the past, especially
after delays or distractor tasks. Agents with common memory architectures
struggle to recall and integrate across multiple timesteps of a past event, or
even to recall the details of a single timestep that is followed by distractor
tasks. To address these limitations, we propose a Hierarchical Transformer
Memory (HTM), which helps agents to remember the past in detail. HTM stores
memories by dividing the past into chunks, and recalls by first performing
high-level attention over coarse summaries of the chunks, and then performing
detailed attention within only the most relevant chunks. An agent with HTM can
therefore ""mentally time-travel"" -- remember past events in detail without
attending to all intervening events. We show that agents with HTM substantially
outperform agents with other memory architectures at tasks requiring long-term
recall, retention, or reasoning over memory. These include recalling where an
object is hidden in a 3D environment, rapidly learning to navigate efficiently
in a new neighborhood, and rapidly learning and retaining new object names.
Agents with HTM can extrapolate to task sequences an order of magnitude longer
than they were trained on, and can even generalize zero-shot from a
meta-learning setting to maintaining knowledge across episodes. HTM improves
agent sample efficiency, generalization, and generality (by solving tasks that
previously required specialized architectures). Our work is a step towards
agents that can learn, interact, and adapt in complex and temporally-extended
environments.","I.2.6,cs.AI,cs.LG,cs.NE"
"DAWSON: A Domain Adaptive Few Shot Generation Framework. Training a Generative Adversarial Networks (GAN) for a new domain from
scratch requires an enormous amount of training data and days of training time.
To this end, we propose DAWSON, a Domain Adaptive FewShot Generation
FrameworkFor GANs based on meta-learning. A major challenge of applying
meta-learning GANs is to obtain gradients for the generator from evaluating it
on development sets due to the likelihood-free nature of GANs. To address this
challenge, we propose an alternative GAN training procedure that naturally
combines the two-step training procedure of GANs and the two-step training
procedure of meta-learning algorithms. DAWSON is a plug-and-play framework that
supports a broad family of meta-learning algorithms and various GANs with
architectural-variants. Based on DAWSON, We also propose MUSIC MATINEE, which
is the first few-shot music generation model. Our experiments show that MUSIC
MATINEE could quickly adapt to new domains with only tens of songs from the
target domains. We also show that DAWSON can learn to generate new digits with
only four samples in the MNIST dataset. We release source codes implementation
of DAWSON in both PyTorch and Tensorflow, generated music samples on two genres
and the lightning video.","cs.LG,cs.SD,eess.AS,stat.ML"
"Model Embedded DRL for Intelligent Greenhouse Control. Greenhouse environment is the key to influence crops production. However, it
is difficult for classical control methods to give precise environment
setpoints, such as temperature, humidity, light intensity and carbon dioxide
concentration for greenhouse because it is uncertain nonlinear system.
Therefore, an intelligent close loop control framework based on model embedded
deep reinforcement learning (MEDRL) is designed for greenhouse environment
control. Specifically, computer vision algorithms are used to recognize growing
periods and sex of crops, followed by the crop growth models, which can be
trained with different growing periods and sex. These model outputs combined
with the cost factor provide the setpoints for greenhouse and feedback to the
control system in real-time. The whole MEDRL system has capability to conduct
optimization control precisely and conveniently, and costs will be greatly
reduced compared with traditional greenhouse control approaches.","cs.LG,cs.SY,eess.SY,stat.ML"
"A Self-Attention Network based Node Embedding Model. Despite several signs of progress have been made recently, limited research
has been conducted for an inductive setting where embeddings are required for
newly unseen nodes -- a setting encountered commonly in practical applications
of deep learning for graph networks. This significantly affects the
performances of downstream tasks such as node classification, link prediction
or community extraction. To this end, we propose SANNE -- a novel unsupervised
embedding model -- whose central idea is to employ a transformer self-attention
network to iteratively aggregate vector representations of nodes in random
walks. Our SANNE aims to produce plausible embeddings not only for present
nodes, but also for newly unseen nodes. Experimental results show that the
proposed SANNE obtains state-of-the-art results for the node classification
task on well-known benchmark datasets.","cs.CL,cs.LG,cs.SI,stat.ML"
"Deep Neural Networks Motivated by Partial Differential Equations. Partial differential equations (PDEs) are indispensable for modeling many
physical phenomena and also commonly used for solving image processing tasks.
In the latter area, PDE-based approaches interpret image data as
discretizations of multivariate functions and the output of image processing
algorithms as solutions to certain PDEs. Posing image processing problems in
the infinite dimensional setting provides powerful tools for their analysis and
solution. Over the last few decades, the reinterpretation of classical image
processing problems through the PDE lens has been creating multiple celebrated
approaches that benefit a vast area of tasks including image segmentation,
denoising, registration, and reconstruction.
  In this paper, we establish a new PDE-interpretation of a class of deep
convolutional neural networks (CNN) that are commonly used to learn from
speech, image, and video data. Our interpretation includes convolution residual
neural networks (ResNet), which are among the most promising approaches for
tasks such as image classification having improved the state-of-the-art
performance in prestigious benchmark challenges. Despite their recent
successes, deep ResNets still face some critical challenges associated with
their design, immense computational costs and memory requirements, and lack of
understanding of their reasoning.
  Guided by well-established PDE theory, we derive three new ResNet
architectures that fall into two new classes: parabolic and hyperbolic CNNs. We
demonstrate how PDE theory can provide new insights and algorithms for deep
learning and demonstrate the competitiveness of three new CNN architectures
using numerical experiments.","65K10,68T45,cs.LG,math.OC,stat.ML"
"Bayesian Regularization for Graphical Models with Unequal Shrinkage. We consider a Bayesian framework for estimating a high-dimensional sparse
precision matrix, in which adaptive shrinkage and sparsity are induced by a
mixture of Laplace priors. Besides discussing our formulation from the Bayesian
standpoint, we investigate the MAP (maximum a posteriori) estimator from a
penalized likelihood perspective that gives rise to a new non-convex penalty
approximating the $\ell_0$ penalty. Optimal error rates for estimation
consistency in terms of various matrix norms along with selection consistency
for sparse structure recovery are shown for the unique MAP estimator under mild
conditions. For fast and efficient computation, an EM algorithm is proposed to
compute the MAP estimator of the precision matrix and (approximate) posterior
probabilities on the edges of the underlying sparse structure. Through
extensive simulation studies and a real application to a call center data, we
have demonstrated the fine performance of our method compared with existing
alternatives.","cs.LG,math.ST,stat.CO,stat.ME,stat.ML,stat.TH"
"GANterfactual - Counterfactual Explanations for Medical Non-Experts using Generative Adversarial Learning. With the ongoing rise of machine learning, the need for methods for
explaining decisions made by artificial intelligence systems is becoming a more
and more important topic. Especially for image classification tasks, many
state-of-the-art tools to explain such classifiers rely on visual highlighting
of important areas of the input data. Contrary, counterfactual explanation
systems try to enable a counterfactual reasoning by modifying the input image
in a way such that the classifier would have made a different prediction. By
doing so, the users of counterfactual explanation systems are equipped with a
completely different kind of explanatory information. However, methods for
generating realistic counterfactual explanations for image classifiers are
still rare. Especially in medical contexts, where relevant information often
consists of textural and structural information, high-quality counterfactual
images have the potential to give meaningful insights into decision processes.
In this work, we present GANterfactual, an approach to generate such
counterfactual image explanations based on adversarial image-to-image
translation techniques. Additionally, we conduct a user study to evaluate our
approach in an exemplary medical use case. Our results show that, in the chosen
medical use-case, counterfactual explanations lead to significantly better
results regarding mental models, explanation satisfaction, trust, emotions, and
self-efficacy than two state-of-the-art systems that work with saliency maps,
namely LIME and LRP.","cs.AI,cs.CV,cs.HC,cs.LG,cs.NE"
"Multiplicative noise and heavy tails in stochastic optimization. Although stochastic optimization is central to modern machine learning, the
precise mechanisms underlying its success, and in particular, the precise role
of the stochasticity, still remain unclear. Modelling stochastic optimization
algorithms as discrete random recurrence relations, we show that multiplicative
noise, as it commonly arises due to variance in local rates of convergence,
results in heavy-tailed stationary behaviour in the parameters. A detailed
analysis is conducted for SGD applied to a simple linear regression problem,
followed by theoretical results for a much larger class of models (including
non-linear and non-convex) and optimizers (including momentum, Adam, and
stochastic Newton), demonstrating that our qualitative results hold much more
generally. In each case, we describe dependence on key factors, including step
size, batch size, and data variability, all of which exhibit similar
qualitative behavior to recent empirical results on state-of-the-art neural
network models from computer vision and natural language processing.
Furthermore, we empirically demonstrate how multiplicative noise and
heavy-tailed structure improve capacity for basin hopping and exploration of
non-convex loss surfaces, over commonly-considered stochastic dynamics with
only additive noise and light-tailed structure.","cs.LG,math.OC,math.ST,stat.ML,stat.TH"
"On the Applicability of Synthetic Data for Face Recognition. Face verification has come into increasing focus in various applications
including the European Entry/Exit System, which integrates face recognition
mechanisms. At the same time, the rapid advancement of biometric authentication
requires extensive performance tests in order to inhibit the discriminatory
treatment of travellers due to their demographic background. However, the use
of face images collected as part of border controls is restricted by the
European General Data Protection Law to be processed for no other reason than
its original purpose. Therefore, this paper investigates the suitability of
synthetic face images generated with StyleGAN and StyleGAN2 to compensate for
the urgent lack of publicly available large-scale test data. Specifically, two
deep learning-based (SER-FIQ, FaceQnet v1) and one standard-based (ISO/IEC TR
29794-5) face image quality assessment algorithm is utilized to compare the
applicability of synthetic face images compared to real face images extracted
from the FRGC dataset. Finally, based on the analysis of impostor score
distributions and utility score distributions, our experiments reveal
negligible differences between StyleGAN vs. StyleGAN2, and further also minor
discrepancies compared to real face images.","cs.CR,cs.CV"
"PAM: Understanding Product Images in Cross Product Category Attribute Extraction. Understanding product attributes plays an important role in improving online
shopping experience for customers and serves as an integral part for
constructing a product knowledge graph. Most existing methods focus on
attribute extraction from text description or utilize visual information from
product images such as shape and color. Compared to the inputs considered in
prior works, a product image in fact contains more information, represented by
a rich mixture of words and visual clues with a layout carefully designed to
impress customers. This work proposes a more inclusive framework that fully
utilizes these different modalities for attribute extraction. Inspired by
recent works in visual question answering, we use a transformer based sequence
to sequence model to fuse representations of product text, Optical Character
Recognition (OCR) tokens and visual objects detected in the product image. The
framework is further extended with the capability to extract attribute value
across multiple product categories with a single model, by training the decoder
to predict both product category and attribute value and conditioning its
output on product category. The model provides a unified attribute extraction
solution desirable at an e-commerce platform that offers numerous product
categories with a diverse body of product attributes. We evaluated the model on
two product attributes, one with many possible values and one with a small set
of possible values, over 14 product categories and found the model could
achieve 15% gain on the Recall and 10% gain on the F1 score compared to
existing methods using text-only features.","cs.CL,cs.CV,cs.LG"
"Spatiotemporal Data Mining: A Survey on Challenges and Open Problems. Spatiotemporal data mining (STDM) discovers useful patterns from the dynamic
interplay between space and time. Several available surveys capture STDM
advances and report a wealth of important progress in this field. However, STDM
challenges and problems are not thoroughly discussed and presented in articles
of their own. We attempt to fill this gap by providing a comprehensive
literature survey on state-of-the-art advances in STDM. We describe the
challenging issues and their causes and open gaps of multiple STDM directions
and aspects. Specifically, we investigate the challenging issues in regards to
spatiotemporal relationships, interdisciplinarity, discretisation, and data
characteristics. Moreover, we discuss the limitations in the literature and
open research problems related to spatiotemporal data representations,
modelling and visualisation, and comprehensiveness of approaches. We explain
issues related to STDM tasks of classification, clustering, hotspot detection,
association and pattern mining, outlier detection, visualisation, visual
analytics, and computer vision tasks. We also highlight STDM issues related to
multiple applications including crime and public safety, traffic and
transportation, earth and environment monitoring, epidemiology, social media,
and Internet of Things.","cs.DB,cs.LG"
"Molecular graph generation with Graph Neural Networks. Drug Discovery is a fundamental and ever-evolving field of research. The
design of new candidate molecules requires large amounts of time and money, and
computational methods are being increasingly employed to cut these costs.
Machine learning methods are ideal for the design of large amounts of potential
new candidate molecules, which are naturally represented as graphs. Graph
generation is being revolutionized by deep learning methods, and molecular
generation is one of its most promising applications. In this paper, we
introduce a sequential molecular graph generator based on a set of graph neural
network modules, which we call MG^2N^2. At each step, a node or a group of
nodes is added to the graph, along with its connections. The modular
architecture simplifies the training procedure, also allowing an independent
retraining of a single module. Sequentiality and modularity make the generation
process interpretable. The use of graph neural networks maximizes the
information in input at each generative step, which consists of the subgraph
produced during the previous steps. Experiments of unconditional generation on
the QM9 and Zinc datasets show that our model is capable of generalizing
molecular patterns seen during the training phase, without overfitting. The
results indicate that our method is competitive, and outperforms challenging
baselines for unconditional generation.","cs.LG,q-bio.BM,stat.ML"
"Gated Graph Sequence Neural Networks. Graph-structured data appears frequently in domains including chemistry,
natural language semantics, social networks, and knowledge bases. In this work,
we study feature learning techniques for graph-structured inputs. Our starting
point is previous work on Graph Neural Networks (Scarselli et al., 2009), which
we modify to use gated recurrent units and modern optimization techniques and
then extend to output sequences. The result is a flexible and broadly useful
class of neural network models that has favorable inductive biases relative to
purely sequence-based models (e.g., LSTMs) when the problem is
graph-structured. We demonstrate the capabilities on some simple AI (bAbI) and
graph algorithm learning tasks. We then show it achieves state-of-the-art
performance on a problem from program verification, in which subgraphs need to
be matched to abstract data structures.","cs.AI,cs.LG,cs.NE,stat.ML"
"Online Sparse Reinforcement Learning. We investigate the hardness of online reinforcement learning in fixed
horizon, sparse linear Markov decision process (MDP), with a special focus on
the high-dimensional regime where the ambient dimension is larger than the
number of episodes. Our contribution is two-fold. First, we provide a lower
bound showing that linear regret is generally unavoidable in this case, even if
there exists a policy that collects well-conditioned data. The lower bound
construction uses an MDP with a fixed number of states while the number of
actions scales with the ambient dimension. Note that when the horizon is fixed
to one, the case of linear stochastic bandits, the linear regret can be
avoided. Second, we show that if the learner has oracle access to a policy that
collects well-conditioned data then a variant of Lasso fitted Q-iteration
enjoys a nearly dimension-free regret of $\tilde{O}( s^{2/3} N^{2/3})$ where
$N$ is the number of episodes and $s$ is the sparsity level. This shows that in
the large-action setting, the difficulty of learning can be attributed to the
difficulty of finding a good exploratory policy.","cs.LG,math.ST,stat.ML,stat.TH"
"BoA-PTA, A Bayesian Optimization Accelerated Error-Free SPICE Solver. One of the greatest challenges in IC design is the repeated executions of
computationally expensive SPICE simulations, particularly when highly complex
chip testing/verification is involved. Recently, pseudo transient analysis
(PTA) has shown to be one of the most promising continuation SPICE solver.
However, the PTA efficiency is highly influenced by the inserted
pseudo-parameters. In this work, we proposed BoA-PTA, a Bayesian optimization
accelerated PTA that can substantially accelerate simulations and improve
convergence performance without introducing extra errors. Furthermore, our
method does not require any pre-computation data or offline training. The
acceleration framework can either be implemented to speed up ongoing repeated
simulations immediately or to improve new simulations of completely different
circuits. BoA-PTA is equipped with cutting-edge machine learning techniques,
e.g., deep learning, Gaussian process, Bayesian optimization, non-stationary
monotonic transformation, and variational inference via parameterization. We
assess BoA-PTA in 43 benchmark circuits against other SOTA SPICE solvers and
demonstrate an average 2.3x (maximum 3.5x) speed-up over the original CEPTA.","I.6.5; I.6.6; J.2; J.6,cs.CE,cs.LG"
"Dynamic RAN Slicing for Service-Oriented Vehicular Networks via Constrained Learning. In this paper, we investigate a radio access network (RAN) slicing problem
for Internet of vehicles (IoV) services with different quality of service (QoS)
requirements, in which multiple logically-isolated slices are constructed on a
common roadside network infrastructure. A dynamic RAN slicing framework is
presented to dynamically allocate radio spectrum and computing resource, and
distribute computation workloads for the slices. To obtain an optimal RAN
slicing policy for accommodating the spatial-temporal dynamics of vehicle
traffic density, we first formulate a constrained RAN slicing problem with the
objective to minimize long-term system cost. This problem cannot be directly
solved by traditional reinforcement learning (RL) algorithms due to complicated
coupled constraints among decisions. Therefore, we decouple the problem into a
resource allocation subproblem and a workload distribution subproblem, and
propose a two-layer constrained RL algorithm, named Resource Allocation and
Workload diStribution (RAWS) to solve them. Specifically, an outer layer first
makes the resource allocation decision via an RL algorithm, and then an inner
layer makes the workload distribution decision via an optimization subroutine.
Extensive trace-driven simulations show that the RAWS effectively reduces the
system cost while satisfying QoS requirements with a high probability, as
compared with benchmarks.","cs.LG,cs.NI"
"Multi-class Generalized Binary Search for Active Inverse Reinforcement Learning. This paper addresses the problem of learning a task from demonstration. We
adopt the framework of inverse reinforcement learning, where tasks are
represented in the form of a reward function. Our contribution is a novel
active learning algorithm that enables the learning agent to query the expert
for more informative demonstrations, thus leading to more sample-efficient
learning. For this novel algorithm (Generalized Binary Search for Inverse
Reinforcement Learning, or GBS-IRL), we provide a theoretical bound on sample
complexity and illustrate its applicability on several different tasks. To our
knowledge, GBS-IRL is the first active IRL algorithm with provable sample
complexity bounds. We also discuss our method in light of other existing
methods in the literature and its general applicability in multi-class
classification problems. Finally, motivated by recent work on learning from
demonstration in robots, we also discuss how different forms of human feedback
can be integrated in a transparent manner in our learning framework.","cs.AI,cs.LG,stat.ML"
"Molecular machine learning with conformer ensembles. Virtual screening can accelerate drug discovery by identifying promising
candidates for experimental evaluation. Machine learning is a powerful method
for screening, as it can learn complex structure-property relationships from
experimental data and make rapid predictions over virtual libraries. Molecules
inherently exist as a three-dimensional ensemble and their biological action
typically occurs through supramolecular recognition. However, most deep
learning approaches to molecular property prediction use a 2D graph
representation as input, and in some cases a single 3D conformation. Here we
investigate how the 3D information of multiple conformers, traditionally known
as 4D information in the cheminformatics community, can improve molecular
property prediction in deep learning models. We introduce multiple deep
learning models that expand upon key architectures such as ChemProp and Schnet,
adding elements such as multiple-conformer inputs and conformer attention. We
then benchmark the performance trade-offs of these models on 2D, 3D and 4D
representations in the prediction of drug activity using a large training set
of geometrically resolved molecules. The new architectures perform
significantly better than 2D models, but their performance is often just as
strong with a single conformer as with many. We also find that 4D deep learning
models learn interpretable attention weights for each conformer.","cs.LG,physics.chem-ph"
"Expectation Learning for Adaptive Crossmodal Stimuli Association. The human brain is able to learn, generalize, and predict crossmodal stimuli.
Learning by expectation fine-tunes crossmodal processing at different levels,
thus enhancing our power of generalization and adaptation in highly dynamic
environments. In this paper, we propose a deep neural architecture trained by
using expectation learning accounting for unsupervised learning tasks. Our
learning model exhibits a self-adaptable behavior, setting the first steps
towards the development of deep learning architectures for crossmodal stimuli
association.","cs.AI,cs.LG,cs.SD,q-bio.NC,stat.ML"
"Marius: Learning Massive Graph Embeddings on a Single Machine. We propose a new framework for computing the embeddings of large-scale graphs
on a single machine. A graph embedding is a fixed length vector representation
for each node (and/or edge-type) in a graph and has emerged as the de-facto
approach to apply modern machine learning on graphs. We identify that current
systems for learning the embeddings of large-scale graphs are bottlenecked by
data movement, which results in poor resource utilization and inefficient
training. These limitations require state-of-the-art systems to distribute
training across multiple machines. We propose Marius, a system for efficient
training of graph embeddings that leverages partition caching and buffer-aware
data orderings to minimize disk access and interleaves data movement with
computation to maximize utilization. We compare Marius against two
state-of-the-art industrial systems on a diverse array of benchmarks. We
demonstrate that Marius achieves the same level of accuracy but is up to one
order of magnitude faster. We also show that Marius can scale training to
datasets an order of magnitude beyond a single machine's GPU and CPU memory
capacity, enabling training of configurations with more than a billion edges
and 550 GB of total parameters on a single machine with 16 GB of GPU memory and
64 GB of CPU memory. Marius is open-sourced at www.marius-project.org.","cs.DB,cs.DC,cs.LG"
"Variance-Aware Regret Bounds for Undiscounted Reinforcement Learning in MDPs. The problem of reinforcement learning in an unknown and discrete Markov
Decision Process (MDP) under the average-reward criterion is considered, when
the learner interacts with the system in a single stream of observations,
starting from an initial state without any reset. We revisit the minimax lower
bound for that problem by making appear the local variance of the bias function
in place of the diameter of the MDP. Furthermore, we provide a novel analysis
of the KL-UCRL algorithm establishing a high-probability regret bound scaling
as $\widetilde {\mathcal O}\Bigl({\textstyle \sqrt{S\sum_{s,a}{\bf
V}^\star_{s,a}T}}\Big)$ for this algorithm for ergodic MDPs, where $S$ denotes
the number of states and where ${\bf V}^\star_{s,a}$ is the variance of the
bias function with respect to the next-state distribution following action $a$
in state $s$. The resulting bound improves upon the best previously known
regret bound $\widetilde {\mathcal O}(DS\sqrt{AT})$ for that algorithm, where
$A$ and $D$ respectively denote the maximum number of actions (per state) and
the diameter of MDP. We finally compare the leading terms of the two bounds in
some benchmark MDPs indicating that the derived bound can provide an order of
magnitude improvement in some cases. Our analysis leverages novel variations of
the transportation lemma combined with Kullback-Leibler concentration
inequalities, that we believe to be of independent interest.","cs.LG,cs.SY,stat.ML"
"Object Exchangeability in Reinforcement Learning: Extended Abstract. Although deep reinforcement learning has advanced significantly over the past
several years, sample efficiency remains a major challenge. Careful choice of
input representations can help improve efficiency depending on the structure
present in the problem. In this work, we present an attention-based method to
project inputs into an efficient representation space that is invariant under
changes to input ordering. We show that our proposed representation results in
a search space that is a factor of m! smaller for inputs of m objects. Our
experiments demonstrate improvements in sample efficiency for policy gradient
methods on a variety of tasks. We show that our representation allows us to
solve problems that are otherwise intractable when using naive approaches.","I.2.6,cs.AI,cs.LG,stat.ML"
"Few-shot time series segmentation using prototype-defined infinite hidden Markov models. We propose a robust framework for interpretable, few-shot analysis of
non-stationary sequential data based on flexible graphical models to express
the structured distribution of sequential events, using prototype radial basis
function (RBF) neural network emissions. A motivational link is demonstrated
between prototypical neural network architectures for few-shot learning and the
proposed RBF network infinite hidden Markov model (RBF-iHMM). We show that RBF
networks can be efficiently specified via prototypes allowing us to express
complex nonstationary patterns, while hidden Markov models are used to infer
principled high-level Markov dynamics. The utility of the framework is
demonstrated on biomedical signal processing applications such as automated
seizure detection from EEG data where RBF networks achieve state-of-the-art
performance using a fraction of the data needed to train long-short-term memory
variational autoencoders.","cs.LG,eess.SP,math.ST,stat.TH"
"Reinforcement Learning with Probabilistic Boolean Network Models of Smart Grid Devices. The area of Smart Power Grids needs to constantly improve its efficiency and
resilience, to pro-vide high quality electrical power, in a resistant grid,
managing faults and avoiding failures. Achieving this requires high component
reliability, adequate maintenance, and a studied failure occurrence. Correct
system operation involves those activities, and novel methodologies to detect,
classify, and isolate faults and failures, model and simulate processes with
predictive algorithms and analytics (using data analysis and asset condition to
plan and perform activities). We show-case the application of a
complex-adaptive, self-organizing modeling method, Probabilistic Boolean
Networks (PBN), as a way towards the understanding of the dynamics of smart
grid devices, and to model and characterize their behavior. This work
demonstrates that PBNs are is equivalent to the standard Reinforcement Learning
Cycle, in which the agent/model has an inter-action with its environment and
receives feedback from it in the form of a reward signal. Differ-ent reward
structures were created in order to characterize preferred behavior. This
information can be used to guide the PBN to avoid fault conditions and
failures.","cs.LG,cs.SY,eess.SY"
"Quaternion Equivariant Capsule Networks for 3D Point Clouds. We present a 3D capsule module for processing point clouds that is
equivariant to 3D rotations and translations, as well as invariant to
permutations of the input points. The operator receives a sparse set of local
reference frames, computed from an input point cloud and establishes end-to-end
transformation equivariance through a novel dynamic routing procedure on
quaternions. Further, we theoretically connect dynamic routing between capsules
to the well-known Weiszfeld algorithm, a scheme for solving \emph{iterative
re-weighted least squares} (IRLS) problems with provable convergence
properties. It is shown that such group dynamic routing can be interpreted as
robust IRLS rotation averaging on capsule votes, where information is routed
based on the final inlier scores. Based on our operator, we build a capsule
network that disentangles geometry from pose, paving the way for more
informative descriptors and a structured latent space. Our architecture allows
joint object classification and orientation estimation without explicit
supervision of rotations. We validate our algorithm empirically on common
benchmark datasets.","cs.CV,cs.GR,cs.LG,cs.RO,stat.ML"
"A Graph Neural Network Approach for Scalable and Dynamic IP Similarity in Enterprise Networks. Measuring similarity between IP addresses is an important task in the daily
operations of any enterprise network. Applications that depend on an IP
similarity measure include measuring correlation between security alerts,
building baselines for behavioral modelling, debugging network failures and
tracking persistent attacks. However, IPs do not have a natural similarity
measure by definition. Deep Learning architectures are a promising solution
here since they are able to learn numerical representations for IPs directly
from data, allowing various distance measures to be applied on the calculated
representations. Current works have utilized Natural Language Processing (NLP)
techniques for learning IP embeddings. However, these approaches have no proper
way to handle out-of-vocabulary (OOV) IPs not seen during training. In this
paper, we propose a novel approach for IP embedding using an adapted graph
neural network (GNN) architecture. This approach has the advantages of working
on the raw data, scalability and, most importantly, induction, i.e. the ability
to measure similarity between previously unseen IPs. Using data from an
enterprise network, our approach is able to identify similarities between local
DNS servers and root DNS servers even though some of these machines are never
encountered during the training phase.","cs.LG,cs.NI"
"Towards Interpretable Sparse Graph Representation Learning with Laplacian Pooling. Recent work in graph neural networks (GNNs) has led to improvements in
molecular activity and property prediction tasks. Unfortunately, GNNs often
fail to capture the relative importance of interactions between molecular
substructures, in part due to the absence of efficient intermediate pooling
steps. To address these issues, we propose LaPool (Laplacian Pooling), a novel,
data-driven, and interpretable hierarchical graph pooling method that takes
into account both node features and graph structure to improve molecular
representation. We benchmark LaPool on molecular graph prediction and
understanding tasks and show that it outperforms recent GNNs. Interestingly,
LaPool also remains competitive on non-molecular tasks. Both quantitative and
qualitative assessments are done to demonstrate LaPool's improved
interpretability and highlight its potential benefits in drug design. Finally,
we demonstrate LaPool's utility for the generation of valid and novel molecules
by incorporating it into an adversarial autoencoder.","cs.LG,q-bio.BM,stat.ML"
"OD-GCN: Object Detection Boosted by Knowledge GCN. Classical CNN based object detection methods only extract the objects' image
features, but do not consider the high-level relationship among objects in
context. In this article, the graph convolutional networks (GCN) is integrated
into the object detection framework to exploit the benefit of category
relationship among objects, which is able to provide extra confidence for any
pre-trained object detection model in our framework. In experiments, we test
several popular base detection models on COCO dataset. The results show
promising improvement on mAP by 1-5pp. In addition, visualized analysis reveals
the benchmark improvement is quite reasonable in human's opinion.","68T45,I.2.10,cs.AI,cs.CV,cs.LG"
"Using Cyber Terrain in Reinforcement Learning for Penetration Testing. Reinforcement learning (RL) has been applied to attack graphs for penetration
testing, however, trained agents do not reflect reality because the attack
graphs lack operational nuances typically captured within the intelligence
preparation of the battlefield (IPB) that include notions of (cyber) terrain.
In particular, current practice constructs attack graphs exclusively using the
Common Vulnerability Scoring System (CVSS) and its components. We present
methods for constructing attack graphs using notions from IPB on cyber terrain
analysis of obstacles, avenues of approach, key terrain, observation and fields
of fire, and cover and concealment. We demonstrate our methods on an example
where firewalls are treated as obstacles and represented in (1) the reward
space and (2) the state dynamics. We show that terrain analysis can be used to
bring realism to attack graphs for RL.","cs.CR,cs.LG,cs.NI"
"Novel semi-metrics for multivariate change point analysis and anomaly detection. This paper proposes a new method for determining similarity and anomalies
between time series, most practically effective in large collections of (likely
related) time series, by measuring distances between structural breaks within
such a collection. We introduce a class of \emph{semi-metric} distance
measures, which we term \emph{MJ distances}. These semi-metrics provide an
advantage over existing options such as the Hausdorff and Wasserstein metrics.
We prove they have desirable properties, including better sensitivity to
outliers, while experiments on simulated data demonstrate that they uncover
similarity within collections of time series more effectively. Semi-metrics
carry a potential disadvantage: without the triangle inequality, they may not
satisfy a ""transitivity property of closeness."" We analyse this failure with
proof and introduce an computational method to investigate, in which we
demonstrate that our semi-metrics violate transitivity infrequently and mildly.
Finally, we apply our methods to cryptocurrency and measles data, introducing a
judicious application of eigenvalue analysis.","cs.LG,math.DS,stat.CO,stat.ME,stat.ML"
"Faster-LTN: a neuro-symbolic, end-to-end object detection architecture. The detection of semantic relationships between objects represented in an
image is one of the fundamental challenges in image interpretation.
Neural-Symbolic techniques, such as Logic Tensor Networks (LTNs), allow the
combination of semantic knowledge representation and reasoning with the ability
to efficiently learn from examples typical of neural networks. We here propose
Faster-LTN, an object detector composed of a convolutional backbone and an LTN.
To the best of our knowledge, this is the first attempt to combine both
frameworks in an end-to-end training setting. This architecture is trained by
optimizing a grounded theory which combines labelled examples with prior
knowledge, in the form of logical axioms. Experimental comparisons show
competitive performance with respect to the traditional Faster R-CNN
architecture.","cs.AI,cs.CV,cs.LO"
"Message Passing in Graph Convolution Networks via Adaptive Filter Banks. Graph convolution networks, like message passing graph convolution networks
(MPGCNs), have been a powerful tool in representation learning of networked
data. However, when data is heterogeneous, most architectures are limited as
they employ a single strategy to handle multi-channel graph signals and they
typically focus on low-frequency information. In this paper, we present a novel
graph convolution operator, termed BankGCN, which keeps benefits of message
passing models, but extends their capabilities beyond `low-pass' features. It
decomposes multi-channel signals on graphs into subspaces and handles
particular information in each subspace with an adapted filter. The filters of
all subspaces have different frequency responses and together form a filter
bank. Furthermore, each filter in the spectral domain corresponds to a message
passing scheme, and diverse schemes are implemented via the filter bank.
Importantly, the filter bank and the signal decomposition are jointly learned
to adapt to the spectral characteristics of data and to target applications.
Furthermore, this is implemented almost without extra parameters in comparison
with most existing MPGCNs. Experimental results show that the proposed
convolution operator permits to achieve excellent performance in graph
classification on a collection of benchmark graph datasets.","cs.AI,cs.LG,eess.SP,stat.ML"
"Near-Term Quantum-Classical Associative Adversarial Networks. We introduce a new hybrid quantum-classical adversarial machine learning
architecture called a quantum-classical associative adversarial network (QAAN).
This architecture consists of a classical generative adversarial network with a
small auxiliary quantum Boltzmann machine that is simultaneously trained on an
intermediate layer of the discriminator of the generative network. We
numerically study the performance of QAANs compared to their classical
counterparts on the MNIST and CIFAR-10 data sets, and show that QAANs attain a
higher quality of learning when evaluated using the Inception score and the
Fr\'{e}chet Inception distance. As the QAAN architecture only relies on
sampling simple local observables of a small quantum Boltzmann machine, this
model is particularly amenable for implementation on the current and next
generations of quantum devices.","cs.LG,quant-ph"
"Nemesyst: A Hybrid Parallelism Deep Learning-Based Framework Applied for Internet of Things Enabled Food Retailing Refrigeration Systems. Deep Learning has attracted considerable attention across multiple
application domains, including computer vision, signal processing and natural
language processing. Although quite a few single node deep learning frameworks
exist, such as tensorflow, pytorch and keras, we still lack a complete
processing structure that can accommodate large scale data processing, version
control, and deployment, all while staying agnostic of any specific single node
framework. To bridge this gap, this paper proposes a new, higher level
framework, i.e. Nemesyst, which uses databases along with model
sequentialisation to allow processes to be fed unique and transformed data at
the point of need. This facilitates near real-time application and makes models
available for further training or use at any node that has access to the
database simultaneously. Nemesyst is well suited as an application framework
for internet of things aggregated control systems, deploying deep learning
techniques to optimise individual machines in massive networks. To demonstrate
this framework, we adopted a case study in a novel domain; deploying deep
learning to optimise the high speed control of electrical power consumed by a
massive internet of things network of retail refrigeration systems in
proportion to load available on the UK National Grid (a demand side response).
The case study demonstrated for the first time in such a setting how deep
learning models, such as Recurrent Neural Networks (vanilla and Long-Short-Term
Memory) and Generative Adversarial Networks paired with Nemesyst, achieve
compelling performance, whilst still being malleable to future adjustments as
both the data and requirements inevitably change over time.","cs.DC,cs.LG,stat.ML"
"RL-NCS: Reinforcement learning based data-driven approach for nonuniform compressed sensing. A reinforcement-learning-based non-uniform compressed sensing (NCS) framework
for time-varying signals is introduced. The proposed scheme, referred to as
RL-NCS, aims to boost the performance of signal recovery through an optimal and
adaptive distribution of sensing energy among two groups of coefficients of the
signal, referred to as the region of interest (ROI) coefficients and non-ROI
coefficients. The coefficients in ROI usually have greater importance and need
to be reconstructed with higher accuracy compared to non-ROI coefficients. In
order to accomplish this task, the ROI is predicted at each time step using two
specific approaches. One of these approaches incorporates a long short-term
memory (LSTM) network for the prediction. The other approach employs the
previous ROI information for predicting the next step ROI. Using the
exploration-exploitation technique, a Q-network learns to choose the best
approach for designing the measurement matrix. Furthermore, a joint loss
function is introduced for the efficient training of the Q-network as well as
the LSTM network. The result indicates a significant performance gain for our
proposed method, even for rapidly varying signals and a reduced number of
measurements.","cs.IT,cs.LG,eess.SP,math.IT"
"Perceiver IO: A General Architecture for Structured Inputs & Outputs. The recently-proposed Perceiver model obtains good results on several domains
(images, audio, multimodal, point clouds) while scaling linearly in compute and
memory with the input size. While the Perceiver supports many kinds of inputs,
it can only produce very simple outputs such as class scores. Perceiver IO
overcomes this limitation without sacrificing the original's appealing
properties by learning to flexibly query the model's latent space to produce
outputs of arbitrary size and semantics. Perceiver IO still decouples model
depth from data size and still scales linearly with data size, but now with
respect to both input and output sizes. The full Perceiver IO model achieves
strong results on tasks with highly structured output spaces, such as natural
language and visual understanding, StarCraft II, and multi-task and multi-modal
domains. As highlights, Perceiver IO matches a Transformer-based BERT baseline
on the GLUE language benchmark without the need for input tokenization and
achieves state-of-the-art performance on Sintel optical flow estimation.","cs.CL,cs.CV,cs.LG,cs.SD,eess.AS"
"Context-Aware Generative Adversarial Privacy. Preserving the utility of published datasets while simultaneously providing
provable privacy guarantees is a well-known challenge. On the one hand,
context-free privacy solutions, such as differential privacy, provide strong
privacy guarantees, but often lead to a significant reduction in utility. On
the other hand, context-aware privacy solutions, such as information theoretic
privacy, achieve an improved privacy-utility tradeoff, but assume that the data
holder has access to dataset statistics. We circumvent these limitations by
introducing a novel context-aware privacy framework called generative
adversarial privacy (GAP). GAP leverages recent advancements in generative
adversarial networks (GANs) to allow the data holder to learn privatization
schemes from the dataset itself. Under GAP, learning the privacy mechanism is
formulated as a constrained minimax game between two players: a privatizer that
sanitizes the dataset in a way that limits the risk of inference attacks on the
individuals' private variables, and an adversary that tries to infer the
private variables from the sanitized dataset. To evaluate GAP's performance, we
investigate two simple (yet canonical) statistical dataset models: (a) the
binary data model, and (b) the binary Gaussian mixture model. For both models,
we derive game-theoretically optimal minimax privacy mechanisms, and show that
the privacy mechanisms learned from data (in a generative adversarial fashion)
match the theoretically optimal ones. This demonstrates that our framework can
be easily applied in practice, even in the absence of dataset statistics.","cs.AI,cs.CR,cs.GT,cs.IT,cs.LG,math.IT"
"Self-Supervised Audio-Visual Co-Segmentation. Segmenting objects in images and separating sound sources in audio are
challenging tasks, in part because traditional approaches require large amounts
of labeled data. In this paper we develop a neural network model for visual
object segmentation and sound source separation that learns from natural videos
through self-supervision. The model is an extension of recently proposed work
that maps image pixels to sounds. Here, we introduce a learning approach to
disentangle concepts in the neural networks, and assign semantic categories to
network feature channels to enable independent image segmentation and sound
source separation after audio-visual training on videos. Our evaluations show
that the disentangled model outperforms several baselines in semantic
segmentation and sound source separation.","cs.CV,cs.SD,eess.AS,eess.IV"
"Disentangling ODE parameters from dynamics in VAEs. Deep networks have become increasingly of interest in dynamical system
prediction, but generalization remains elusive. In this work, we consider the
physical parameters of ODEs as factors of variation of the data generating
process. By leveraging ideas from supervised disentanglement in VAEs, we aim to
separate the ODE parameters from the dynamics in the latent space. Experiments
show that supervised disentanglement allows VAEs to capture the variability in
the dynamics and extrapolate better to ODE parameter spaces that were not
present in the training data.","cs.AI,cs.LG,cs.NA,math.NA"
"Identification of Latent Variables From Graphical Model Residuals. Graph-based causal discovery methods aim to capture conditional
independencies consistent with the observed data and differentiate causal
relationships from indirect or induced ones. Successful construction of
graphical models of data depends on the assumption of causal sufficiency: that
is, that all confounding variables are measured. When this assumption is not
met, learned graphical structures may become arbitrarily incorrect and effects
implied by such models may be wrongly attributed, carry the wrong magnitude, or
mis-represent direction of correlation. Wide application of graphical models to
increasingly less curated ""big data"" draws renewed attention to the unobserved
confounder problem.
  We present a novel method that aims to control for the latent space when
estimating a DAG by iteratively deriving proxies for the latent space from the
residuals of the inferred model. Under mild assumptions, our method improves
structural inference of Gaussian graphical models and enhances identifiability
of the causal effect. In addition, when the model is being used to predict
outcomes, it un-confounds the coefficients on the parents of the outcomes and
leads to improved predictive performance when out-of-sample regime is very
different from the training data. We show that any improvement of prediction of
an outcome is intrinsically capped and cannot rise beyond a certain limit as
compared to the confounded model. We extend our methodology beyond GGMs to
ordinal variables and nonlinear cases. Our R package provides both PCA and
autoencoder implementations of the methodology, suitable for GGMs with some
guarantees and for better performance in general cases but without such
guarantees.","cs.LG,q-bio.QM,stat.ML"
"Towards Automated Satellite Conjunction Management with Bayesian Deep Learning. After decades of space travel, low Earth orbit is a junkyard of discarded
rocket bodies, dead satellites, and millions of pieces of debris from
collisions and explosions. Objects in high enough altitudes do not re-enter and
burn up in the atmosphere, but stay in orbit around Earth for a long time. With
a speed of 28,000 km/h, collisions in these orbits can generate fragments and
potentially trigger a cascade of more collisions known as the Kessler syndrome.
This could pose a planetary challenge, because the phenomenon could escalate to
the point of hindering future space operations and damaging satellite
infrastructure critical for space and Earth science applications. As commercial
entities place mega-constellations of satellites in orbit, the burden on
operators conducting collision avoidance manoeuvres will increase. For this
reason, development of automated tools that predict potential collision events
(conjunctions) is critical. We introduce a Bayesian deep learning approach to
this problem, and develop recurrent neural network architectures (LSTMs) that
work with time series of conjunction data messages (CDMs), a standard data
format used by the space community. We show that our method can be used to
model all CDM features simultaneously, including the time of arrival of future
CDMs, providing predictions of conjunction event evolution with associated
uncertainties.","62P35,68T05,68T37,G.3; I.2.6; J.2,cs.LG,stat.ML"
"Towards Non-I.I.D. and Invisible Data with FedNAS: Federated Deep Learning via Neural Architecture Search. Federated Learning (FL) has been proved to be an effective learning framework
when data cannot be centralized due to privacy, communication costs, and
regulatory restrictions. When training deep learning models under an FL
setting, people employ the predefined model architecture discovered in the
centralized environment. However, this predefined architecture may not be the
optimal choice because it may not fit data with non-identical and independent
distribution (non-IID). Thus, we advocate automating federated learning
(AutoFL) to improve model accuracy and reduce the manual design effort. We
specifically study AutoFL via Neural Architecture Search (NAS), which can
automate the design process. We propose a Federated NAS (FedNAS) algorithm to
help scattered workers collaboratively searching for a better architecture with
higher accuracy. We also build a system based on FedNAS. Our experiments on
non-IID dataset show that the architecture searched by FedNAS can outperform
the manually predefined architecture.","cs.CV,cs.DC,cs.LG,cs.MA,stat.ML"
"Eikonal Region-based Active Contours for Image Segmentation. The minimal path model based on the Eikonal partial differential equation
(PDE) has served as a fundamental tool for the applications of image
segmentation and boundary detection in the passed three decades. However, the
existing minimal paths-based image segmentation approaches commonly rely on the
image boundary features, potentially limiting their performance in some
situations. In this paper, we introduce a new variational image segmentation
model based on the minimal path framework and the Eikonal PDE, where the
region-based functional that defines the homogeneity criteria can be taken into
account for estimating the associated geodesic paths. This is done by
establishing a geodesic curve interpretation to the region-based active contour
evolution problem. The image segmentation processing is carried out in an
iterative manner in our approach. A crucial ingredient in each iteration is to
construct an asymmetric Randers geodesic metric using a sufficiently small
vector field, such that a set of geodesic paths can be tracked from the
geodesic distance map which is the solution to an Eikonal PDE. The object
boundary can be delineated by the concatenation of the final geodesic paths. We
invoke the Finsler variant of the fast marching method to estimate the geodesic
distance map, yielding an efficient implementation of the proposed Eikonal
region-based active contour model. Experimental results on both of the
synthetic and real images exhibit that our model indeed achieves encouraging
segmentation performance.","cs.CG,cs.CV"
"Unsupervisedly Training GANs for Segmenting Digital Pathology with Automatically Generated Annotations. Recently, generative adversarial networks exhibited excellent performances in
semi-supervised image analysis scenarios. In this paper, we go even further by
proposing a fully unsupervised approach for segmentation applications with
prior knowledge of the objects' shapes. We propose and investigate different
strategies to generate simulated label data and perform image-to-image
translation between the image and the label domain using an adversarial model.
Specifically, we assess the impact of the annotation model's accuracy as well
as the effect of simulating additional low-level image features. For
experimental evaluation, we consider the segmentation of the glomeruli, an
application scenario from renal pathology. Experiments provide proof of concept
and also confirm that the strategy for creating the simulated label data is of
particular relevance considering the stability of GAN trainings.","68T45,cs.CV"
"Decentralized Reinforcement Learning: Global Decision-Making via Local Economic Transactions. This paper seeks to establish a framework for directing a society of simple,
specialized, self-interested agents to solve what traditionally are posed as
monolithic single-agent sequential decision problems. What makes it challenging
to use a decentralized approach to collectively optimize a central objective is
the difficulty in characterizing the equilibrium strategy profile of
non-cooperative games. To overcome this challenge, we design a mechanism for
defining the learning environment of each agent for which we know that the
optimal solution for the global objective coincides with a Nash equilibrium
strategy profile of the agents optimizing their own local objectives. The
society functions as an economy of agents that learn the credit assignment
process itself by buying and selling to each other the right to operate on the
environment state. We derive a class of decentralized reinforcement learning
algorithms that are broadly applicable not only to standard reinforcement
learning but also for selecting options in semi-MDPs and dynamically composing
computation graphs. Lastly, we demonstrate the potential advantages of a
society's inherent modular structure for more efficient transfer learning.","cs.GT,cs.LG,cs.MA,cs.NE,stat.ML"
"A Group-Theoretic Framework for Data Augmentation. Data augmentation is a widely used trick when training deep neural networks:
in addition to the original data, properly transformed data are also added to
the training set. However, to the best of our knowledge, a clear mathematical
framework to explain the performance benefits of data augmentation is not
available. In this paper, we develop such a theoretical framework. We show data
augmentation is equivalent to an averaging operation over the orbits of a
certain group that keeps the data distribution approximately invariant. We
prove that it leads to variance reduction. We study empirical risk
minimization, and the examples of exponential families, linear regression, and
certain two-layer neural networks. We also discuss how data augmentation could
be used in problems with symmetry where other approaches are prevalent, such as
in cryo-electron microscopy (cryo-EM).","cs.LG,math.ST,stat.ML,stat.TH"
"Improving Botnet Detection with Recurrent Neural Network and Transfer Learning. Botnet detection is a critical step in stopping the spread of botnets and
preventing malicious activities. However, reliable detection is still a
challenging task, due to a wide variety of botnets involving ever-increasing
types of devices and attack vectors. Recent approaches employing machine
learning (ML) showed improved performance than earlier ones, but these ML-
based approaches still have significant limitations. For example, most ML
approaches can not incorporate sequential pattern analysis techniques key to
detect some classes of botnets. Another common shortcoming of ML-based
approaches is the need to retrain neural networks in order to detect the
evolving botnets; however, the training process is time-consuming and requires
significant efforts to label the training data. For fast-evolving botnets, it
might take too long to create sufficient training samples before the botnets
have changed again. To address these challenges, we propose a novel botnet
detection method, built upon Recurrent Variational Autoencoder (RVAE) that
effectively captures sequential characteristics of botnet activities. In the
experiment, this semi-supervised learning method achieves better detection
accuracy than similar learning methods, especially on hard to detect classes.
Additionally, we devise a transfer learning framework to learn from a
well-curated source data set and transfer the knowledge to a target problem
domain not seen before. Tests show that the true-positive rate (TPR) with
transfer learning is higher than the RVAE semi-supervised learning method
trained using the target data set (91.8% vs. 68.3%).","cs.CR,cs.LG,cs.NI"
"GAEA: Graph Augmentation for Equitable Access via Reinforcement Learning. Disparate access to resources by different subpopulations is a prevalent
issue in societal and sociotechnical networks. For example, urban
infrastructure networks may enable certain racial groups to more easily access
resources such as high-quality schools, grocery stores, and polling places.
Similarly, social networks within universities and organizations may enable
certain groups to more easily access people with valuable information or
influence. Here we introduce a new class of problems, Graph Augmentation for
Equitable Access (GAEA), to enhance equity in networked systems by editing
graph edges under budget constraints. We prove such problems are NP-hard, and
cannot be approximated within a factor of $(1-\tfrac{1}{3e})$. We develop a
principled, sample- and time- efficient Markov Reward Process (MRP)-based
mechanism design framework for GAEA. Our algorithm outperforms baselines on a
diverse set of synthetic graphs. We further demonstrate the method on
real-world networks, by merging public census, school, and transportation
datasets for the city of Chicago and applying our algorithm to find
human-interpretable edits to the bus network that enhance equitable access to
high-quality schools across racial groups. Further experiments on Facebook
networks of universities yield sets of new social connections that would
increase equitable access to certain attributed nodes across gender groups.","cs.AI,cs.CY,cs.LG,cs.SI"
"Domain-Aware Dynamic Networks. Deep neural networks with more parameters and FLOPs have higher capacity and
generalize better to diverse domains. But to be deployed on edge devices, the
model's complexity has to be constrained due to limited compute resource. In
this work, we propose a method to improve the model capacity without increasing
inference-time complexity. Our method is based on an assumption of data
locality: for an edge device, within a short period of time, the input data to
the device are sampled from a single domain with relatively low diversity.
Therefore, it is possible to utilize a specialized, low-complexity model to
achieve good performance in that input domain. To leverage this, we propose
Domain-aware Dynamic Network (DDN), which is a high-capacity dynamic network in
which each layer contains multiple weights. During inference, based on the
input domain, DDN dynamically combines those weights into one single weight
that specializes in the given domain. This way, DDN can keep the inference-time
complexity low but still maintain a high capacity. Experiments show that
without increasing the parameters, FLOPs, and actual latency, DDN achieves up
to 2.6\% higher AP50 than a static network on the BDD100K object-detection
benchmark.","cs.CV,cs.DC,cs.LG,stat.ML"
"The H3D Dataset for Full-Surround 3D Multi-Object Detection and Tracking in Crowded Urban Scenes. 3D multi-object detection and tracking are crucial for traffic scene
understanding. However, the community pays less attention to these areas due to
the lack of a standardized benchmark dataset to advance the field. Moreover,
existing datasets (e.g., KITTI) do not provide sufficient data and labels to
tackle challenging scenes where highly interactive and occluded traffic
participants are present. To address the issues, we present the Honda Research
Institute 3D Dataset (H3D), a large-scale full-surround 3D multi-object
detection and tracking dataset collected using a 3D LiDAR scanner. H3D
comprises of 160 crowded and highly interactive traffic scenes with a total of
1 million labeled instances in 27,721 frames. With unique dataset size, rich
annotations, and complex scenes, H3D is gathered to stimulate research on
full-surround 3D multi-object detection and tracking. To effectively and
efficiently annotate a large-scale 3D point cloud dataset, we propose a
labeling methodology to speed up the overall annotation cycle. A standardized
benchmark is created to evaluate full-surround 3D multi-object detection and
tracking algorithms. 3D object detection and tracking algorithms are trained
and tested on H3D. Finally, sources of errors are discussed for the development
of future algorithms.","cs.CV,cs.RO"
"Winograd Algorithm for AdderNet. Adder neural network (AdderNet) is a new kind of deep model that replaces the
original massive multiplications in convolutions by additions while preserving
the high performance. Since the hardware complexity of additions is much lower
than that of multiplications, the overall energy consumption is thus reduced
significantly. To further optimize the hardware overhead of using AdderNet,
this paper studies the winograd algorithm, which is a widely used fast
algorithm for accelerating convolution and saving the computational costs.
Unfortunately, the conventional Winograd algorithm cannot be directly applied
to AdderNets since the distributive law in multiplication is not valid for the
l1-norm. Therefore, we replace the element-wise multiplication in the Winograd
equation by additions and then develop a new set of transform matrixes that can
enhance the representation ability of output features to maintain the
performance. Moreover, we propose the l2-to-l1 training strategy to mitigate
the negative impacts caused by formal inconsistency. Experimental results on
both FPGA and benchmarks show that the new method can further reduce the energy
consumption without affecting the accuracy of the original AdderNet.","cs.CV,cs.LG,cs.NE"
"Forecasting Across Time Series Databases using Recurrent Neural Networks on Groups of Similar Series: A Clustering Approach. With the advent of Big Data, nowadays in many applications databases
containing large quantities of similar time series are available. Forecasting
time series in these domains with traditional univariate forecasting procedures
leaves great potentials for producing accurate forecasts untapped. Recurrent
neural networks (RNNs), and in particular Long Short-Term Memory (LSTM)
networks, have proven recently that they are able to outperform
state-of-the-art univariate time series forecasting methods in this context
when trained across all available time series. However, if the time series
database is heterogeneous, accuracy may degenerate, so that on the way towards
fully automatic forecasting methods in this space, a notion of similarity
between the time series needs to be built into the methods. To this end, we
present a prediction model that can be used with different types of RNN models
on subgroups of similar time series, which are identified by time series
clustering techniques. We assess our proposed methodology using LSTM networks,
a widely popular RNN variant. Our method achieves competitive results on
benchmarking datasets under competition evaluation procedures. In particular,
in terms of mean sMAPE accuracy, it consistently outperforms the baseline LSTM
model and outperforms all other methods on the CIF2016 forecasting competition
dataset.","cs.DB,cs.LG,econ.EM,stat.AP,stat.ML"
"Knowledge Graph Driven Approach to Represent Video Streams for Spatiotemporal Event Pattern Matching in Complex Event Processing. Complex Event Processing (CEP) is an event processing paradigm to perform
real-time analytics over streaming data and match high-level event patterns.
Presently, CEP is limited to process structured data stream. Video streams are
complicated due to their unstructured data model and limit CEP systems to
perform matching over them. This work introduces a graph-based structure for
continuous evolving video streams, which enables the CEP system to query
complex video event patterns. We propose the Video Event Knowledge Graph
(VEKG), a graph driven representation of video data. VEKG models video objects
as nodes and their relationship interaction as edges over time and space. It
creates a semantic knowledge representation of video data derived from the
detection of high-level semantic concepts from the video using an ensemble of
deep learning models. A CEP-based state optimization - VEKG-Time Aggregated
Graph (VEKG-TAG) is proposed over VEKG representation for faster event
detection. VEKG-TAG is a spatiotemporal graph aggregation method that provides
a summarized view of the VEKG graph over a given time length. We defined a set
of nine event pattern rules for two domains (Activity Recognition and Traffic
Management), which act as a query and applied over VEKG graphs to discover
complex event patterns. To show the efficacy of our approach, we performed
extensive experiments over 801 video clips across 10 datasets. The proposed
VEKG approach was compared with other state-of-the-art methods and was able to
detect complex event patterns over videos with F-Score ranging from 0.44 to
0.90. In the given experiments, the optimized VEKG-TAG was able to reduce 99%
and 93% of VEKG nodes and edges, respectively, with 5.19X faster search time,
achieving sub-second median latency of 4-20 milliseconds.","cs.AI,cs.CV,cs.DB,cs.MM"
"Deep Gaussian Process Emulation using Stochastic Imputation. We propose a novel deep Gaussian process (DGP) inference method for computer
model emulation using stochastic imputation. By stochastically imputing the
latent layers, the approach transforms the DGP into the linked GP, a
state-of-the-art surrogate model formed by linking a system of feed-forward
coupled GPs. This transformation renders a simple while efficient DGP training
procedure that only involves optimizations of conventional stationary GPs. In
addition, the analytically tractable mean and variance of the linked GP allows
one to implement predictions from DGP emulators in a fast and accurate manner.
We demonstrate the method in a series of synthetic examples and real-world
applications, and show that it is a competitive candidate for efficient DGP
surrogate modeling in comparison to the variational inference and the
fully-Bayesian approach. A $\texttt{Python}$ package $\texttt{dgpsi}$
implementing the method is also produced and available at
https://github.com/mingdeyu/DGP.","cs.LG,stat.AP,stat.CO,stat.ML"
"Jacks of All Trades, Masters Of None: Addressing Distributional Shift and Obtrusiveness via Transparent Patch Attacks. We focus on the development of effective adversarial patch attacks and -- for
the first time -- jointly address the antagonistic objectives of attack success
and obtrusiveness via the design of novel semi-transparent patches. This work
is motivated by our pursuit of a systematic performance analysis of patch
attack robustness with regard to geometric transformations. Specifically, we
first elucidate a) key factors underpinning patch attack success and b) the
impact of distributional shift between training and testing/deployment when
cast under the Expectation over Transformation (EoT) formalism. By focusing our
analysis on three principal classes of transformations (rotation, scale, and
location), our findings provide quantifiable insights into the design of
effective patch attacks and demonstrate that scale, among all factors,
significantly impacts patch attack success. Working from these findings, we
then focus on addressing how to overcome the principal limitations of scale for
the deployment of attacks in real physical settings: namely the obtrusiveness
of large patches. Our strategy is to turn to the novel design of
irregularly-shaped, semi-transparent partial patches which we construct via a
new optimization process that jointly addresses the antagonistic goals of
mitigating obtrusiveness and maximizing effectiveness. Our study -- we hope --
will help encourage more focus in the community on the issues of obtrusiveness,
scale, and success in patch attacks.","cs.CR,cs.CV,cs.LG"
"Fast and Uncertainty-Aware Directional Message Passing for Non-Equilibrium Molecules. Many important tasks in chemistry revolve around molecules during reactions.
This requires predictions far from the equilibrium, while most recent work in
machine learning for molecules has been focused on equilibrium or
near-equilibrium states. In this paper we aim to extend this scope in three
ways. First, we propose the DimeNet++ model, which is 8x faster and 10% more
accurate than the original DimeNet on the QM9 benchmark of equilibrium
molecules. Second, we validate DimeNet++ on highly reactive molecules by
developing the challenging COLL dataset, which contains distorted
configurations of small molecules during collisions. Finally, we investigate
ensembling and mean-variance estimation for uncertainty quantification with the
goal of accelerating the exploration of the vast space of non-equilibrium
structures. Our DimeNet++ implementation as well as the COLL dataset are
available online.","cs.LG,physics.chem-ph,physics.comp-ph"
"Editing Conditional Radiance Fields. A neural radiance field (NeRF) is a scene model supporting high-quality view
synthesis, optimized per scene. In this paper, we explore enabling user editing
of a category-level NeRF - also known as a conditional radiance field - trained
on a shape category. Specifically, we introduce a method for propagating coarse
2D user scribbles to the 3D space, to modify the color or shape of a local
region. First, we propose a conditional radiance field that incorporates new
modular network components, including a shape branch that is shared across
object instances. Observing multiple instances of the same category, our model
learns underlying part semantics without any supervision, thereby allowing the
propagation of coarse 2D user scribbles to the entire 3D region (e.g., chair
seat). Next, we propose a hybrid network update strategy that targets specific
network components, which balances efficiency and accuracy. During user
interaction, we formulate an optimization problem that both satisfies the
user's constraints and preserves the original object structure. We demonstrate
our approach on various editing tasks over three shape datasets and show that
it outperforms prior neural editing approaches. Finally, we edit the appearance
and shape of a real photograph and show that the edit propagates to
extrapolated novel views.","cs.CV,cs.GR,cs.LG"
"Graph Coding for Model Selection and Anomaly Detection in Gaussian Graphical Models. A classic application of description length is for model selection with the
minimum description length (MDL) principle. The focus of this paper is to
extend description length for data analysis beyond simple model selection and
sequences of scalars. More specifically, we extend the description length for
data analysis in Gaussian graphical models. These are powerful tools to model
interactions among variables in a sequence of i.i.d Gaussian data in the form
of a graph. Our method uses universal graph coding methods to accurately
account for model complexity, and therefore provide a more rigorous approach
for graph model selection. The developed method is tested with synthetic and
electrocardiogram (ECG) data to find the graph model and anomaly in Gaussian
graphical models. The experiments show that our method gives better performance
compared to commonly used methods.","cs.IT,cs.LG,math.IT"
"Intelligence, physics and information -- the tradeoff between accuracy and simplicity in machine learning. How can we enable machines to make sense of the world, and become better at
learning? To approach this goal, I believe viewing intelligence in terms of
many integral aspects, and also a universal two-term tradeoff between task
performance and complexity, provides two feasible perspectives. In this thesis,
I address several key questions in some aspects of intelligence, and study the
phase transitions in the two-term tradeoff, using strategies and tools from
physics and information. Firstly, how can we make the learning models more
flexible and efficient, so that agents can learn quickly with fewer examples?
Inspired by how physicists model the world, we introduce a paradigm and an AI
Physicist agent for simultaneously learning many small specialized models
(theories) and the domain they are accurate, which can then be simplified,
unified and stored, facilitating few-shot learning in a continual way.
Secondly, for representation learning, when can we learn a good representation,
and how does learning depend on the structure of the dataset? We approach this
question by studying phase transitions when tuning the tradeoff hyperparameter.
In the information bottleneck, we theoretically show that these phase
transitions are predictable and reveal structure in the relationships between
the data, the model, the learned representation and the loss landscape.
Thirdly, how can agents discover causality from observations? We address part
of this question by introducing an algorithm that combines prediction and
minimizing information from the input, for exploratory causal discovery from
observational time series. Fourthly, to make models more robust to label noise,
we introduce Rank Pruning, a robust algorithm for classification with noisy
labels. I believe that building on the work of my thesis we will be one step
closer to enable more intelligent machines that can make sense of the world.","cs.LG,physics.data-an,stat.ML"
"Information-Theoretic Understanding of Population Risk Improvement with Model Compression. We show that model compression can improve the population risk of a
pre-trained model, by studying the tradeoff between the decrease in the
generalization error and the increase in the empirical risk with model
compression. We first prove that model compression reduces an
information-theoretic bound on the generalization error; this allows for an
interpretation of model compression as a regularization technique to avoid
overfitting. We then characterize the increase in empirical risk with model
compression using rate distortion theory. These results imply that the
population risk could be improved by model compression if the decrease in
generalization error exceeds the increase in empirical risk. We show through a
linear regression example that such a decrease in population risk due to model
compression is indeed possible. Our theoretical results further suggest that
the Hessian-weighted $K$-means clustering compression approach can be improved
by regularizing the distance between the clustering centers. We provide
experiments with neural networks to support our theoretical assertions.","cs.IT,cs.LG,math.IT,stat.ML"
"Exploiting Edge Features in Graph Neural Networks. Edge features contain important information about graphs. However, current
state-of-the-art neural network models designed for graph learning, e.g. graph
convolutional networks (GCN) and graph attention networks (GAT), adequately
utilize edge features, especially multi-dimensional edge features. In this
paper, we build a new framework for a family of new graph neural network models
that can more sufficiently exploit edge features, including those of undirected
or multi-dimensional edges. The proposed framework can consolidate current
graph neural network models; e.g. graph convolutional networks (GCN) and graph
attention networks (GAT). The proposed framework and new models have the
following novelties: First, we propose to use doubly stochastic normalization
of graph edge features instead of the commonly used row or symmetric
normalization approches used in current graph neural networks. Second, we
construct new formulas for the operations in each individual layer so that they
can handle multi-dimensional edge features. Third, for the proposed new
framework, edge features are adaptive across network layers. As a result, our
proposed new framework and new models can exploit a rich source of graph
information. We apply our new models to graph node classification on several
citation networks, whole graph classification, and regression on several
molecular datasets. Compared with the current state-of-the-art methods, i.e.
GCNs and GAT, our models obtain better performance, which testify to the
importance of exploiting edge features in graph neural networks.","cs.LG,cs.SI,stat.ML"
"Predicting Blood Pressure Response to Fluid Bolus Therapy Using Attention-Based Neural Networks for Clinical Interpretability. Determining whether hypotensive patients in intensive care units (ICUs)
should receive fluid bolus therapy (FBT) has been an extremely challenging task
for intensive care physicians as the corresponding increase in blood pressure
has been hard to predict. Our study utilized regression models and
attention-based recurrent neural network (RNN) algorithms and a multi-clinical
information system large-scale database to build models that can predict the
successful response to FBT among hypotensive patients in ICUs. We investigated
both time-aggregated modeling using logistic regression algorithms with
regularization and time-series modeling using the long short term memory
network (LSTM) and the gated recurrent units network (GRU) with the attention
mechanism for clinical interpretability. Among all modeling strategies, the
stacked LSTM with the attention mechanism yielded the most predictable model
with the highest accuracy of 0.852 and area under the curve (AUC) value of
0.925. The study results may help identify hypotensive patients in ICUs who
will have sufficient blood pressure recovery after FBT.","cs.LG,physics.med-ph,q-bio.QM,stat.ML"
"Inferring Granger Causality from Irregularly Sampled Time Series. Continuous, automated surveillance systems that incorporate machine learning
models are becoming increasingly more common in healthcare environments. These
models can capture temporally dependent changes across multiple patient
variables and can enhance a clinician's situational awareness by providing an
early warning alarm of an impending adverse event such as sepsis. However, most
commonly used methods, e.g., XGBoost, fail to provide an interpretable
mechanism for understanding why a model produced a sepsis alarm at a given
time. The black-box nature of many models is a severe limitation as it prevents
clinicians from independently corroborating those physiologic features that
have contributed to the sepsis alarm. To overcome this limitation, we propose a
generalized linear model (GLM) approach to fit a Granger causal graph based on
the physiology of several major sepsis-associated derangements (SADs). We adopt
a recently developed stochastic monotone variational inequality-based estimator
coupled with forwarding feature selection to learn the graph structure from
both continuous and discrete-valued as well as regularly and irregularly
sampled time series. Most importantly, we develop a non-asymptotic upper bound
on the estimation error for any monotone link function in the GLM. We conduct
real-data experiments and demonstrate that our proposed method can achieve
comparable performance to popular and powerful prediction methods such as
XGBoost while simultaneously maintaining a high level of interpretability.","cs.LG,math.ST,stat.AP,stat.ME,stat.TH"
"An End-to-End Framework for Molecular Conformation Generation via Bilevel Programming. Predicting molecular conformations (or 3D structures) from molecular graphs
is a fundamental problem in many applications. Most existing approaches are
usually divided into two steps by first predicting the distances between atoms
and then generating a 3D structure through optimizing a distance geometry
problem. However, the distances predicted with such two-stage approaches may
not be able to consistently preserve the geometry of local atomic
neighborhoods, making the generated structures unsatisfying. In this paper, we
propose an end-to-end solution for molecular conformation prediction called
ConfVAE based on the conditional variational autoencoder framework.
Specifically, the molecular graph is first encoded in a latent space, and then
the 3D structures are generated by solving a principled bilevel optimization
program. Extensive experiments on several benchmark data sets prove the
effectiveness of our proposed approach over existing state-of-the-art
approaches. Code is available at
\url{https://github.com/MinkaiXu/ConfVAE-ICML21}.","cs.LG,q-bio.BM"
"Accelerating Neural ODEs Using Model Order Reduction. Embedding nonlinear dynamical systems into artificial neural networks is a
powerful new formalism for machine learning. By parameterizing ordinary
differential equations (ODEs) as neural network layers, these Neural ODEs are
memory-efficient to train, process time-series naturally and incorporate
knowledge of physical systems into deep learning models. However, the practical
applications of Neural ODEs are limited due to long inference times, because
the outputs of the embedded ODE layers are computed numerically with
differential equation solvers that can be computationally demanding. Here we
show that mathematical model order reduction methods can be used for
compressing and accelerating Neural ODEs by accurately simulating the
continuous nonlinear dynamics in low-dimensional subspaces. We implement our
novel compression method by developing Neural ODEs that integrate the necessary
subspace-projection and interpolation operations as layers of the neural
network. We validate our model reduction approach by comparing it to two
established acceleration methods from the literature in two classification
asks. In compressing convolutional and recurrent Neural ODE architectures, we
achieve the best balance between speed and accuracy when compared to the other
two acceleration methods. Based on our results, our integration of model order
reduction with Neural ODEs can facilitate efficient, dynamical system-driven
deep learning in resource-constrained applications.","34C20,cs.LG,math.DS"
"Spectral Bias and Task-Model Alignment Explain Generalization in Kernel Regression and Infinitely Wide Neural Networks. Generalization beyond a training dataset is a main goal of machine learning,
but theoretical understanding of generalization remains an open problem for
many models. The need for a new theory is exacerbated by recent observations in
deep neural networks where overparameterization leads to better performance,
contradicting the conventional wisdom from classical statistics. In this paper,
we investigate generalization error for kernel regression, which, besides being
a popular machine learning method, also includes infinitely overparameterized
neural networks trained with gradient descent. We use techniques from
statistical mechanics to derive an analytical expression for generalization
error applicable to any kernel or data distribution. We present applications of
our theory to real and synthetic datasets, and for many kernels including those
that arise from training deep neural networks in the infinite-width limit. We
elucidate an inductive bias of kernel regression to explain data with ""simple
functions"", which are identified by solving a kernel eigenfunction problem on
the data distribution. This notion of simplicity allows us to characterize
whether a kernel is compatible with a learning task, facilitating good
generalization performance from a small number of training examples. We show
that more data may impair generalization when noisy or not expressible by the
kernel, leading to non-monotonic learning curves with possibly many peaks. To
further understand these phenomena, we turn to the broad class of rotation
invariant kernels, which is relevant to training deep neural networks in the
infinite-width limit, and present a detailed mathematical analysis of them when
data is drawn from a spherically symmetric distribution and the number of input
dimensions is large.","cond-mat.dis-nn,cs.LG,stat.ML"
"U-Time: A Fully Convolutional Network for Time Series Segmentation Applied to Sleep Staging. Neural networks are becoming more and more popular for the analysis of
physiological time-series. The most successful deep learning systems in this
domain combine convolutional and recurrent layers to extract useful features to
model temporal relations. Unfortunately, these recurrent models are difficult
to tune and optimize. In our experience, they often require task-specific
modifications, which makes them challenging to use for non-experts. We propose
U-Time, a fully feed-forward deep learning approach to physiological time
series segmentation developed for the analysis of sleep data. U-Time is a
temporal fully convolutional network based on the U-Net architecture that was
originally proposed for image segmentation. U-Time maps sequential inputs of
arbitrary length to sequences of class labels on a freely chosen temporal
scale. This is done by implicitly classifying every individual time-point of
the input signal and aggregating these classifications over fixed intervals to
form the final predictions. We evaluated U-Time for sleep stage classification
on a large collection of sleep electroencephalography (EEG) datasets. In all
cases, we found that U-Time reaches or outperforms current state-of-the-art
deep learning models while being much more robust in the training process and
without requiring architecture or hyperparameter adaptation across tasks.","cs.LG,eess.SP,stat.ML"
"Symbolic Pregression: Discovering Physical Laws from Distorted Video. We present a method for unsupervised learning of equations of motion for
objects in raw and optionally distorted unlabeled video. We first train an
autoencoder that maps each video frame into a low-dimensional latent space
where the laws of motion are as simple as possible, by minimizing a combination
of non-linearity, acceleration and prediction error. Differential equations
describing the motion are then discovered using Pareto-optimal symbolic
regression. We find that our pre-regression (""pregression"") step is able to
rediscover Cartesian coordinates of unlabeled moving objects even when the
video is distorted by a generalized lens. Using intuition from multidimensional
knot-theory, we find that the pregression step is facilitated by first adding
extra latent space dimensions to avoid topological problems during training and
then removing these extra dimensions via principal component analysis.","cs.AI,cs.CV,cs.LG,physics.comp-ph,stat.ML"
"The watershed concept and its use in segmentation : a brief history. The watershed is one of the most used tools in image segmentation. We present
how its concept is born and developed over time. Its implementation as an
algorithm or a hardwired device evolved together with the technology which
allowed it. We present also how it is used in practice, first together with
markers, and later introduced in a multiscale framework, in order to produce
not a unique partition but a complete hierarchy.","05C85,68U10,cs.CV"
"Can the Transformer Be Used as a Drop-in Replacement for RNNs in Text-Generating GANs?. In this paper we address the problem of fine-tuned text generation with a
limited computational budget. For that, we use a well-performing text
generative adversarial network (GAN) architecture - Diversity-Promoting GAN
(DPGAN), and attempted a drop-in replacement of the LSTM layer with a
self-attention-based Transformer layer in order to leverage their efficiency.
The resulting Self-Attention DPGAN (SADPGAN) was evaluated for performance,
quality and diversity of generated text and stability. Computational
experiments suggested that a transformer architecture is unable to drop-in
replace the LSTM layer, under-performing during the pre-training phase and
undergoing a complete mode collapse during the GAN tuning phase. Our results
suggest that the transformer architecture need to be adapted before it can be
used as a replacement for RNNs in text-generating GANs.","68T05,68T50,I.2.7,cs.CL,cs.LG"
"Learning to Reason in Large Theories without Imitation. In this paper, we demonstrate how to do automated theorem proving in the
presence of a large knowledge base of potential premises without learning from
human proofs. We suggest an exploration mechanism that mixes in additional
premises selected by a tf-idf (term frequency-inverse document frequency) based
lookup in a deep reinforcement learning scenario. This helps with exploring and
learning which premises are relevant for proving a new theorem. Our experiments
show that the theorem prover trained with this exploration mechanism
outperforms provers that are trained only on human proofs. It approaches the
performance of a prover trained by a combination of imitation and reinforcement
learning. We perform multiple experiments to understand the importance of the
underlying assumptions that make our exploration approach work, thus explaining
our design choices.","cs.AI,cs.LG,cs.LO,stat.ML"
"Multivariate Time Series Forecasting with Transfer Entropy Graph. Multivariate time series (MTS) forecasting is an important problem in many
fields. Accurate forecasting results can effectively help decision-making. To
date, many MTS forecasting methods have been proposed and widely applied.
However, these methods assume that the predicted value of a single variable is
affected by all other variables, which ignores the causal relationship among
variables. To address the above issue, a novel end-to-end deep learning model,
termed graph neural network with transfer entropy (TEGNN) is proposed in this
paper. To characterize the causal information among variables, the transfer
entropy (TE) graph is introduced in our model, where each variable is regarded
as a graph node and each edge represents the casual relationship between
variables. In addition, convolutional neural network (CNN) filters with
different perception scales are used for time series feature extraction, which
is used to generate the feature of each node. Finally, graph neural network
(GNN) is adopted to tackle the forecasting problem of graph structure generated
by MTS. Three benchmark datasets from the real world are used to evaluate the
proposed TEGNN and the comprehensive experiments show that the proposed method
achieves state-of-the-art results in MTS forecasting task.","cs.LG,eess.SP"
"On Deep Representation Learning from Noisy Web Images. The keep-growing content of Web images may be the next important data source
to scale up deep neural networks, which recently obtained a great success in
the ImageNet classification challenge and related tasks. This prospect,
however, has not been validated on convolutional networks (convnet) -- one of
best performing deep models -- because of their supervised regime. While
unsupervised alternatives are not so good as convnet in generalizing the
learned model to new domains, we use convnet to leverage semi-supervised
representation learning. Our approach is to use massive amounts of unlabeled
and noisy Web images to train convnets as general feature detectors despite
challenges coming from data such as high level of mislabeled data, outliers,
and data biases. Extensive experiments are conducted at several data scales,
different network architectures, and data reranking techniques. The learned
representations are evaluated on nine public datasets of various topics. The
best results obtained by our convnets, trained on 3.14 million Web images,
outperform AlexNet trained on 1.2 million clean images of ILSVRC 2012 and is
closing the gap with VGG-16. These prominent results suggest a budget solution
to use deep learning in practice and motivate more research in semi-supervised
representation learning.","cs.CV,cs.MM"
"Self-Supervised Learning of Contextual Embeddings for Link Prediction in Heterogeneous Networks. Representation learning methods for heterogeneous networks produce a
low-dimensional vector embedding for each node that is typically fixed for all
tasks involving the node. Many of the existing methods focus on obtaining a
static vector representation for a node in a way that is agnostic to the
downstream application where it is being used. In practice, however, downstream
tasks such as link prediction require specific contextual information that can
be extracted from the subgraphs related to the nodes provided as input to the
task. To tackle this challenge, we develop SLiCE, a framework bridging static
representation learning methods using global information from the entire graph
with localized attention driven mechanisms to learn contextual node
representations. We first pre-train our model in a self-supervised manner by
introducing higher-order semantic associations and masking nodes, and then
fine-tune our model for a specific link prediction task. Instead of training
node representations by aggregating information from all semantic neighbors
connected via metapaths, we automatically learn the composition of different
metapaths that characterize the context for a specific task without the need
for any pre-defined metapaths. SLiCE significantly outperforms both static and
contextual embedding learning methods on several publicly available benchmark
network datasets. We also interpret the semantic association matrix and provide
its utility and relevance in making successful link predictions between
heterogeneous nodes in the network.","cs.LG,cs.SI,stat.ML"
"Robust Differentially Private Training of Deep Neural Networks. Differentially private stochastic gradient descent (DPSGD) is a variation of
stochastic gradient descent based on the Differential Privacy (DP) paradigm
which can mitigate privacy threats arising from the presence of sensitive
information in training data. One major drawback of training deep neural
networks with DPSGD is a reduction in the model's accuracy. In this paper, we
propose an alternative method for preserving data privacy based on introducing
noise through learnable probability distributions, which leads to a significant
improvement in the utility of the resulting private models. We also demonstrate
that normalization layers have a large beneficial impact on the performance of
deep neural networks with noisy parameters. In particular, we show that
contrary to general belief, a large amount of random noise can be added to the
weights of neural networks without harming the performance, once the networks
are augmented with normalization layers. We hypothesize that this robustness is
a consequence of the scale invariance property of normalization operators.
Building on these observations, we propose a new algorithmic technique for
training deep neural networks under very low privacy budgets by sampling
weights from Gaussian distributions and utilizing batch or layer normalization
techniques to prevent performance degradation. Our method outperforms previous
approaches, including DPSGD, by a substantial margin on a comprehensive set of
experiments on Computer Vision and Natural Language Processing tasks. In
particular, we obtain a 20 percent accuracy improvement over DPSGD on the MNIST
and CIFAR10 datasets with DP-privacy budgets of $\varepsilon = 0.05$ and
$\varepsilon = 2.0$, respectively. Our code is available online:
https://github.com/uds-lsv/SIDP.","cs.CR,cs.LG,stat.ML"
"Nonparametric Density Estimation for High-Dimensional Data - Algorithms and Applications. Density Estimation is one of the central areas of statistics whose purpose is
to estimate the probability density function underlying the observed data. It
serves as a building block for many tasks in statistical inference,
visualization, and machine learning. Density Estimation is widely adopted in
the domain of unsupervised learning especially for the application of
clustering. As big data become pervasive in almost every area of data sciences,
analyzing high-dimensional data that have many features and variables appears
to be a major focus in both academia and industry. High-dimensional data pose
challenges not only from the theoretical aspects of statistical inference, but
also from the algorithmic/computational considerations of machine learning and
data analytics. This paper reviews a collection of selected nonparametric
density estimation algorithms for high-dimensional data, some of them are
recently published and provide interesting mathematical insights. The important
application domain of nonparametric density estimation, such as { modal
clustering}, are also included in this paper. Several research directions
related to density estimation and high-dimensional data analysis are suggested
by the authors.","cs.LG,stat.CO,stat.ML"
"Multi-Modal Aesthetic Assessment for MObile Gaming Image. With the proliferation of various gaming technology, services, game styles,
and platforms, multi-dimensional aesthetic assessment of the gaming contents is
becoming more and more important for the gaming industry. Depending on the
diverse needs of diversified game players, game designers, graphical
developers, etc. in particular conditions, multi-modal aesthetic assessment is
required to consider different aesthetic dimensions/perspectives. Since there
are different underlying relationships between different aesthetic dimensions,
e.g., between the `Colorfulness' and `Color Harmony', it could be advantageous
to leverage effective information attached in multiple relevant dimensions. To
this end, we solve this problem via multi-task learning. Our inclination is to
seek and learn the correlations between different aesthetic relevant dimensions
to further boost the generalization performance in predicting all the aesthetic
dimensions. Therefore, the `bottleneck' of obtaining good predictions with
limited labeled data for one individual dimension could be unplugged by
harnessing complementary sources of other dimensions, i.e., augment the
training data indirectly by sharing training information across dimensions.
According to experimental results, the proposed model outperforms
state-of-the-art aesthetic metrics significantly in predicting four gaming
aesthetic dimensions.","68U10,J.0,cs.AI,cs.CV"
"Convolutional Embedded Networks for Population Scale Clustering and Bio-ancestry Inferencing. The study of genetic variants can help find correlating population groups to
identify cohorts that are predisposed to common diseases and explain
differences in disease susceptibility and how patients react to drugs. Machine
learning algorithms are increasingly being applied to identify interacting GVs
to understand their complex phenotypic traits. Since the performance of a
learning algorithm not only depends on the size and nature of the data but also
on the quality of underlying representation, deep neural networks can learn
non-linear mappings that allow transforming GVs data into more clustering and
classification friendly representations than manual feature selection. In this
paper, we proposed convolutional embedded networks in which we combine two DNN
architectures called convolutional embedded clustering and convolutional
autoencoder classifier for clustering individuals and predicting geographic
ethnicity based on GVs, respectively. We employed CAE-based representation
learning on 95 million GVs from the 1000 genomes and Simons genome diversity
projects. Quantitative and qualitative analyses with a focus on accuracy and
scalability show that our approach outperforms state-of-the-art approaches such
as VariantSpark and ADMIXTURE. In particular, CEC can cluster targeted
population groups in 22 hours with an adjusted rand index of 0.915, the
normalized mutual information of 0.92, and the clustering accuracy of 89%.
Contrarily, the CAE classifier can predict the geographic ethnicity of unknown
samples with an F1 and Mathews correlation coefficient(MCC) score of 0.9004 and
0.8245, respectively. To provide interpretations of the predictions, we
identify significant biomarkers using gradient boosted trees(GBT) and SHAP.
Overall, our approach is transparent and faster than the baseline methods, and
scalable for 5% to 100% of the full human genome.","cs.LG,q-bio.QM,stat.ML"
"Chow-Liu++: Optimal Prediction-Centric Learning of Tree Ising Models. We consider the problem of learning a tree-structured Ising model from data,
such that subsequent predictions computed using the model are accurate.
Concretely, we aim to learn a model such that posteriors $P(X_i|X_S)$ for small
sets of variables $S$ are accurate. Since its introduction more than 50 years
ago, the Chow-Liu algorithm, which efficiently computes the maximum likelihood
tree, has been the benchmark algorithm for learning tree-structured graphical
models. A bound on the sample complexity of the Chow-Liu algorithm with respect
to the prediction-centric local total variation loss was shown in [BK19]. While
those results demonstrated that it is possible to learn a useful model even
when recovering the true underlying graph is impossible, their bound depends on
the maximum strength of interactions and thus does not achieve the
information-theoretic optimum. In this paper, we introduce a new algorithm that
carefully combines elements of the Chow-Liu algorithm with tree metric
reconstruction methods to efficiently and optimally learn tree Ising models
under a prediction-centric loss. Our algorithm is robust to model
misspecification and adversarial corruptions. In contrast, we show that the
celebrated Chow-Liu algorithm can be arbitrarily suboptimal.","cs.DS,cs.IT,cs.LG,math.IT,math.ST,stat.TH"
"word2vec, node2vec, graph2vec, X2vec: Towards a Theory of Vector Embeddings of Structured Data. Vector representations of graphs and relational structures, whether
hand-crafted feature vectors or learned representations, enable us to apply
standard data analysis and machine learning techniques to the structures. A
wide range of methods for generating such embeddings have been studied in the
machine learning and knowledge representation literature. However, vector
embeddings have received relatively little attention from a theoretical point
of view.
  Starting with a survey of embedding techniques that have been used in
practice, in this paper we propose two theoretical approaches that we see as
central for understanding the foundations of vector embeddings. We draw
connections between the various approaches and suggest directions for future
research.","cs.DB,cs.DM,cs.LG,stat.ML"
"Efficiently Constructing Adversarial Examples by Feature Watermarking. With the increasing attentions of deep learning models, attacks are also
upcoming for such models. For example, an attacker may carefully construct
images in specific ways (also referred to as adversarial examples) aiming to
mislead the deep learning models to output incorrect classification results.
Similarly, many efforts are proposed to detect and mitigate adversarial
examples, usually for certain dedicated attacks. In this paper, we propose a
novel digital watermark based method to generate adversarial examples for deep
learning models. Specifically, partial main features of the watermark image are
embedded into the host image invisibly, aiming to tamper and damage the
recognition capabilities of the deep learning models. We devise an efficient
mechanism to select host images and watermark images, and utilize the improved
discrete wavelet transform (DWT) based Patchwork watermarking algorithm and the
modified discrete cosine transform (DCT) based Patchwork watermarking
algorithm. The experimental results showed that our scheme is able to generate
a large number of adversarial examples efficiently. In addition, we find that
using the extracted features of the image as the watermark images, can increase
the success rate of an attack under certain conditions with minimal changes to
the host image. To ensure repeatability, reproducibility, and code sharing, the
source code is available on GitHub","I.2.0,cs.CR,cs.CV"
"On the Universality of Rotation Equivariant Point Cloud Networks. Learning functions on point clouds has applications in many fields, including
computer vision, computer graphics, physics, and chemistry. Recently, there has
been a growing interest in neural architectures that are invariant or
equivariant to all three shape-preserving transformations of point clouds:
translation, rotation, and permutation.
  In this paper, we present a first study of the approximation power of these
architectures. We first derive two sufficient conditions for an equivariant
architecture to have the universal approximation property, based on a novel
characterization of the space of equivariant polynomials. We then use these
conditions to show that two recently suggested models are universal, and for
devising two other novel universal architectures.","cs.CG,cs.LG"
"How Well Can Generative Adversarial Networks Learn Densities: A Nonparametric View. We study in this paper the rate of convergence for learning densities under
the Generative Adversarial Networks (GAN) framework, borrowing insights from
nonparametric statistics. We introduce an improved GAN estimator that achieves
a faster rate, through simultaneously leveraging the level of smoothness in the
target density and the evaluation metric, which in theory remedies the mode
collapse problem reported in the literature. A minimax lower bound is
constructed to show that when the dimension is large, the exponent in the rate
for the new GAN estimator is near optimal. One can view our results as
answering in a quantitative way how well GAN learns a wide range of densities
with different smoothness properties, under a hierarchy of evaluation metrics.
As a byproduct, we also obtain improved generalization bounds for GAN with
deeper ReLU discriminator network.","cs.LG,math.ST,stat.ML,stat.TH"
"Optimal Rates of Distributed Regression with Imperfect Kernels. Distributed machine learning systems have been receiving increasing
attentions for their efficiency to process large scale data. Many distributed
frameworks have been proposed for different machine learning tasks. In this
paper, we study the distributed kernel regression via the divide and conquer
approach. This approach has been proved asymptotically minimax optimal if the
kernel is perfectly selected so that the true regression function lies in the
associated reproducing kernel Hilbert space. However, this is usually, if not
always, impractical because kernels that can only be selected via prior
knowledge or a tuning process are hardly perfect. Instead it is more common
that the kernel is good enough but imperfect in the sense that the true
regression can be well approximated by but does not lie exactly in the kernel
space. We show distributed kernel regression can still achieves capacity
independent optimal rate in this case. To this end, we first establish a
general framework that allows to analyze distributed regression with response
weighted base algorithms by bounding the error of such algorithms on a single
data set, provided that the error bounds has factored the impact of the
unexplained variance of the response variable. Then we perform a leave one out
analysis of the kernel ridge regression and bias corrected kernel ridge
regression, which in combination with the aforementioned framework allows us to
derive sharp error bounds and capacity independent optimal rates for the
associated distributed kernel regression algorithms. As a byproduct of the
thorough analysis, we also prove the kernel ridge regression can achieve rates
faster than $N^{-1}$ (where $N$ is the sample size) in the noise free setting
which, to our best knowledge, are first observed and novel in regression
learning.","68Q32,68T05,68W15,cs.DC,cs.LG,math.ST,stat.ML,stat.TH"
"Finding the Needle in the Haystack with Convolutions: on the benefits of architectural bias. Despite the phenomenal success of deep neural networks in a broad range of
learning tasks, there is a lack of theory to understand the way they work. In
particular, Convolutional Neural Networks (CNNs) are known to perform much
better than Fully-Connected Networks (FCNs) on spatially structured data: the
architectural structure of CNNs benefits from prior knowledge on the features
of the data, for instance their translation invariance. The aim of this work is
to understand this fact through the lens of dynamics in the loss landscape.
  We introduce a method that maps a CNN to its equivalent FCN (denoted as
eFCN). Such an embedding enables the comparison of CNN and FCN training
dynamics directly in the FCN space. We use this method to test a new training
protocol, which consists in training a CNN, embedding it to FCN space at a
certain ``relax time'', then resuming the training in FCN space. We observe
that for all relax times, the deviation from the CNN subspace is small, and the
final performance reached by the eFCN is higher than that reachable by a
standard FCN of same architecture. More surprisingly, for some intermediate
relax times, the eFCN outperforms the CNN it stemmed, by combining the prior
information of the CNN and the expressivity of the FCN in a complementary way.
The practical interest of our protocol is limited by the very large size of the
highly sparse eFCN. However, it offers interesting insights into the
persistence of architectural bias under stochastic gradient dynamics. It shows
the existence of some rare basins in the FCN loss landscape associated with
very good generalization. These can only be accessed thanks to the CNN prior,
which helps navigate the landscape during the early stages of optimization.","cond-mat.dis-nn,cond-mat.stat-mech,cs.LG,stat.ML"
"DeepGaze II: Reading fixations from deep features trained on object recognition. Here we present DeepGaze II, a model that predicts where people look in
images. The model uses the features from the VGG-19 deep neural network trained
to identify objects in images. Contrary to other saliency models that use deep
features, here we use the VGG features for saliency prediction with no
additional fine-tuning (rather, a few readout layers are trained on top of the
VGG features to predict saliency). The model is therefore a strong test of
transfer learning. After conservative cross-validation, DeepGaze II explains
about 87% of the explainable information gain in the patterns of fixations and
achieves top performance in area under the curve metrics on the MIT300 hold-out
benchmark. These results corroborate the finding from DeepGaze I (which
explained 56% of the explainable information gain), that deep features trained
on object recognition provide a versatile feature space for performing related
visual tasks. We explore the factors that contribute to this success and
present several informative image examples. A web service is available to
compute model predictions at http://deepgaze.bethgelab.org.","cs.CV,q-bio.NC,stat.AP"
"Learning in Sinusoidal Spaces with Physics-Informed Neural Networks. A physics-informed neural network (PINN) uses physics-augmented loss
functions, e.g., incorporating the residual term from governing differential
equations, to ensure its output is consistent with fundamental physics laws.
However, it turns out to be difficult to train an accurate PINN model for many
problems in practice. In this paper, we address this issue through a novel
perspective on the merits of learning in sinusoidal spaces with PINNs. By
analyzing asymptotic behavior at model initialization, we first prove that a
PINN of increasing size (i.e., width and depth) induces a bias towards flat
outputs. Notably, a flat function is a trivial solution to many physics
differential equations, hence, deceptively minimizing the residual term of the
augmented loss while being far from the true solution. We then show that the
sinusoidal mapping of inputs, in an architecture we label as sf-PINN, is able
to elevate output variability, thus avoiding being trapped in the deceptive
local minimum. In addition, the level of variability can be effectively
modulated to match high-frequency patterns in the problem at hand. A key facet
of this paper is the comprehensive empirical study that demonstrates the
efficacy of learning in sinusoidal spaces with PINNs for a wide range of
forward and inverse modelling problems spanning multiple physics domains.","cs.AI,cs.CE,cs.LG,physics.comp-ph"
"From Multiview Image Curves to 3D Drawings. Reconstructing 3D scenes from multiple views has made impressive strides in
recent years, chiefly by correlating isolated feature points, intensity
patterns, or curvilinear structures. In the general setting - without
controlled acquisition, abundant texture, curves and surfaces following
specific models or limiting scene complexity - most methods produce unorganized
point clouds, meshes, or voxel representations, with some exceptions producing
unorganized clouds of 3D curve fragments. Ideally, many applications require
structured representations of curves, surfaces and their spatial relationships.
This paper presents a step in this direction by formulating an approach that
combines 2D image curves into a collection of 3D curves, with topological
connectivity between them represented as a 3D graph. This results in a 3D
drawing, which is complementary to surface representations in the same sense as
a 3D scaffold complements a tent taut over it. We evaluate our results against
truth on synthetic and real datasets.","53A20,65D17,68U05,68U10,I.4.8; I.4.10; I.4.6; I.3.5; J.6,cs.CG,cs.CV,cs.GR,cs.RO"
"Quantum Ensemble for Classification. A powerful way to improve performance in machine learning is to construct an
ensemble that combines the predictions of multiple models. Ensemble methods are
often much more accurate and lower variance than the individual classifiers
that make them up but have high requirements in terms of memory and
computational time. In fact, a large number of alternative algorithms is
usually adopted, each requiring to query all available data. We propose a new
quantum algorithm that exploits quantum superposition, entanglement and
interference to build an ensemble of classification models. Thanks to the
generation of the several quantum trajectories in superposition, we obtain $B$
transformations of the quantum state which encodes the training set in only
$log\left(B\right)$ operations. This implies an exponential speed-up in the
size of the ensemble with respect to classical methods. Furthermore, when
considering the overall cost of the algorithm, we show that the training of a
single weak classifier impacts additively rather than multiplicatively, as it
usually happens. We also present small-scale experiments, defining a quantum
version of the cosine classifier and using the IBM qiskit environment to show
how the algorithm works.","cs.LG,quant-ph,stat.ML"
"On the equivalence of molecular graph convolution and molecular wave function with poor basis set. In this study, we demonstrate that the linear combination of atomic orbitals
(LCAO), an approximation of quantum physics introduced by Pauling and
Lennard-Jones in the 1920s, corresponds to graph convolutional networks (GCNs)
for molecules. However, GCNs involve unnecessary nonlinearity and deep
architecture. We also verify that molecular GCNs are based on a poor basis
function set compared with the standard one used in theoretical calculations or
quantum chemical simulations. From these observations, we describe the quantum
deep field (QDF), a machine learning (ML) model based on an underlying quantum
physics, in particular the density functional theory (DFT). We believe that the
QDF model can be easily understood because it can be regarded as a single
linear layer GCN. Moreover, it uses two vanilla feedforward neural networks to
learn an energy functional and a Hohenberg--Kohn map that have nonlinearities
inherent in quantum physics and the DFT. For molecular energy prediction tasks,
we demonstrated the viability of an ``extrapolation,'' in which we trained a
QDF model with small molecules, tested it with large molecules, and achieved
high extrapolation performance. This will lead to reliable and practical
applications for discovering effective materials. The implementation is
available at https://github.com/masashitsubaki/QuantumDeepField_molecule.","cond-mat.mtrl-sci,cs.LG,physics.chem-ph"
"Let's Get Dirty: GAN Based Data Augmentation for Camera Lens Soiling Detection in Autonomous Driving. Wide-angle fisheye cameras are commonly used in automated driving for parking
and low-speed navigation tasks. Four of such cameras form a surround-view
system that provides a complete and detailed view of the vehicle. These cameras
are directly exposed to harsh environmental settings and can get soiled very
easily by mud, dust, water, frost. Soiling on the camera lens can severely
degrade the visual perception algorithms, and a camera cleaning system
triggered by a soiling detection algorithm is increasingly being deployed.
While adverse weather conditions, such as rain, are getting attention recently,
there is only limited work on general soiling. The main reason is the
difficulty in collecting a diverse dataset as it is a relatively rare event. We
propose a novel GAN based algorithm for generating unseen patterns of soiled
images. Additionally, the proposed method automatically provides the
corresponding soiling masks eliminating the manual annotation cost.
Augmentation of the generated soiled images for training improves the accuracy
of soiling detection tasks significantly by 18% demonstrating its usefulness.
The manually annotated soiling dataset and the generated augmentation dataset
will be made public. We demonstrate the generalization of our fisheye trained
GAN model on the Cityscapes dataset. We provide an empirical evaluation of the
degradation of the semantic segmentation algorithm with the soiled data.","cs.CV,cs.LG,cs.RO"
"TOG: Targeted Adversarial Objectness Gradient Attacks on Real-time Object Detection Systems. The rapid growth of real-time huge data capturing has pushed the deep
learning and data analytic computing to the edge systems. Real-time object
recognition on the edge is one of the representative deep neural network (DNN)
powered edge systems for real-world mission-critical applications, such as
autonomous driving and augmented reality. While DNN powered object detection
edge systems celebrate many life-enriching opportunities, they also open doors
for misuse and abuse. This paper presents three Targeted adversarial Objectness
Gradient attacks, coined as TOG, which can cause the state-of-the-art deep
object detection networks to suffer from object-vanishing, object-fabrication,
and object-mislabeling attacks. We also present a universal objectness gradient
attack to use adversarial transferability for black-box attacks, which is
effective on any inputs with negligible attack time cost, low human
perceptibility, and particularly detrimental to object detection edge systems.
We report our experimental measurements using two benchmark datasets (PASCAL
VOC and MS COCO) on two state-of-the-art detection algorithms (YOLO and SSD).
The results demonstrate serious adversarial vulnerabilities and the compelling
need for developing robust object detection systems.","cs.CR,cs.CV,cs.LG,stat.ML"
"Co-attentional Transformers for Story-Based Video Understanding. Inspired by recent trends in vision and language learning, we explore
applications of attention mechanisms for visio-lingual fusion within an
application to story-based video understanding. Like other video-based QA
tasks, video story understanding requires agents to grasp complex temporal
dependencies. However, as it focuses on the narrative aspect of video it also
requires understanding of the interactions between different characters, as
well as their actions and their motivations. We propose a novel co-attentional
transformer model to better capture long-term dependencies seen in visual
stories such as dramas and measure its performance on the video question
answering task. We evaluate our approach on the recently introduced DramaQA
dataset which features character-centered video story understanding questions.
Our model outperforms the baseline model by 8 percentage points overall, at
least 4.95 and up to 12.8 percentage points on all difficulty levels and
manages to beat the winner of the DramaQA challenge.","cs.AI,cs.CL,cs.CV"
"Adaptive Discretization for Episodic Reinforcement Learning in Metric Spaces. We present an efficient algorithm for model-free episodic reinforcement
learning on large (potentially continuous) state-action spaces. Our algorithm
is based on a novel $Q$-learning policy with adaptive data-driven
discretization. The central idea is to maintain a finer partition of the
state-action space in regions which are frequently visited in historical
trajectories, and have higher payoff estimates. We demonstrate how our adaptive
partitions take advantage of the shape of the optimal $Q$-function and the
joint space, without sacrificing the worst-case performance. In particular, we
recover the regret guarantees of prior algorithms for continuous state-action
spaces, which additionally require either an optimal discretization as input,
and/or access to a simulation oracle. Moreover, experiments demonstrate how our
algorithm automatically adapts to the underlying structure of the problem,
resulting in much better performance compared both to heuristics and
$Q$-learning with uniform discretization.","68Q32,I.2.6,cs.LG,stat.ML"
"Efficient Inference via Universal LSH Kernel. Large machine learning models achieve unprecedented performance on various
tasks and have evolved as the go-to technique. However, deploying these compute
and memory hungry models on resource constraint environments poses new
challenges. In this work, we propose mathematically provable Representer
Sketch, a concise set of count arrays that can approximate the inference
procedure with simple hashing computations and aggregations. Representer Sketch
builds upon the popular Representer Theorem from kernel literature, hence the
name, providing a generic fundamental alternative to the problem of efficient
inference that goes beyond the popular approach such as quantization, iterative
pruning and knowledge distillation. A neural network function is transformed to
its weighted kernel density representation, which can be very efficiently
estimated with our sketching algorithm. Empirically, we show that Representer
Sketch achieves up to 114x reduction in storage requirement and 59x reduction
in computation complexity without any drop in accuracy.","cs.AI,cs.DS,cs.LG"
"Metareasoning in Modular Software Systems: On-the-Fly Configuration using Reinforcement Learning with Rich Contextual Representations. Assemblies of modular subsystems are being pressed into service to perform
sensing, reasoning, and decision making in high-stakes, time-critical tasks in
such areas as transportation, healthcare, and industrial automation. We address
the opportunity to maximize the utility of an overall computing system by
employing reinforcement learning to guide the configuration of the set of
interacting modules that comprise the system. The challenge of doing
system-wide optimization is a combinatorial problem. Local attempts to boost
the performance of a specific module by modifying its configuration often leads
to losses in overall utility of the system's performance as the distribution of
inputs to downstream modules changes drastically. We present metareasoning
techniques which consider a rich representation of the input, monitor the state
of the entire pipeline, and adjust the configuration of modules on-the-fly so
as to maximize the utility of a system's operation. We show significant
improvement in both real-world and synthetic pipelines across a variety of
reinforcement learning techniques.","cs.AI,cs.LG,cs.SE,stat.ML"
"A Gentle Introduction to Conformal Prediction and Distribution-Free Uncertainty Quantification. Black-box machine learning learning methods are now routinely used in
high-risk settings, like medical diagnostics, which demand uncertainty
quantification to avoid consequential model failures. Distribution-free
uncertainty quantification (distribution-free UQ) is a user-friendly paradigm
for creating statistically rigorous confidence intervals/sets for such
predictions. Critically, the intervals/sets are valid without distributional
assumptions or model assumptions, with explicit guarantees with finitely many
datapoints. Moreover, they adapt to the difficulty of the input; when the input
example is difficult, the uncertainty intervals/sets are large, signaling that
the model might be wrong. Without much work, one can use distribution-free
methods on any underlying algorithm, such as a neural network, to produce
confidence sets guaranteed to contain the ground truth with a user-specified
probability, such as 90%. Indeed, the methods are easy-to-understand and
general, applying to many modern prediction problems arising in the fields of
computer vision, natural language processing, deep reinforcement learning, and
so on. This hands-on introduction is aimed at a reader interested in the
practical implementation of distribution-free UQ, including conformal
prediction and related methods, who is not necessarily a statistician. We will
include many explanatory illustrations, examples, and code samples in Python,
with PyTorch syntax. The goal is to provide the reader a working understanding
of distribution-free UQ, allowing them to put confidence intervals on their
algorithms, with one self-contained document.","cs.AI,cs.LG,math.ST,stat.ME,stat.ML,stat.TH"
"CopulaGNN: Towards Integrating Representational and Correlational Roles of Graphs in Graph Neural Networks. Graph-structured data are ubiquitous. However, graphs encode diverse types of
information and thus play different roles in data representation. In this
paper, we distinguish the \textit{representational} and the
\textit{correlational} roles played by the graphs in node-level prediction
tasks, and we investigate how Graph Neural Network (GNN) models can effectively
leverage both types of information. Conceptually, the representational
information provides guidance for the model to construct better node features;
while the correlational information indicates the correlation between node
outcomes conditional on node features. Through a simulation study, we find that
many popular GNN models are incapable of effectively utilizing the
correlational information. By leveraging the idea of the copula, a principled
way to describe the dependence among multivariate random variables, we offer a
general solution. The proposed Copula Graph Neural Network (CopulaGNN) can take
a wide range of GNN models as base models and utilize both representational and
correlational information stored in the graphs. Experimental results on two
types of regression tasks verify the effectiveness of the proposed method.","cs.LG,stat.ML"
"On the price of explainability for some clustering problems. The price of explainability for a clustering task can be defined as the
unavoidable loss,in terms of the objective function, if we force the final
partition to be explainable.
  Here, we study this price for the following clustering problems: $k$-means,
$k$-medians, $k$-centers and maximum-spacing. We provide upper and lower bounds
for a natural model where explainability is achieved via decision trees. For
the $k$-means and $k$-medians problems our upper bounds improve those obtained
by [Moshkovitz et. al, ICML 20] for low dimensions.
  Another contribution is a simple and efficient algorithm for building
explainable clusterings for the $k$-means problem. We provide empirical
evidence that its performance is better than the current state of the art for
decision-tree based explainable clustering.","cs.DS,cs.LG"
"Multi-Agent Generative Adversarial Imitation Learning. Imitation learning algorithms can be used to learn a policy from expert
demonstrations without access to a reward signal. However, most existing
approaches are not applicable in multi-agent settings due to the existence of
multiple (Nash) equilibria and non-stationary environments. We propose a new
framework for multi-agent imitation learning for general Markov games, where we
build upon a generalized notion of inverse reinforcement learning. We further
introduce a practical multi-agent actor-critic algorithm with good empirical
performance. Our method can be used to imitate complex behaviors in
high-dimensional environments with multiple cooperative or competing agents.","cs.AI,cs.LG,cs.MA,stat.ML"
"Geometry-aware Transformer for molecular property prediction. Recently, graph neural networks (GNNs) have achieved remarkable performances
for quantum mechanical problems. However, a graph convolution can only cover a
localized region, and cannot capture long-range interactions of atoms. This
behavior is contrary to theoretical interatomic potentials, which is a
fundamental limitation of the spatial based GNNs. In this work, we propose a
novel attention-based framework for molecular property prediction tasks. We
represent a molecular conformation as a discrete atomic sequence combined by
atom-atom distance attributes, named Geometry-aware Transformer (GeoT). In
particular, we adopt a Transformer architecture, which has been widely used for
sequential data. Our proposed model trains sequential representations of
molecular graphs based on globally constructed attentions, maintaining all
spatial arrangements of atom pairs. Our method does not suffer from cost
intensive computations, such as angle calculations. The experimental results on
several public benchmarks and visualization maps verified that keeping the
long-range interatomic attributes can significantly improve the model
predictability.","cs.LG,physics.chem-ph"
"Three-Stream 3D/1D CNN for Fine-Grained Action Classification and Segmentation in Table Tennis. This paper proposes a fusion method of modalities extracted from video
through a three-stream network with spatio-temporal and temporal convolutions
for fine-grained action classification in sport. It is applied to TTStroke-21
dataset which consists of untrimmed videos of table tennis games. The goal is
to detect and classify table tennis strokes in the videos, the first step of a
bigger scheme aiming at giving feedback to the players for improving their
performance. The three modalities are raw RGB data, the computed optical flow
and the estimated pose of the player. The network consists of three branches
with attention blocks. Features are fused at the latest stage of the network
using bilinear layers. Compared to previous approaches, the use of three
modalities allows faster convergence and better performances on both tasks:
classification of strokes with known temporal boundaries and joint segmentation
and classification. The pose is also further investigated in order to offer
richer feedback to the athletes.","cs.AI,cs.CV,cs.HC,cs.LG,cs.MM"
"Disparity Image Segmentation For ADAS. We present a simple solution for segmenting grayscale images using existing
Connected Component Labeling (CCL) algorithms (which are generally applied to
binary images), which was efficient enough to be implemented in a constrained
(embedded automotive) architecture. Our solution customizes the region growing
and merging approach, and is primarily targeted for stereoscopic disparity
images where nearer objects carry more relevance. We provide results from a
standard OpenCV implementation for some basic cases and an image from the
Tsukuba stereo-pair dataset.","68U10,68W99,I.4.6; I.4.5,cs.CV"
"N-BEATS neural network for mid-term electricity load forecasting. This paper addresses the mid-term electricity load forecasting problem.
Solving this problem is necessary for power system operation and planning as
well as for negotiating forward contracts in deregulated energy markets. We
show that our proposed deep neural network modeling approach based on the deep
neural architecture is effective at solving the mid-term electricity load
forecasting problem. Proposed neural network has high expressive power to solve
non-linear stochastic forecasting problems with time series including trends,
seasonality and significant random fluctuations. At the same time, it is simple
to implement and train, it does not require signal preprocessing, and it is
equipped with a forecast bias reduction mechanism. We compare our approach
against ten baseline methods, including classical statistical methods, machine
learning and hybrid approaches, on 35 monthly electricity demand time series
for European countries. The empirical study shows that proposed neural network
clearly outperforms all competitors in terms of both accuracy and forecast
bias. Code is available here: https://github.com/boreshkinai/nbeats-midterm.","cs.LG,eess.SP"
"DermGAN: Synthetic Generation of Clinical Skin Images with Pathology. Despite the recent success in applying supervised deep learning to medical
imaging tasks, the problem of obtaining large and diverse expert-annotated
datasets required for the development of high performant models remains
particularly challenging. In this work, we explore the possibility of using
Generative Adverserial Networks (GAN) to synthesize clinical images with skin
condition. We propose DermGAN, an adaptation of the popular Pix2Pix
architecture, to create synthetic images for a pre-specified skin condition
while being able to vary its size, location and the underlying skin color. We
demonstrate that the generated images are of high fidelity using objective GAN
evaluation metrics. In a Human Turing test, we note that the synthetic images
are not only visually similar to real images, but also embody the respective
skin condition in dermatologists' eyes. Finally, when using the synthetic
images as a data augmentation technique for training a skin condition
classifier, we observe that the model performs comparably to the baseline model
overall while improving on rare but malignant conditions.","cs.CV,cs.LG"
"Robust Identifiability in Linear Structural Equation Models of Causal Inference. In this work, we consider the problem of robust parameter estimation from
observational data in the context of linear structural equation models (LSEMs).
LSEMs are a popular and well-studied class of models for inferring causality in
the natural and social sciences. One of the main problems related to LSEMs is
to recover the model parameters from the observational data. Under various
conditions on LSEMs and the model parameters the prior work provides efficient
algorithms to recover the parameters. However, these results are often about
generic identifiability. In practice, generic identifiability is not sufficient
and we need robust identifiability: small changes in the observational data
should not affect the parameters by a huge amount. Robust identifiability has
received far less attention and remains poorly understood. Sankararaman et al.
(2019) recently provided a set of sufficient conditions on parameters under
which robust identifiability is feasible. However, a limitation of their work
is that their results only apply to a small sub-class of LSEMs, called
``bow-free paths.'' In this work, we significantly extend their work along
multiple dimensions. First, for a large and well-studied class of LSEMs, namely
``bow free'' models, we provide a sufficient condition on model parameters
under which robust identifiability holds, thereby removing the restriction of
paths required by prior work. We then show that this sufficient condition holds
with high probability which implies that for a large set of parameters robust
identifiability holds and that for such parameters, existing algorithms already
achieve robust identifiability. Finally, we validate our results on both
simulated and real-world datasets.","cs.AI,cs.DS,cs.LG,stat.ML"
"Efficiently Training On-Policy Actor-Critic Networks in Robotic Deep Reinforcement Learning with Demonstration-like Sampled Exploration. In complex environments with high dimension, training a reinforcement
learning (RL) model from scratch often suffers from lengthy and tedious
collection of agent-environment interactions. Instead, leveraging expert
demonstration to guide RL agent can boost sample efficiency and improve final
convergence. In order to better integrate expert prior with on-policy RL
models, we propose a generic framework for Learning from Demonstration (LfD)
based on actor-critic algorithms. Technically, we first employ K-Means
clustering to evaluate the similarity of sampled exploration with demonstration
data. Then we increase the likelihood of actions in similar frames by modifying
the gradient update strategy to leverage demonstration. We conduct experiments
on 4 standard benchmark environments in Mujoco and 2 self-designed robotic
environments. Results show that, under certain condition, our algorithm can
improve sample efficiency by 20% ~ 40%. By combining our framework with
on-policy algorithms, RL models can accelerate convergence and obtain better
final mean episode rewards especially in complex robotic context where
interactions are expensive.","cs.HC,cs.LG,cs.RO,cs.SY,eess.SY"
"Unsupervised Transfer Learning via BERT Neuron Selection. Recent advancements in language representation models such as BERT have led
to a rapid improvement in numerous natural language processing tasks. However,
language models usually consist of a few hundred million trainable parameters
with embedding space distributed across multiple layers, thus making them
challenging to be fine-tuned for a specific task or to be transferred to a new
domain. To determine whether there are task-specific neurons that can be
exploited for unsupervised transfer learning, we introduce a method for
selecting the most important neurons to solve a specific classification task.
This algorithm is further extended to multi-source transfer learning by
computing the importance of neurons for several single-source transfer learning
scenarios between different subsets of data sources. Besides, a task-specific
fingerprint for each data source is obtained based on the percentage of the
selected neurons in each layer. We perform extensive experiments in
unsupervised transfer learning for sentiment analysis, natural language
inference and sentence similarity, and compare our results with the existing
literature and baselines. Significantly, we found that the source and target
data sources with higher degrees of similarity between their task-specific
fingerprints demonstrate a better transferability property. We conclude that
our method can lead to better performance using just a few hundred
task-specific and interpretable neurons.","cs.CL,cs.LG,stat.ML"
"GANs for Urban Design. Development and diffusion of machine learning and big data tools provide a
new tool for architects and urban planners that could be used as analytical or
design instruments. The topic investigated in this paper is the application of
Generative Adversarial Networks to the design of an urban block. The research
presents a flexible model able to adapt to the morphological characteristics of
a city. This method does not define explicitly any of the parameters of an
urban block typical for a city, the algorithm learns them from the existing
urban context. This approach has been applied to the cities with different
morphology: Milan, Amsterdam, Tallinn, Turin, and Bengaluru in order to see the
performance of the model and the possibility of style translation between
different cities. The data are gathered from Openstreetmap and Open Data
portals of the cities. This research presents the results of the experiments
and their quantitative and qualitative evaluation.","68T07,J.6; I.4.9; I.2.10; J.5,cs.CV,cs.LG"
"Learning Arbitrary Statistical Mixtures of Discrete Distributions. We study the problem of learning from unlabeled samples very general
statistical mixture models on large finite sets. Specifically, the model to be
learned, $\vartheta$, is a probability distribution over probability
distributions $p$, where each such $p$ is a probability distribution over $[n]
= \{1,2,\dots,n\}$. When we sample from $\vartheta$, we do not observe $p$
directly, but only indirectly and in very noisy fashion, by sampling from $[n]$
repeatedly, independently $K$ times from the distribution $p$. The problem is
to infer $\vartheta$ to high accuracy in transportation (earthmover) distance.
  We give the first efficient algorithms for learning this mixture model
without making any restricting assumptions on the structure of the distribution
$\vartheta$. We bound the quality of the solution as a function of the size of
the samples $K$ and the number of samples used. Our model and results have
applications to a variety of unsupervised learning scenarios, including
learning topic models and collaborative filtering.","cs.DS,cs.LG"
"Bidirectional Scene Text Recognition with a Single Decoder. Scene Text Recognition (STR) is the problem of recognizing the correct word
or character sequence in a cropped word image. To obtain more robust output
sequences, the notion of bidirectional STR has been introduced. So far,
bidirectional STRs have been implemented by using two separate decoders; one
for left-to-right decoding and one for right-to-left. Having two separate
decoders for almost the same task with the same output space is undesirable
from a computational and optimization point of view. We introduce the
bidirectional Scene Text Transformer (Bi-STET), a novel bidirectional STR
method with a single decoder for bidirectional text decoding. With its single
decoder, Bi-STET outperforms methods that apply bidirectional decoding by using
two separate decoders while also being more efficient than those methods,
Furthermore, we achieve or beat state-of-the-art (SOTA) methods on all STR
benchmarks with Bi-STET. Finally, we provide analyses and insights into the
performance of Bi-STET.","cs.CL,cs.CV,cs.LG"
"Which Neural Network to Choose for Post-Fault Localization, Dynamic State Estimation and Optimal Measurement Placement in Power Systems?. We consider a power transmission system monitored with Phasor Measurement
Units (PMUs) placed at significant, but not all, nodes of the system. Assuming
that a sufficient number of distinct single-line faults, specifically pre-fault
state and (not cleared) post-fault state, are recorded by the PMUs and are
available for training, we, first, design a comprehensive sequence of Neural
Networks (NNs) locating the faulty line. Performance of different NNs in the
sequence, including Linear Regression, Feed-Forward NN, AlexNet, Graphical
Convolutional NN, Neural Linear ODE and Neural Graph-based ODE, ordered
according to the type and amount of the power flow physics involved, are
compared for different levels of observability. Second, we build a sequence of
advanced Power-System-Dynamics-Informed and Neural-ODE based Machine Learning
schemes trained, given pre-fault state, to predict the post-fault state and
also, in parallel, to estimate system parameters. Finally, third, and
continuing to work with the first (fault localization) setting we design a
(NN-based) algorithm which discovers optimal PMU placement.","cs.LG,physics.data-an,physics.soc-ph,stat.ML"
"A Reinforcement Learning Based Approach to Play Calling in Football. With the vast amount of data collected on football and the growth of
computing abilities, many games involving decision choices can be optimized.
The underlying rule is the maximization of an expected utility of outcomes and
the law of large numbers. The data available allows us to compute with high
accuracy the probabilities of outcomes of decisions and the well defined points
system in the game allows us to have the necessary terminal utilities. With
some well established theory we can then optimize choices at a single play
level.","cs.LG,stat.AP,stat.ME,stat.ML"
"Demographic-Guided Attention in Recurrent Neural Networks for Modeling Neuropathophysiological Heterogeneity. Heterogeneous presentation of a neurological disorder suggests potential
differences in the underlying pathophysiological changes that occur in the
brain. We propose to model heterogeneous patterns of functional network
differences using a demographic-guided attention (DGA) mechanism for recurrent
neural network models for prediction from functional magnetic resonance imaging
(fMRI) time-series data. The context computed from the DGA head is used to help
focus on the appropriate functional networks based on individual demographic
information. We demonstrate improved classification on 3 subsets of the ABIDE I
dataset used in published studies that have previously produced
state-of-the-art results, evaluating performance under a leave-one-site-out
cross-validation framework for better generalizeability to new data. Finally,
we provide examples of interpreting functional network differences based on
individual demographic variables.","cs.CV,cs.LG,eess.IV,q-bio.QM,stat.AP"
"Robust Max Entrywise Error Bounds for Sparse Tensor Estimation via Similarity Based Collaborative Filtering. Consider the task of estimating a 3-order $n \times n \times n$ tensor from
noisy observations of randomly chosen entries in the sparse regime. We
introduce a similarity based collaborative filtering algorithm for sparse
tensor estimation and argue that it achieves sample complexity that nearly
matches the conjectured computationally efficient lower bound on the sample
complexity for the setting of low-rank tensors. Our algorithm uses the matrix
obtained from the flattened tensor to compute similarity, and estimates the
tensor entries using a nearest neighbor estimator. We prove that the algorithm
recovers a low rank tensor with maximum entry-wise error (MEE) and
mean-squared-error (MSE) decaying to $0$ as long as each entry is observed
independently with probability $p = \Omega(n^{-3/2 + \kappa})$ for any
arbitrarily small $\kappa > 0$. % as long as tensor has finite rank $r =
\Theta(1)$. More generally, we establish robustness of the estimator, showing
that when arbitrary noise bounded by $\epsilon \geq 0$ is added to each
observation, the estimation error with respect to MEE and MSE degrades by ${\sf
poly}(\epsilon)$. Consequently, even if the tensor may not have finite rank but
can be approximated within $\epsilon \geq 0$ by a finite rank tensor, then the
estimation error converges to ${\sf poly}(\epsilon)$. Our analysis sheds
insight into the conjectured sample complexity lower bound, showing that it
matches the connectivity threshold of the graph used by our algorithm for
estimating similarity between coordinates.","cs.DS,cs.LG,stat.ML"
"Cross-Domain Adversarial Auto-Encoder. In this paper, we propose the Cross-Domain Adversarial Auto-Encoder (CDAAE)
to address the problem of cross-domain image inference, generation and
transformation. We make the assumption that images from different domains share
the same latent code space for content, while having separate latent code space
for style. The proposed framework can map cross-domain data to a latent code
vector consisting of a content part and a style part. The latent code vector is
matched with a prior distribution so that we can generate meaningful samples
from any part of the prior space. Consequently, given a sample of one domain,
our framework can generate various samples of the other domain with the same
content of the input. This makes the proposed framework different from the
current work of cross-domain transformation. Besides, the proposed framework
can be trained with both labeled and unlabeled data, which makes it also
suitable for domain adaptation. Experimental results on data sets SVHN, MNIST
and CASIA show the proposed framework achieved visually appealing performance
for image generation task. Besides, we also demonstrate the proposed method
achieved superior results for domain adaptation. Code of our experiments is
available in https://github.com/luckycallor/CDAAE.","cs.AI,cs.CV,cs.IT,math.IT"
"Solvable Model for Inheriting the Regularization through Knowledge Distillation. In recent years the empirical success of transfer learning with neural
networks has stimulated an increasing interest in obtaining a theoretical
understanding of its core properties. Knowledge distillation where a smaller
neural network is trained using the outputs of a larger neural network is a
particularly interesting case of transfer learning. In the present work, we
introduce a statistical physics framework that allows an analytic
characterization of the properties of knowledge distillation (KD) in shallow
neural networks. Focusing the analysis on a solvable model that exhibits a
non-trivial generalization gap, we investigate the effectiveness of KD. We are
able to show that, through KD, the regularization properties of the larger
teacher model can be inherited by the smaller student and that the yielded
generalization performance is closely linked to and limited by the optimality
of the teacher. Finally, we analyze the double descent phenomenology that can
arise in the considered KD setting.","cond-mat.dis-nn,cs.LG,cs.NE"
"Understanding overfitting peaks in generalization error: Analytical risk curves for $l_2$ and $l_1$ penalized interpolation. Traditionally in regression one minimizes the number of fitting parameters or
uses smoothing/regularization to trade training (TE) and generalization error
(GE). Driving TE to zero by increasing fitting degrees of freedom (dof) is
expected to increase GE. However modern big-data approaches, including deep
nets, seem to over-parametrize and send TE to zero (data interpolation) without
impacting GE. Overparametrization has the benefit that global minima of the
empirical loss function proliferate and become easier to find. These phenomena
have drawn theoretical attention. Regression and classification algorithms have
been shown that interpolate data but also generalize optimally. An interesting
related phenomenon has been noted: the existence of non-monotonic risk curves,
with a peak in GE with increasing dof. It was suggested that this peak
separates a classical regime from a modern regime where over-parametrization
improves performance. Similar over-fitting peaks were reported previously
(statistical physics approach to learning) and attributed to increased fitting
model flexibility. We introduce a generative and fitting model pair
(""Misparametrized Sparse Regression"" or MiSpaR) and show that the overfitting
peak can be dissociated from the point at which the fitting function gains
enough dof's to match the data generative model and thus provides good
generalization. This complicates the interpretation of overfitting peaks as
separating a ""classical"" from a ""modern"" regime. Data interpolation itself
cannot guarantee good generalization: we need to study the interpolation with
different penalty terms. We present analytical formulae for GE curves for
MiSpaR with $l_2$ and $l_1$ penalties, in the interpolating limit
$\lambda\rightarrow 0$.These risk curves exhibit important differences and help
elucidate the underlying phenomena.","cs.LG,physics.data-an,stat.ML"
"A Quantum Computational Approach to Correspondence Problems on Point Sets. Modern adiabatic quantum computers (AQC) are already used to solve difficult
combinatorial optimisation problems in various domains of science. Currently,
only a few applications of AQC in computer vision have been demonstrated. We
review AQC and derive a new algorithm for correspondence problems on point sets
suitable for execution on AQC. Our algorithm has a subquadratic computational
complexity of the state preparation. Examples of successful transformation
estimation and point set alignment by simulated sampling are shown in the
systematic experimental evaluation. Finally, we analyse the differences in the
solutions and the corresponding energy values.","cs.CV,cs.ET,quant-ph"
"Associative Memory using Dictionary Learning and Expander Decoding. An associative memory is a framework of content-addressable memory that
stores a collection of message vectors (or a dataset) over a neural network
while enabling a neurally feasible mechanism to recover any message in the
dataset from its noisy version. Designing an associative memory requires
addressing two main tasks: 1) learning phase: given a dataset, learn a concise
representation of the dataset in the form of a graphical model (or a neural
network), 2) recall phase: given a noisy version of a message vector from the
dataset, output the correct message vector via a neurally feasible algorithm
over the network learnt during the learning phase. This paper studies the
problem of designing a class of neural associative memories which learns a
network representation for a large dataset that ensures correction against a
large number of adversarial errors during the recall phase. Specifically, the
associative memories designed in this paper can store dataset containing
$\exp(n)$ $n$-length message vectors over a network with $O(n)$ nodes and can
tolerate $\Omega(\frac{n}{{\rm polylog} n})$ adversarial errors. This paper
carries out this memory design by mapping the learning phase and recall phase
to the tasks of dictionary learning with a square dictionary and iterative
error correction in an expander code, respectively.","cs.IT,cs.LG,math.IT,stat.ML"
"libmolgrid: GPU Accelerated Molecular Gridding for Deep Learning Applications. There are many ways to represent a molecule as input to a machine learning
model and each is associated with loss and retention of certain kinds of
information. In the interest of preserving three-dimensional spatial
information, including bond angles and torsions, we have developed libmolgrid,
a general-purpose library for representing three-dimensional molecules using
multidimensional arrays. This library also provides functionality for composing
batches of data suited to machine learning workflows, including data
augmentation, class balancing, and example stratification according to a
regression variable or data subgroup, and it further supports temporal and
spatial recurrences over that data to facilitate work with recurrent neural
networks, dynamical data, and size extensive modeling. It was designed for
seamless integration with popular deep learning frameworks, including Caffe,
PyTorch, and Keras, providing good performance by leveraging graphical
processing units (GPUs) for computationally-intensive tasks and efficient
memory usage through the use of memory views over preallocated buffers.
libmolgrid is a free and open source project that is actively supported,
serving the growing need in the molecular modeling community for tools that
streamline the process of data ingestion, representation construction, and
principled machine learning model development.","cs.LG,physics.chem-ph,q-bio.BM"
"Flexpoint: An Adaptive Numerical Format for Efficient Training of Deep Neural Networks. Deep neural networks are commonly developed and trained in 32-bit floating
point format. Significant gains in performance and energy efficiency could be
realized by training and inference in numerical formats optimized for deep
learning. Despite advances in limited precision inference in recent years,
training of neural networks in low bit-width remains a challenging problem.
Here we present the Flexpoint data format, aiming at a complete replacement of
32-bit floating point format training and inference, designed to support modern
deep network topologies without modifications. Flexpoint tensors have a shared
exponent that is dynamically adjusted to minimize overflows and maximize
available dynamic range. We validate Flexpoint by training AlexNet, a deep
residual network and a generative adversarial network, using a simulator
implemented with the neon deep learning framework. We demonstrate that 16-bit
Flexpoint closely matches 32-bit floating point in training all three models,
without any need for tuning of model hyperparameters. Our results suggest
Flexpoint as a promising numerical format for future hardware for training and
inference.","cs.LG,cs.NA,stat.ML"
"Learning Modality-Invariant Representations for Speech and Images. In this paper, we explore the unsupervised learning of a semantic embedding
space for co-occurring sensory inputs. Specifically, we focus on the task of
learning a semantic vector space for both spoken and handwritten digits using
the TIDIGITs and MNIST datasets. Current techniques encode image and
audio/textual inputs directly to semantic embeddings. In contrast, our
technique maps an input to the mean and log variance vectors of a diagonal
Gaussian from which sample semantic embeddings are drawn. In addition to
encouraging semantic similarity between co-occurring inputs,our loss function
includes a regularization term borrowed from variational autoencoders (VAEs)
which drives the posterior distributions over embeddings to be unit Gaussian.
We can use this regularization term to filter out modality information while
preserving semantic information. We speculate this technique may be more
broadly applicable to other areas of cross-modality/domain information
retrieval and transfer learning.","cs.CL,cs.CV,cs.LG"
"Recurrent Attention Walk for Semi-supervised Classification. In this paper, we study the graph-based semi-supervised learning for
classifying nodes in attributed networks, where the nodes and edges possess
content information. Recent approaches like graph convolution networks and
attention mechanisms have been proposed to ensemble the first-order neighbors
and incorporate the relevant neighbors. However, it is costly (especially in
memory) to consider all neighbors without a prior differentiation. We propose
to explore the neighborhood in a reinforcement learning setting and find a walk
path well-tuned for classifying the unlabelled target nodes. We let an agent
(of node classification task) walk over the graph and decide where to direct to
maximize classification accuracy. We define the graph walk as a partially
observable Markov decision process (POMDP). The proposed method is flexible for
working in both transductive and inductive setting. Extensive experiments on
four datasets demonstrate that our proposed method outperforms several
state-of-the-art methods. Several case studies also illustrate the meaningful
movement trajectory made by the agent.","cs.LG,cs.SI,stat.ML"
"Learning Sparse Nonparametric DAGs. We develop a framework for learning sparse nonparametric directed acyclic
graphs (DAGs) from data. Our approach is based on a recent algebraic
characterization of DAGs that led to a fully continuous program for score-based
learning of DAG models parametrized by a linear structural equation model
(SEM). We extend this algebraic characterization to nonparametric SEM by
leveraging nonparametric sparsity based on partial derivatives, resulting in a
continuous optimization problem that can be applied to a variety of
nonparametric and semiparametric models including GLMs, additive noise models,
and index models as special cases. Unlike existing approaches that require
specific modeling choices, loss functions, or algorithms, we present a
completely general framework that can be applied to general nonlinear models
(e.g. without additive noise), general differentiable loss functions, and
generic black-box optimization routines. The code is available at
https://github.com/xunzheng/notears.","cs.LG,stat.ME,stat.ML"
"Towards Automatic Construction of Diverse, High-quality Image Dataset. The availability of labeled image datasets has been shown critical for
high-level image understanding, which continuously drives the progress of
feature designing and models developing. However, constructing labeled image
datasets is laborious and monotonous. To eliminate manual annotation, in this
work, we propose a novel image dataset construction framework by employing
multiple textual queries. We aim at collecting diverse and accurate images for
given queries from the Web. Specifically, we formulate noisy textual queries
removing and noisy images filtering as a multi-view and multi-instance learning
problem separately. Our proposed approach not only improves the accuracy but
also enhances the diversity of the selected images. To verify the effectiveness
of our proposed approach, we construct an image dataset with 100 categories.
The experiments show significant performance gains by using the generated data
of our approach on several tasks, such as image classification, cross-dataset
generalization, and object detection. The proposed method also consistently
outperforms existing weakly supervised and web-supervised approaches.","cs.CV,cs.MM"
"Lithium-ion Battery State of Health Estimation based on Cycle Synchronization using Dynamic Time Warping. The state of health (SOH) estimation plays an essential role in
battery-powered applications to avoid unexpected breakdowns due to battery
capacity fading. However, few studies have paid attention to the problem of
uneven length of degrading cycles, simply employing manual operation or leaving
to the automatic processing mechanism of advanced machine learning models, like
long short-term memory (LSTM). As a result, this causes information loss and
caps the full capability of the data-driven SOH estimation models. To address
this challenge, this paper proposes an innovative cycle synchronization way to
change the existing coordinate system using dynamic time warping, not only
enabling the equal length inputs of the estimation model but also preserving
all information. By exploiting the time information of the time series, the
proposed method embeds the time index and the original measurements into a
novel indicator to reflect the battery degradation status, which could have the
same length over cycles. Adopting the LSTM as the basic estimation model, the
cycle synchronization-based SOH model could significantly improve the
prediction accuracy by more than 30% compared to the traditional LSTM.","cs.LG,eess.SP"
"Optimal Actor-Critic Policy with Optimized Training Datasets. Actor-critic (AC) algorithms are known for their efficacy and high
performance in solving reinforcement learning problems, but they also suffer
from low sampling efficiency. An AC based policy optimization process is
iterative and needs to frequently access the agent-environment system to
evaluate and update the policy by rolling out the policy, collecting rewards
and states (i.e. samples), and learning from them. It ultimately requires a
huge number of samples to learn an optimal policy. To improve sampling
efficiency, we propose a strategy to optimize the training dataset that
contains significantly less samples collected from the AC process. The dataset
optimization is made of a best episode only operation, a policy
parameter-fitness model, and a genetic algorithm module. The optimal policy
network trained by the optimized training dataset exhibits superior performance
compared to many contemporary AC algorithms in controlling autonomous dynamical
systems. Evaluation on standard benchmarks show that the method improves
sampling efficiency, ensures faster convergence to optima, and is more
data-efficient than its counterparts.","cs.LG,cs.SY,eess.SY"
"A Finite Time Analysis of Two Time-Scale Actor Critic Methods. Actor-critic (AC) methods have exhibited great empirical success compared
with other reinforcement learning algorithms, where the actor uses the policy
gradient to improve the learning policy and the critic uses temporal difference
learning to estimate the policy gradient. Under the two time-scale learning
rate schedule, the asymptotic convergence of AC has been well studied in the
literature. However, the non-asymptotic convergence and finite sample
complexity of actor-critic methods are largely open. In this work, we provide a
non-asymptotic analysis for two time-scale actor-critic methods under
non-i.i.d. setting. We prove that the actor-critic method is guaranteed to find
a first-order stationary point (i.e., $\|\nabla J(\boldsymbol{\theta})\|_2^2
\le \epsilon$) of the non-concave performance function
$J(\boldsymbol{\theta})$, with $\mathcal{\tilde{O}}(\epsilon^{-2.5})$ sample
complexity. To the best of our knowledge, this is the first work providing
finite-time analysis and sample complexity bound for two time-scale
actor-critic methods.","cs.LG,math.OC,stat.ML"
"xFraud: Explainable Fraud Transaction Detection on Heterogeneous Graphs. At online retail platforms, it is crucial to actively detect risks of
fraudulent transactions to improve our customer experience, minimize loss, and
prevent unauthorized chargebacks. Traditional rule-based methods and simple
feature-based models are either inefficient or brittle and uninterpretable. The
graph structure that exists among the heterogeneous typed entities of the
transaction logs is informative and difficult to fake. To utilize the
heterogeneous graph relationships and enrich the explainability, we present
xFraud, an explainable Fraud transaction prediction system. xFraud is composed
of a predictor which learns expressive representations for malicious
transaction detection from the heterogeneous transaction graph via a
self-attentive heterogeneous graph neural network, and an explainer that
generates meaningful and human understandable explanations from graphs to
facilitate further process in business unit. In our experiments with xFraud on
two real transaction networks with up to ten millions transactions, we are able
to achieve an area under a curve (AUC) score that outperforms baseline models
and graph embedding methods. In addition, we show how the explainer could
benefit the understanding towards model predictions and enhance model
trustworthiness for real-world fraud transaction cases.","I.2.6,cs.AI,cs.LG,cs.SI"
"Policy Optimization Provably Converges to Nash Equilibria in Zero-Sum Linear Quadratic Games. We study the global convergence of policy optimization for finding the Nash
equilibria (NE) in zero-sum linear quadratic (LQ) games. To this end, we first
investigate the landscape of LQ games, viewing it as a nonconvex-nonconcave
saddle-point problem in the policy space. Specifically, we show that despite
its nonconvexity and nonconcavity, zero-sum LQ games have the property that the
stationary point of the objective function with respect to the linear feedback
control policies constitutes the NE of the game. Building upon this, we develop
three projected nested-gradient methods that are guaranteed to converge to the
NE of the game. Moreover, we show that all of these algorithms enjoy both
globally sublinear and locally linear convergence rates. Simulation results are
also provided to illustrate the satisfactory convergence properties of the
algorithms. To the best of our knowledge, this work appears to be the first one
to investigate the optimization landscape of LQ games, and provably show the
convergence of policy optimization methods to the Nash equilibria. Our work
serves as an initial step toward understanding the theoretical aspects of
policy-based reinforcement learning algorithms for zero-sum Markov games in
general.","cs.GT,cs.LG,cs.SY,math.OC,stat.ML"
"Restoration of Atmospheric Turbulence-distorted Images via RPCA and Quasiconformal Maps. We address the problem of restoring a high-quality image from an observed
image sequence strongly distorted by atmospheric turbulence. A novel algorithm
is proposed in this paper to reduce geometric distortion as well as
space-and-time-varying blur due to strong turbulence. By considering a suitable
energy functional, our algorithm first obtains a sharp reference image and a
subsampled image sequence containing sharp and mildly distorted image frames
with respect to the reference image. The subsampled image sequence is then
stabilized by applying the Robust Principal Component Analysis (RPCA) on the
deformation fields between image frames and warping the image frames by a
quasiconformal map associated with the low-rank part of the deformation matrix.
After image frames are registered to the reference image, the low-rank part of
them are deblurred via a blind deconvolution, and the deblurred frames are then
fused with the enhanced sparse part. Experiments have been carried out on both
synthetic and real turbulence-distorted video. Results demonstrate that our
method is effective in alleviating distortions and blur, restoring image
details and enhancing visual quality.","cs.CG,cs.CV,cs.GR"
"Machine learning in resting-state fMRI analysis. Machine learning techniques have gained prominence for the analysis of
resting-state functional Magnetic Resonance Imaging (rs-fMRI) data. Here, we
present an overview of various unsupervised and supervised machine learning
applications to rs-fMRI. We present a methodical taxonomy of machine learning
methods in resting-state fMRI. We identify three major divisions of
unsupervised learning methods with regard to their applications to rs-fMRI,
based on whether they discover principal modes of variation across space, time
or population. Next, we survey the algorithms and rs-fMRI feature
representations that have driven the success of supervised subject-level
predictions. The goal is to provide a high-level overview of the burgeoning
field of rs-fMRI from the perspective of machine learning applications.","cs.CV,cs.LG,q-bio.QM,stat.ML"
"Increased performance in DDM analysis by calculating structure functions through Fourier transform in time. Differential Dynamic Microscopy (DDM) is the combination of optical
microscopy to statistical analysis to obtain information about the dynamical
behaviour of a variety of samples spanning from soft matter physics to biology.
In DDM, the dynamical evolution of the samples is investigated separately at
different length scales and extracted from a set of images recorded at
different times. A specific result of interest is the structure function that
can be computed via spatial Fourier transforms and differences of signals. In
this work, we present an algorithm to efficiently process a set of images
according to the DDM analysis scheme. We bench-marked the new approach against
the state-of-the-art algorithm reported in previous work. The new
implementation computes the DDM analysis faster, thanks to an additional
Fourier transform in time instead of performing differences of signals. This
allows obtaining very fast analysis also in CPU based machine. In order to test
the new code, we performed the DDM analysis over sets of more than 1000 images
with and without the help of GPU hardware acceleration. As an example, for
images of $512 \times 512$ pixels, the new algorithm is 10 times faster than
the previous GPU code. Without GPU hardware acceleration and for the same set
of images, we found that the new algorithm is 300 faster than the old one both
running only on the CPU.","cs.CV,eess.IV,physics.app-ph,physics.data-an"
"GMM-Based Hidden Markov Random Field for Color Image and 3D Volume Segmentation. In this project, we first study the Gaussian-based hidden Markov random field
(HMRF) model and its expectation-maximization (EM) algorithm. Then we
generalize it to Gaussian mixture model-based hidden Markov random field. The
algorithm is implemented in MATLAB. We also apply this algorithm to color image
segmentation problems and 3D volume segmentation problems.",cs.CV
"Reconstruction of Convex Polytope Compositions from 3D Point-clouds. Reconstructing a composition (union) of convex polytopes that perfectly fits
the corresponding input point-cloud is a hard optimization problem with
interesting applications in reverse engineering and rigid body dynamics
simulations. We propose a pipeline that first extracts a set of planes, then
partitions the input point-cloud into weakly convex clusters and finally
generates a set of convex polytopes as the intersection of fitted planes for
each partition. Finding the best-fitting convex polytopes is formulated as a
combinatorial optimization problem over the set of fitted planes and is solved
using an Evolutionary Algorithm. For convex clustering, we employ two different
methods and detail their strengths and weaknesses in a thorough evaluation
based on multiple input data-sets.","cs.CG,cs.CV"
"Towards Explainable Anticancer Compound Sensitivity Prediction via Multimodal Attention-based Convolutional Encoders. In line with recent advances in neural drug design and sensitivity
prediction, we propose a novel architecture for interpretable prediction of
anticancer compound sensitivity using a multimodal attention-based
convolutional encoder. Our model is based on the three key pillars of drug
sensitivity: compounds' structure in the form of a SMILES sequence, gene
expression profiles of tumors and prior knowledge on intracellular interactions
from protein-protein interaction networks. We demonstrate that our multiscale
convolutional attention-based (MCA) encoder significantly outperforms a
baseline model trained on Morgan fingerprints, a selection of encoders based on
SMILES as well as previously reported state of the art for multimodal drug
sensitivity prediction (R2 = 0.86 and RMSE = 0.89). Moreover, the
explainability of our approach is demonstrated by a thorough analysis of the
attention weights. We show that the attended genes significantly enrich
apoptotic processes and that the drug attention is strongly correlated with a
standard chemical structure similarity index. Finally, we report a case study
of two receptor tyrosine kinase (RTK) inhibitors acting on a leukemia cell
line, showcasing the ability of the model to focus on informative genes and
submolecular regions of the two compounds. The demonstrated generalizability
and the interpretability of our model testify its potential for in-silico
prediction of anticancer compound efficacy on unseen cancer cells, positioning
it as a valid solution for the development of personalized therapies as well as
for the evaluation of candidate compounds in de novo drug design.","cs.AI,cs.LG,q-bio.QM,stat.ML"
"Natural Wake-Sleep Algorithm. The benefits of using the natural gradient are well known in a wide range of
optimization problems. However, for the training of common neural networks the
resulting increase in computational complexity sets a limitation to its
practical application. Helmholtz Machines are a particular type of generative
model composed of two Sigmoid Belief Networks (SBNs), acting as an encoder and
a decoder, commonly trained using the Wake-Sleep (WS) algorithm and its
reweighted version RWS. For SBNs, it has been shown how the locality of the
connections in the graphical structure induces sparsity in the Fisher
information matrix. The resulting block diagonal structure can be efficiently
exploited to reduce the computational complexity of the Fisher matrix inversion
and thus compute the natural gradient exactly, without the need of
approximations. We present a geometric adaptation of well-known methods from
the literature, introducing the Natural Wake-Sleep (NWS) and the Natural
Reweighted Wake-Sleep (NRWS) algorithms. We present an experimental analysis of
the novel geometrical algorithms based on the convergence speed and the value
of the log-likelihood, both with respect to the number of iterations and the
time complexity and demonstrating improvements on these aspects over their
respective non-geometric baselines.","68T07,cs.LG,stat.ML"
"Geometry-Aware Neighborhood Search for Learning Local Models for Image Reconstruction. Local learning of sparse image models has proven to be very effective to
solve inverse problems in many computer vision applications. To learn such
models, the data samples are often clustered using the K-means algorithm with
the Euclidean distance as a dissimilarity metric. However, the Euclidean
distance may not always be a good dissimilarity measure for comparing data
samples lying on a manifold. In this paper, we propose two algorithms for
determining a local subset of training samples from which a good local model
can be computed for reconstructing a given input test sample, where we take
into account the underlying geometry of the data. The first algorithm, called
Adaptive Geometry-driven Nearest Neighbor search (AGNN), is an adaptive scheme
which can be seen as an out-of-sample extension of the replicator graph
clustering method for local model learning. The second method, called
Geometry-driven Overlapping Clusters (GOC), is a less complex nonadaptive
alternative for training subset selection. The proposed AGNN and GOC methods
are evaluated in image super-resolution, deblurring and denoising applications
and shown to outperform spectral clustering, soft clustering, and geodesic
distance based subset selection in most settings.","cs.CV,cs.IT,math.IT,math.OC"
"Geometric learning of the conformational dynamics of molecules using dynamic graph neural networks. We apply a temporal edge prediction model for weighted dynamic graphs to
predict time-dependent changes in molecular structure. Each molecule is
represented as a complete graph in which each atom is a vertex and all vertex
pairs are connected by an edge weighted by the Euclidean distance between atom
pairs. We ingest a sequence of complete molecular graphs into a dynamic graph
neural network (GNN) to predict the graph at the next time step. Our dynamic
GNN predicts atom-to-atom distances with a mean absolute error of 0.017 \r{A},
which is considered ``chemically accurate'' for molecular simulations. We also
explored the transferability of a trained network to new molecular systems and
found that finetuning with less than 10% of the total trajectory provides a
mean absolute error of the same order of magnitude as that when training from
scratch on the full molecular trajectory.","cs.LG,physics.chem-ph"
"Deep Multi-attribute Graph Representation Learning on Protein Structures. Graphs as a type of data structure have recently attracted significant
attention. Representation learning of geometric graphs has achieved great
success in many fields including molecular, social, and financial networks. It
is natural to present proteins as graphs in which nodes represent the residues
and edges represent the pairwise interactions between residues. However, 3D
protein structures have rarely been studied as graphs directly. The challenges
include: 1) Proteins are complex macromolecules composed of thousands of atoms
making them much harder to model than micro-molecules. 2) Capturing the
long-range pairwise relations for protein structure modeling remains
under-explored. 3) Few studies have focused on learning the different
attributes of proteins together. To address the above challenges, we propose a
new graph neural network architecture to represent the proteins as 3D graphs
and predict both distance geometric graph representation and dihedral geometric
graph representation together. This gives a significant advantage because this
network opens a new path from the sequence to structure. We conducted extensive
experiments on four different datasets and demonstrated the effectiveness of
the proposed method.","cs.LG,q-bio.BM,q-bio.QM"
"Visual Relationship Detection with Visual-Linguistic Knowledge from Multimodal Representations. Visual relationship detection aims to reason over relationships among salient
objects in images, which has drawn increasing attention over the past few
years. Inspired by human reasoning mechanisms, it is believed that external
visual commonsense knowledge is beneficial for reasoning visual relationships
of objects in images, which is however rarely considered in existing methods.
In this paper, we propose a novel approach named Relational Visual-Linguistic
Bidirectional Encoder Representations from Transformers (RVL-BERT), which
performs relational reasoning with both visual and language commonsense
knowledge learned via self-supervised pre-training with multimodal
representations. RVL-BERT also uses an effective spatial module and a novel
mask attention module to explicitly capture spatial information among the
objects. Moreover, our model decouples object detection from visual
relationship recognition by taking in object names directly, enabling it to be
used on top of any object detection system. We show through quantitative and
qualitative experiments that, with the transferred knowledge and novel modules,
RVL-BERT achieves competitive results on two challenging visual relationship
detection datasets. The source code is available at
https://github.com/coldmanck/RVL-BERT.","cs.AI,cs.CL,cs.CV,cs.LG"
"Mixed integer programming formulation of unsupervised learning. A novel formulation and training procedure for full Boltzmann machines in
terms of a mixed binary quadratic feasibility problem is given. As a proof of
concept, the theory is analytically and numerically tested on XOR patterns.","cond-mat.dis-nn,cs.LG,stat.ML"
"Kernel Alignment Risk Estimator: Risk Prediction from Training Data. We study the risk (i.e. generalization error) of Kernel Ridge Regression
(KRR) for a kernel $K$ with ridge $\lambda>0$ and i.i.d. observations. For
this, we introduce two objects: the Signal Capture Threshold (SCT) and the
Kernel Alignment Risk Estimator (KARE). The SCT $\vartheta_{K,\lambda}$ is a
function of the data distribution: it can be used to identify the components of
the data that the KRR predictor captures, and to approximate the (expected) KRR
risk. This then leads to a KRR risk approximation by the KARE $\rho_{K,
\lambda}$, an explicit function of the training data, agnostic of the true data
distribution. We phrase the regression problem in a functional setting. The key
results then follow from a finite-size analysis of the Stieltjes transform of
general Wishart random matrices. Under a natural universality assumption (that
the KRR moments depend asymptotically on the first two moments of the
observations) we capture the mean and variance of the KRR predictor. We
numerically investigate our findings on the Higgs and MNIST datasets for
various classical kernels: the KARE gives an excellent approximation of the
risk, thus supporting our universality assumption. Using the KARE, one can
compare choices of Kernels and hyperparameters directly from the training set.
The KARE thus provides a promising data-dependent procedure to select Kernels
that generalize well.","cs.LG,math.PR,stat.ML"
"Nonparametric clustering for image segmentation. Image segmentation aims at identifying regions of interest within an image,
by grouping pixels according to their properties. This task resembles the
statistical one of clustering, yet many standard clustering methods fail to
meet the basic requirements of image segmentation: segment shapes are often
biased toward predetermined shapes and their number is rarely determined
automatically. Nonparametric clustering is, in principle, free from these
limitations and turns out to be particularly suitable for the task of image
segmentation. This is also witnessed by several operational analogies, as, for
instance, the resort to topological data analysis and spatial tessellation in
both the frameworks. We discuss the application of nonparametric clustering to
image segmentation and provide an algorithm specific for this task. Pixel
similarity is evaluated in terms of density of the color representation and the
adjacency structure of the pixels is exploited to introduce a simple, yet
effective method to identify image segments as disconnected high-density
regions. The proposed method works both to segment an image and to detect its
boundaries and can be seen as a generalization to color images of the class of
thresholding methods.","62P99,cs.CV,eess.IV,stat.AP"
"Digital rock reconstruction with user-defined properties using conditional generative adversarial networks. Uncertainty is ubiquitous with flow in subsurface rocks because of their
inherent heterogeneity and lack of in-situ measurements. To complete
uncertainty analysis in a multi-scale manner, it is a prerequisite to provide
sufficient rock samples. Even though the advent of digital rock technology
offers opportunities to reproduce rocks, it still cannot be utilized to provide
massive samples due to its high cost, thus leading to the development of
diversified mathematical methods. Among them, two-point statistics (TPS) and
multi-point statistics (MPS) are commonly utilized, which feature incorporating
low-order and high-order statistical information, respectively. Recently,
generative adversarial networks (GANs) are becoming increasingly popular since
they can reproduce training images with excellent visual and consequent
geologic realism. However, standard GANs can only incorporate information from
data, while leaving no interface for user-defined properties, and thus may
limit the representativeness of reconstructed samples. In this study, we
propose conditional GANs for digital rock reconstruction, aiming to reproduce
samples not only similar to the real training data, but also satisfying
user-specified properties. In fact, the proposed framework can realize the
targets of MPS and TPS simultaneously by incorporating high-order information
directly from rock images with the GANs scheme, while preserving low-order
counterparts through conditioning. We conduct three reconstruction experiments,
and the results demonstrate that rock type, rock porosity, and correlation
length can be successfully conditioned to affect the reconstructed rock images.
Furthermore, in contrast to existing GANs, the proposed conditioning enables
learning of multiple rock types simultaneously, and thus invisibly saves
computational cost.","cs.AI,cs.CV,cs.LG,physics.data-an"
"Weakly-correlated synapses promote dimension reduction in deep neural networks. By controlling synaptic and neural correlations, deep learning has achieved
empirical successes in improving classification performances. How synaptic
correlations affect neural correlations to produce disentangled hidden
representations remains elusive. Here we propose a simplified model of
dimension reduction, taking into account pairwise correlations among synapses,
to reveal the mechanism underlying how the synaptic correlations affect
dimension reduction. Our theory determines the synaptic-correlation scaling
form requiring only mathematical self-consistency, for both binary and
continuous synapses. The theory also predicts that weakly-correlated synapses
encourage dimension reduction compared to their orthogonal counterparts. In
addition, these synapses slow down the decorrelation process along the network
depth. These two computational roles are explained by the proposed mean-field
equation. The theoretical predictions are in excellent agreement with numerical
simulations, and the key features are also captured by a deep learning with
Hebbian rules.","cond-mat.dis-nn,cond-mat.stat-mech,cs.LG,q-bio.NC,stat.ML"
"DeepExpress: Heterogeneous and Coupled Sequence Modeling for Express Delivery Prediction. The prediction of express delivery sequence, i.e., modeling and estimating
the volumes of daily incoming and outgoing parcels for delivery, is critical
for online business, logistics, and positive customer experience, and
specifically for resource allocation optimization and promotional activity
arrangement. A precise estimate of consumer delivery requests has to involve
sequential factors such as shopping behaviors, weather conditions, events,
business campaigns, and their couplings. Besides, conventional sequence
prediction assumes a stable sequence evolution, failing to address complex
nonlinear sequences and various feature effects in the above multi-source data.
Although deep networks and attention mechanisms demonstrate the potential of
complex sequence modeling, extant networks ignore the heterogeneous and
coupling situation between features and sequences, resulting in weak prediction
accuracy. To address these issues, we propose DeepExpress - a deep-learning
based express delivery sequence prediction model, which extends the classic
seq2seq framework to learning complex coupling between sequence and features.
DeepExpress leverages an express delivery seq2seq learning, a
carefully-designed heterogeneous feature representation, and a novel joint
training attention mechanism to adaptively map heterogeneous data, and capture
sequence-feature coupling for precise estimation. Experimental results on
real-world data demonstrate that the proposed method outperforms both shallow
and deep baseline models.","cs.AI,cs.LG"
"Toric grammars: a new statistical approach to natural language modeling. We propose a new statistical model for computational linguistics. Rather than
trying to estimate directly the probability distribution of a random sentence
of the language, we define a Markov chain on finite sets of sentences with many
finite recurrent communicating classes and define our language model as the
invariant probability measures of the chain on each recurrent communicating
class. This Markov chain, that we call a communication model, recombines at
each step randomly the set of sentences forming its current state, using some
grammar rules. When the grammar rules are fixed and known in advance instead of
being estimated on the fly, we can prove supplementary mathematical properties.
In particular, we can prove in this case that all states are recurrent states,
so that the chain defines a partition of its state space into finite recurrent
communicating classes. We show that our approach is a decisive departure from
Markov models at the sentence level and discuss its relationships with Context
Free Grammars. Although the toric grammars we use are closely related to
Context Free Grammars, the way we generate the language from the grammar is
qualitatively different. Our communication model has two purposes. On the one
hand, it is used to define indirectly the probability distribution of a random
sentence of the language. On the other hand it can serve as a (crude) model of
language transmission from one speaker to another speaker through the
communication of a (large) set of sentences.","03B65,60J20,62M09,62P99,68T50,91E40,91F20,cs.CL,math.PR,stat.ML"
"ESPRESSO: Entropy and ShaPe awaRe timE-Series SegmentatiOn for processing heterogeneous sensor data. Extracting informative and meaningful temporal segments from high-dimensional
wearable sensor data, smart devices, or IoT data is a vital preprocessing step
in applications such as Human Activity Recognition (HAR), trajectory
prediction, gesture recognition, and lifelogging. In this paper, we propose
ESPRESSO (Entropy and ShaPe awaRe timE-Series SegmentatiOn), a hybrid
segmentation model for multi-dimensional time-series that is formulated to
exploit the entropy and temporal shape properties of time-series. ESPRESSO
differs from existing methods that focus upon particular statistical or
temporal properties of time-series exclusively. As part of model development, a
novel temporal representation of time-series $WCAC$ was introduced along with a
greedy search approach that estimate segments based upon the entropy metric.
ESPRESSO was shown to offer superior performance to four state-of-the-art
methods across seven public datasets of wearable and wear-free sensing. In
addition, we undertake a deeper investigation of these datasets to understand
how ESPRESSO and its constituent methods perform with respect to different
dataset characteristics. Finally, we provide two interesting case-studies to
show how applying ESPRESSO can assist in inferring daily activity routines and
the emotional state of humans.","cs.CV,cs.DB,cs.IT,cs.LG,eess.SP,math.IT,stat.ML"
"Deep Convolutional Neural Network Ensembles using ECOC. Deep neural networks have enhanced the performance of decision making systems
in many applications including image understanding, and further gains can be
achieved by constructing ensembles. However, designing an ensemble of deep
networks is often not very beneficial since the time needed to train the
networks is very high or the performance gain obtained is not very significant.
In this paper, we analyse error correcting output coding (ECOC) framework to be
used as an ensemble technique for deep networks and propose different design
strategies to address the accuracy-complexity trade-off. We carry out an
extensive comparative study between the introduced ECOC designs and the
state-of-the-art ensemble techniques such as ensemble averaging and gradient
boosting decision trees. Furthermore, we propose a combinatory technique which
is shown to achieve the highest classification performance amongst all.",",68T07,I.5.2; I.2.0,cs.LG,stat.ML"
"Deep Learning for Molecular Graphs with Tiered Graph Autoencoders and Graph Prediction. Tiered graph autoencoders provide the architecture and mechanisms for
learning tiered latent representations and latent spaces for molecular graphs
that explicitly represent and utilize groups (e.g., functional groups). This
enables the utilization and exploration of tiered molecular latent spaces,
either individually - the node (atom) tier, the group tier, or the graph
(molecule) tier - or jointly, as well as navigation across the tiers. In this
paper, we discuss the use of tiered graph autoencoders together with graph
prediction for molecular graphs. We show features of molecular graphs used, and
groups in molecular graphs identified for some sample molecules. We briefly
review graph prediction and the QM9 dataset for background information, and
discuss the use of tiered graph embeddings for graph prediction, particularly
weighted group pooling. We find that functional groups and ring groups
effectively capture and represent the chemical essence of molecular graphs
(structures). Further, tiered graph autoencoders and graph prediction together
provide effective, efficient and interpretable deep learning for molecular
graphs, with the former providing unsupervised, transferable learning and the
latter providing supervised, task-optimized learning.","cs.LG,q-bio.BM"
"Generating Diverse Programs with Instruction Conditioned Reinforced Adversarial Learning. Advances in Deep Reinforcement Learning have led to agents that perform well
across a variety of sensory-motor domains. In this work, we study the setting
in which an agent must learn to generate programs for diverse scenes
conditioned on a given symbolic instruction. Final goals are specified to our
agent via images of the scenes. A symbolic instruction consistent with the goal
images is used as the conditioning input for our policies. Since a single
instruction corresponds to a diverse set of different but still consistent
end-goal images, the agent needs to learn to generate a distribution over
programs given an instruction. We demonstrate that with simple changes to the
reinforced adversarial learning objective, we can learn instruction conditioned
policies to achieve the corresponding diverse set of goals. Most importantly,
our agent's stochastic policy is shown to more accurately capture the diversity
in the goal distribution than a fixed pixel-based reward function baseline. We
demonstrate the efficacy of our approach on two domains: (1) drawing MNIST
digits with a paint software conditioned on instructions and (2) constructing
scenes in a 3D editor that satisfies a certain instruction.","cs.CL,cs.CV,cs.LG,stat.ML"
"Bayesian Optimization for Probabilistic Programs. We present the first general purpose framework for marginal maximum a
posteriori estimation of probabilistic program variables. By using a series of
code transformations, the evidence of any probabilistic program, and therefore
of any graphical model, can be optimized with respect to an arbitrary subset of
its sampled variables. To carry out this optimization, we develop the first
Bayesian optimization package to directly exploit the source code of its
target, leading to innovations in problem-independent hyperpriors, unbounded
optimization, and implicit constraint satisfaction; delivering significant
performance improvements over prominent existing packages. We present
applications of our method to a number of tasks including engineering design
and parameter optimization.","cs.AI,cs.PL,stat.CO,stat.ML"
"READ: Recursive Autoencoders for Document Layout Generation. Layout is a fundamental component of any graphic design. Creating large
varieties of plausible document layouts can be a tedious task, requiring
numerous constraints to be satisfied, including local ones relating different
semantic elements and global constraints on the general appearance and spacing.
In this paper, we present a novel framework, coined READ, for REcursive
Autoencoders for Document layout generation, to generate plausible 2D layouts
of documents in large quantities and varieties. First, we devise an exploratory
recursive method to extract a structural decomposition of a single document.
Leveraging a dataset of documents annotated with labeled bounding boxes, our
recursive neural network learns to map the structural representation, given in
the form of a simple hierarchy, to a compact code, the space of which is
approximated by a Gaussian distribution. Novel hierarchies can be sampled from
this space, obtaining new document layouts. Moreover, we introduce a
combinatorial metric to measure structural similarity among document layouts.
We deploy it to show that our method is able to generate highly variable and
realistic layouts. We further demonstrate the utility of our generated layouts
in the context of standard detection tasks on documents, showing that detection
performance improves when the training data is augmented with generated
documents whose layouts are produced by READ.","cs.CV,cs.GR,cs.IR"
"Shape-based defect classification for Non Destructive Testing. The aim of this work is to classify the aerospace structure defects detected
by eddy current non-destructive testing. The proposed method is based on the
assumption that the defect is bound to the reaction of the probe coil impedance
during the test. Impedance plane analysis is used to extract a feature vector
from the shape of the coil impedance in the complex plane, through the use of
some geometric parameters. Shape recognition is tested with three different
machine-learning based classifiers: decision trees, neural networks and Naive
Bayes. The performance of the proposed detection system are measured in terms
of accuracy, sensitivity, specificity, precision and Matthews correlation
coefficient. Several experiments are performed on dataset of eddy current
signal samples for aircraft structures. The obtained results demonstrate the
usefulness of our approach and the competiveness against existing descriptors.","cs.CE,cs.CV"
"Matrix Shuffle-Exchange Networks for Hard 2D Tasks. Convolutional neural networks have become the main tools for processing
two-dimensional data. They work well for images, yet convolutions have a
limited receptive field that prevents its applications to more complex 2D
tasks. We propose a new neural model, called Matrix Shuffle-Exchange network,
that can efficiently exploit long-range dependencies in 2D data and has
comparable speed to a convolutional neural network. It is derived from Neural
Shuffle-Exchange network and has $\mathcal{O}( \log{n})$ layers and
$\mathcal{O}( n^2 \log{n})$ total time and space complexity for processing a $n
\times n$ data matrix. We show that the Matrix Shuffle-Exchange network is
well-suited for algorithmic and logical reasoning tasks on matrices and dense
graphs, exceeding convolutional and graph neural network baselines. Its
distinct advantage is the capability of retaining full long-range dependency
modelling when generalizing to larger instances - much larger than could be
processed with models equipped with a dense attention mechanism.","cs.LG,stat.ML"
"COALA: Co-Aligned Autoencoders for Learning Semantically Enriched Audio Representations. Audio representation learning based on deep neural networks (DNNs) emerged as
an alternative approach to hand-crafted features. For achieving high
performance, DNNs often need a large amount of annotated data which can be
difficult and costly to obtain. In this paper, we propose a method for learning
audio representations, aligning the learned latent representations of audio and
associated tags. Aligning is done by maximizing the agreement of the latent
representations of audio and tags, using a contrastive loss. The result is an
audio embedding model which reflects acoustic and semantic characteristics of
sounds. We evaluate the quality of our embedding model, measuring its
performance as a feature extractor on three different tasks (namely, sound
event recognition, and music genre and musical instrument classification), and
investigate what type of characteristics the model captures. Our results are
promising, sometimes in par with the state-of-the-art in the considered tasks
and the embeddings produced with our method are well correlated with some
acoustic descriptors.","cs.IR,cs.LG,eess.AS,stat.ML"
"Semi-Supervised Learning on Graphs with Feature-Augmented Graph Basis Functions. For semi-supervised learning on graphs, we study how initial kernels in a
supervised learning regime can be augmented with additional information from
known priors or from unsupervised learning outputs. These augmented kernels are
constructed in a simple update scheme based on the Schur-Hadamard product of
the kernel with additional feature kernels. As generators of the positive
definite kernels we will focus on graph basis functions (GBF) that allow to
include geometric information of the graph via the graph Fourier transform.
Using a regularized least squares (RLS) approach for machine learning, we will
test the derived augmented kernels for the classification of data on graphs.","cs.LG,cs.NA,eess.SP,math.NA"
"Emerging Trends in Federated Learning: From Model Fusion to Federated X Learning. Federated learning is a new learning paradigm that decouples data collection
and model training via multi-party computation and model aggregation. As a
flexible learning setting, federated learning has the potential to integrate
with other learning frameworks. We conduct a focused survey of federated
learning in conjunction with other learning algorithms. Specifically, we
explore various learning algorithms to improve the vanilla federated averaging
algorithm and review model fusion methods such as adaptive aggregation,
regularization, clustered methods, and Bayesian methods. Following the emerging
trends, we also discuss federated learning in the intersection with other
learning paradigms, termed as federated x learning, where x includes multitask
learning, meta-learning, transfer learning, unsupervised learning, and
reinforcement learning. This survey reviews the state of the art, challenges,
and future directions.","cs.DC,cs.LG"
"Process Model Forecasting Using Time Series Analysis of Event Sequence Data. Process analytics is an umbrella of data-driven techniques which includes
making predictions for individual process instances or overall process models.
At the instance level, various novel techniques have been recently devised,
tackling next activity, remaining time, and outcome prediction. At the model
level, there is a notable void. It is the ambition of this paper to fill this
gap. To this end, we develop a technique to forecast the entire process model
from historical event data. A forecasted model is a will-be process model
representing a probable future state of the overall process. Such a forecast
helps to investigate the consequences of drift and emerging bottlenecks. Our
technique builds on a representation of event data as multiple time series,
each capturing the evolution of a behavioural aspect of the process model, such
that corresponding forecasting techniques can be applied. Our implementation
demonstrates the accuracy of our technique on real-world event log data.","cs.DB,cs.LG"
"Improving Learning Effectiveness For Object Detection and Classification in Cluttered Backgrounds. Usually, Neural Networks models are trained with a large dataset of images in
homogeneous backgrounds. The issue is that the performance of the network
models trained could be significantly degraded in a complex and heterogeneous
environment. To mitigate the issue, this paper develops a framework that
permits to autonomously generate a training dataset in heterogeneous cluttered
backgrounds. It is clear that the learning effectiveness of the proposed
framework should be improved in complex and heterogeneous environments,
compared with the ones with the typical dataset. In our framework, a
state-of-the-art image segmentation technique called DeepLab is used to extract
objects of interest from a picture and Chroma-key technique is then used to
merge the extracted objects of interest into specific heterogeneous
backgrounds. The performance of the proposed framework is investigated through
empirical tests and compared with that of the model trained with the COCO
dataset. The results show that the proposed framework outperforms the model
compared. This implies that the learning effectiveness of the framework
developed is superior to the models with the typical dataset.","cs.CV,cs.LG,eess.IV"
"Tensorized Transformer for Dynamical Systems Modeling. The identification of nonlinear dynamics from observations is essential for
the alignment of the theoretical ideas and experimental data. The last, in
turn, is often corrupted by the side effects and noise of different natures, so
probabilistic approaches could give a more general picture of the process. At
the same time, high-dimensional probabilities modeling is a challenging and
data-intensive task. In this paper, we establish a parallel between the
dynamical systems modeling and language modeling tasks. We propose a
transformer-based model that incorporates geometrical properties of the data
and provide an iterative training algorithm allowing the fine-grid
approximation of the conditional probabilities of high-dimensional dynamical
systems.","cs.LG,math.DS,stat.ML"
"Lattice-Free MMI Adaptation Of Self-Supervised Pretrained Acoustic Models. In this work, we propose lattice-free MMI (LFMMI) for supervised adaptation
of self-supervised pretrained acoustic model. We pretrain a Transformer model
on thousand hours of untranscribed Librispeech data followed by supervised
adaptation with LFMMI on three different datasets. Our results show that
fine-tuning with LFMMI, we consistently obtain relative WER improvements of 10%
and 35.3% on the clean and other test sets of Librispeech (100h), 10.8% on
Switchboard (300h), and 4.3% on Swahili (38h) and 4.4% on Tagalog (84h)
compared to the baseline trained only with supervised data.","cs.LG,cs.SD,eess.AS"
"RST-MODNet: Real-time Spatio-temporal Moving Object Detection for Autonomous Driving. Moving Object Detection (MOD) is a critical task for autonomous vehicles as
moving objects represent higher collision risk than static ones. The trajectory
of the ego-vehicle is planned based on the future states of detected moving
objects. It is quite challenging as the ego-motion has to be modelled and
compensated to be able to understand the motion of the surrounding objects. In
this work, we propose a real-time end-to-end CNN architecture for MOD utilizing
spatio-temporal context to improve robustness. We construct a novel time-aware
architecture exploiting temporal motion information embedded within sequential
images in addition to explicit motion maps using optical flow images.We
demonstrate the impact of our algorithm on KITTI dataset where we obtain an
improvement of 8% relative to the baselines. We compare our algorithm with
state-of-the-art methods and achieve competitive results on KITTI-Motion
dataset in terms of accuracy at three times better run-time. The proposed
algorithm runs at 23 fps on a standard desktop GPU targeting deployment on
embedded platforms.","cs.CV,cs.LG,cs.RO,stat.ML"
"Global Image Segmentation Process using Machine Learning algorithm & Convolution Neural Network method for Self- Driving Vehicles. In autonomous Vehicles technology Image segmentation was a major problem in
visual perception. This image segmentation process is mainly used in medical
applications. Here we adopted an image segmentation process to visual
perception tasks for predicting the agents on the surrounding environment,
identifying the road boundaries and tracking the line markings. Main objective
of the paper is to divide the input images using the image segmentation process
and Convolution Neural Network method for efficient results of visual
perception. For Sampling assume a local city data-set samples and validation
process done in Jupyter Notebook using Python language. We proposed this image
segmentation method planning to standard and further the development of
state-of-the art methods for visual inspection system understanding. The
experimental results achieves 73% mean IOU. Our method also achieves 90 FPS
inference speed and using a NVDIA GeForce GTX 1050 GPU.","cs.CV,cs.GT"
"Mean Oriented Riesz Features for Micro Expression Classification. Micro-expressions are brief and subtle facial expressions that go on and off
the face in a fraction of a second. This kind of facial expressions usually
occurs in high stake situations and is considered to reflect a human's real
intent. There has been some interest in micro-expression analysis, however, a
great majority of the methods are based on classically established computer
vision methods such as local binary patterns, histogram of gradients and
optical flow. A novel methodology for micro-expression recognition using the
Riesz pyramid, a multi-scale steerable Hilbert transform is presented. In fact,
an image sequence is transformed with this tool, then the image phase
variations are extracted and filtered as proxies for motion. Furthermore, the
dominant orientation constancy from the Riesz transform is exploited to average
the micro-expression sequence into an image pair. Based on that, the Mean
Oriented Riesz Feature description is introduced. Finally the performance of
our methods are tested in two spontaneous micro-expressions databases and
compared to state-of-the-art methods.","68T10,68U10,cs.CV"
"Learning to Reason in Large Theories without Imitation. In this paper, we demonstrate how to do automated theorem proving in the
presence of a large knowledge base of potential premises without learning from
human proofs. We suggest an exploration mechanism that mixes in additional
premises selected by a tf-idf (term frequency-inverse document frequency) based
lookup in a deep reinforcement learning scenario. This helps with exploring and
learning which premises are relevant for proving a new theorem. Our experiments
show that the theorem prover trained with this exploration mechanism
outperforms provers that are trained only on human proofs. It approaches the
performance of a prover trained by a combination of imitation and reinforcement
learning. We perform multiple experiments to understand the importance of the
underlying assumptions that make our exploration approach work, thus explaining
our design choices.","cs.AI,cs.LG,cs.LO,stat.ML"
"Capsule Attention for Multimodal EEG-EOG Representation Learning with Application to Driver Vigilance Estimation. Driver vigilance estimation is an important task for transportation safety.
Wearable and portable brain-computer interface devices provide a powerful means
for real-time monitoring of the vigilance level of drivers to help with
avoiding distracted or impaired driving. In this paper, we propose a novel
multimodal architecture for in-vehicle vigilance estimation from
Electroencephalogram and Electrooculogram. To enable the system to focus on the
most salient parts of the learned multimodal representations, we propose an
architecture composed of a capsule attention mechanism following a deep Long
Short-Term Memory (LSTM) network. Our model learns hierarchical dependencies in
the data through the LSTM and capsule feature representation layers. To better
explore the discriminative ability of the learned representations, we study the
effect of the proposed capsule attention mechanism including the number of
dynamic routing iterations as well as other parameters. Experiments show the
robustness of our method by outperforming other solutions and baseline
techniques, setting a new state-of-the-art. We then provide an analysis on
different frequency bands and brain regions to evaluate their suitability for
driver vigilance estimation. Lastly, an analysis on the role of capsule
attention, multimodality, and robustness to noise is performed, highlighting
the advantages of our approach.","cs.CV,cs.LG,eess.SP,stat.ML"
"Network Clustering by Embedding of Attribute-augmented Graphs. In this paper we propose a new approach to detect clusters in undirected
graphs with attributed vertices. The aim is to group vertices which are similar
not only in terms of structural connectivity but also in terms of attribute
values. We incorporate structural and attribute similarities between the
vertices in an augmented graph by creating additional vertices and edges as
proposed in [5, 27]. The augmented graph is embedded in a Euclidean space
associated to its Laplacian and apply a modified K-means algorithm to identify
clusters. The modified K-means uses a vector distance measure where to each
original vertex is assigned a vector-valued set of coordinates depending on
both structural connectivity and attribute similarities. To define the
coordinate vectors we employ an adaptive AMG (Algebraic MultiGrid) method to
identify the coordinate directions in the embedding Euclidean space extending
our previous result for graphs without attributes. We demonstrate the
effectiveness of our proposed clustering method on both synthetic and
real-world attributed graphs.","05C50,05C70,65M55,cs.LG,cs.NA,math.NA,math.ST,stat.TH"
"Deep Hierarchical Reinforcement Learning Based Recommendations via Multi-goals Abstraction. The recommender system is an important form of intelligent application, which
assists users to alleviate from information redundancy. Among the metrics used
to evaluate a recommender system, the metric of conversion has become more and
more important. The majority of existing recommender systems perform poorly on
the metric of conversion due to its extremely sparse feedback signal. To tackle
this challenge, we propose a deep hierarchical reinforcement learning based
recommendation framework, which consists of two components, i.e., high-level
agent and low-level agent. The high-level agent catches long-term sparse
conversion signals, and automatically sets abstract goals for low-level agent,
while the low-level agent follows the abstract goals and interacts with
real-time environment. To solve the inherent problem in hierarchical
reinforcement learning, we propose a novel deep hierarchical reinforcement
learning algorithm via multi-goals abstraction (HRL-MG). Our proposed algorithm
contains three characteristics: 1) the high-level agent generates multiple
goals to guide the low-level agent in different stages, which reduces the
difficulty of approaching high-level goals; 2) different goals share the same
state encoder parameters, which increases the update frequency of the
high-level agent and thus accelerates the convergence of our proposed
algorithm; 3) an appreciate benefit assignment function is designed to allocate
rewards in each goal so as to coordinate different goals in a consistent
direction. We evaluate our proposed algorithm based on a real-world e-commerce
dataset and validate its effectiveness.","cs.AI,cs.IR,cs.LG"
"rx-anon -- A Novel Approach on the De-Identification of Heterogeneous Data based on a Modified Mondrian Algorithm. Traditional approaches for data anonymization consider relational data and
textual data independently. We propose rx-anon, an anonymization approach for
heterogeneous semi-structured documents composed of relational and textual
attributes. We map sensitive terms extracted from the text to the structured
data. This allows us to use concepts like k-anonymity to generate a joined,
privacy-preserved version of the heterogeneous data input. We introduce the
concept of redundant sensitive information to consistently anonymize the
heterogeneous data. To control the influence of anonymization over unstructured
textual data versus structured data attributes, we introduce a modified,
parameterized Mondrian algorithm. The parameter $\lambda$ allows to give
different weight on the relational and textual attributes during the
anonymization process. We evaluate our approach with two real-world datasets
using a Normalized Certainty Penalty score, adapted to the problem of jointly
anonymizing relational and textual data. The results show that our approach is
capable of reducing information loss by using the tuning parameter to control
the Mondrian partitioning while guaranteeing k-anonymity for relational
attributes as well as for sensitive terms. As rx-anon is a framework approach,
it can be reused and extended by other anonymization algorithms, privacy
models, and textual similarity metrics.","cs.CR,cs.DB,cs.LG"
"Meta-Learning for Koopman Spectral Analysis with Short Time-series. Koopman spectral analysis has attracted attention for nonlinear dynamical
systems since we can analyze nonlinear dynamics with a linear regime by
embedding data into a Koopman space by a nonlinear function. For the analysis,
we need to find appropriate embedding functions. Although several neural
network-based methods have been proposed for learning embedding functions,
existing methods require long time-series for training neural networks. This
limitation prohibits performing Koopman spectral analysis in applications where
only short time-series are available. In this paper, we propose a meta-learning
method for estimating embedding functions from unseen short time-series by
exploiting knowledge learned from related but different time-series. With the
proposed method, a representation of a given short time-series is obtained by a
bidirectional LSTM for extracting its properties. The embedding function of the
short time-series is modeled by a neural network that depends on the
time-series representation. By sharing the LSTM and neural networks across
multiple time-series, we can learn common knowledge from different time-series
while modeling time-series-specific embedding functions with the time-series
representation. Our model is trained such that the expected test prediction
error is minimized with the episodic training framework. We experimentally
demonstrate that the proposed method achieves better performance in terms of
eigenvalue estimation and future prediction than existing methods.","cs.LG,math.DS,stat.ML"
"Echo State Networks trained by Tikhonov least squares are L2() approximators of ergodic dynamical systems. Echo State Networks (ESNs) are a class of single-layer recurrent neural
networks with randomly generated internal weights, and a single layer of
tuneable outer weights, which are usually trained by regularised linear least
squares regression. Remarkably, ESNs still enjoy the universal approximation
property despite the training procedure being entirely linear. In this paper,
we prove that an ESN trained on a sequence of observations from an ergodic
dynamical system (with invariant measure $\mu$) using Tikhonov least squares
regression against a set of targets, will approximate the target function in
the $L^2(\mu)$ norm. In the special case that the targets are future
observations, the ESN is learning the next step map, which allows time series
forecasting. We demonstrate the theory numerically by training an ESN using
Tikhonov least squares on a sequence of scalar observations of the Lorenz
system.","cs.LG,math.DS,stat.ML"
"Figure Captioning with Reasoning and Sequence-Level Training. Figures, such as bar charts, pie charts, and line plots, are widely used to
convey important information in a concise format. They are usually
human-friendly but difficult for computers to process automatically. In this
work, we investigate the problem of figure captioning where the goal is to
automatically generate a natural language description of the figure. While
natural image captioning has been studied extensively, figure captioning has
received relatively little attention and remains a challenging problem. First,
we introduce a new dataset for figure captioning, FigCAP, based on FigureQA.
Second, we propose two novel attention mechanisms. To achieve accurate
generation of labels in figures, we propose Label Maps Attention. To model the
relations between figure labels, we propose Relation Maps Attention. Third, we
use sequence-level training with reinforcement learning in order to directly
optimizes evaluation metrics, which alleviates the exposure bias issue and
further improves the models in generating long captions. Extensive experiments
show that the proposed method outperforms the baselines, thus demonstrating a
significant potential for the automatic captioning of vast repositories of
figures.","cs.CL,cs.CV"
"Region segmentation via deep learning and convex optimization. In this paper, we propose a method to segment regions in three-dimensional
point clouds. We assume that (i) the shape and the number of regions in the
point cloud are not known and (ii) the point cloud may be noisy. The method
consists of two steps. In the first step we use a deep neural network to
predict the probability that a pair of small patches from the point cloud
belongs to the same region. In the second step, we use a convex-optimization
based method to improve the predictions of the network by enforcing consistency
constraints. We evaluate the accuracy of our method on a custom dataset of
convex polyhedra, where the regions correspond to the faces of the polyhedra.
The method can be seen as a robust and flexible alternative to the famous
region growing segmentation algorithm. All reported results are reproducible
and come with easy to use code that could serve as a baseline for future
research.","65D19,68T45,I.4.6; I.2.10,cs.CV,cs.LG,eess.IV"
"Power Grid Cascading Failure Mitigation by Reinforcement Learning. This paper proposes a cascading failure mitigation strategy based on
Reinforcement Learning (RL). The motivation of the Multi-Stage Cascading
Failure (MSCF) problem and its connection with the challenge of climate change
are introduced. The bottom-level corrective control of the MCSF problem is
formulated based on DCOPF (Direct Current Optimal Power Flow). Then, to
mitigate the MSCF issue by a high-level RL-based strategy, physics-informed
reward, action, and state are devised. Besides, both shallow and deep neural
network architectures are tested. Experiments on the IEEE 118-bus system by the
proposed mitigation strategy demonstrate a promising performance in reducing
system collapses.","68T01,68T07,F.2.2; K.3.2,cs.LG,math.DS,math.OC"
"CrossWalk: Fairness-enhanced Node Representation Learning. The potential for machine learning systems to amplify social inequities and
unfairness is receiving increasing popular and academic attention. Much recent
work has focused on developing algorithmic tools to assess and mitigate such
unfairness. However, there is little work on enhancing fairness in graph
algorithms. Here, we develop a simple, effective and general method, CrossWalk,
that enhances fairness of various graph algorithms, including influence
maximization, link prediction and node classification, applied to node
embeddings. CrossWalk is applicable to any random walk based node
representation learning algorithm, such as DeepWalk and Node2Vec. The key idea
is to bias random walks to cross group boundaries, by upweighting edges which
(1) are closer to the groups' peripheries or (2) connect different groups in
the network. CrossWalk pulls nodes that are near groups' peripheries towards
their neighbors from other groups in the embedding space, while preserving the
necessary structural information from the graph. Extensive experiments show the
effectiveness of our algorithm to enhance fairness in various graph algorithms,
including influence maximization, link prediction and node classification in
synthetic and real networks, with only a very small decrease in performance.","cs.LG,cs.SI,stat.ML"
"Learning Curriculum Policies for Reinforcement Learning. Curriculum learning in reinforcement learning is a training methodology that
seeks to speed up learning of a difficult target task, by first training on a
series of simpler tasks and transferring the knowledge acquired to the target
task. Automatically choosing a sequence of such tasks (i.e. a curriculum) is an
open problem that has been the subject of much recent work in this area. In
this paper, we build upon a recent method for curriculum design, which
formulates the curriculum sequencing problem as a Markov Decision Process. We
extend this model to handle multiple transfer learning algorithms, and show for
the first time that a curriculum policy over this MDP can be learned from
experience. We explore various representations that make this possible, and
evaluate our approach by learning curriculum policies for multiple agents in
two different domains. The results show that our method produces curricula that
can train agents to perform on a target task as fast or faster than existing
methods.","cs.AI,cs.LG,stat.ML"
"Node Copying for Protection Against Graph Neural Network Topology Attacks. Adversarial attacks can affect the performance of existing deep learning
models. With the increased interest in graph based machine learning techniques,
there have been investigations which suggest that these models are also
vulnerable to attacks. In particular, corruptions of the graph topology can
degrade the performance of graph based learning algorithms severely. This is
due to the fact that the prediction capability of these algorithms relies
mostly on the similarity structure imposed by the graph connectivity.
Therefore, detecting the location of the corruption and correcting the induced
errors becomes crucial. There has been some recent work which tackles the
detection problem, however these methods do not address the effect of the
attack on the downstream learning task. In this work, we propose an algorithm
that uses node copying to mitigate the degradation in classification that is
caused by adversarial attacks. The proposed methodology is applied only after
the model for the downstream task is trained and the added computation cost
scales well for large graphs. Experimental results show the effectiveness of
our approach for several real world datasets.","cs.CR,cs.LG,cs.SI"
"Supervised Learning on Relational Databases with Graph Neural Networks. The majority of data scientists and machine learning practitioners use
relational data in their work [State of ML and Data Science 2017, Kaggle,
Inc.]. But training machine learning models on data stored in relational
databases requires significant data extraction and feature engineering efforts.
These efforts are not only costly, but they also destroy potentially important
relational structure in the data. We introduce a method that uses Graph Neural
Networks to overcome these challenges. Our proposed method outperforms
state-of-the-art automatic feature engineering methods on two out of three
datasets.","cs.AI,cs.DB,cs.LG,stat.ML"
"NLS: an accurate and yet easy-to-interpret regression method. An important feature of successful supervised machine learning applications
is to be able to explain the predictions given by the regression or
classification model being used. However, most state-of-the-art models that
have good predictive power lead to predictions that are hard to interpret.
Thus, several model-agnostic interpreters have been developed recently as a way
of explaining black-box classifiers. In practice, using these methods is a slow
process because a novel fitting is required for each new testing instance, and
several non-trivial choices must be made. We develop NLS (neural local
smoother), a method that is complex enough to give good predictions, and yet
gives solutions that are easy to be interpreted without the need of using a
separate interpreter. The key idea is to use a neural network that imposes a
local linear shape to the output layer. We show that NLS leads to predictive
power that is comparable to state-of-the-art machine learning models, and yet
is easier to interpret.","cs.LG,stat.ME,stat.ML"
"Towards Multi-agent Reinforcement Learning for Wireless Network Protocol Synthesis. This paper proposes a multi-agent reinforcement learning based medium access
framework for wireless networks. The access problem is formulated as a Markov
Decision Process (MDP), and solved using reinforcement learning with every
network node acting as a distributed learning agent. The solution components
are developed step by step, starting from a single-node access scenario in
which a node agent incrementally learns to control MAC layer packet loads for
reining in self-collisions. The strategy is then scaled up for multi-node
fully-connected scenarios by using more elaborate reward structures. It also
demonstrates preliminary feasibility for more general partially connected
topologies. It is shown that by learning to adjust MAC layer transmission
probabilities, the protocol is not only able to attain theoretical maximum
throughput at an optimal load, but unlike classical approaches, it can also
retain that maximum throughput at higher loading conditions. Additionally, the
mechanism is agnostic to heterogeneous loading while preserving that feature.
It is also shown that access priorities of the protocol across nodes can be
parametrically adjusted. Finally, it is also shown that the online learning
feature of reinforcement learning is able to make the protocol adapt to
time-varying loading conditions.","cs.AI,cs.LG,cs.SY,eess.SY"
"Hard and Soft EM in Bayesian Network Learning from Incomplete Data. Incomplete data are a common feature in many domains, from clinical trials to
industrial applications. Bayesian networks (BNs) are often used in these
domains because of their graphical and causal interpretations. BN parameter
learning from incomplete data is usually implemented with the
Expectation-Maximisation algorithm (EM), which computes the relevant sufficient
statistics (""soft EM"") using belief propagation. Similarly, the Structural
Expectation-Maximisation algorithm (Structural EM) learns the network structure
of the BN from those sufficient statistics using algorithms designed for
complete data. However, practical implementations of parameter and structure
learning often impute missing data (""hard EM"") to compute sufficient statistics
instead of using belief propagation, for both ease of implementation and
computational speed. In this paper, we investigate the question: what is the
impact of using imputation instead of belief propagation on the quality of the
resulting BNs? From a simulation study using synthetic data and reference BNs,
we find that it is possible to recommend one approach over the other in several
scenarios based on the characteristics of the data. We then use this
information to build a simple decision tree to guide practitioners in choosing
the EM algorithm best suited to their problem.","cs.LG,stat.ME,stat.ML"
"Least $k$th-Order and Rnyi Generative Adversarial Networks. We investigate the use of parametrized families of information-theoretic
measures to generalize the loss functions of generative adversarial networks
(GANs) with the objective of improving performance. A new generator loss
function, called least $k$th-order GAN (L$k$GAN), is first introduced,
generalizing the least squares GANs (LSGANs) by using a $k$th order absolute
error distortion measure with $k \geq 1$ (which recovers the LSGAN loss
function when $k=2$). It is shown that minimizing this generalized loss
function under an (unconstrained) optimal discriminator is equivalent to
minimizing the $k$th-order Pearson-Vajda divergence. Another novel GAN
generator loss function is next proposed in terms of R\'{e}nyi cross-entropy
functionals with order $\alpha >0$, $\alpha\neq 1$. It is demonstrated that
this R\'{e}nyi-centric generalized loss function, which provably reduces to the
original GAN loss function as $\alpha\to1$, preserves the equilibrium point
satisfied by the original GAN based on the Jensen-R\'{e}nyi divergence, a
natural extension of the Jensen-Shannon divergence.
  Experimental results indicate that the proposed loss functions, applied to
the MNIST and CelebA datasets, under both DCGAN and StyleGAN architectures,
confer performance benefits by virtue of the extra degrees of freedom provided
by the parameters $k$ and $\alpha$, respectively. More specifically,
experiments show improvements with regard to the quality of the generated
images as measured by the Fr\'echet Inception Distance (FID) score and training
stability. While it was applied to GANs in this study, the proposed approach is
generic and can be used in other applications of information theory to deep
learning, e.g., the issues of fairness or privacy in artificial intelligence.","cs.IT,cs.LG,math.IT,stat.ML"
"Generative Models and Model Criticism via Optimized Maximum Mean Discrepancy. We propose a method to optimize the representation and distinguishability of
samples from two probability distributions, by maximizing the estimated power
of a statistical test based on the maximum mean discrepancy (MMD). This
optimized MMD is applied to the setting of unsupervised learning by generative
adversarial networks (GAN), in which a model attempts to generate realistic
samples, and a discriminator attempts to tell these apart from data samples. In
this context, the MMD may be used in two roles: first, as a discriminator,
either directly on the samples, or on features of the samples. Second, the MMD
can be used to evaluate the performance of a generative model, by testing the
model's samples against a reference data set. In the latter role, the optimized
MMD is particularly helpful, as it gives an interpretable indication of how the
model and data distributions differ, even in cases where individual model
samples are not easily distinguished either by eye or by classifier.","cs.AI,cs.LG,cs.NE,stat.ME,stat.ML"
"Gated Graph Recursive Neural Networks for Molecular Property Prediction. Molecule property prediction is a fundamental problem for computer-aided drug
discovery and materials science. Quantum-chemical simulations such as density
functional theory (DFT) have been widely used for calculating the molecule
properties, however, because of the heavy computational cost, it is difficult
to search a huge number of potential chemical compounds. Machine learning
methods for molecular modeling are attractive alternatives, however, the
development of expressive, accurate, and scalable graph neural networks for
learning molecular representations is still challenging. In this work, we
propose a simple and powerful graph neural networks for molecular property
prediction. We model a molecular as a directed complete graph in which each
atom has a spatial position, and introduce a recursive neural network with
simple gating function. We also feed input embeddings for every layers as skip
connections to accelerate the training. Experimental results show that our
model achieves the state-of-the-art performance on the standard benchmark
dataset for molecular property prediction.","cs.LG,q-bio.BM,stat.ML"
"A Practical Method for Constructing Equivariant Multilayer Perceptrons for Arbitrary Matrix Groups. Symmetries and equivariance are fundamental to the generalization of neural
networks on domains such as images, graphs, and point clouds. Existing work has
primarily focused on a small number of groups, such as the translation,
rotation, and permutation groups. In this work we provide a completely general
algorithm for solving for the equivariant layers of matrix groups. In addition
to recovering solutions from other works as special cases, we construct
multilayer perceptrons equivariant to multiple groups that have never been
tackled before, including $\mathrm{O}(1,3)$, $\mathrm{O}(5)$, $\mathrm{Sp}(n)$,
and the Rubik's cube group. Our approach outperforms non-equivariant baselines,
with applications to particle physics and dynamical systems. We release our
software library to enable researchers to construct equivariant layers for
arbitrary matrix groups.","cs.LG,math.DS,stat.ML"
"Design of Spectrum Sensing Policy for Multi-user Multi-band Cognitive Radio Network. Finding an optimal sensing policy for a particular access policy and sensing
scheme is a laborious combinatorial problem that requires the system model
parameters to be known. In practise the parameters or the model itself may not
be completely known making reinforcement learning methods appealing. In this
paper a non-parametric reinforcement learning-based method is developed for
sensing and accessing multi-band radio spectrum in multi-user cognitive radio
networks. A suboptimal sensing policy search algorithm is proposed for a
particular multi-user multi-band access policy and the randomized
Chair-Varshney rule. The randomized Chair-Varshney rule is used to reduce the
probability of false alarms under a constraint on the probability of detection
that protects the primary user. The simulation results show that the proposed
method achieves a sum profit (e.g. data rate) close to the optimal sensing
policy while achieving the desired probability of detection.","cs.LG,cs.NI"
"Voice2Series: Reprogramming Acoustic Models for Time Series Classification. Learning to classify time series with limited data is a practical yet
challenging problem. Current methods are primarily based on hand-designed
feature extraction rules or domain-specific data augmentation. Motivated by the
advances in deep speech processing models and the fact that voice data are
univariate temporal signals, in this paper, we propose Voice2Series (V2S), a
novel end-to-end approach that reprograms acoustic models for time series
classification, through input transformation learning and output label mapping.
Leveraging the representation learning power of a large-scale pre-trained
speech processing model, on 30 different time series tasks we show that V2S
either outperforms or is tied with state-of-the-art methods on 20 tasks, and
improves their average accuracy by 1.84%. We further provide a theoretical
justification of V2S by proving its population risk is upper bounded by the
source risk and a Wasserstein distance accounting for feature alignment via
reprogramming. Our results offer new and effective means to time series
classification.","cs.AI,cs.LG,cs.NE,cs.SD,eess.AS"
"Steinhaus Filtration and Stable Paths in the Mapper. Two central concepts from topological data analysis are persistence and the
Mapper construction. Persistence employs a sequence of objects built on data
called a filtration. A Mapper produces insightful summaries of data, and has
found widespread applications in diverse areas.
  We define a new filtration called the cover filtration built from a single
cover based on a generalized Steinhaus distance, which is a generalization of
Jaccard distance. We prove a stability result: the cover filtrations of two
covers are $\alpha/m$ interleaved, where $\alpha$ is a bound on bottleneck
distance between covers and $m$ is the size of smallest set in either cover. We
also show our construction is equivalent to the Cech filtration under certain
settings, and the Vietoris-Rips filtration completely determines the cover
filtration in all cases. We then develop a theory for stable paths within this
filtration. Unlike standard results on stability in topological persistence,
our definition of path stability aligns exactly with the above result on
stability of cover filtration.
  We demonstrate how our framework can be employed in a variety of applications
where a metric is not obvious but a cover is readily available. First we
present a new model for recommendation systems using cover filtration. For an
explicit example, stable paths identified on a movies data set represent
sequences of movies constituting gentle transitions from one genre to another.
As a second application in explainable machine learning, we apply the Mapper
for model induction, providing explanations in the form of paths between
subpopulations. Stable paths in the Mapper from a supervised machine learning
model trained on the FashionMNIST data set provide improved explanations of
relationships between subpopulations of images.","cs.CG,cs.LG,math.AT"
"Text-to-Image Generation Grounded by Fine-Grained User Attention. Localized Narratives is a dataset with detailed natural language descriptions
of images paired with mouse traces that provide a sparse, fine-grained visual
grounding for phrases. We propose TReCS, a sequential model that exploits this
grounding to generate images. TReCS uses descriptions to retrieve segmentation
masks and predict object labels aligned with mouse traces. These alignments are
used to select and position masks to generate a fully covered segmentation
canvas; the final image is produced by a segmentation-to-image generator using
this canvas. This multi-step, retrieval-based approach outperforms existing
direct text-to-image generation models on both automatic metrics and human
evaluations: overall, its generated images are more photo-realistic and better
match descriptions.","cs.AI,cs.CV"
"A Novel Intrinsic Measure of Data Separability. In machine learning, the performance of a classifier depends on both the
classifier model and the separability/complexity of datasets. To quantitatively
measure the separability of datasets, we create an intrinsic measure -- the
Distance-based Separability Index (DSI), which is independent of the classifier
model. We consider the situation in which different classes of data are mixed
in the same distribution to be the most difficult for classifiers to separate.
We then formally show that the DSI can indicate whether the distributions of
datasets are identical for any dimensionality. And we verify the DSI to be an
effective separability measure by comparing to several state-of-the-art
separability/complexity measures using synthetic and real datasets. Having
demonstrated the DSI's ability to compare distributions of samples, we also
discuss some of its other promising applications, such as measuring the
performance of generative adversarial networks (GANs) and evaluating the
results of clustering methods.","cs.LG,math.ST,stat.TH"
"TF-Replicator: Distributed Machine Learning for Researchers. We describe TF-Replicator, a framework for distributed machine learning
designed for DeepMind researchers and implemented as an abstraction over
TensorFlow. TF-Replicator simplifies writing data-parallel and model-parallel
research code. The same models can be effortlessly deployed to different
cluster architectures (i.e. one or many machines containing CPUs, GPUs or TPU
accelerators) using synchronous or asynchronous training regimes. To
demonstrate the generality and scalability of TF-Replicator, we implement and
benchmark three very different models: (1) A ResNet-50 for ImageNet
classification, (2) a SN-GAN for class-conditional ImageNet image generation,
and (3) a D4PG reinforcement learning agent for continuous control. Our results
show strong scalability performance without demanding any distributed systems
expertise of the user. The TF-Replicator programming model will be open-sourced
as part of TensorFlow 2.0 (see
https://github.com/tensorflow/community/pull/25).","cs.AI,cs.DC,cs.LG,stat.ML"
"Proximal Reliability Optimization for Reinforcement Learning. Despite the numerous advances, reinforcement learning remains away from
widespread acceptance for autonomous controller design as compared to classical
methods due to lack of ability to effectively tackle the reality gap. The
reliance on absolute or deterministic reward as a metric for optimization
process renders reinforcement learning highly susceptible to changes in problem
dynamics. We introduce a novel framework that effectively quantizes the
uncertainty of the design space and induces robustness in controllers by
switching to a reliability-based optimization routine. The data efficiency of
the method is maintained to match reward based optimization methods by
employing a model-based approach. We prove the stability of learned
neuro-controllers in both static and dynamic environments on classical
reinforcement learning tasks such as Cart Pole balancing and Inverted Pendulum.","cs.LG,cs.SY,stat.ML"
"ViT-Inception-GAN for Image Colourising. Studies involving colourising images has been garnering researchers' keen
attention over time, assisted by significant advances in various Machine
Learning techniques and compute power availability. Traditionally, colourising
images have been an intricate task that gave a substantial degree of freedom
during the assignment of chromatic information. In our proposed method, we
attempt to colourise images using Vision Transformer - Inception - Generative
Adversarial Network (ViT-I-GAN), which has an Inception-v3 fusion embedding in
the generator. For a stable and robust network, we have used Vision Transformer
(ViT) as the discriminator. We trained the model on the Unsplash and the COCO
dataset for demonstrating the improvement made by the Inception-v3 embedding.
We have compared the results between ViT-GANs with and without Inception-v3
embedding.","cs.CV,cs.LG"
"An autoencoder wavelet based deep neural network with attention mechanism for multistep prediction of plant growth. Multi-step prediction is considered of major significance for time series
analysis in many real life problems. Existing methods mainly focus on
one-step-ahead forecasting, since multiple step forecasting generally fails due
to accumulation of prediction errors. This paper presents a novel approach for
predicting plant growth in agriculture, focusing on prediction of plant Stem
Diameter Variations (SDV). The proposed approach consists of three main steps.
At first, wavelet decomposition is applied to the original data, as to
facilitate model fitting and reduce noise in them. Then an encoder-decoder
framework is developed using Long Short Term Memory (LSTM) and used for
appropriate feature extraction from the data. Finally, a recurrent neural
network including LSTM and an attention mechanism is proposed for modelling
long-term dependencies in the time series data. Experimental results are
presented which illustrate the good performance of the proposed approach and
that it significantly outperforms the existing models, in terms of error
criteria such as RMSE, MAE and MAPE.",cs.LG
"Perturbation Bounds for Procrustes, Classical Scaling, and Trilateration, with Applications to Manifold Learning. One of the common tasks in unsupervised learning is dimensionality reduction,
where the goal is to find meaningful low-dimensional structures hidden in
high-dimensional data. Sometimes referred to as manifold learning, this problem
is closely related to the problem of localization, which aims at embedding a
weighted graph into a low-dimensional Euclidean space. Several methods have
been proposed for localization, and also manifold learning. Nonetheless, the
robustness property of most of them is little understood. In this paper, we
obtain perturbation bounds for classical scaling and trilateration, which are
then applied to derive performance bounds for Isomap, Landmark Isomap, and
Maximum Variance Unfolding. A new perturbation bound for procrustes analysis
plays a key role.","cs.LG,cs.NA,math.NA,stat.ML"
"NudgeSeg: Zero-Shot Object Segmentation by Repeated Physical Interaction. Recent advances in object segmentation have demonstrated that deep neural
networks excel at object segmentation for specific classes in color and depth
images. However, their performance is dictated by the number of classes and
objects used for training, thereby hindering generalization to never seen
objects or zero-shot samples. To exacerbate the problem further, object
segmentation using image frames rely on recognition and pattern matching cues.
Instead, we utilize the 'active' nature of a robot and their ability to
'interact' with the environment to induce additional geometric constraints for
segmenting zero-shot samples.
  In this paper, we present the first framework to segment unknown objects in a
cluttered scene by repeatedly 'nudging' at the objects and moving them to
obtain additional motion cues at every step using only a monochrome monocular
camera. We call our framework NudgeSeg. These motion cues are used to refine
the segmentation masks. We successfully test our approach to segment novel
objects in various cluttered scenes and provide an extensive study with image
and motion segmentation methods. We show an impressive average detection rate
of over 86% on zero-shot objects.","cs.CV,cs.RO"
"Identification of Model Uncertainty via Optimal Design of Experiments Applied to a Mechanical Press. In engineering applications almost all processes are described with the help
of models. Especially forming machines heavily rely on mathematical models for
control and condition monitoring. Inaccuracies during the modeling,
manufacturing and assembly of these machines induce model uncertainty which
impairs the controller's performance. In this paper we propose an approach to
identify model uncertainty using parameter identification, optimal design of
experiments and hypothesis testing. The experimental setup is characterized by
optimal sensor positions such that specific model parameters can be determined
with minimal variance. This allows for the computation of confidence regions in
which the real parameters or the parameter estimates from different test sets
have to lie. We claim that inconsistencies in the estimated parameter values,
considering their approximated confidence ellipsoids as well, cannot be
explained by data uncertainty but are indicators of model uncertainty. The
proposed method is demonstrated using a component of the 3D Servo Press, a
multi-technology forming machine that combines spindles with eccentric servo
drives.","cs.LG,math.OC,stat.AP,stat.ML"
"LAMP: Large Deep Nets with Automated Model Parallelism for Image Segmentation. Deep Learning (DL) models are becoming larger, because the increase in model
size might offer significant accuracy gain. To enable the training of large
deep networks, data parallelism and model parallelism are two well-known
approaches for parallel training. However, data parallelism does not help
reduce memory footprint per device. In this work, we introduce Large deep 3D
ConvNets with Automated Model Parallelism (LAMP) and investigate the impact of
both input's and deep 3D ConvNets' size on segmentation accuracy. Through
automated model parallelism, it is feasible to train large deep 3D ConvNets
with a large input patch, even the whole image. Extensive experiments
demonstrate that, facilitated by the automated model parallelism, the
segmentation accuracy can be improved through increasing model size and input
context size, and large input yields significant inference speedup compared
with sliding window of small patches in the inference. Code is
available\footnote{https://monai.io/research/lamp-automated-model-parallelism}.","cs.CV,cs.DC,cs.LG,cs.NE,eess.IV"
"Hypotheses testing on infinite random graphs. Drawing on some recent results that provide the formalism necessary to
definite stationarity for infinite random graphs, this paper initiates the
study of statistical and learning questions pertaining to these objects.
Specifically, a criterion for the existence of a consistent test for complex
hypotheses is presented, generalizing the corresponding results on time series.
As an application, it is shown how one can test that a tree has the Markov
property, or, more generally, to estimate its memory.","cs.IT,cs.LG,math.IT,math.ST,stat.ML,stat.TH"
"Learning to Transfer Dynamic Models of Underactuated Soft Robotic Hands. Transfer learning is a popular approach to bypassing data limitations in one
domain by leveraging data from another domain. This is especially useful in
robotics, as it allows practitioners to reduce data collection with physical
robots, which can be time-consuming and cause wear and tear. The most common
way of doing this with neural networks is to take an existing neural network,
and simply train it more with new data. However, we show that in some
situations this can lead to significantly worse performance than simply using
the transferred model without adaptation. We find that a major cause of these
problems is that models trained on small amounts of data can have chaotic or
divergent behavior in some regions. We derive an upper bound on the Lyapunov
exponent of a trained transition model, and demonstrate two approaches that
make use of this insight. Both show significant improvement over traditional
fine-tuning. Experiments performed on real underactuated soft robotic hands
clearly demonstrate the capability to transfer a dynamic model from one hand to
another.","cs.LG,cs.SY,eess.SY,stat.ML"
"Interpolated Convolutional Networks for 3D Point Cloud Understanding. Point cloud is an important type of 3D representation. However, directly
applying convolutions on point clouds is challenging due to the sparse,
irregular and unordered data structure. In this paper, we propose a novel
Interpolated Convolution operation, InterpConv, to tackle the point cloud
feature learning and understanding problem. The key idea is to utilize a set of
discrete kernel weights and interpolate point features to neighboring
kernel-weight coordinates by an interpolation function for convolution. A
normalization term is introduced to handle neighborhoods of different sparsity
levels. Our InterpConv is shown to be permutation and sparsity invariant, and
can directly handle irregular inputs. We further design Interpolated
Convolutional Neural Networks (InterpCNNs) based on InterpConv layers to handle
point cloud recognition tasks including shape classification, object part
segmentation and indoor scene semantic parsing. Experiments show that the
networks can capture both fine-grained local structures and global shape
context information effectively. The proposed approach achieves
state-of-the-art performance on public benchmarks including ModelNet40,
ShapeNet Parts and S3DIS.","cs.CG,cs.CV,eess.IV"
"3D attention mechanism for fine-grained classification of table tennis strokes using a Twin Spatio-Temporal Convolutional Neural Networks. The paper addresses the problem of recognition of actions in video with low
inter-class variability such as Table Tennis strokes. Two stream, ""twin""
convolutional neural networks are used with 3D convolutions both on RGB data
and optical flow. Actions are recognized by classification of temporal windows.
We introduce 3D attention modules and examine their impact on classification
efficiency. In the context of the study of sportsmen performances, a corpus of
the particular actions of table tennis strokes is considered. The use of
attention blocks in the network speeds up the training step and improves the
classification scores up to 5% with our twin model. We visualize the impact on
the obtained features and notice correlation between attention and player
movements and position. Score comparison of state-of-the-art action
classification method and proposed approach with attentional blocks is
performed on the corpus. Proposed model with attention blocks outperforms
previous model without them and our baseline.","cs.CV,cs.HC,cs.LG,cs.MM"
"Stochastic Primal-Dual Methods and Sample Complexity of Reinforcement Learning. We study the online estimation of the optimal policy of a Markov decision
process (MDP). We propose a class of Stochastic Primal-Dual (SPD) methods which
exploit the inherent minimax duality of Bellman equations. The SPD methods
update a few coordinates of the value and policy estimates as a new state
transition is observed. These methods use small storage and has low
computational complexity per iteration. The SPD methods find an
absolute-$\epsilon$-optimal policy, with high probability, using
$\mathcal{O}\left(\frac{|\mathcal{S}|^4 |\mathcal{A}|^2\sigma^2
}{(1-\gamma)^6\epsilon^2} \right)$ iterations/samples for the infinite-horizon
discounted-reward MDP and $\mathcal{O}\left(\frac{|\mathcal{S}|^4
|\mathcal{A}|^2H^6\sigma^2 }{\epsilon^2} \right)$ for the finite-horizon MDP.","cs.AI,math.OC,stat.ML"
"Medicine Strip Identification using 2-D Cepstral Feature Extraction and Multiclass Classification Methods. Misclassification of medicine is perilous to the health of a patient, more so
if the said patient is visually impaired or simply did not recognize the color,
shape or type of medicine strip. This paper proposes a method for
identification of medicine strips by 2-D cepstral analysis of their images
followed by performing classification that has been done using the K-Nearest
Neighbor (KNN), Support Vector Machine (SVM) and Logistic Regression (LR)
Classifiers. The 2-D cepstral features extracted are extremely distinct to a
medicine strip and consequently make identifying them exceptionally accurate.
This paper also proposes the Color Gradient and Pill shape Feature (CGPF)
extraction procedure and discusses the Binary Robust Invariant Scalable
Keypoints (BRISK) algorithm as well. The mentioned algorithms were implemented
and their identification results have been compared.","cs.CV,cs.IR,eess.IV"
"DropEdge: Towards Deep Graph Convolutional Networks on Node Classification. \emph{Over-fitting} and \emph{over-smoothing} are two main obstacles of
developing deep Graph Convolutional Networks (GCNs) for node classification. In
particular, over-fitting weakens the generalization ability on small dataset,
while over-smoothing impedes model training by isolating output representations
from the input features with the increase in network depth. This paper proposes
DropEdge, a novel and flexible technique to alleviate both issues. At its core,
DropEdge randomly removes a certain number of edges from the input graph at
each training epoch, acting like a data augmenter and also a message passing
reducer. Furthermore, we theoretically demonstrate that DropEdge either reduces
the convergence speed of over-smoothing or relieves the information loss caused
by it. More importantly, our DropEdge is a general skill that can be equipped
with many other backbone models (e.g. GCN, ResGCN, GraphSAGE, and JKNet) for
enhanced performance. Extensive experiments on several benchmarks verify that
DropEdge consistently improves the performance on a variety of both shallow and
deep GCNs. The effect of DropEdge on preventing over-smoothing is empirically
visualized and validated as well. Codes are released
on~\url{https://github.com/DropEdge/DropEdge}.","cs.LG,cs.NI,stat.ML"
"Iterative graph cuts for image segmentation with a nonlinear statistical shape prior. Shape-based regularization has proven to be a useful method for delineating
objects within noisy images where one has prior knowledge of the shape of the
targeted object. When a collection of possible shapes is available, the
specification of a shape prior using kernel density estimation is a natural
technique. Unfortunately, energy functionals arising from kernel density
estimation are of a form that makes them impossible to directly minimize using
efficient optimization algorithms such as graph cuts. Our main contribution is
to show how one may recast the energy functional into a form that is
minimizable iteratively and efficiently using graph cuts.","cs.CV,math.OC,physics.data-an,q-bio.QM,stat.AP"
"Generating Human Action Videos by Coupling 3D Game Engines and Probabilistic Graphical Models. Deep video action recognition models have been highly successful in recent
years but require large quantities of manually annotated data, which are
expensive and laborious to obtain. In this work, we investigate the generation
of synthetic training data for video action recognition, as synthetic data have
been successfully used to supervise models for a variety of other computer
vision tasks. We propose an interpretable parametric generative model of human
action videos that relies on procedural generation, physics models and other
components of modern game engines. With this model we generate a diverse,
realistic, and physically plausible dataset of human action videos, called PHAV
for ""Procedural Human Action Videos"". PHAV contains a total of 39,982 videos,
with more than 1,000 examples for each of 35 action categories. Our video
generation approach is not limited to existing motion capture sequences: 14 of
these 35 categories are procedurally defined synthetic actions. In addition,
each video is represented with 6 different data modalities, including RGB,
optical flow and pixel-level semantic labels. These modalities are generated
almost simultaneously using the Multiple Render Targets feature of modern GPUs.
In order to leverage PHAV, we introduce a deep multi-task (i.e. that considers
action classes from multiple datasets) representation learning architecture
that is able to simultaneously learn from synthetic and real video datasets,
even when their action categories differ. Our experiments on the UCF-101 and
HMDB-51 benchmarks suggest that combining our large set of synthetic videos
with small real-world datasets can boost recognition performance. Our approach
also significantly outperforms video representations produced by fine-tuning
state-of-the-art unsupervised generative models of videos.","cs.CV,cs.LG,cs.MM"
"Foley Music: Learning to Generate Music from Videos. In this paper, we introduce Foley Music, a system that can synthesize
plausible music for a silent video clip about people playing musical
instruments. We first identify two key intermediate representations for a
successful video to music generator: body keypoints from videos and MIDI events
from audio recordings. We then formulate music generation from videos as a
motion-to-MIDI translation problem. We present a Graph$-$Transformer framework
that can accurately predict MIDI event sequences in accordance with the body
movements. The MIDI event can then be converted to realistic music using an
off-the-shelf music synthesizer tool. We demonstrate the effectiveness of our
models on videos containing a variety of music performances. Experimental
results show that our model outperforms several existing systems in generating
music that is pleasant to listen to. More importantly, the MIDI representations
are fully interpretable and transparent, thus enabling us to perform music
editing flexibly. We encourage the readers to watch the demo video with audio
turned on to experience the results.","cs.CV,cs.LG,cs.SD,eess.AS"
"Robust Risk-Sensitive Reinforcement Learning Agents for Trading Markets. Trading markets represent a real-world financial application to deploy
reinforcement learning agents, however, they carry hard fundamental challenges
such as high variance and costly exploration. Moreover, markets are inherently
a multiagent domain composed of many actors taking actions and changing the
environment. To tackle these type of scenarios agents need to exhibit certain
characteristics such as risk-awareness, robustness to perturbations and low
learning variance. We take those as building blocks and propose a family of
four algorithms. First, we contribute with two algorithms that use risk-averse
objective functions and variance reduction techniques. Then, we augment the
framework to multi-agent learning and assume an adversary which can take over
and perturb the learning process. Our third and fourth algorithms perform well
under this setting and balance theoretical guarantees with practical use.
Additionally, we consider the multi-agent nature of the environment and our
work is the first one extending empirical game theory analysis for multi-agent
learning by considering risk-sensitive payoffs.","cs.LG,cs.MA"
"Self-supervised Learning of Point Clouds via Orientation Estimation. Point clouds provide a compact and efficient representation of 3D shapes.
While deep neural networks have achieved impressive results on point cloud
learning tasks, they require massive amounts of manually labeled data, which
can be costly and time-consuming to collect. In this paper, we leverage 3D
self-supervision for learning downstream tasks on point clouds with fewer
labels. A point cloud can be rotated in infinitely many ways, which provides a
rich label-free source for self-supervision. We consider the auxiliary task of
predicting rotations that in turn leads to useful features for other tasks such
as shape classification and 3D keypoint prediction. Using experiments on
ShapeNet and ModelNet, we demonstrate that our approach outperforms the
state-of-the-art. Moreover, features learned by our model are complementary to
other self-supervised methods and combining them leads to further performance
improvement.","cs.CV,cs.GR,cs.LG"
"Multivariate Quantile Bayesian Structural Time Series (MQBSTS) Model. In this paper, we propose the multivariate quantile Bayesian structural time
series (MQBSTS) model for the joint quantile time series forecast, which is the
first such model for correlated multivariate time series to the author's best
knowledge. The MQBSTS model also enables quantile based feature selection in
its regression component where each time series has its own pool of
contemporaneous external time series predictors, which is the first time that a
fully data-driven quantile feature selection technique applicable to time
series data to the author's best knowledge. Different from most machine
learning algorithms, the MQBSTS model has very few hyper-parameters to tune,
requires small datasets to train, converges fast, and is executable on ordinary
personal computers. Extensive examinations on simulated data and empirical data
confirmed that the MQBSTS model has superior performance in feature selection,
parameter estimation, and forecast.","cs.LG,math.ST,stat.ML,stat.TH"
"Expressive power of tensor-network factorizations for probabilistic modeling, with applications from hidden Markov models to quantum machine learning. Tensor-network techniques have enjoyed outstanding success in physics, and
have recently attracted attention in machine learning, both as a tool for the
formulation of new learning algorithms and for enhancing the mathematical
understanding of existing methods. Inspired by these developments, and the
natural correspondence between tensor networks and probabilistic graphical
models, we provide a rigorous analysis of the expressive power of various
tensor-network factorizations of discrete multivariate probability
distributions. These factorizations include non-negative tensor-trains/MPS,
which are in correspondence with hidden Markov models, and Born machines, which
are naturally related to local quantum circuits. When used to model probability
distributions, they exhibit tractable likelihoods and admit efficient learning
algorithms. Interestingly, we prove that there exist probability distributions
for which there are unbounded separations between the resource requirements of
some of these tensor-network factorizations. Particularly surprising is the
fact that using complex instead of real tensors can lead to an arbitrarily
large reduction in the number of parameters of the network. Additionally, we
introduce locally purified states (LPS), a new factorization inspired by
techniques for the simulation of quantum systems, with provably better
expressive power than all other representations considered. The ramifications
of this result are explored through numerical experiments. Our findings imply
that LPS should be considered over hidden Markov models, and furthermore
provide guidelines for the design of local quantum circuits for probabilistic
modeling.","cond-mat.str-el,cs.LG,math.OC,quant-ph,stat.ML"
"HERS Superpixels: Deep Affinity Learning for Hierarchical Entropy Rate Segmentation. Superpixels serve as a powerful preprocessing tool in many computer vision
tasks. By using superpixel representation, the number of image primitives can
be largely reduced by orders of magnitudes. The majority of superpixel methods
use handcrafted features, which usually do not translate well into strong
adherence to object boundaries. A few recent superpixel methods have introduced
deep learning into the superpixel segmentation process. However, none of these
methods is able to produce superpixels in near real-time, which is crucial to
the applicability of a superpixel method in practice. In this work, we propose
a two-stage graph-based framework for superpixel segmentation. In the first
stage, we introduce an efficient Deep Affinity Learning (DAL) network that
learns pairwise pixel affinities by aggregating multi-scale information. In the
second stage, we propose a highly efficient superpixel method called
Hierarchical Entropy Rate Segmentation (HERS). Using the learned affinities
from the first stage, HERS builds a hierarchical tree structure that can
produce any number of highly adaptive superpixels instantaneously. We
demonstrate, through visual and numerical experiments, the effectiveness and
efficiency of our method compared to various state-of-the-art superpixel
methods.","I.4; I.5,cs.CV,stat.AP,stat.ML"
"DyERNIE: Dynamic Evolution of Riemannian Manifold Embeddings for Temporal Knowledge Graph Completion. There has recently been increasing interest in learning representations of
temporal knowledge graphs (KGs), which record the dynamic relationships between
entities over time. Temporal KGs often exhibit multiple simultaneous
non-Euclidean structures, such as hierarchical and cyclic structures. However,
existing embedding approaches for temporal KGs typically learn entity
representations and their dynamic evolution in the Euclidean space, which might
not capture such intrinsic structures very well. To this end, we propose Dy-
ERNIE, a non-Euclidean embedding approach that learns evolving entity
representations in a product of Riemannian manifolds, where the composed spaces
are estimated from the sectional curvatures of underlying data. Product
manifolds enable our approach to better reflect a wide variety of geometric
structures on temporal KGs. Besides, to capture the evolutionary dynamics of
temporal KGs, we let the entity representations evolve according to a velocity
vector defined in the tangent space at each timestamp. We analyze in detail the
contribution of geometric spaces to representation learning of temporal KGs and
evaluate our model on temporal knowledge graph completion tasks. Extensive
experiments on three real-world datasets demonstrate significantly improved
performance, indicating that the dynamics of multi-relational graph data can be
more properly modeled by the evolution of embeddings on Riemannian manifolds.","cs.AI,cs.CL,cs.LG"
"Optimizing Molecules using Efficient Queries from Property Evaluations. Machine learning has shown potential for optimizing existing molecules with
more desirable properties, a critical step towards accelerating new chemical
discovery. In this work, we propose QMO, a generic query-based molecule
optimization framework that exploits latent embeddings from a molecule
autoencoder. QMO improves the desired properties of an input molecule based on
efficient queries, guided by a set of molecular property predictions and
evaluation metrics. We show that QMO outperforms existing methods in the
benchmark tasks of optimizing molecules for drug likeliness and solubility
under similarity constraints. We also demonstrate significant property
improvement using QMO on two new and challenging tasks that are also important
in real-world discovery problems: (i) optimizing existing SARS-CoV-2 Main
Protease inhibitors toward higher binding affinity; and (ii) improving known
antimicrobial peptides towards lower toxicity. Results from QMO show high
consistency with external validations, suggesting effective means of
facilitating molecule optimization problems with design constraints.","cs.LG,q-bio.BM"
"Convolutional Neural Networks over Tree Structures for Programming Language Processing. Programming language processing (similar to natural language processing) is a
hot research topic in the field of software engineering; it has also aroused
growing interest in the artificial intelligence community. However, different
from a natural language sentence, a program contains rich, explicit, and
complicated structural information. Hence, traditional NLP models may be
inappropriate for programs. In this paper, we propose a novel tree-based
convolutional neural network (TBCNN) for programming language processing, in
which a convolution kernel is designed over programs' abstract syntax trees to
capture structural information. TBCNN is a generic architecture for programming
language processing; our experiments show its effectiveness in two different
program analysis tasks: classifying programs according to functionality, and
detecting code snippets of certain patterns. TBCNN outperforms baseline
methods, including several neural models for NLP.","cs.LG,cs.NE,cs.SE"
"Re-balancing Variational Autoencoder Loss for Molecule Sequence Generation. Molecule generation is to design new molecules with specific chemical
properties and further to optimize the desired chemical properties. Following
previous work, we encode molecules into continuous vectors in the latent space
and then decode the vectors into molecules under the variational autoencoder
(VAE) framework. We investigate the posterior collapse problem of current
RNN-based VAEs for molecule sequence generation. For the first time, we find
that underestimated reconstruction loss leads to posterior collapse, and
provide both theoretical and experimental evidence. We propose an effective and
efficient solution to fix the problem and avoid posterior collapse. Without
bells and whistles, our method achieves SOTA reconstruction accuracy and
competitive validity on the ZINC 250K dataset. When generating 10,000 unique
valid SMILES from random prior sampling, it costs JT-VAE1450s while our method
only needs 9s. Our implementation is at
https://github.com/chaoyan1037/Re-balanced-VAE.","cs.LG,physics.chem-ph,stat.ML"
"Robust Value Iteration for Continuous Control Tasks. When transferring a control policy from simulation to a physical system, the
policy needs to be robust to variations in the dynamics to perform well.
Commonly, the optimal policy overfits to the approximate model and the
corresponding state-distribution, often resulting in failure to trasnfer
underlying distributional shifts. In this paper, we present Robust Fitted Value
Iteration, which uses dynamic programming to compute the optimal value function
on the compact state domain and incorporates adversarial perturbations of the
system dynamics. The adversarial perturbations encourage a optimal policy that
is robust to changes in the dynamics. Utilizing the continuous-time perspective
of reinforcement learning, we derive the optimal perturbations for the states,
actions, observations and model parameters in closed-form. Notably, the
resulting algorithm does not require discretization of states or actions.
Therefore, the optimal adversarial perturbations can be efficiently
incorporated in the min-max value function update. We apply the resulting
algorithm to the physical Furuta pendulum and cartpole. By changing the masses
of the systems we evaluate the quantitative and qualitative performance across
different model parameters. We show that robust value iteration is more robust
compared to deep reinforcement learning algorithm and the non-robust version of
the algorithm. Videos of the experiments are shown at
https://sites.google.com/view/rfvi","cs.LG,cs.RO,cs.SY,eess.SY"
"Large Scale Image Completion via Co-Modulated Generative Adversarial Networks. Numerous task-specific variants of conditional generative adversarial
networks have been developed for image completion. Yet, a serious limitation
remains that all existing algorithms tend to fail when handling large-scale
missing regions. To overcome this challenge, we propose a generic new approach
that bridges the gap between image-conditional and recent modulated
unconditional generative architectures via co-modulation of both conditional
and stochastic style representations. Also, due to the lack of good
quantitative metrics for image completion, we propose the new Paired/Unpaired
Inception Discriminative Score (P-IDS/U-IDS), which robustly measures the
perceptual fidelity of inpainted images compared to real images via linear
separability in a feature space. Experiments demonstrate superior performance
in terms of both quality and diversity over state-of-the-art methods in
free-form image completion and easy generalization to image-to-image
translation. Code is available at https://github.com/zsyzzsoft/co-mod-gan.","cs.CV,cs.GR,cs.LG"
"Induction of Non-monotonic Logic Programs To Explain Statistical Learning Models. We present a fast and scalable algorithm to induce non-monotonic logic
programs from statistical learning models. We reduce the problem of search for
best clauses to instances of the High-Utility Itemset Mining (HUIM) problem. In
the HUIM problem, feature values and their importance are treated as
transactions and utilities respectively. We make use of TreeExplainer, a fast
and scalable implementation of the Explainable AI tool SHAP, to extract locally
important features and their weights from ensemble tree models. Our experiments
with UCI standard benchmarks suggest a significant improvement in terms of
classification evaluation metrics and running time of the training algorithm
compared to ALEPH, a state-of-the-art Inductive Logic Programming (ILP) system.","cs.AI,cs.LG,cs.LO"
"Is Q-Learning Minimax Optimal? A Tight Sample Complexity Analysis. Q-learning, which seeks to learn the optimal Q-function of a Markov decision
process (MDP) in a model-free fashion, lies at the heart of reinforcement
learning. When it comes to the synchronous setting (such that independent
samples for all state-action pairs are drawn from a generative model in each
iteration), substantial progress has been made recently towards understanding
the sample efficiency of Q-learning. Take a $\gamma$-discounted
infinite-horizon MDP with state space $\mathcal{S}$ and action space
$\mathcal{A}$: to yield an entrywise $\varepsilon$-accurate estimate of the
optimal Q-function, state-of-the-art theory for Q-learning proves that a sample
size on the order of
$\frac{|\mathcal{S}||\mathcal{A}|}{(1-\gamma)^5\varepsilon^{2}}$ is sufficient,
which, however, fails to match with the existing minimax lower bound. This
gives rise to natural questions: what is the sharp sample complexity of
Q-learning? Is Q-learning provably sub-optimal? In this work, we settle these
questions by (1) demonstrating that the sample complexity of Q-learning is at
most on the order of
$\frac{|\mathcal{S}||\mathcal{A}|}{(1-\gamma)^4\varepsilon^2}$ (up to some log
factor) for any $0<\varepsilon <1$, and (2) developing a matching lower bound
to confirm the sharpness of our result. Our findings unveil both the
effectiveness and limitation of Q-learning: its sample complexity matches that
of speedy Q-learning without requiring extra computation and storage, albeit
still being considerably higher than the minimax lower bound.","cs.IT,cs.LG,math.IT,math.OC,math.ST,stat.ML,stat.TH"
"ChemoVerse: Manifold traversal of latent spaces for novel molecule discovery. In order to design a more potent and effective chemical entity, it is
essential to identify molecular structures with the desired chemical
properties. Recent advances in generative models using neural networks and
machine learning are being widely used by many emerging startups and
researchers in this domain to design virtual libraries of drug-like compounds.
Although these models can help a scientist to produce novel molecular
structures rapidly, the challenge still exists in the intelligent exploration
of the latent spaces of generative models, thereby reducing the randomness in
the generative procedure. In this work we present a manifold traversal with
heuristic search to explore the latent chemical space. Different heuristics and
scores such as the Tanimoto coefficient, synthetic accessibility, binding
activity, and QED drug-likeness can be incorporated to increase the validity
and proximity for desired molecular properties of the generated molecules. For
evaluating the manifold traversal exploration, we produce the latent chemical
space using various generative models such as grammar variational autoencoders
(with and without attention) as they deal with the randomized generation and
validity of compounds. With this novel traversal method, we are able to find
more unseen compounds and more specific regions to mine in the latent space.
Finally, these components are brought together in a simple platform allowing
users to perform search, visualization and selection of novel generated
compounds.","cs.LG,q-bio.BM"
"Decision Tree Classification on Outsourced Data. This paper proposes a client-server decision tree learning method for
outsourced private data. The privacy model is anatomization/fragmentation: the
server sees data values, but the link between sensitive and identifying
information is encrypted with a key known only to clients. Clients have limited
processing and storage capability. Both sensitive and identifying information
thus are stored on the server. The approach presented also retains most
processing at the server, and client-side processing is amortized over
predictions made by the clients. Experiments on various datasets show that the
method produces decision trees approaching the accuracy of a non-private
decision tree, while substantially reducing the client's computing resource
requirements.","H.2.8; H.2.7,cs.CR,cs.DB,cs.LG"
"Robust Non-Rigid Registration with Reweighted Position and Transformation Sparsity. Non-rigid registration is challenging because it is ill-posed with high
degrees of freedom and is thus sensitive to noise and outliers. We propose a
robust non-rigid registration method using reweighted sparsities on position
and transformation to estimate the deformations between 3-D shapes. We
formulate the energy function with position and transformation sparsity on both
the data term and the smoothness term, and define the smoothness constraint
using local rigidity. The double sparsity based non-rigid registration model is
enhanced with a reweighting scheme, and solved by transferring the model into
four alternately-optimized subproblems which have exact solutions and
guaranteed convergence. Experimental results on both public datasets and real
scanned datasets show that our method outperforms the state-of-the-art methods
and is more robust to noise and outliers than conventional non-rigid
registration methods.","cs.CG,cs.CV,cs.GR"
"Tracking Instances as Queries. Recently, query based deep networks catch lots of attention owing to their
end-to-end pipeline and competitive results on several fundamental computer
vision tasks, such as object detection, semantic segmentation, and instance
segmentation. However, how to establish a query based video instance
segmentation (VIS) framework with elegant architecture and strong performance
remains to be settled. In this paper, we present \textbf{QueryTrack} (i.e.,
tracking instances as queries), a unified query based VIS framework fully
leveraging the intrinsic one-to-one correspondence between instances and
queries in QueryInst. The proposed method obtains 52.7 / 52.3 AP on
YouTube-VIS-2019 / 2021 datasets, which wins the 2-nd place in the YouTube-VIS
Challenge at CVPR 2021 \textbf{with a single online end-to-end model, single
scale testing \& modest amount of training data}. We also provide
QueryTrack-ResNet-50 baseline results on YouTube-VIS-2021 val set as references
for the VIS community.","cs.AI,cs.CV,cs.MM"
"Neural Network for Weighted Signal Temporal Logic. In this paper, we propose a neuro-symbolic framework called weighted Signal
Temporal Logic Neural Network (wSTL-NN) that combines the characteristics of
neural networks and temporal logics. Weighted Signal Temporal Logic (wSTL)
formulas are recursively composed of subformulas that are combined using
logical and temporal operators. The quantitative semantics of wSTL is defined
such that the quantitative satisfaction of subformulas with higher weights has
more influence on the quantitative satisfaction of the overall wSTL formula. In
the wSTL-NN, each neuron corresponds to a wSTL subformula, and its output
corresponds to the quantitative satisfaction of the formula. We use wSTL-NN to
represent wSTL formulas as features to classify time series data. STL features
are more explainable than those used in classical methods. The wSTL-NN is
end-to-end differentiable, which allows learning of wSTL formulas to be done
using back-propagation. To reduce the number of weights, we introduce two
techniques to sparsify the wSTL-NN.We apply our framework to an occupancy
detection time-series dataset to learn a classifier that predicts the occupancy
status of an office room.","cs.LG,cs.NE"
"Hypothesize and Bound: A Computational Focus of Attention Mechanism for Simultaneous N-D Segmentation, Pose Estimation and Classification Using Shape Priors. Given the ever increasing bandwidth of the visual information available to
many intelligent systems, it is becoming essential to endow them with a sense
of what is worthwhile their attention and what can be safely disregarded. This
article presents a general mathematical framework to efficiently allocate the
available computational resources to process the parts of the input that are
relevant to solve a given perceptual problem. By this we mean to find the
hypothesis H (i.e., the state of the world) that maximizes a function L(H),
representing how well each hypothesis ""explains"" the input. Given the large
bandwidth of the sensory input, fully evaluating L(H) for each hypothesis H is
computationally infeasible (e.g., because it would imply checking a large
number of pixels). To address this problem we propose a mathematical framework
with two key ingredients. The first one is a Bounding Mechanism (BM) to compute
lower and upper bounds of L(H), for a given computational budget. These bounds
are much cheaper to compute than L(H) itself, can be refined at any time by
increasing the budget allocated to a hypothesis, and are frequently enough to
discard a hypothesis. To compute these bounds, we develop a novel theory of
shapes and shape priors. The second ingredient is a Focus of Attention
Mechanism (FoAM) to select which hypothesis' bounds should be refined next,
with the goal of discarding non-optimal hypotheses with the least amount of
computation. The proposed framework: 1) is very efficient since most hypotheses
are discarded with minimal computation; 2) is parallelizable; 3) is guaranteed
to find the globally optimal hypothesis; and 4) its running time depends on the
problem at hand, not on the bandwidth of the input. We instantiate the proposed
framework for the problem of simultaneously estimating the class, pose, and a
noiseless version of a 2D shape in a 2D image.","cs.CG,cs.CV,cs.GR,cs.LG"
"Offline detection of change-points in the mean for stationary graph signals. This paper addresses the problem of segmenting a stream of graph signals: we
aim to detect changes in the mean of the multivariate signal defined over the
nodes of a known graph. We propose an offline algorithm that relies on the
concept of graph signal stationarity and allows the convenient translation of
the problem from the original vertex domain to the spectral domain (Graph
Fourier Transform), where it is much easier to solve. Although the obtained
spectral representation is sparse in real applications, to the best of our
knowledge this property has not been much exploited in the existing related
literature. Our main contribution is a change-point detection algorithm that
adopts a model selection perspective, which takes into account the sparsity of
the spectral representation and determines automatically the number of
change-points. Our detector comes with a proof of a non-asymptotic oracle
inequality, numerical experiments demonstrate the validity of our method.","I.2.6,cs.LG,stat.AP,stat.ML"
"On the equivalence of molecular graph convolution and molecular wave function with poor basis set. In this study, we demonstrate that the linear combination of atomic orbitals
(LCAO), an approximation of quantum physics introduced by Pauling and
Lennard-Jones in the 1920s, corresponds to graph convolutional networks (GCNs)
for molecules. However, GCNs involve unnecessary nonlinearity and deep
architecture. We also verify that molecular GCNs are based on a poor basis
function set compared with the standard one used in theoretical calculations or
quantum chemical simulations. From these observations, we describe the quantum
deep field (QDF), a machine learning (ML) model based on an underlying quantum
physics, in particular the density functional theory (DFT). We believe that the
QDF model can be easily understood because it can be regarded as a single
linear layer GCN. Moreover, it uses two vanilla feedforward neural networks to
learn an energy functional and a Hohenberg--Kohn map that have nonlinearities
inherent in quantum physics and the DFT. For molecular energy prediction tasks,
we demonstrated the viability of an ``extrapolation,'' in which we trained a
QDF model with small molecules, tested it with large molecules, and achieved
high extrapolation performance. This will lead to reliable and practical
applications for discovering effective materials. The implementation is
available at https://github.com/masashitsubaki/QuantumDeepField_molecule.","cond-mat.mtrl-sci,cs.LG,physics.chem-ph"
"libmolgrid: GPU Accelerated Molecular Gridding for Deep Learning Applications. There are many ways to represent a molecule as input to a machine learning
model and each is associated with loss and retention of certain kinds of
information. In the interest of preserving three-dimensional spatial
information, including bond angles and torsions, we have developed libmolgrid,
a general-purpose library for representing three-dimensional molecules using
multidimensional arrays. This library also provides functionality for composing
batches of data suited to machine learning workflows, including data
augmentation, class balancing, and example stratification according to a
regression variable or data subgroup, and it further supports temporal and
spatial recurrences over that data to facilitate work with recurrent neural
networks, dynamical data, and size extensive modeling. It was designed for
seamless integration with popular deep learning frameworks, including Caffe,
PyTorch, and Keras, providing good performance by leveraging graphical
processing units (GPUs) for computationally-intensive tasks and efficient
memory usage through the use of memory views over preallocated buffers.
libmolgrid is a free and open source project that is actively supported,
serving the growing need in the molecular modeling community for tools that
streamline the process of data ingestion, representation construction, and
principled machine learning model development.","cs.LG,physics.chem-ph,q-bio.BM"
"How to choose an Explainability Method? Towards a Methodical Implementation of XAI in Practice. Explainability is becoming an important requirement for organizations that
make use of automated decision-making due to regulatory initiatives and a shift
in public awareness. Various and significantly different algorithmic methods to
provide this explainability have been introduced in the field, but the existing
literature in the machine learning community has paid little attention to the
stakeholder whose needs are rather studied in the human-computer interface
community. Therefore, organizations that want or need to provide this
explainability are confronted with the selection of an appropriate method for
their use case. In this paper, we argue there is a need for a methodology to
bridge the gap between stakeholder needs and explanation methods. We present
our ongoing work on creating this methodology to help data scientists in the
process of providing explainability to stakeholders. In particular, our
contributions include documents used to characterize XAI methods and user
requirements (shown in Appendix), which our methodology builds upon.","cs.AI,cs.CY,cs.LG"
"PointTrackNet: An End-to-End Network For 3-D Object Detection and Tracking From Point Clouds. Recent machine learning-based multi-object tracking (MOT) frameworks are
becoming popular for 3-D point clouds. Most traditional tracking approaches use
filters (e.g., Kalman filter or particle filter) to predict object locations in
a time sequence, however, they are vulnerable to extreme motion conditions,
such as sudden braking and turning. In this letter, we propose PointTrackNet,
an end-to-end 3-D object detection and tracking network, to generate foreground
masks, 3-D bounding boxes, and point-wise tracking association displacements
for each detected object. The network merely takes as input two adjacent
point-cloud frames. Experimental results on the KITTI tracking dataset show
competitive results over the state-of-the-arts, especially in the irregularly
and rapidly changing scenarios.","cs.CV,cs.LG,cs.RO"
"Almost exact recovery in noisy semi-supervised learning. Graph-based semi-supervised learning methods combine the graph structure and
labeled data to classify unlabeled data. In this work, we study the effect of a
noisy oracle on classification. In particular, we derive the Maximum A
Posteriori (MAP) estimator for clustering a Degree Corrected Stochastic Block
Model (DC-SBM) when a noisy oracle reveals a fraction of the labels. We then
propose an algorithm derived from a continuous relaxation of the MAP, and we
establish its consistency. Numerical experiments show that our approach
achieves promising performance on synthetic and real data sets, even in the
case of very noisy labeled data.","62F12,62H30,68T10,cs.LG,math.ST,stat.ML,stat.TH"
"Unsupervised Feature Learning for Environmental Sound Classification Using Weighted Cycle-Consistent Generative Adversarial Network. In this paper we propose a novel environmental sound classification approach
incorporating unsupervised feature learning from codebook via spherical
$K$-Means++ algorithm and a new architecture for high-level data augmentation.
The audio signal is transformed into a 2D representation using a discrete
wavelet transform (DWT). The DWT spectrograms are then augmented by a novel
architecture for cycle-consistent generative adversarial network. This
high-level augmentation bootstraps generated spectrograms in both intra and
inter class manners by translating structural features from sample to sample. A
codebook is built by coding the DWT spectrograms with the speeded-up robust
feature detector (SURF) and the K-Means++ algorithm. The Random Forest is our
final learning algorithm which learns the environmental sound classification
task from the clustered codewords in the codebook. Experimental results in four
benchmarking environmental sound datasets (ESC-10, ESC-50, UrbanSound8k, and
DCASE-2017) have shown that the proposed classification approach outperforms
the state-of-the-art classifiers in the scope, including advanced and dense
convolutional neural networks such as AlexNet and GoogLeNet, improving the
classification rate between 3.51% and 14.34%, depending on the dataset.","cs.LG,cs.SD,eess.AS,stat.ML"
"MEG Decoding Across Subjects. Brain decoding is a data analysis paradigm for neuroimaging experiments that
is based on predicting the stimulus presented to the subject from the
concurrent brain activity. In order to make inference at the group level, a
straightforward but sometimes unsuccessful approach is to train a classifier on
the trials of a group of subjects and then to test it on unseen trials from new
subjects. The extreme difficulty is related to the structural and functional
variability across the subjects. We call this approach ""decoding across
subjects"". In this work, we address the problem of decoding across subjects for
magnetoencephalographic (MEG) experiments and we provide the following
contributions: first, we formally describe the problem and show that it belongs
to a machine learning sub-field called transductive transfer learning (TTL).
Second, we propose to use a simple TTL technique that accounts for the
differences between train data and test data. Third, we propose the use of
ensemble learning, and specifically of stacked generalization, to address the
variability across subjects within train data, with the aim of producing more
stable classifiers. On a face vs. scramble task MEG dataset of 16 subjects, we
compare the standard approach of not modelling the differences across subjects,
to the proposed one of combining TTL and ensemble learning. We show that the
proposed approach is consistently more accurate than the standard one.","cs.LG,q-bio.NC,stat.ML"
"Spatiotemporal convolutional network for time-series prediction and causal inference. Making predictions in a robust way is not easy for nonlinear systems. In this
work, a neural network computing framework, i.e., a spatiotemporal
convolutional network (STCN), was developed to efficiently and accurately
render a multistep-ahead prediction of a time series by employing a
spatial-temporal information (STI) transformation. The STCN combines the
advantages of both the temporal convolutional network (TCN) and the STI
equation, which maps the high-dimensional/spatial data to the future temporal
values of a target variable, thus naturally providing the prediction of the
target variable. From the observed variables, the STCN also infers the causal
factors of the target variable in the sense of Granger causality, which are in
turn selected as effective spatial information to improve the prediction
robustness. The STCN was successfully applied to both benchmark systems and
real-world datasets, all of which show superior and robust performance in
multistep-ahead prediction, even when the data were perturbed by noise. From
both theoretical and computational viewpoints, the STCN has great potential in
practical applications in artificial intelligence (AI) or machine learning
fields as a model-free method based only on the observed data, and also opens a
new way to explore the observed high-dimensional data in a dynamical manner for
machine learning.","I.2.1,cs.AI,cs.LG,math.DS"
"Bag-of-Words Method Applied to Accelerometer Measurements for the Purpose of Classification and Energy Estimation. Accelerometer measurements are the prime type of sensor information most
think of when seeking to measure physical activity. On the market, there are
many fitness measuring devices which aim to track calories burned and steps
counted through the use of accelerometers. These measurements, though good
enough for the average consumer, are noisy and unreliable in terms of the
precision of measurement needed in a scientific setting. The contribution of
this paper is an innovative and highly accurate regression method which uses an
intermediary two-stage classification step to better direct the regression of
energy expenditure values from accelerometer counts.
  We show that through an additional unsupervised layer of intermediate feature
construction, we can leverage latent patterns within accelerometer counts to
provide better grounds for activity classification than expert-constructed
timeseries features. For this, our approach utilizes a mathematical model
originating in natural language processing, the bag-of-words model, that has in
the past years been appearing in diverse disciplines outside of the natural
language processing field such as image processing. Further emphasizing the
natural language connection to stochastics, we use a gaussian mixture model to
learn the dictionary upon which the bag-of-words model is built. Moreover, we
show that with the addition of these features, we're able to improve regression
root mean-squared error of energy expenditure by approximately 1.4 units over
existing state-of-the-art methods.","cs.LG,stat.ML"
"Temporal Clustering of Time Series via Threshold Autoregressive Models: Application to Commodity Prices. This study aimed to find temporal clusters for several commodity prices using
the threshold non-linear autoregressive model. It is expected that the process
of determining the commodity groups that are time-dependent will advance the
current knowledge about the dynamics of co-moving and coherent prices, and can
serve as a basis for multivariate time series analyses. The clustering of
commodity prices was examined using the proposed clustering approach based on
time series models to incorporate the time varying properties of price series
into the clustering scheme. Accordingly, the primary aim in this study was
grouping time series according to the similarity between their Data Generating
Mechanisms (DGMs) rather than comparing pattern similarities in the time series
traces. The approximation to the DGM of each series was accomplished using
threshold autoregressive models, which are recognized for their ability to
represent nonlinear features in time series, such as abrupt changes,
time-irreversibility and regime-shifting behavior. Through the use of the
proposed approach, one can determine and monitor the set of co-moving time
series variables across the time dimension. Furthermore, generating a time
varying commodity price index and sub-indexes can become possible.
Consequently, we conducted a simulation study to assess the effectiveness of
the proposed clustering approach and the results are presented for both the
simulated and real data sets.","stat.AP,stat.ME,stat.ML"
"Unsupervised Progressive Learning and the STAM Architecture. We first pose the Unsupervised Progressive Learning (UPL) problem: an online
representation learning problem in which the learner observes a non-stationary
and unlabeled data stream, learning a growing number of features that persist
over time even though the data is not stored or replayed. To solve the UPL
problem we propose the Self-Taught Associative Memory (STAM) architecture.
Layered hierarchies of STAM modules learn based on a combination of online
clustering, novelty detection, forgetting outliers, and storing only
prototypical features rather than specific examples. We evaluate STAM
representations using clustering and classification tasks. While there are no
existing learning scenarios that are directly comparable to UPL, we compare the
STAM architecture with two recent continual learning models, Memory Aware
Synapses (MAS) and Gradient Episodic Memories (GEM), after adapting them in the
UPL setting.","I.2.6,cs.LG,q-bio.NC,stat.ML"
"A Type II Fuzzy Entropy Based Multi-Level Image Thresholding Using Adaptive Plant Propagation Algorithm. One of the most straightforward, direct and efficient approaches to Image
Segmentation is Image Thresholding. Multi-level Image Thresholding is an
essential viewpoint in many image processing and Pattern Recognition based
real-time applications which can effectively and efficiently classify the
pixels into various groups denoting multiple regions in an Image. Thresholding
based Image Segmentation using fuzzy entropy combined with intelligent
optimization approaches are commonly used direct methods to properly identify
the thresholds so that they can be used to segment an Image accurately. In this
paper a novel approach for multi-level image thresholding is proposed using
Type II Fuzzy sets combined with Adaptive Plant Propagation Algorithm (APPA).
Obtaining the optimal thresholds for an image by maximizing the entropy is
extremely tedious and time consuming with increase in the number of thresholds.
Hence, Adaptive Plant Propagation Algorithm (APPA), a memetic algorithm based
on plant intelligence, is used for fast and efficient selection of optimal
thresholds. This fact is reasonably justified by comparing the accuracy of the
outcomes and computational time consumed by other modern state-of-the-art
algorithms such as Particle Swarm Optimization (PSO), Gravitational Search
Algorithm (GSA) and Genetic Algorithm (GA).","cs.CV,math.OC,physics.data-an,stat.CO"
"Unsupervised Feature Learning for Environmental Sound Classification Using Weighted Cycle-Consistent Generative Adversarial Network. In this paper we propose a novel environmental sound classification approach
incorporating unsupervised feature learning from codebook via spherical
$K$-Means++ algorithm and a new architecture for high-level data augmentation.
The audio signal is transformed into a 2D representation using a discrete
wavelet transform (DWT). The DWT spectrograms are then augmented by a novel
architecture for cycle-consistent generative adversarial network. This
high-level augmentation bootstraps generated spectrograms in both intra and
inter class manners by translating structural features from sample to sample. A
codebook is built by coding the DWT spectrograms with the speeded-up robust
feature detector (SURF) and the K-Means++ algorithm. The Random Forest is our
final learning algorithm which learns the environmental sound classification
task from the clustered codewords in the codebook. Experimental results in four
benchmarking environmental sound datasets (ESC-10, ESC-50, UrbanSound8k, and
DCASE-2017) have shown that the proposed classification approach outperforms
the state-of-the-art classifiers in the scope, including advanced and dense
convolutional neural networks such as AlexNet and GoogLeNet, improving the
classification rate between 3.51% and 14.34%, depending on the dataset.","cs.LG,cs.SD,eess.AS,stat.ML"
"Benchmarking Perturbation-based Saliency Maps for Explaining Atari Agents. Recent years saw a plethora of work on explaining complex intelligent agents.
One example is the development of several algorithms that generate saliency
maps which show how much each pixel attributed to the agents' decision.
However, most evaluations of such saliency maps focus on image classification
tasks. As far as we know, there is no work that thoroughly compares different
saliency maps for Deep Reinforcement Learning agents. This paper compares four
perturbation-based approaches to create saliency maps for Deep Reinforcement
Learning agents trained on four different Atari 2600 games. All four approaches
work by perturbing parts of the input and measuring how much this affects the
agent's output. The approaches are compared using three computational metrics:
dependence on the learned parameters of the agent (sanity checks), faithfulness
to the agent's reasoning (input degradation), and run-time. In particular,
during the sanity checks we find issues with two approaches and propose a
solution to fix one of those issues.","cs.AI,cs.LG,cs.NE"
"Lipophilicity Prediction with Multitask Learning and Molecular Substructures Representation. Lipophilicity is one of the factors determining the permeability of the cell
membrane to a drug molecule. Hence, accurate lipophilicity prediction is an
essential step in the development of new drugs. In this paper, we introduce a
novel approach to encoding additional graph information by extracting molecular
substructures. By adding a set of generalized atomic features of these
substructures to an established Direct Message Passing Neural Network (D-MPNN)
we were able to achieve a new state-of-the-art result at the task of prediction
of two main lipophilicity coefficients, namely logP and logD descriptors. We
further improve our approach by employing a multitask approach to predict logP
and logD values simultaneously. Additionally, we present a study of the model
performance on symmetric and asymmetric molecules, that may yield insight for
further research.","cs.LG,q-bio.QM"
"Context-based Image Segment Labeling (CBISL). Working with images, one often faces problems with incomplete or unclear
information. Image inpainting can be used to restore missing image regions but
focuses, however, on low-level image features such as pixel intensity, pixel
gradient orientation, and color. This paper aims to recover semantic image
features (objects and positions) in images. Based on published gated PixelCNNs,
we demonstrate a new approach referred to as quadro-directional PixelCNN to
recover missing objects and return probable positions for objects based on the
context. We call this approach context-based image segment labeling (CBISL).
The results suggest that our four-directional model outperforms one-directional
models (gated PixelCNN) and returns a human-comparable performance.","68T45,J.6; J.0,cs.CV"
"Fairness-Aware Neural Ryni Minimization for Continuous Features. The past few years have seen a dramatic rise of academic and societal
interest in fair machine learning. While plenty of fair algorithms have been
proposed recently to tackle this challenge for discrete variables, only a few
ideas exist for continuous ones. The objective in this paper is to ensure some
independence level between the outputs of regression models and any given
continuous sensitive variables. For this purpose, we use the
Hirschfeld-Gebelein-R\'enyi (HGR) maximal correlation coefficient as a fairness
metric. We propose two approaches to minimize the HGR coefficient. First, by
reducing an upper bound of the HGR with a neural network estimation of the
$\chi^{2}$ divergence. Second, by minimizing the HGR directly with an
adversarial neural network architecture. The idea is to predict the output Y
while minimizing the ability of an adversarial neural network to find the
estimated transformations which are required to predict the HGR coefficient. We
empirically assess and compare our approaches and demonstrate significant
improvements on previously presented work in the field.","cs.AI,cs.CY,cs.LG,stat.ML"
"Robust MAML: Prioritization task buffer with adaptive learning process for model-agnostic meta-learning. Model agnostic meta-learning (MAML) is a popular state-of-the-art
meta-learning algorithm that provides good weight initialization of a model
given a variety of learning tasks. The model initialized by provided weight can
be fine-tuned to an unseen task despite only using a small amount of samples
and within a few adaptation steps. MAML is simple and versatile but requires
costly learning rate tuning and careful design of the task distribution which
affects its scalability and generalization. This paper proposes a more robust
MAML based on an adaptive learning scheme and a prioritization task buffer(PTB)
referred to as Robust MAML (RMAML) for improving scalability of training
process and alleviating the problem of distribution mismatch. RMAML uses
gradient-based hyper-parameter optimization to automatically find the optimal
learning rate and uses the PTB to gradually adjust train-ing task distribution
toward testing task distribution over the course of training. Experimental
results on meta reinforcement learning environments demonstrate a substantial
performance gain as well as being less sensitive to hyper-parameter choice and
robust to distribution mismatch.","cs.AI,cs.LG,cs.SY,eess.SY"
"Improving Cross-Corpus Speech Emotion Recognition with Adversarial Discriminative Domain Generalization (ADDoG). Automatic speech emotion recognition provides computers with critical context
to enable user understanding. While methods trained and tested within the same
dataset have been shown successful, they often fail when applied to unseen
datasets. To address this, recent work has focused on adversarial methods to
find more generalized representations of emotional speech. However, many of
these methods have issues converging, and only involve datasets collected in
laboratory conditions. In this paper, we introduce Adversarial Discriminative
Domain Generalization (ADDoG), which follows an easier to train ""meet in the
middle"" approach. The model iteratively moves representations learned for each
dataset closer to one another, improving cross-dataset generalization. We also
introduce Multiclass ADDoG, or MADDoG, which is able to extend the proposed
method to more than two datasets, simultaneously. Our results show consistent
convergence for the introduced methods, with significantly improved results
when not using labels from the target dataset. We also show how, in most cases,
ADDoG and MADDoG can be used to improve upon baseline state-of-the-art methods
when target dataset labels are added and in-the-wild data are considered. Even
though our experiments focus on cross-corpus speech emotion, these methods
could be used to remove unwanted factors of variation in other settings.","cs.LG,cs.SD,eess.AS,stat.ML"
"Localization of Autonomous Vehicles: Proof of Concept for A Computer Vision Approach. This paper introduces a visual-based localization method for autonomous
vehicles (AVs) that operate in the absence of any complicated hardware system
but a single camera. Visual localization refers to techniques that aim to find
the location of an object based on visual information of its surrounding area.
The problem of localization has been of interest for many years. However,
visual localization is a relatively new subject in the literature of
transportation. Moreover, the inevitable application of this type of
localization in the context of autonomous vehicles demands special attention
from the transportation community to this problem. This study proposes a
two-step localization method that requires a database of geotagged images and a
camera mounted on a vehicle that can take pictures while the car is moving. The
first step which is image retrieval uses SIFT local feature descriptor to find
an initial location for the vehicle using image matching. The next step is to
utilize the Kalman filter to estimate a more accurate location for the vehicle
as it is moving. All stages of the introduced method are implemented as a
complete system using different Python libraries. The proposed system is tested
on the KITTI dataset and has shown an average accuracy of 2 meters in finding
the final location of the vehicle.","cs.CV,stat.AP"
"Cost-effective Variational Active Entity Resolution. Accurately identifying different representations of the same real-world
entity is an integral part of data cleaning and many methods have been proposed
to accomplish it. The challenges of this entity resolution task that demand so
much research attention are often rooted in the task-specificity and
user-dependence of the process. Adopting deep learning techniques has the
potential to lessen these challenges. In this paper, we set out to devise an
entity resolution method that builds on the robustness conferred by deep
autoencoders to reduce human-involvement costs. Specifically, we reduce the
cost of training deep entity resolution models by performing unsupervised
representation learning. This unveils a transferability property of the
resulting model that can further reduce the cost of applying the approach to
new datasets by means of transfer learning. Finally, we reduce the cost of
labelling training data through an active learning approach that builds on the
properties conferred by the use of deep autoencoders. Empirical evaluation
confirms the accomplishment of our cost-reduction desideratum while achieving
comparable effectiveness with state-of-the-art alternatives.","cs.DB,cs.LG"
"Controlling Level of Unconsciousness by Titrating Propofol with Deep Reinforcement Learning. Reinforcement Learning (RL) can be used to fit a mapping from patient state
to a medication regimen. Prior studies have used deterministic and value-based
tabular learning to learn a propofol dose from an observed anesthetic state.
Deep RL replaces the table with a deep neural network and has been used to
learn medication regimens from registry databases. Here we perform the first
application of deep RL to closed-loop control of anesthetic dosing in a
simulated environment. We use the cross-entropy method to train a deep neural
network to map an observed anesthetic state to a probability of infusing a
fixed propofol dosage. During testing, we implement a deterministic policy that
transforms the probability of infusion to a continuous infusion rate. The model
is trained and tested on simulated pharmacokinetic/pharmacodynamic models with
randomized parameters to ensure robustness to patient variability. The deep RL
agent significantly outperformed a proportional-integral-derivative controller
(median absolute performance error 1.7% +/- 0.6 and 3.4% +/- 1.2). Modeling
continuous input variables instead of a table affords more robust pattern
recognition and utilizes our prior domain knowledge. Deep RL learned a smooth
policy with a natural interpretation to data scientists and anesthesia care
providers alike.","cs.AI,cs.LG,stat.ML"
"Classification of EEG Signal based on non-Gaussian Neutral Vector. In the design of brain-computer interface systems, classification of
Electroencephalogram (EEG) signals is the essential part and a challenging
task. Recently, as the marginalized discrete wavelet transform (mDWT)
representations can reveal features related to the transient nature of the EEG
signals, the mDWT coefficients have been frequently used in EEG signal
classification. In our previous work, we have proposed a super-Dirichlet
distribution-based classifier, which utilized the nonnegative and sum-to-one
properties of the mDWT coefficients. The proposed classifier performed better
than the state-of-the-art support vector machine-based classifier. In this
paper, we further study the neutrality of the mDWT coefficients. Assuming the
mDWT vector coefficients to be a neutral vector, we transform them non-linearly
into a set of independent scalar coefficients. Feature selection strategy is
proposed on the transformed feature domain. Experimental results show that the
feature selection strategy helps improving the classification accuracy.","cs.LG,eess.SP,stat.ML"
"Techniques Toward Optimizing Viewability in RTB Ad Campaigns Using Reinforcement Learning. Reinforcement learning (RL) is an effective technique for training
decision-making agents through interactions with their environment. The advent
of deep learning has been associated with highly notable successes with
sequential decision making problems - such as defeating some of the
highest-ranked human players at Go. In digital advertising, real-time bidding
(RTB) is a common method of allocating advertising inventory through real-time
auctions. Bidding strategies need to incorporate logic for dynamically
adjusting parameters in order to deliver pre-assigned campaign goals. Here we
discuss techniques toward using RL to train bidding agents. As a campaign
metric we particularly focused on viewability: the percentage of inventory
which goes on to be viewed by an end user.
  This paper is presented as a survey of techniques and experiments which we
developed through the course of this research. We discuss expanding our
training data to include edge cases by training on simulated interactions. We
discuss the experimental results comparing the performance of several promising
RL algorithms, and an approach to hyperparameter optimization of an
actor/critic training pipeline through Bayesian optimization. Finally, we
present live-traffic tests of some of our RL agents against a rule-based
feedback-control approach, demonstrating the potential for this method as well
as areas for further improvement. This paper therefore presents an arrangement
of our findings in this quickly developing field, and ways that it can be
applied to an RTB use case.","cs.AI,cs.IR,cs.LG"
"Demystification of Few-shot and One-shot Learning. Few-shot and one-shot learning have been the subject of active and intensive
research in recent years, with mounting evidence pointing to successful
implementation and exploitation of few-shot learning algorithms in practice.
Classical statistical learning theories do not fully explain why few- or
one-shot learning is at all possible since traditional generalisation bounds
normally require large training and testing samples to be meaningful. This
sharply contrasts with numerous examples of successful one- and few-shot
learning systems and applications.
  In this work we present mathematical foundations for a theory of one-shot and
few-shot learning and reveal conditions specifying when such learning schemes
are likely to succeed. Our theory is based on intrinsic properties of
high-dimensional spaces. We show that if the ambient or latent decision space
of a learning machine is sufficiently high-dimensional than a large class of
objects in this space can indeed be easily learned from few examples provided
that certain data non-concentration conditions are met.","68T05,68T07,cs.AI,cs.LG,math.ST,stat.TH"
"High-dimensional Multivariate Time Series Forecasting in IoT Applications using Embedding Non-stationary Fuzzy Time Series. In Internet of things (IoT), data is continuously recorded from different
data sources and devices can suffer faults in their embedded electronics, thus
leading to a high-dimensional data sets and concept drift events. Therefore,
methods that are capable of high-dimensional non-stationary time series are of
great value in IoT applications. Fuzzy Time Series (FTS) models stand out as
data-driven non-parametric models of easy implementation and high accuracy.
Unfortunately, FTS encounters difficulties when dealing with data sets of many
variables and scenarios with concept drift. We present a new approach to handle
high-dimensional non-stationary time series, by projecting the original
high-dimensional data into a low dimensional embedding space and using FTS
approach. Combining these techniques enables a better representation of the
complex content of non-stationary multivariate time series and accurate
forecasts. Our model is able to explain 98% of the variance and reach 11.52% of
RMSE, 2.68% of MAE and 2.91% of MAPE.","I.2.6,cs.AI,cs.LG,cs.SY,eess.SP,eess.SY"
"A Comparative Analysis of Feature Selection Methods for Biomarker Discovery in Study of Toxicant-treated Atlantic Cod (Gadus morhua) Liver. Univariate and multivariate feature selection methods can be used for
biomarker discovery in analysis of toxicant exposure. Among the univariate
methods, differential expression analysis (DEA) is often applied for its
simplicity and interpretability. A characteristic of methods for DEA is that
they treat genes individually, disregarding the correlation that exists between
them. On the other hand, some multivariate feature selection methods are
proposed for biomarker discovery. Provided with various biomarker discovery
methods, how to choose the most suitable method for a specific dataset becomes
a problem. In this paper, we present a framework for comparison of potential
biomarker discovery methods: three methods that stem from different theories
are compared by how stable they are and how well they can improve the
classification accuracy. The three methods we have considered are: Significance
Analysis of Microarrays (SAM) which identifies the differentially expressed
genes; minimum Redundancy Maximum Relevance (mRMR) based on information theory;
and Characteristic Direction (GeoDE) inspired by a graphical perspective.
Tested on the gene expression data from two experiments exposing the cod fish
to two different toxicants (MeHg and PCB 153), different methods stand out in
different cases, so a decision upon the most suitable method should be made
based on the dataset under study and the research interest.","cs.LG,q-bio.QM,stat.ML"
"A Limited-Capacity Minimax Theorem for Non-Convex Games or: How I Learned to Stop Worrying about Mixed-Nash and Love Neural Nets. Adversarial training, a special case of multi-objective optimization, is an
increasingly prevalent machine learning technique: some of its most notable
applications include GAN-based generative modeling and self-play techniques in
reinforcement learning which have been applied to complex games such as Go or
Poker. In practice, a \emph{single} pair of networks is typically trained in
order to find an approximate equilibrium of a highly nonconcave-nonconvex
adversarial problem. However, while a classic result in game theory states such
an equilibrium exists in concave-convex games, there is no analogous guarantee
if the payoff is nonconcave-nonconvex. Our main contribution is to provide an
approximate minimax theorem for a large class of games where the players pick
neural networks including WGAN, StarCraft II, and Blotto Game. Our findings
rely on the fact that despite being nonconcave-nonconvex with respect to the
neural networks parameters, these games are concave-convex with respect to the
actual models (e.g., functions or distributions) represented by these neural
networks.","cs.GT,cs.LG,stat.ML"
"Modelling the influence of data structure on learning in neural networks: the hidden manifold model. Understanding the reasons for the success of deep neural networks trained
using stochastic gradient-based methods is a key open problem for the nascent
theory of deep learning. The types of data where these networks are most
successful, such as images or sequences of speech, are characterised by
intricate correlations. Yet, most theoretical work on neural networks does not
explicitly model training data, or assumes that elements of each data sample
are drawn independently from some factorised probability distribution. These
approaches are thus by construction blind to the correlation structure of
real-world data sets and their impact on learning in neural networks. Here, we
introduce a generative model for structured data sets that we call the hidden
manifold model (HMM). The idea is to construct high-dimensional inputs that lie
on a lower-dimensional manifold, with labels that depend only on their position
within this manifold, akin to a single layer decoder or generator in a
generative adversarial network. We demonstrate that learning of the hidden
manifold model is amenable to an analytical treatment by proving a ""Gaussian
Equivalence Property"" (GEP), and we use the GEP to show how the dynamics of
two-layer neural networks trained using one-pass stochastic gradient descent is
captured by a set of integro-differential equations that track the performance
of the network at all times. This permits us to analyse in detail how a neural
network learns functions of increasing complexity during training, how its
performance depends on its size and how it is impacted by parameters such as
the learning rate or the dimension of the hidden manifold.","cond-mat.dis-nn,cond-mat.stat-mech,cs.LG,stat.ML"
"IPF for Discrete Chain Factor Graphs. Iterative Proportional Fitting (IPF), combined with EM, is commonly used as
an algorithm for likelihood maximization in undirected graphical models. In
this paper, we present two iterative algorithms that generalize upon IPF. The
first one is for likelihood maximization in discrete chain factor graphs, which
we define as a wide class of discrete variable models including undirected
graphical models and Bayesian networks, but also chain graphs and sigmoid
belief networks. The second one is for conditional likelihood maximization in
standard undirected models and Bayesian networks. In both algorithms, the
iteration steps are expressed in closed form. Numerical simulations show that
the algorithms are competitive with state of the art methods.","cs.AI,cs.LG,stat.ML"
"Using Physics-Informed Super-Resolution Generative Adversarial Networks for Subgrid Modeling in Turbulent Reactive Flows. Turbulence is still one of the main challenges for accurately predicting
reactive flows. Therefore, the development of new turbulence closures which can
be applied to combustion problems is essential. Data-driven modeling has become
very popular in many fields over the last years as large, often extensively
labeled, datasets became available and training of large neural networks became
possible on GPUs speeding up the learning process tremendously. However, the
successful application of deep neural networks in fluid dynamics, for example
for subgrid modeling in the context of large-eddy simulations (LESs), is still
challenging. Reasons for this are the large amount of degrees of freedom in
realistic flows, the high requirements with respect to accuracy and error
robustness, as well as open questions, such as the generalization capability of
trained neural networks in such high-dimensional, physics-constrained
scenarios. This work presents a novel subgrid modeling approach based on a
generative adversarial network (GAN), which is trained with unsupervised deep
learning (DL) using adversarial and physics-informed losses. A two-step
training method is used to improve the generalization capability, especially
extrapolation, of the network. The novel approach gives good results in a
priori as well as a posteriori tests with decaying turbulence including
turbulent mixing. The applicability of the network in complex combustion
scenarios is furthermore discussed by employing it to a reactive LES of the
Spray A case defined by the Engine Combustion Network (ECN).","cs.GR,cs.LG,physics.comp-ph,physics.flu-dyn,stat.ML"
"Cluster coloring of the Self-Organizing Map: An information visualization perspective. This paper takes an information visualization perspective to visual
representations in the general SOM paradigm. This involves viewing SOM-based
visualizations through the eyes of Bertin's and Tufte's theories on data
graphics. The regular grid shape of the Self-Organizing Map (SOM), while being
a virtue for linking visualizations to it, restricts representation of cluster
structures. From the viewpoint of information visualization, this paper
provides a general, yet simple, solution to projection-based coloring of the
SOM that reveals structures. First, the proposed color space is easy to
construct and customize to the purpose of use, while aiming at being
perceptually correct and informative through two separable dimensions. Second,
the coloring method is not dependent on any specific method of projection, but
is rather modular to fit any objective function suitable for the task at hand.
The cluster coloring is illustrated on two datasets: the iris data, and welfare
and poverty indicators.","cs.HC,cs.LG"
"On the geometry of generalization and memorization in deep neural networks. Understanding how large neural networks avoid memorizing training data is key
to explaining their high generalization performance. To examine the structure
of when and where memorization occurs in a deep network, we use a recently
developed replica-based mean field theoretic geometric analysis method. We find
that all layers preferentially learn from examples which share features, and
link this behavior to generalization performance. Memorization predominately
occurs in the deeper layers, due to decreasing object manifolds' radius and
dimension, whereas early layers are minimally affected. This predicts that
generalization can be restored by reverting the final few layer weights to
earlier epochs before significant memorization occurred, which is confirmed by
the experiments. Additionally, by studying generalization under different model
sizes, we reveal the connection between the double descent phenomenon and the
underlying model geometry. Finally, analytical analysis shows that networks
avoid memorization early in training because close to initialization, the
gradient contribution from permuted examples are small. These findings provide
quantitative evidence for the structure of memorization across layers of a deep
neural network, the drivers for such structure, and its connection to manifold
geometric properties.","cond-mat.dis-nn,cs.LG,stat.ML"
"Exploring grid topology reconfiguration using a simple deep reinforcement learning approach. System operators are faced with increasingly volatile operating conditions.
In order to manage system reliability in a cost-effective manner, control room
operators are turning to computerised decision support tools based on AI and
machine learning. Specifically, Reinforcement Learning (RL) is a promising
technique to train agents that suggest grid control actions to operators. In
this paper, a simple baseline approach is presented using RL to represent an
artificial control room operator that can operate a IEEE 14-bus test case for a
duration of 1 week. This agent takes topological switching actions to control
power flows on the grid, and is trained on only a single well-chosen scenario.
The behaviour of this agent is tested on different time-series of generation
and demand, demonstrating its ability to operate the grid successfully in 965
out of 1000 scenarios. The type and variability of topologies suggested by the
agent are analysed across the test scenarios, demonstrating efficient and
diverse agent behaviour.","cs.LG,cs.SY,eess.SY"
"Are scene graphs good enough to improve Image Captioning?. Many top-performing image captioning models rely solely on object features
computed with an object detection model to generate image descriptions.
However, recent studies propose to directly use scene graphs to introduce
information about object relations into captioning, hoping to better describe
interactions between objects. In this work, we thoroughly investigate the use
of scene graphs in image captioning. We empirically study whether using
additional scene graph encoders can lead to better image descriptions and
propose a conditional graph attention network (C-GAT), where the image
captioning decoder state is used to condition the graph updates. Finally, we
determine to what extent noise in the predicted scene graphs influence caption
quality. Overall, we find no significant difference between models that use
scene graph features and models that only use object detection features across
different captioning metrics, which suggests that existing scene graph
generation models are still too noisy to be useful in image captioning.
Moreover, although the quality of predicted scene graphs is very low in
general, when using high quality scene graphs we obtain gains of up to 3.3
CIDEr compared to a strong Bottom-Up Top-Down baseline. We open source code to
reproduce all our experiments in
https://github.com/iacercalixto/butd-image-captioning.","68T45,68T50,I.2.7; I.2.10,cs.CL,cs.CV"
"Fast Hypergraph Regularized Nonnegative Tensor Ring Factorization Based on Low-Rank Approximation. For the high dimensional data representation, nonnegative tensor ring (NTR)
decomposition equipped with manifold learning has become a promising model to
exploit the multi-dimensional structure and extract the feature from tensor
data. However, the existing methods such as graph regularized tensor ring
decomposition (GNTR) only models the pair-wise similarities of objects. For
tensor data with complex manifold structure, the graph can not exactly
construct similarity relationships. In this paper, in order to effectively
utilize the higher-dimensional and complicated similarities among objects, we
introduce hypergraph to the framework of NTR to further enhance the feature
extraction, upon which a hypergraph regularized nonnegative tensor ring
decomposition (HGNTR) method is developed. To reduce the computational
complexity and suppress the noise, we apply the low-rank approximation trick to
accelerate HGNTR (called LraHGNTR). Our experimental results show that compared
with other state-of-the-art algorithms, the proposed HGNTR and LraHGNTR can
achieve higher performance in clustering tasks, in addition, LraHGNTR can
greatly reduce running time without decreasing accuracy.","cs.LG,cs.NA,math.NA"
"The autofeat Python Library for Automated Feature Engineering and Selection. This paper describes the autofeat Python library, which provides scikit-learn
style linear regression and classification models with automated feature
engineering and selection capabilities. Complex non-linear machine learning
models, such as neural networks, are in practice often difficult to train and
even harder to explain to non-statisticians, who require transparent analysis
results as a basis for important business decisions. While linear models are
efficient and intuitive, they generally provide lower prediction accuracies.
Our library provides a multi-step feature engineering and selection process,
where first a large pool of non-linear features is generated, from which then a
small and robust set of meaningful features is selected, which improve the
prediction accuracy of a linear model while retaining its interpretability.","cs.LG,stat.ML"
"Continuous-Time Birth-Death MCMC for Bayesian Regression Tree Models. Decision trees are flexible models that are well suited for many statistical
regression problems. In a Bayesian framework for regression trees, Markov Chain
Monte Carlo (MCMC) search algorithms are required to generate samples of tree
models according to their posterior probabilities. The critical component of
such an MCMC algorithm is to construct good Metropolis-Hastings steps for
updating the tree topology. However, such algorithms frequently suffering from
local mode stickiness and poor mixing. As a result, the algorithms are slow to
converge. Hitherto, authors have primarily used discrete-time birth/death
mechanisms for Bayesian (sums of) regression tree models to explore the model
space. These algorithms are efficient only if the acceptance rate is high which
is not always the case. Here we overcome this issue by developing a new search
algorithm which is based on a continuous-time birth-death Markov process. This
search algorithm explores the model space by jumping between parameter spaces
corresponding to different tree structures. In the proposed algorithm, the
moves between models are always accepted which can dramatically improve the
convergence and mixing properties of the MCMC algorithm. We provide theoretical
support of the algorithm for Bayesian regression tree models and demonstrate
its performance.","cs.LG,stat.CO,stat.ME,stat.ML"
"Estimating Vector Fields from Noisy Time Series. While there has been a surge of recent interest in learning differential
equation models from time series, methods in this area typically cannot cope
with highly noisy data. We break this problem into two parts: (i) approximating
the unknown vector field (or right-hand side) of the differential equation, and
(ii) dealing with noise. To deal with (i), we describe a neural network
architecture consisting of tensor products of one-dimensional neural shape
functions. For (ii), we propose an alternating minimization scheme that
switches between vector field training and filtering steps, together with
multiple trajectories of training data. We find that the neural shape function
architecture retains the approximation properties of dense neural networks,
enables effective computation of vector field error, and allows for graphical
interpretability, all for data/systems in any finite dimension $d$. We also
study the combination of either our neural shape function method or existing
differential equation learning methods with alternating minimization and
multiple trajectories. We find that retrofitting any learning method in this
way boosts the method's robustness to noise. While in their raw form the
methods struggle with 1% Gaussian noise, after retrofitting, they learn
accurate vector fields from data with 10% Gaussian noise.","cs.LG,cs.SY,eess.SY,math.DS,math.OC,stat.ML"
"Identifying Pairs in Simulated Bio-Medical Time-Series. The paper presents a time-series-based classification approach to identify
similarities in pairs of simulated human-generated patterns. An example for a
pattern is a time-series representing a heart rate during a specific
time-range, wherein the time-series is a sequence of data points that represent
the changes in the heart rate values. A bio-medical simulator system was
developed to acquire a collection of 7,871 price patterns of financial
instruments. The financial instruments traded in real-time on three American
stock exchanges, NASDAQ, NYSE, and AMEX, simulate bio-medical measurements. The
system simulates a human in which each price pattern represents one bio-medical
sensor. Data provided during trading hours from the stock exchanges allowed
real-time classification. Classification is based on new machine learning
techniques: self-labeling, which allows the application of supervised learning
methods on unlabeled time-series and similarity ranking, which applied on a
decision tree learning algorithm to classify time-series regardless of type and
quantity.","cs.CE,cs.LG"
"Long Distance Relationships without Time Travel: Boosting the Performance of a Sparse Predictive Autoencoder in Sequence Modeling. In sequence learning tasks such as language modelling, Recurrent Neural
Networks must learn relationships between input features separated by time.
State of the art models such as LSTM and Transformer are trained by
backpropagation of losses into prior hidden states and inputs held in memory.
This allows gradients to flow from present to past and effectively learn with
perfect hindsight, but at a significant memory cost. In this paper we show that
it is possible to train high performance recurrent networks using information
that is local in time, and thereby achieve a significantly reduced memory
footprint. We describe a predictive autoencoder called bRSM featuring recurrent
connections, sparse activations, and a boosting rule for improved cell
utilization. The architecture demonstrates near optimal performance on a
non-deterministic (stochastic) partially-observable sequence learning task
consisting of high-Markov-order sequences of MNIST digits. We find that this
model learns these sequences faster and more completely than an LSTM, and offer
several possible explanations why the LSTM architecture might struggle with the
partially observable sequence structure in this task. We also apply our model
to a next word prediction task on the Penn Treebank (PTB) dataset. We show that
a 'flattened' RSM network, when paired with a modern semantic word embedding
and the addition of boosting, achieves 103.5 PPL (a 20-point improvement over
the best N-gram models), beating ordinary RNNs trained with BPTT and
approaching the scores of early LSTM implementations. This work provides
encouraging evidence that strong results on challenging tasks such as language
modelling may be possible using less memory intensive, biologically-plausible
training regimes.","68T05,68T30,I.2.7; I.2.4,cs.CL,cs.LG,cs.NE,stat.ML"
"Second-Order Neural ODE Optimizer. We propose a novel second-order optimization framework for training the
emerging deep continuous-time models, specifically the Neural Ordinary
Differential Equations (Neural ODEs). Since their training already involves
expensive gradient computation by solving a backward ODE, deriving efficient
second-order methods becomes highly nontrivial. Nevertheless, inspired by the
recent Optimal Control (OC) interpretation of training deep networks, we show
that a specific continuous-time OC methodology, called Differential
Programming, can be adopted to derive backward ODEs for higher-order
derivatives at the same O(1) memory cost. We further explore a low-rank
representation of the second-order derivatives and show that it leads to
efficient preconditioned updates with the aid of Kronecker-based factorization.
The resulting method converges much faster than first-order baselines in
wall-clock time, and the improvement remains consistent across various
applications, e.g. image classification, generative flow, and time-series
prediction. Our framework also enables direct architecture optimization, such
as the integration time of Neural ODEs, with second-order feedback policies,
strengthening the OC perspective as a principled tool of analyzing optimization
in deep learning.","cs.LG,cs.SY,eess.SY,math.OC"
"VESICLE: Volumetric Evaluation of Synaptic Interfaces using Computer vision at Large Scale. An open challenge problem at the forefront of modern neuroscience is to
obtain a comprehensive mapping of the neural pathways that underlie human brain
function; an enhanced understanding of the wiring diagram of the brain promises
to lead to new breakthroughs in diagnosing and treating neurological disorders.
Inferring brain structure from image data, such as that obtained via electron
microscopy (EM), entails solving the problem of identifying biological
structures in large data volumes. Synapses, which are a key communication
structure in the brain, are particularly difficult to detect due to their small
size and limited contrast. Prior work in automated synapse detection has relied
upon time-intensive biological preparations (post-staining, isotropic slice
thicknesses) in order to simplify the problem.
  This paper presents VESICLE, the first known approach designed for mammalian
synapse detection in anisotropic, non-post-stained data. Our methods explicitly
leverage biological context, and the results exceed existing synapse detection
methods in terms of accuracy and scalability. We provide two different
approaches - one a deep learning classifier (VESICLE-CNN) and one a lightweight
Random Forest approach (VESICLE-RF) to offer alternatives in the
performance-scalability space. Addressing this synapse detection challenge
enables the analysis of high-throughput imaging data soon expected to reach
petabytes of data, and provide tools for more rapid estimation of brain-graphs.
Finally, to facilitate community efforts, we developed tools for large-scale
object detection, and demonstrated this framework to find $\approx$ 50,000
synapses in 60,000 $\mu m ^3$ (220 GB on disk) of electron microscopy data.","cs.CE,cs.CV,q-bio.QM"
"Is Long Horizon Reinforcement Learning More Difficult Than Short Horizon Reinforcement Learning?. Learning to plan for long horizons is a central challenge in episodic
reinforcement learning problems. A fundamental question is to understand how
the difficulty of the problem scales as the horizon increases. Here the natural
measure of sample complexity is a normalized one: we are interested in the
number of episodes it takes to provably discover a policy whose value is
$\varepsilon$ near to that of the optimal value, where the value is measured by
the normalized cumulative reward in each episode. In a COLT 2018 open problem,
Jiang and Agarwal conjectured that, for tabular, episodic reinforcement
learning problems, there exists a sample complexity lower bound which exhibits
a polynomial dependence on the horizon -- a conjecture which is consistent with
all known sample complexity upper bounds. This work refutes this conjecture,
proving that tabular, episodic reinforcement learning is possible with a sample
complexity that scales only logarithmically with the planning horizon. In other
words, when the values are appropriately normalized (to lie in the unit
interval), this results shows that long horizon RL is no more difficult than
short horizon RL, at least in a minimax sense. Our analysis introduces two
ideas: (i) the construction of an $\varepsilon$-net for optimal policies whose
log-covering number scales only logarithmically with the planning horizon, and
(ii) the Online Trajectory Synthesis algorithm, which adaptively evaluates all
policies in a given policy class using sample complexity that scales with the
log-covering number of the given policy class. Both may be of independent
interest.","cs.AI,cs.LG,math.OC,stat.ML"
"Generating Electronic Health Records with Multiple Data Types and Constraints. Sharing electronic health records (EHRs) on a large scale may lead to privacy
intrusions. Recent research has shown that risks may be mitigated by simulating
EHRs through generative adversarial network (GAN) frameworks. Yet the methods
developed to date are limited because they 1) focus on generating data of a
single type (e.g., diagnosis codes), neglecting other data types (e.g.,
demographics, procedures or vital signs) and 2) do not represent constraints
between features. In this paper, we introduce a method to simulate EHRs
composed of multiple data types by 1) refining the GAN model, 2) accounting for
feature constraints, and 3) incorporating key utility measures for such
generation tasks. Our analysis with over $770,000$ EHRs from Vanderbilt
University Medical Center demonstrates that the new model achieves higher
performance in terms of retaining basic statistics, cross-feature correlations,
latent structural properties, feature constraints and associated patterns from
real data, without sacrificing privacy.","cs.CY,cs.LG,stat.ML"
"Herded Gibbs Sampling. The Gibbs sampler is one of the most popular algorithms for inference in
statistical models. In this paper, we introduce a herding variant of this
algorithm, called herded Gibbs, that is entirely deterministic. We prove that
herded Gibbs has an $O(1/T)$ convergence rate for models with independent
variables and for fully connected probabilistic graphical models. Herded Gibbs
is shown to outperform Gibbs in the tasks of image denoising with MRFs and
named entity recognition with CRFs. However, the convergence for herded Gibbs
for sparsely connected probabilistic graphical models is still an open problem.","cs.LG,stat.CO,stat.ML"
"Flexibly Regularized Mixture Models and Application to Image Segmentation. Probabilistic finite mixture models are widely used for unsupervised
clustering. These models can often be improved by adapting them to the topology
of the data. For instance, in order to classify spatially adjacent data points
similarly, it is common to introduce a Laplacian constraint on the posterior
probability that each data point belongs to a class. Alternatively, the mixing
probabilities can be treated as free parameters, while assuming Gauss-Markov or
more complex priors to regularize those mixing probabilities. However, these
approaches are constrained by the shape of the prior and often lead to
complicated or intractable inference. Here, we propose a new parametrization of
the Dirichlet distribution to flexibly regularize the mixing probabilities of
over-parametrized mixture distributions. Using the Expectation-Maximization
algorithm, we show that our approach allows us to define any linear update rule
for the mixing probabilities, including spatial smoothing regularization as a
special case. We then show that this flexible design can be extended to share
class information between multiple mixture models. We apply our algorithm to
artificial and natural image segmentation tasks, and we provide quantitative
and qualitative comparison of the performance of Gaussian and Student-t
mixtures on the Berkeley Segmentation Dataset. We also demonstrate how to
propagate class information across the layers of deep convolutional neural
networks in a probabilistically optimal way, suggesting a new interpretation
for feedback signals in biological visual systems. Our flexible approach can be
easily generalized to adapt probabilistic mixture models to arbitrary data
topologies.","cs.CV,cs.LG,q-bio.NC"
"Kernel methods library for pattern analysis and machine learning in python. Kernel methods have proven to be powerful techniques for pattern analysis and
machine learning (ML) in a variety of domains. However, many of their original
or advanced implementations remain in Matlab. With the incredible rise and
adoption of Python in the ML and data science world, there is a clear need for
a well-defined library that enables not only the use of popular kernels, but
also allows easy definition of customized kernels to fine-tune them for diverse
applications. The kernelmethods library fills that important void in the python
ML ecosystem in a domain-agnostic fashion, allowing the sample data type to be
anything from numerical, categorical, graphs or a combination of them. In
addition, this library provides a number of well-defined classes to make
various kernel-based operations efficient (for large scale datasets), modular
(for ease of domain adaptation), and inter-operable (across different
ecosystems). The library is available at
https://github.com/raamana/kernelmethods.","cs.CV,cs.LG,stat.CO,stat.ML"
"On Connections between Constrained Optimization and Reinforcement Learning. Dynamic Programming (DP) provides standard algorithms to solve Markov
Decision Processes. However, these algorithms generally do not optimize a
scalar objective function. In this paper, we draw connections between DP and
(constrained) convex optimization. Specifically, we show clear links in the
algorithmic structure between three DP schemes and optimization algorithms. We
link Conservative Policy Iteration to Frank-Wolfe, Mirror-Descent Modified
Policy Iteration to Mirror Descent, and Politex (Policy Iteration Using Expert
Prediction) to Dual Averaging. These abstract DP schemes are representative of
a number of (deep) Reinforcement Learning (RL) algorithms. By highlighting
these connections (most of which have been noticed earlier, but in a scattered
way), we would like to encourage further studies linking RL and convex
optimization, that could lead to the design of new, more efficient, and better
understood RL algorithms.","cs.LG,math.OC,stat.ML"
"Conditional Teacher-Student Learning. The teacher-student (T/S) learning has been shown to be effective for a
variety of problems such as domain adaptation and model compression. One
shortcoming of the T/S learning is that a teacher model, not always perfect,
sporadically produces wrong guidance in form of posterior probabilities that
misleads the student model towards a suboptimal performance. To overcome this
problem, we propose a conditional T/S learning scheme, in which a ""smart""
student model selectively chooses to learn from either the teacher model or the
ground truth labels conditioned on whether the teacher can correctly predict
the ground truth. Unlike a naive linear combination of the two knowledge
sources, the conditional learning is exclusively engaged with the teacher model
when the teacher model's prediction is correct, and otherwise backs off to the
ground truth. Thus, the student model is able to learn effectively from the
teacher and even potentially surpass the teacher. We examine the proposed
learning scheme on two tasks: domain adaptation on CHiME-3 dataset and speaker
adaptation on Microsoft short message dictation dataset. The proposed method
achieves 9.8% and 12.8% relative word error rate reductions, respectively, over
T/S learning for environment adaptation and speaker-independent model for
speaker adaptation.","cs.CL,cs.LG,cs.SD,eess.AS,stat.ML"
"Scalable Linear Causal Inference for Irregularly Sampled Time Series with Long Range Dependencies. Linear causal analysis is central to a wide range of important application
spanning finance, the physical sciences, and engineering. Much of the existing
literature in linear causal analysis operates in the time domain.
Unfortunately, the direct application of time domain linear causal analysis to
many real-world time series presents three critical challenges: irregular
temporal sampling, long range dependencies, and scale. Moreover, real-world
data is often collected at irregular time intervals across vast arrays of
decentralized sensors and with long range dependencies which make naive time
domain correlation estimators spurious. In this paper we present a frequency
domain based estimation framework which naturally handles irregularly sampled
data and long range dependencies while enabled memory and communication
efficient distributed processing of time series data. By operating in the
frequency domain we eliminate the need to interpolate and help mitigate the
effects of long range dependencies. We implement and evaluate our new work-flow
in the distributed setting using Apache Spark and demonstrate on both Monte
Carlo simulations and high-frequency financial trading that we can accurately
recover causal structure at scale.","cs.LG,stat.ME"
"Power System Event Identification based on Deep Neural Network with Information Loading. Online power system event identification and classification is crucial to
enhancing the reliability of transmission systems. In this paper, we develop a
deep neural network (DNN) based approach to identify and classify power system
events by leveraging real-world measurements from hundreds of phasor
measurement units (PMUs) and labels from thousands of events. Two innovative
designs are embedded into the baseline model built on convolutional neural
networks (CNNs) to improve the event classification accuracy. First, we propose
a graph signal processing based PMU sorting algorithm to improve the learning
efficiency of CNNs. Second, we deploy information loading based regularization
to strike the right balance between memorization and generalization for the
DNN. Numerical studies results based on real-world dataset from the Eastern
Interconnection of the U.S power transmission grid show that the combination of
PMU based sorting and the information loading based regularization techniques
help the proposed DNN approach achieve highly accurate event identification and
classification results.","cs.LG,cs.SY,eess.SY"
"DisCoveR: Accurate & Efficient Discovery of Declarative Process Models. Declarative process modeling formalisms - which capture high-level process
constraints - have seen growing interest, especially for modeling flexible
processes. This paper presents DisCoveR, an extremely efficient and accurate
declarative miner for learning Dynamic Condition Response (DCR) Graphs from
event logs. We precisely formalize the algorithm, describe a highly efficient
bit vector implementation and rigorously evaluate performance against two other
declarative miners, representing the state-of-the-art in Declare and DCR Graphs
mining. DisCoveR outperforms each of these w.r.t. a binary classification task,
achieving an average accuracy of 96.2% in the Process Discovery Contest 2019.
Due to its linear time complexity, DisCoveR also achieves run-times 1-2 orders
of magnitude below its declarative counterparts. Finally, we show how the miner
has been integrated in a state-of-the-art declarative process modeling
framework as a model recommendation tool, discuss how discovery can play an
integral part of the modeling task and report on how the integration has
improved the modeling experience of end-users.","cs.FL,cs.LG,cs.SE,stat.ML"
"ProcK: Machine Learning for Knowledge-Intensive Processes. Process mining deals with extraction of knowledge from business process
execution logs. Traditional process mining tasks, like process model generation
or conformance checking, rely on a minimalistic feature set where each event is
characterized only by its case identifier, activity type, and timestamp. In
contrast, the success of modern machine learning is based on models that take
any available data as direct input and build layers of features automatically
during training. In this work, we introduce ProcK (Process & Knowledge), a
novel pipeline to build business process prediction models that take into
account both sequential data in the form of event logs and rich semantic
information represented in a graph-structured knowledge base. The hybrid
approach enables ProcK to flexibly make use of all information residing in the
databases of organizations. Components to extract inter-linked event logs and
knowledge bases from relational databases are part of the pipeline. We
demonstrate the power of ProcK by training it for prediction tasks on the OULAD
e-learning dataset, where we achieve state-of-the-art performance on the tasks
of predicting student dropout from courses and predicting their success. We
also apply our method on a number of additional machine learning tasks,
including exam score prediction and early predictions that only take into
account data recorded during the first weeks of the courses.","I.2.6; H.4.0,cs.DB,cs.LG,cs.NE"
"Investigating Object Compositionality in Generative Adversarial Networks. Deep generative models seek to recover the process with which the observed
data was generated. They may be used to synthesize new samples or to
subsequently extract representations. Successful approaches in the domain of
images are driven by several core inductive biases. However, a bias to account
for the compositional way in which humans structure a visual scene in terms of
objects has frequently been overlooked. In this work, we investigate object
compositionality as an inductive bias for Generative Adversarial Networks
(GANs). We present a minimal modification of a standard generator to
incorporate this inductive bias and find that it reliably learns to generate
images as compositions of objects. Using this general design as a backbone, we
then propose two useful extensions to incorporate dependencies among objects
and background. We extensively evaluate our approach on several multi-object
image datasets and highlight the merits of incorporating structure for
representation learning purposes. In particular, we find that our structured
GANs are better at generating multi-object images that are more faithful to the
reference distribution. More so, we demonstrate how, by leveraging the
structure of the learned generative process, one can `invert' the learned
generative model to perform unsupervised instance segmentation. On the
challenging CLEVR dataset, it is shown how our approach is able to improve over
other recent purely unsupervised object-centric approaches to image generation.","I.2.6,cs.CV,cs.NE"
"A Hybrid Inference System for Improved Curvature Estimation in the Level-Set Method Using Machine Learning. We present a novel hybrid strategy based on machine learning to improve
curvature estimation in the level-set method. The proposed inference system
couples enhanced neural networks with standard numerical schemes to compute
curvature more accurately. The core of our hybrid framework is a switching
mechanism that relies on well established numerical techniques to gauge
curvature. If the curvature magnitude is larger than a resolution-dependent
threshold, it uses a neural network to yield a better approximation. Our
networks are multilayer perceptrons fitted to synthetic data sets composed of
sinusoidal- and circular-interface samples at various configurations. To reduce
data set size and training complexity, we leverage the problem's characteristic
symmetry and build our models on just half of the curvature spectrum. These
savings lead to a powerful inference system able to outperform any of its
numerical or neural component alone. Experiments with static, smooth interfaces
show that our hybrid solver is notably superior to conventional numerical
methods in coarse grids and along steep interface regions. Compared to prior
research, we have observed outstanding gains in precision after training the
regression model with data pairs from more than a single interface type and
transforming data with specialized input preprocessing. In particular, our
findings confirm that machine learning is a promising venue for reducing or
removing mass loss in the level-set method.","65N06,65Z05,68T99,I.2.6; G.1.8,cs.LG,cs.NA,math.NA"
"GLAMOUR: Graph Learning over Macromolecule Representations. The near-infinite chemical diversity of natural and artificial macromolecules
arises from the vast range of possible component monomers, linkages, and
polymers topologies. This enormous variety contributes to the ubiquity and
indispensability of macromolecules but hinders the development of general
machine learning methods with macromolecules as input. To address this, we
developed GLAMOUR, a framework for chemistry-informed graph representation of
macromolecules that enables quantifying structural similarity, and
interpretable supervised learning for macromolecules.","J.2.4; J.3.1,cs.CY,cs.LG,q-bio.BM,q-bio.QM,stat.ML"
"Deep Reinforcement Learning for Wireless Scheduling with Multiclass Services. In this paper, we investigate the problem of scheduling and resource
allocation over a time varying set of clients with heterogeneous demands.In
this context, a service provider has to schedule traffic destined to users with
different classes of requirements and to allocate bandwidth resources over time
as a means to efficiently satisfy service demands within a limited time
horizon. This is a highly intricate problem, in particular in wireless
communication systems, and solutions may involve tools stemming from diverse
fields, including combinatorics and constrained optimization. Although recent
work has successfully proposed solutions based on Deep Reinforcement Learning
(DRL), the challenging setting of heterogeneous user traffic and demands has
not been addressed. We propose a deep deterministic policy gradient algorithm
that combines state-of-the-art techniques, namely Distributional RL and Deep
Sets, to train a model for heterogeneous traffic scheduling. We test on diverse
scenarios with different time dependence dynamics, users' requirements, and
resources available, demonstrating consistent results using both synthetic and
real data. We evaluate the algorithm on a wireless communication setting using
both synthetic and real data and show significant gains in terms of Quality of
Service (QoS) defined by the classes, against state-of-the-art conventional
algorithms from combinatorics, optimization and scheduling metric(e.g.
Knapsack, Integer Linear Programming, Frank-Wolfe, Exponential Rule).","cs.IT,cs.LG,cs.NI,math.IT"
"Neural Circuit Synthesis from Specification Patterns. We train hierarchical Transformers on the task of synthesizing hardware
circuits directly out of high-level logical specifications in linear-time
temporal logic (LTL). The LTL synthesis problem is a well-known algorithmic
challenge with a long history and an annual competition is organized to track
the improvement of algorithms and tooling over time. New approaches using
machine learning might open a lot of possibilities in this area, but suffer
from the lack of sufficient amounts of training data. In this paper, we
consider a method to generate large amounts of additional training data, i.e.,
pairs of specifications and circuits implementing them. We ensure that this
synthetic data is sufficiently close to human-written specifications by mining
common patterns from the specifications used in the synthesis competitions. We
show that hierarchical Transformers trained on this synthetic data solve a
significant portion of problems from the synthesis competitions, and even
out-of-distribution examples from a recent case study.","cs.LG,cs.LO"
"Liver Segmentation in Abdominal CT Images via Auto-Context Neural Network and Self-Supervised Contour Attention. Accurate image segmentation of the liver is a challenging problem owing to
its large shape variability and unclear boundaries. Although the applications
of fully convolutional neural networks (CNNs) have shown groundbreaking
results, limited studies have focused on the performance of generalization. In
this study, we introduce a CNN for liver segmentation on abdominal computed
tomography (CT) images that shows high generalization performance and accuracy.
To improve the generalization performance, we initially propose an auto-context
algorithm in a single CNN. The proposed auto-context neural network exploits an
effective high-level residual estimation to obtain the shape prior. Identical
dual paths are effectively trained to represent mutual complementary features
for an accurate posterior analysis of a liver. Further, we extend our network
by employing a self-supervised contour scheme. We trained sparse contour
features by penalizing the ground-truth contour to focus more contour
attentions on the failures. The experimental results show that the proposed
network results in better accuracy when compared to the state-of-the-art
networks by reducing 10.31% of the Hausdorff distance. We used 180 abdominal CT
images for training and validation. Two-fold cross-validation is presented for
a comparison with the state-of-the-art neural networks. Novel multiple N-fold
cross-validations are conducted to verify the performance of generalization.
The proposed network showed the best generalization performance among the
networks. Additionally, we present a series of ablation experiments that
comprehensively support the importance of the underlying concepts.","68U10,cs.CV"
"Intelligent Roundabout Insertion using Deep Reinforcement Learning. An important topic in the autonomous driving research is the development of
maneuver planning systems. Vehicles have to interact and negotiate with each
other so that optimal choices, in terms of time and safety, are taken. For this
purpose, we present a maneuver planning module able to negotiate the entering
in busy roundabouts. The proposed module is based on a neural network trained
to predict when and how entering the roundabout throughout the whole duration
of the maneuver. Our model is trained with a novel implementation of A3C, which
we will call Delayed A3C (D-A3C), in a synthetic environment where vehicles
move in a realistic manner with interaction capabilities. In addition, the
system is trained such that agents feature a unique tunable behavior, emulating
real world scenarios where drivers have their own driving styles. Similarly,
the maneuver can be performed using different aggressiveness levels, which is
particularly useful to manage busy scenarios where conservative rule-based
policies would result in undefined waits.","cs.AI,cs.LG,cs.MA,cs.RO"
"Reconstructing a dynamical system and forecasting time series by self-consistent deep learning. We introduce a self-consistent deep-learning framework which, for a noisy
deterministic time series, provides unsupervised filtering, state-space
reconstruction, identification of the underlying differential equations and
forecasting. Without a priori information on the signal, we embed the time
series in a state space, where deterministic structures, i.e. attractors, are
revealed. Under the assumption that the evolution of solution trajectories is
described by an unknown dynamical system, we filter out stochastic outliers.
The embedding function, the solution trajectories and the dynamical systems are
constructed using deep neural networks, respectively. By exploiting the
differentiability of the neural solution trajectory, the neural dynamical
system is defined locally at each time, mitigating the need for propagating
gradients through numerical solvers. On a chaotic time series masked by
additive Gaussian noise, we demonstrate the filtering ability and the
predictive power of the proposed framework.","cs.LG,math.DS"
"E(n) Equivariant Normalizing Flows. This paper introduces a generative model equivariant to Euclidean symmetries:
E(n) Equivariant Normalizing Flows (E-NFs). To construct E-NFs, we take the
discriminative E(n) graph neural networks and integrate them as a differential
equation to obtain an invertible equivariant function: a continuous-time
normalizing flow. We demonstrate that E-NFs considerably outperform baselines
and existing methods from the literature on particle systems such as DW4 and
LJ13, and on molecules from QM9 in terms of log-likelihood. To the best of our
knowledge, this is the first flow that jointly generates molecule features and
positions in 3D.","cs.LG,physics.chem-ph,stat.ML"
"Directional Message Passing for Molecular Graphs. Graph neural networks have recently achieved great successes in predicting
quantum mechanical properties of molecules. These models represent a molecule
as a graph using only the distance between atoms (nodes). They do not, however,
consider the spatial direction from one atom to another, despite directional
information playing a central role in empirical potentials for molecules, e.g.
in angular potentials. To alleviate this limitation we propose directional
message passing, in which we embed the messages passed between atoms instead of
the atoms themselves. Each message is associated with a direction in coordinate
space. These directional message embeddings are rotationally equivariant since
the associated directions rotate with the molecule. We propose a message
passing scheme analogous to belief propagation, which uses the directional
information by transforming messages based on the angle between them.
Additionally, we use spherical Bessel functions and spherical harmonics to
construct theoretically well-founded, orthogonal representations that achieve
better performance than the currently prevalent Gaussian radial basis
representations while using fewer than 1/4 of the parameters. We leverage these
innovations to construct the directional message passing neural network
(DimeNet). DimeNet outperforms previous GNNs on average by 76% on MD17 and by
31% on QM9. Our implementation is available online.","cs.LG,physics.comp-ph,stat.ML"
"Predicting times of waiting on red signals using BERT. We present a method for approximating outcomes of road traffic simulations
using BERT-based models, which may find applications in, e.g., optimizing
traffic signal settings, especially with the presence of autonomous and
connected vehicles. The experiments were conducted on a dataset generated using
the Traffic Simulation Framework software runs on a realistic road network. The
BERT-based models were compared with 4 other types of machine learning models
(LightGBM, fully connected neural networks and 2 types of graph neural
networks) and gave the best results in terms of all the considered metrics.","I.2.6,cs.LG"
"On the capacity of deep generative networks for approximating distributions. We study the efficacy and efficiency of deep generative networks for
approximating probability distributions. We prove that neural networks can
transform a low-dimensional source distribution to a distribution that is
arbitrarily close to a high-dimensional target distribution, when the closeness
are measured by Wasserstein distances and maximum mean discrepancy. Upper
bounds of the approximation error are obtained in terms of the width and depth
of neural network. Furthermore, it is shown that the approximation error in
Wasserstein distance grows at most linearly on the ambient dimension and that
the approximation order only depends on the intrinsic dimension of the target
distribution. On the contrary, when $f$-divergences are used as metrics of
distributions, the approximation property is different. We show that in order
to approximate the target distribution in $f$-divergences, the dimension of the
source distribution cannot be smaller than the intrinsic dimension of the
target distribution.","cs.LG,math.PR,math.ST,stat.ML,stat.TH"
"CLUTRR: A Diagnostic Benchmark for Inductive Reasoning from Text. The recent success of natural language understanding (NLU) systems has been
troubled by results highlighting the failure of these models to generalize in a
systematic and robust way. In this work, we introduce a diagnostic benchmark
suite, named CLUTRR, to clarify some key issues related to the robustness and
systematicity of NLU systems. Motivated by classic work on inductive logic
programming, CLUTRR requires that an NLU system infer kinship relations between
characters in short stories. Successful performance on this task requires both
extracting relationships between entities, as well as inferring the logical
rules governing these relationships. CLUTRR allows us to precisely measure a
model's ability for systematic generalization by evaluating on held-out
combinations of logical rules, and it allows us to evaluate a model's
robustness by adding curated noise facts. Our empirical results highlight a
substantial performance gap between state-of-the-art NLU models (e.g., BERT and
MAC) and a graph neural network model that works directly with symbolic
inputs---with the graph-based model exhibiting both stronger generalization and
greater robustness.","cs.CL,cs.LG,cs.LO,stat.ML"
"Does the Adam Optimizer Exacerbate Catastrophic Forgetting?. Catastrophic forgetting remains a severe hindrance to the broad application
of artificial neural networks (ANNs), however, it continues to be a poorly
understood phenomenon. Despite the extensive amount of work on catastrophic
forgetting, we argue that it is still unclear how exactly the phenomenon should
be quantified, and, moreover, to what degree all of the choices we make when
designing learning systems affect the amount of catastrophic forgetting. We use
various testbeds from the reinforcement learning and supervised learning
literature to (1) provide evidence that the choice of which modern
gradient-based optimization algorithm is used to train an ANN has a significant
impact on the amount of catastrophic forgetting and show that-surprisingly-in
many instances classical algorithms such as vanilla SGD experience less
catastrophic forgetting than the more modern algorithms such as Adam. We
empirically compare four different existing metrics for quantifying
catastrophic forgetting and (2) show that the degree to which the learning
systems experience catastrophic forgetting is sufficiently sensitive to the
metric used that a change from one principled metric to another is enough to
change the conclusions of a study dramatically. Our results suggest that a much
more rigorous experimental methodology is required when looking at catastrophic
forgetting. Based on our results, we recommend inter-task forgetting in
supervised learning must be measured with both retention and relearning metrics
concurrently, and intra-task forgetting in reinforcement learning must-at the
very least-be measured with pairwise interference.","I.2.6,cs.AI,cs.LG,stat.ML"
"Machine Learning by Unitary Tensor Network of Hierarchical Tree Structure. The resemblance between the methods used in quantum-many body physics and in
machine learning has drawn considerable attention. In particular, tensor
networks (TNs) and deep learning architectures bear striking similarities to
the extent that TNs can be used for machine learning. Previous results used
one-dimensional TNs in image recognition, showing limited scalability and
flexibilities. In this work, we train two-dimensional hierarchical TNs to solve
image recognition problems, using a training algorithm derived from the
multi-scale entanglement renormalization ansatz. This approach introduces
mathematical connections among quantum many-body physics, quantum information
theory, and machine learning. While keeping the TN unitary in the training
phase, TN states are defined, which encode classes of images into quantum
many-body states. We study the quantum features of the TN states, including
quantum entanglement and fidelity. We find these quantities could be properties
that characterize the image classes, as well as the machine learning tasks.","cond-mat.str-el,physics.comp-ph,quant-ph,stat.ML"
"A PAC-Bayesian bound for Lifelong Learning. Transfer learning has received a lot of attention in the machine learning
community over the last years, and several effective algorithms have been
developed. However, relatively little is known about their theoretical
properties, especially in the setting of lifelong learning, where the goal is
to transfer information to tasks for which no data have been observed so far.
In this work we study lifelong learning from a theoretical perspective. Our
main result is a PAC-Bayesian generalization bound that offers a unified view
on existing paradigms for transfer learning, such as the transfer of parameters
or the transfer of low-dimensional representations. We also use the bound to
derive two principled lifelong learning algorithms, and we show that these
yield results comparable with existing methods.","68T05,cs.LG,stat.ML"
"Learning to Have an Ear for Face Super-Resolution. We propose a novel method to use both audio and a low-resolution image to
perform extreme face super-resolution (a 16x increase of the input size). When
the resolution of the input image is very low (e.g., 8x8 pixels), the loss of
information is so dire that important details of the original identity have
been lost and audio can aid the recovery of a plausible high-resolution image.
In fact, audio carries information about facial attributes, such as gender and
age. To combine the aural and visual modalities, we propose a method to first
build the latent representations of a face from the lone audio track and then
from the lone low-resolution image. We then train a network to fuse these two
representations. We show experimentally that audio can assist in recovering
attributes such as the gender, the age and the identity, and thus improve the
correctness of the high-resolution image reconstruction process. Our procedure
does not make use of human annotation and thus can be easily trained with
existing video datasets. Moreover, we show that our model builds a factorized
representation of images and audio as it allows one to mix low-resolution
images and audio from different videos and to generate realistic faces with
semantically meaningful combinations.","cs.CV,eess.AS,eess.IV"
"Interaction-aware Factorization Machines for Recommender Systems. Factorization Machine (FM) is a widely used supervised learning approach by
effectively modeling of feature interactions. Despite the successful
application of FM and its many deep learning variants, treating every feature
interaction fairly may degrade the performance. For example, the interactions
of a useless feature may introduce noises; the importance of a feature may also
differ when interacting with different features. In this work, we propose a
novel model named \emph{Interaction-aware Factorization Machine} (IFM) by
introducing Interaction-Aware Mechanism (IAM), which comprises the
\emph{feature aspect} and the \emph{field aspect}, to learn flexible
interactions on two levels. The feature aspect learns feature interaction
importance via an attention network while the field aspect learns the feature
interaction effect as a parametric similarity of the feature interaction vector
and the corresponding field interaction prototype. IFM introduces more
structured control and learns feature interaction importance in a stratified
manner, which allows for more leverage in tweaking the interactions on both
feature-wise and field-wise levels. Besides, we give a more generalized
architecture and propose Interaction-aware Neural Network (INN) and DeepIFM to
capture higher-order interactions. To further improve both the performance and
efficiency of IFM, a sampling scheme is developed to select interactions based
on the field aspect importance. The experimental results from two well-known
datasets show the superiority of the proposed models over the state-of-the-art
methods.","cs.IR,cs.LG,cs.SI,stat.ML"
"Robust guarantees for learning an autoregressive filter. The optimal predictor for a linear dynamical system (with hidden state and
Gaussian noise) takes the form of an autoregressive linear filter, namely the
Kalman filter. However, a fundamental problem in reinforcement learning and
control theory is to make optimal predictions in an unknown dynamical system.
To this end, we take the approach of directly learning an autoregressive filter
for time-series prediction under unknown dynamics. Our analysis differs from
previous statistical analyses in that we regress not only on the inputs to the
dynamical system, but also the outputs, which is essential to dealing with
process noise. The main challenge is to estimate the filter under worst case
input (in $\mathcal H_\infty$ norm), for which we use an $L^\infty$-based
objective rather than ordinary least-squares. For learning an autoregressive
model, our algorithm has optimal sample complexity in terms of the rollout
length, which does not seem to be attained by naive least-squares.","cs.LG,math.OC,stat.ML"
"Subspace Clustering using Ensembles of $K$-Subspaces. Subspace clustering is the unsupervised grouping of points lying near a union
of low-dimensional linear subspaces. Algorithms based directly on geometric
properties of such data tend to either provide poor empirical performance, lack
theoretical guarantees, or depend heavily on their initialization. We present a
novel geometric approach to the subspace clustering problem that leverages
ensembles of the K-subspaces (KSS) algorithm via the evidence accumulation
clustering framework. Our algorithm, referred to as ensemble K-subspaces
(EKSS), forms a co-association matrix whose (i,j)th entry is the number of
times points i and j are clustered together by several runs of KSS with random
initializations. We prove general recovery guarantees for any algorithm that
forms an affinity matrix with entries close to a monotonic transformation of
pairwise absolute inner products. We then show that a specific instance of EKSS
results in an affinity matrix with entries of this form, and hence our proposed
algorithm can provably recover subspaces under similar conditions to
state-of-the-art algorithms. The finding is, to the best of our knowledge, the
first recovery guarantee for evidence accumulation clustering and for KSS
variants. We show on synthetic data that our method performs well in the
traditionally challenging settings of subspaces with large intersection,
subspaces with small principal angles, and noisy data. Finally, we evaluate our
algorithm on six common benchmark datasets and show that unlike existing
methods, EKSS achieves excellent empirical performance when there are both a
small and large number of points per subspace.","cs.CV,cs.LG,stat.ML"
"The H3D Dataset for Full-Surround 3D Multi-Object Detection and Tracking in Crowded Urban Scenes. 3D multi-object detection and tracking are crucial for traffic scene
understanding. However, the community pays less attention to these areas due to
the lack of a standardized benchmark dataset to advance the field. Moreover,
existing datasets (e.g., KITTI) do not provide sufficient data and labels to
tackle challenging scenes where highly interactive and occluded traffic
participants are present. To address the issues, we present the Honda Research
Institute 3D Dataset (H3D), a large-scale full-surround 3D multi-object
detection and tracking dataset collected using a 3D LiDAR scanner. H3D
comprises of 160 crowded and highly interactive traffic scenes with a total of
1 million labeled instances in 27,721 frames. With unique dataset size, rich
annotations, and complex scenes, H3D is gathered to stimulate research on
full-surround 3D multi-object detection and tracking. To effectively and
efficiently annotate a large-scale 3D point cloud dataset, we propose a
labeling methodology to speed up the overall annotation cycle. A standardized
benchmark is created to evaluate full-surround 3D multi-object detection and
tracking algorithms. 3D object detection and tracking algorithms are trained
and tested on H3D. Finally, sources of errors are discussed for the development
of future algorithms.","cs.CV,cs.RO"
"Zero-Shot Reinforcement Learning with Deep Attention Convolutional Neural Networks. Simulation-to-simulation and simulation-to-real world transfer of neural
network models have been a difficult problem. To close the reality gap, prior
methods to simulation-to-real world transfer focused on domain adaptation,
decoupling perception and dynamics and solving each problem separately, and
randomization of agent parameters and environment conditions to expose the
learning agent to a variety of conditions. While these methods provide
acceptable performance, the computational complexity required to capture a
large variation of parameters for comprehensive scenarios on a given task such
as autonomous driving or robotic manipulation is high. Our key contribution is
to theoretically prove and empirically demonstrate that a deep attention
convolutional neural network (DACNN) with specific visual sensor configuration
performs as well as training on a dataset with high domain and parameter
variation at lower computational complexity. Specifically, the attention
network weights are learned through policy optimization to focus on local
dependencies that lead to optimal actions, and does not require tuning in
real-world for generalization. Our new architecture adapts perception with
respect to the control objective, resulting in zero-shot learning without
pre-training a perception network. To measure the impact of our new deep
network architecture on domain adaptation, we consider autonomous driving as a
use case. We perform an extensive set of experiments in
simulation-to-simulation and simulation-to-real scenarios to compare our
approach to several baselines including the current state-of-art models.","cs.LG,cs.RO,cs.SY,eess.SY"
"On the mapping between Hopfield networks and Restricted Boltzmann Machines. Hopfield networks (HNs) and Restricted Boltzmann Machines (RBMs) are two
important models at the interface of statistical physics, machine learning, and
neuroscience. Recently, there has been interest in the relationship between HNs
and RBMs, due to their similarity under the statistical mechanics formalism. An
exact mapping between HNs and RBMs has been previously noted for the special
case of orthogonal (uncorrelated) encoded patterns. We present here an exact
mapping in the case of correlated pattern HNs, which are more broadly
applicable to existing datasets. Specifically, we show that any HN with $N$
binary variables and $p<N$ arbitrary binary patterns can be transformed into an
RBM with $N$ binary visible variables and $p$ gaussian hidden variables. We
outline the conditions under which the reverse mapping exists, and conduct
experiments on the MNIST dataset which suggest the mapping provides a useful
initialization to the RBM weights. We discuss extensions, the potential
importance of this correspondence for the training of RBMs, and for
understanding the performance of deep architectures which utilize RBMs.","cond-mat.dis-nn,cond-mat.stat-mech,cs.LG"
"Federated Reinforcement Distillation with Proxy Experience Memory. In distributed reinforcement learning, it is common to exchange the
experience memory of each agent and thereby collectively train their local
models. The experience memory, however, contains all the preceding state
observations and their corresponding policies of the host agent, which may
violate the privacy of the agent. To avoid this problem, in this work, we
propose a privacy-preserving distributed reinforcement learning (RL) framework,
termed federated reinforcement distillation (FRD). The key idea is to exchange
a proxy experience memory comprising a pre-arranged set of states and
time-averaged policies, thereby preserving the privacy of actual experiences.
Based on an advantage actor-critic RL architecture, we numerically evaluate the
effectiveness of FRD and investigate how the performance of FRD is affected by
the proxy memory structure and different memory exchanging rules.","cs.LG,cs.MA,cs.NI,stat.ML"
"ExplAIn: Explanatory Artificial Intelligence for Diabetic Retinopathy Diagnosis. In recent years, Artificial Intelligence (AI) has proven its relevance for
medical decision support. However, the ""black-box"" nature of successful AI
algorithms still holds back their wide-spread deployment. In this paper, we
describe an eXplanatory Artificial Intelligence (XAI) that reaches the same
level of performance as black-box AI, for the task of classifying Diabetic
Retinopathy (DR) severity using Color Fundus Photography (CFP). This algorithm,
called ExplAIn, learns to segment and categorize lesions in images; the final
image-level classification directly derives from these multivariate lesion
segmentations. The novelty of this explanatory framework is that it is trained
from end to end, with image supervision only, just like black-box AI
algorithms: the concepts of lesions and lesion categories emerge by themselves.
For improved lesion localization, foreground/background separation is trained
through self-supervision, in such a way that occluding foreground pixels
transforms the input image into a healthy-looking image. The advantage of such
an architecture is that automatic diagnoses can be explained simply by an image
and/or a few sentences. ExplAIn is evaluated at the image level and at the
pixel level on various CFP image datasets. We expect this new framework, which
jointly offers high classification performance and explainability, to
facilitate AI deployment.",cs.CV
"Joint learning of variational representations and solvers for inverse problems with partially-observed data. Designing appropriate variational regularization schemes is a crucial part of
solving inverse problems, making them better-posed and guaranteeing that the
solution of the associated optimization problem satisfies desirable properties.
Recently, learning-based strategies have appeared to be very efficient for
solving inverse problems, by learning direct inversion schemes or plug-and-play
regularizers from available pairs of true states and observations. In this
paper, we go a step further and design an end-to-end framework allowing to
learn actual variational frameworks for inverse problems in such a supervised
setting. The variational cost and the gradient-based solver are both stated as
neural networks using automatic differentiation for the latter. We can jointly
learn both components to minimize the data reconstruction error on the true
states. This leads to a data-driven discovery of variational models. We
consider an application to inverse problems with incomplete datasets (image
inpainting and multivariate time series interpolation). We experimentally
illustrate that this framework can lead to a significant gain in terms of
reconstruction performance, including w.r.t. the direct minimization of the
variational formulation derived from the known generative model.","cs.LG,eess.IV,eess.SP,stat.ML"
"Induction of Subgoal Automata for Reinforcement Learning. In this work we present ISA, a novel approach for learning and exploiting
subgoals in reinforcement learning (RL). Our method relies on inducing an
automaton whose transitions are subgoals expressed as propositional formulas
over a set of observable events. A state-of-the-art inductive logic programming
system is used to learn the automaton from observation traces perceived by the
RL agent. The reinforcement learning and automaton learning processes are
interleaved: a new refined automaton is learned whenever the RL agent generates
a trace not recognized by the current automaton. We evaluate ISA in several
gridworld problems and show that it performs similarly to a method for which
automata are given in advance. We also show that the learned automata can be
exploited to speed up convergence through reward shaping and transfer learning
across multiple tasks. Finally, we analyze the running time and the number of
traces that ISA needs to learn an automata, and the impact that the number of
observable events has on the learner's performance.","cs.AI,cs.LG,cs.LO,stat.ML"
"Vehicle Local Position Estimation System. In this paper, a robust vehicle local position estimation with the help of
single camera sensor and GPS is presented. A modified Inverse Perspective
Mapping, illuminant Invariant techniques and object detection based approach is
used to localize the vehicle in the road. Vehicles current lane, its position
from road boundary and other cars are used to define its local position. For
this purpose Lane markings are detected using a Laplacian edge feature, robust
to shadowing. Effect of shadowing and extra sun light are removed using Lab
color space and illuminant invariant techniques. Lanes are assumed to be as
parabolic model and fitted using robust RANSAC. This method can reliably detect
all lanes of the road, estimate lane departure angle and local position of
vehicle relative to lanes, road boundary and other cars. Different type of
obstacle like pedestrians, vehicles are detected using HOG feature based
deformable part model.","68T45,cs.CV"
"Acceleration Method for Learning Fine-Layered Optical Neural Networks. An optical neural network (ONN) is a promising system due to its high-speed
and low-power operation. Its linear unit performs a multiplication of an input
vector and a weight matrix in optical analog circuits. Among them, a circuit
with a multiple-layered structure of programmable Mach-Zehnder interferometers
(MZIs) can realize a specific class of unitary matrices with a limited number
of MZIs as its weight matrix. The circuit is effective for balancing the number
of programmable MZIs and ONN performance. However, it takes a lot of time to
learn MZI parameters of the circuit with a conventional automatic
differentiation (AD), which machine learning platforms are equipped with. To
solve the time-consuming problem, we propose an acceleration method for
learning MZI parameters. We create customized complex-valued derivatives for an
MZI, exploiting Wirtinger derivatives and a chain rule. They are incorporated
into our newly developed function module implemented in C++ to collectively
calculate their values in a multi-layered structure. Our method is simple,
fast, and versatile as well as compatible with the conventional AD. We
demonstrate that our method works 20 times faster than the conventional AD when
a pixel-by-pixel MNIST task is performed in a complex-valued recurrent neural
network with an MZI-based hidden unit.","cs.LG,physics.optics,quant-ph,stat.ML"
"Increasing the adversarial robustness and explainability of capsule networks with $$-capsules. In this paper we introduce a new inductive bias for capsule networks and call
networks that use this prior $\gamma$-capsule networks. Our inductive bias that
is inspired by TE neurons of the inferior temporal cortex increases the
adversarial robustness and the explainability of capsule networks. A
theoretical framework with formal definitions of $\gamma$-capsule networks and
metrics for evaluation are also provided. Under our framework we show that
common capsule networks do not necessarily make use of this inductive bias. For
this reason we introduce a novel routing algorithm and use a different training
algorithm to be able to implement $\gamma$-capsule networks. We then show
experimentally that $\gamma$-capsule networks are indeed more transparent and
more robust against adversarial attacks than regular capsule networks.","cs.AI,cs.LG,stat.ML"
"Spectral Image Segmentation with Global Appearance Modeling. We introduce a new spectral method for image segmentation that incorporates
long range relationships for global appearance modeling. The approach combines
two different graphs, one is a sparse graph that captures spatial relationships
between nearby pixels and another is a dense graph that captures pairwise
similarity between all pairs of pixels. We extend the spectral method for
Normalized Cuts to this setting by combining the transition matrices of Markov
chains associated with each graph. We also derive an efficient method that uses
importance sampling for sparsifying the dense graph of appearance
relationships. This leads to a practical algorithm for segmenting
high-resolution images. The resulting method can segment challenging images
without any filtering or pre-processing.","I.4; I.5,cs.CV,cs.LG,eess.IV"
"Error estimate for a universal function approximator of ReLU network with a local connection. Neural networks have shown high successful performance in a wide range of
tasks, but further studies are needed to improve its performance. We analyze
the approximation error of the specific neural network architecture with a
local connection and higher application than one with the full connection
because the local-connected network can be used to explain diverse neural
networks such as CNNs. Our error estimate depends on two parameters: one
controlling the depth of the hidden layer, and the other, the width of the
hidden layers.","cs.IT,cs.LG,math.IT,stat.ML"
"Modern Hopfield Networks and Attention for Immune Repertoire Classification. A central mechanism in machine learning is to identify, store, and recognize
patterns. How to learn, access, and retrieve such patterns is crucial in
Hopfield networks and the more recent transformer architectures. We show that
the attention mechanism of transformer architectures is actually the update
rule of modern Hopfield networks that can store exponentially many patterns. We
exploit this high storage capacity of modern Hopfield networks to solve a
challenging multiple instance learning (MIL) problem in computational biology:
immune repertoire classification. Accurate and interpretable machine learning
methods solving this problem could pave the way towards new vaccines and
therapies, which is currently a very relevant research topic intensified by the
COVID-19 crisis. Immune repertoire classification based on the vast number of
immunosequences of an individual is a MIL problem with an unprecedentedly
massive number of instances, two orders of magnitude larger than currently
considered problems, and with an extremely low witness rate. In this work, we
present our novel method DeepRC that integrates transformer-like attention, or
equivalently modern Hopfield networks, into deep learning architectures for
massive MIL such as immune repertoire classification. We demonstrate that
DeepRC outperforms all other methods with respect to predictive performance on
large-scale experiments, including simulated and real-world virus infection
data, and enables the extraction of sequence motifs that are connected to a
given disease class. Source code and datasets: https://github.com/ml-jku/DeepRC","cs.LG,q-bio.BM,stat.ML"
"Time Delay Estimation of Traffic Congestion Propagation based on Transfer Entropy. Considering how congestion will propagate in the near future, understanding
traffic congestion propagation has become crucial in GPS navigation systems for
providing users with a more accurate estimated time of arrival (ETA). However,
providing the exact ETA during congestion is a challenge owing to the complex
propagation process between roads and high uncertainty regarding the future
behavior of the process. Recent studies have focused on finding frequent
congestion propagation patterns and determining the propagation probabilities.
By contrast, this study proposes a novel time delay estimation method for
traffic congestion propagation between roads using lag-specific transfer
entropy (TE). Nonlinear normalization with a sliding window is used to
effectively reveal the causal relationship between the source and target time
series in calculating the TE. Moreover, Markov bootstrap techniques were
adopted to quantify the uncertainty in the time delay estimator. To the best of
our knowledge, the time delay estimation method presented in this article is
the first to determine the time delay between roads for any congestion
propagation pattern. The proposed method was validated using simulated data as
well as real user trajectory data obtained from a major GPS navigation system
applied in South Korea.","cs.LG,cs.SY,eess.SY,stat.ML"
"Differentiable Reasoning on Large Knowledge Bases and Natural Language. Reasoning with knowledge expressed in natural language and Knowledge Bases
(KBs) is a major challenge for Artificial Intelligence, with applications in
machine reading, dialogue, and question answering. General neural architectures
that jointly learn representations and transformations of text are very
data-inefficient, and it is hard to analyse their reasoning process. These
issues are addressed by end-to-end differentiable reasoning systems such as
Neural Theorem Provers (NTPs), although they can only be used with small-scale
symbolic KBs. In this paper we first propose Greedy NTPs (GNTPs), an extension
to NTPs addressing their complexity and scalability limitations, thus making
them applicable to real-world datasets. This result is achieved by dynamically
constructing the computation graph of NTPs and including only the most
promising proof paths during inference, thus obtaining orders of magnitude more
efficient models. Then, we propose a novel approach for jointly reasoning over
KBs and textual mentions, by embedding logic facts and natural language
sentences in a shared embedding space. We show that GNTPs perform on par with
NTPs at a fraction of their cost while achieving competitive link prediction
results on large datasets, providing explanations for predictions, and inducing
interpretable models. Source code, datasets, and supplementary material are
available online at https://github.com/uclnlp/gntp.","cs.CL,cs.LG,cs.LO"
"Blind Hyperspectral-Multispectral Image Fusion via Graph Laplacian Regularization. Fusing a low-resolution hyperspectral image (HSI) and a high-resolution
multispectral image (MSI) of the same scene leads to a super-resolution image
(SRI), which is information rich spatially and spectrally. In this paper, we
super-resolve the HSI using the graph Laplacian defined on the MSI. Unlike many
existing works, we don't assume prior knowledge about the spatial degradation
from SRI to HSI, nor a perfectly aligned HSI and MSI pair. Our algorithm
progressively alternates between finding the blur kernel and fusing HSI with
MSI, generating accurate estimations of the blur kernel and the SRI at
convergence. Experiments on various datasets demonstrate the advantages of the
proposed algorithm in the quality of fusion and its capability in dealing with
unknown spatial degradation.","65D18,68U10,cs.CV"
"Concurrent Neural Network : A model of competition between times series. Competition between times series often arises in sales prediction, when
similar products are on sale on a marketplace. This article provides a model of
the presence of cannibalization between times series. This model creates a
""competitiveness"" function that depends on external features such as price and
margin. It also provides a theoretical guaranty on the error of the model under
some reasonable conditions, and implement this model using a neural network to
compute this competitiveness function. This implementation outperforms other
traditional time series methods and classical neural networks for market share
prediction on a real-world data set.","cs.LG,stat.AP,stat.ML"
"Using Cyber Terrain in Reinforcement Learning for Penetration Testing. Reinforcement learning (RL) has been applied to attack graphs for penetration
testing, however, trained agents do not reflect reality because the attack
graphs lack operational nuances typically captured within the intelligence
preparation of the battlefield (IPB) that include notions of (cyber) terrain.
In particular, current practice constructs attack graphs exclusively using the
Common Vulnerability Scoring System (CVSS) and its components. We present
methods for constructing attack graphs using notions from IPB on cyber terrain
analysis of obstacles, avenues of approach, key terrain, observation and fields
of fire, and cover and concealment. We demonstrate our methods on an example
where firewalls are treated as obstacles and represented in (1) the reward
space and (2) the state dynamics. We show that terrain analysis can be used to
bring realism to attack graphs for RL.","cs.CR,cs.LG,cs.NI"
"Preserved central model for faster bidirectional compression in distributed settings. We develop a new approach to tackle communication constraints in a
distributed learning problem with a central server. We propose and analyze a
new algorithm that performs bidirectional compression and achieves the same
convergence rate as algorithms using only uplink (from the local workers to the
central server) compression. To obtain this improvement, we design MCM, an
algorithm such that the downlink compression only impacts local models, while
the global model is preserved. As a result, and contrary to previous works, the
gradients on local servers are computed on perturbed models. Consequently,
convergence proofs are more challenging and require a precise control of this
perturbation. To ensure it, MCM additionally combines model compression with a
memory mechanism. This analysis opens new doors, e.g. incorporating worker
dependent randomized-models and partial participation.","cs.DC,cs.LG,math.ST,stat.TH"
"Understanding training and generalization in deep learning by Fourier analysis. Background: It is still an open research area to theoretically understand why
Deep Neural Networks (DNNs)---equipped with many more parameters than training
data and trained by (stochastic) gradient-based methods---often achieve
remarkably low generalization error. Contribution: We study DNN training by
Fourier analysis. Our theoretical framework explains: i) DNN with (stochastic)
gradient-based methods often endows low-frequency components of the target
function with a higher priority during the training; ii) Small initialization
leads to good generalization ability of DNN while preserving the DNN's ability
to fit any function. These results are further confirmed by experiments of DNNs
fitting the following datasets, that is, natural images, one-dimensional
functions and MNIST dataset.","68Q32,68T01,I.2.6,cs.AI,cs.LG,math.OC,math.ST,stat.ML,stat.TH"
"Consistency of Cheeger and Ratio Graph Cuts. This paper establishes the consistency of a family of graph-cut-based
algorithms for clustering of data clouds. We consider point clouds obtained as
samples of a ground-truth measure. We investigate approaches to clustering
based on minimizing objective functionals defined on proximity graphs of the
given sample. Our focus is on functionals based on graph cuts like the Cheeger
and ratio cuts. We show that minimizers of the these cuts converge as the
sample size increases to a minimizer of a corresponding continuum cut (which
partitions the ground truth measure). Moreover, we obtain sharp conditions on
how the connectivity radius can be scaled with respect to the number of sample
points for the consistency to hold. We provide results for two-way and for
multiway cuts. Furthermore we provide numerical experiments that illustrate the
results and explore the optimality of scaling in dimension two.","49J55,60D05,62G20,62H30,68R10,91C20,cs.LG,math.ST,stat.ML,stat.TH"
"Robust Group Synchronization via Cycle-Edge Message Passing. We propose a general framework for solving the group synchronization problem,
where we focus on the setting of adversarial or uniform corruption and
sufficiently small noise. Specifically, we apply a novel message passing
procedure that uses cycle consistency information in order to estimate the
corruption levels of group ratios and consequently solve the synchronization
problem in our setting. We first explain why the group cycle consistency
information is essential for effectively solving group synchronization
problems. We then establish exact recovery and linear convergence guarantees
for the proposed message passing procedure under a deterministic setting with
adversarial corruption. These guarantees hold as long as the ratio of corrupted
cycles per edge is bounded by a reasonable constant. We also establish the
stability of the proposed procedure to sub-Gaussian noise. We further establish
exact recovery with high probability under a common uniform corruption model.","62G35,68Q25,68Q87,68W40,90-08,93E10,cs.IT,math.IT,math.OC,math.PR,stat.ML"
"On Function Approximation in Reinforcement Learning: Optimism in the Face of Large State Spaces. The classical theory of reinforcement learning (RL) has focused on tabular
and linear representations of value functions. Further progress hinges on
combining RL with modern function approximators such as kernel functions and
deep neural networks, and indeed there have been many empirical successes that
have exploited such combinations in large-scale applications. There are
profound challenges, however, in developing a theory to support this
enterprise, most notably the need to take into consideration the
exploration-exploitation tradeoff at the core of RL in conjunction with the
computational and statistical tradeoffs that arise in modern
function-approximation-based learning systems. We approach these challenges by
studying an optimistic modification of the least-squares value iteration
algorithm, in the context of the action-value function
  represented by a kernel function or an overparameterized neural network. We
establish both polynomial runtime complexity and polynomial sample complexity
for this algorithm, without additional assumptions on the data-generating
model. In particular, we prove that the algorithm incurs an
$\tilde{\mathcal{O}}(\delta_{\mathcal{F}} H^2 \sqrt{T})$ regret, where
$\delta_{\mathcal{F}}$ characterizes the intrinsic complexity of the function
class $\mathcal{F}$, $H$ is the length of each episode, and $T$ is the total
number of episodes. Our regret bounds are independent of the number of states,
a result which exhibits clearly the benefit of function approximation in RL.","cs.AI,cs.LG,math.OC,math.ST,stat.ML,stat.TH"
"BatVision with GCC-PHAT Features for Better Sound to Vision Predictions. Inspired by sophisticated echolocation abilities found in nature, we train a
generative adversarial network to predict plausible depth maps and grayscale
layouts from sound. To achieve this, our sound-to-vision model processes
binaural echo-returns from chirping sounds. We build upon previous work with
BatVision that consists of a sound-to-vision model and a self-collected dataset
using our mobile robot and low-cost hardware. We improve on the previous model
by introducing several changes to the model, which leads to a better depth and
grayscale estimation, and increased perceptual quality. Rather than using raw
binaural waveforms as input, we generate generalized cross-correlation (GCC)
features and use these as input instead. In addition, we change the model
generator and base it on residual learning and use spectral normalization in
the discriminator. We compare and present both quantitative and qualitative
improvements over our previous BatVision model.","cs.CV,cs.RO,cs.SD,eess.AS"
"LP-SparseMAP: Differentiable Relaxed Optimization for Sparse Structured Prediction. Structured prediction requires manipulating a large number of combinatorial
structures, e.g., dependency trees or alignments, either as latent or output
variables. Recently, the SparseMAP method has been proposed as a
differentiable, sparse alternative to maximum a posteriori (MAP) and marginal
inference. SparseMAP returns a combination of a small number of structures, a
desirable property in some downstream applications. However, SparseMAP requires
a tractable MAP inference oracle. This excludes, e.g., loopy graphical models
or factor graphs with logic constraints, which generally require approximate
inference. In this paper, we introduce LP-SparseMAP, an extension of SparseMAP
that addresses this limitation via a local polytope relaxation. LP-SparseMAP
uses the flexible and powerful domain specific language of factor graphs for
defining and backpropagating through arbitrary hidden structure, supporting
coarse decompositions, hard logic constraints, and higher-order correlations.
We derive the forward and backward algorithms needed for using LP-SparseMAP as
a hidden or output layer. Experiments in three structured prediction tasks show
benefits compared to SparseMAP and Structured SVM.","cs.CL,cs.LG,stat.ML"
"Stochastic Gradient MCMC for State Space Models. State space models (SSMs) are a flexible approach to modeling complex time
series. However, inference in SSMs is often computationally prohibitive for
long time series. Stochastic gradient MCMC (SGMCMC) is a popular method for
scalable Bayesian inference for large independent data. Unfortunately when
applied to dependent data, such as in SSMs, SGMCMC's stochastic gradient
estimates are biased as they break crucial temporal dependencies. To alleviate
this, we propose stochastic gradient estimators that control this bias by
performing additional computation in a `buffer' to reduce breaking
dependencies. Furthermore, we derive error bounds for this bias and show a
geometric decay under mild conditions. Using these estimators, we develop novel
SGMCMC samplers for discrete, continuous and mixed-type SSMs with analytic
message passing. Our experiments on real and synthetic data demonstrate the
effectiveness of our SGMCMC algorithms compared to batch MCMC, allowing us to
scale inference to long time series with millions of time points.","cs.LG,stat.CO,stat.ML"
"Variational Gaussian Process Dynamical Systems. High dimensional time series are endemic in applications of machine learning
such as robotics (sensor data), computational biology (gene expression data),
vision (video sequences) and graphics (motion capture data). Practical
nonlinear probabilistic approaches to this data are required. In this paper we
introduce the variational Gaussian process dynamical system. Our work builds on
recent variational approximations for Gaussian process latent variable models
to allow for nonlinear dimensionality reduction simultaneously with learning a
dynamical prior in the latent space. The approach also allows for the
appropriate dimensionality of the latent space to be automatically determined.
We demonstrate the model on a human motion capture data set and a series of
high resolution video sequences.","58E30,60G15 (Primary),62-09,G.3; G.1.2; I.2.6; I.5.4,cs.AI,cs.CV,math.PR,stat.ML"
"Exploring Deep and Recurrent Architectures for Optimal Control. Sophisticated multilayer neural networks have achieved state of the art
results on multiple supervised tasks. However, successful applications of such
multilayer networks to control have so far been limited largely to the
perception portion of the control pipeline. In this paper, we explore the
application of deep and recurrent neural networks to a continuous,
high-dimensional locomotion task, where the network is used to represent a
control policy that maps the state of the system (represented by joint angles)
directly to the torques at each joint. By using a recent reinforcement learning
algorithm called guided policy search, we can successfully train neural network
controllers with thousands of parameters, allowing us to compare a variety of
architectures. We discuss the differences between the locomotion control task
and previous supervised perception tasks, present experimental results
comparing various architectures, and discuss future directions in the
application of techniques from deep learning to the problem of optimal control.","cs.AI,cs.LG,cs.NE,cs.RO,cs.SY"
"Molecular Mechanics-Driven Graph Neural Network with Multiplex Graph for Molecular Structures. The prediction of physicochemical properties from molecular structures is a
crucial task for artificial intelligence aided molecular design. A growing
number of Graph Neural Networks (GNNs) have been proposed to address this
challenge. These models improve their expressive power by incorporating
auxiliary information in molecules while inevitably increase their
computational complexity. In this work, we aim to design a GNN which is both
powerful and efficient for molecule structures. To achieve such goal, we
propose a molecular mechanics-driven approach by first representing each
molecule as a two-layer multiplex graph, where one layer contains only local
connections that mainly capture the covalent interactions and another layer
contains global connections that can simulate non-covalent interactions. Then
for each layer, a corresponding message passing module is proposed to balance
the trade-off of expression power and computational complexity. Based on these
two modules, we build Multiplex Molecular Graph Neural Network (MXMNet). When
validated by the QM9 dataset for small molecules and PDBBind dataset for large
protein-ligand complexes, MXMNet achieves superior results to the existing
state-of-the-art models under restricted resources.","cs.LG,physics.comp-ph,q-bio.QM"
"Automatic Testing With Reusable Adversarial Agents. Autonomous systems such as self-driving cars and general-purpose robots are
safety-critical systems that operate in highly uncertain and dynamic
environments. We propose an interactive multi-agent framework where the
system-under-design is modeled as an ego agent and its environment is modeled
by a number of adversarial (ado) agents. For example, a self-driving car is an
ego agent whose behavior is influenced by ado agents such as pedestrians,
bicyclists, traffic lights, road geometry etc. Given a logical specification of
the correct behavior of the ego agent, and a set of constraints that encode
reasonable adversarial behavior, our framework reduces the adversarial testing
problem to the problem of synthesizing controllers for (constrained) ado agents
that cause the ego agent to violate its specifications. Specifically, we
explore the use of tabular and deep reinforcement learning approaches for
synthesizing adversarial agents. We show that ado agents trained in this
fashion are better than traditional falsification or testing techniques because
they can generalize to ego agents and environments that differ from the
original ego agent. We demonstrate the efficacy of our technique on two
real-world case studies from the domain of self-driving cars.","cs.AI,cs.LG,cs.SY,eess.SY"
"Designing an Effective Metric Learning Pipeline for Speaker Diarization. State-of-the-art speaker diarization systems utilize knowledge from external
data, in the form of a pre-trained distance metric, to effectively determine
relative speaker identities to unseen data. However, much of recent focus has
been on choosing the appropriate feature extractor, ranging from pre-trained
$i-$vectors to representations learned via different sequence modeling
architectures (e.g. 1D-CNNs, LSTMs, attention models), while adopting
off-the-shelf metric learning solutions. In this paper, we argue that,
regardless of the feature extractor, it is crucial to carefully design a metric
learning pipeline, namely the loss function, the sampling strategy and the
discrimnative margin parameter, for building robust diarization systems.
Furthermore, we propose to adopt a fine-grained validation process to obtain a
comprehensive evaluation of the generalization power of metric learning
pipelines. To this end, we measure diarization performance across different
language speakers, and variations in the number of speakers in a recording.
Using empirical studies, we provide interesting insights into the effectiveness
of different design choices and make recommendations.","cs.LG,cs.SD,eess.AS,stat.ML"
"BELT: Block-wise Missing Embedding Learning Transformer. Matrix completion has attracted attention in many fields, including
statistics, applied mathematics, and electrical engineering. Most of the works
focus on the independent sampling models under which the observed entries are
sampled independently. Motivated by applications in the integration of multiple
Electronic Health Record (EHR) datasets, we propose the method {\bf B}lock-wise
missing {\bf E}mbedding {\bf L}earning {\bf T}ransformer (BELT) to treat
row-wise/column-wise missingness. Specifically, BELT can recover block-wise
missing matrices efficiently when every pair of matrices has an overlap. Our
idea is to exploit the orthogonal Procrustes problem to align the eigenspace of
the two sub-matrices using their overlap, then complete the missing blocks by
the inner product of the two low-rank components. Besides, we prove the
statistical rate for the eigenspace of the underlying matrix, which is
comparable to the rate under the independently missing assumption. Simulation
studies show that the method performs well under a variety of configurations.
In the real data analysis, the method is applied to two tasks: (i) the
integrating of several point-wise mutual information matrices built by English
EHR and Chinese medical text data, and (ii) the machine translation between
English and Chinese medical concepts. Our method shows an advantage over
existing methods.","cs.LG,math.ST,stat.AP,stat.ME,stat.ML,stat.TH"
"Visual Steering for One-Shot Deep Neural Network Synthesis. Recent advancements in the area of deep learning have shown the effectiveness
of very large neural networks in several applications. However, as these deep
neural networks continue to grow in size, it becomes more and more difficult to
configure their many parameters to obtain good results. Presently, analysts
must experiment with many different configurations and parameter settings,
which is labor-intensive and time-consuming. On the other hand, the capacity of
fully automated techniques for neural network architecture search is limited
without the domain knowledge of human experts. To deal with the problem, we
formulate the task of neural network architecture optimization as a graph space
exploration, based on the one-shot architecture search technique. In this
approach, a super-graph of all candidate architectures is trained in one-shot
and the optimal neural network is identified as a sub-graph. In this paper, we
present a framework that allows analysts to effectively build the solution
sub-graph space and guide the network search by injecting their domain
knowledge. Starting with the network architecture space composed of basic
neural network components, analysts are empowered to effectively select the
most promising components via our one-shot search scheme. Applying this
technique in an iterative manner allows analysts to converge to the best
performing neural network architecture for a given application. During the
exploration, analysts can use their domain knowledge aided by cues provided
from a scatterplot visualization of the search space to edit different
components and guide the search for faster convergence. We designed our
interface in collaboration with several deep learning researchers and its final
effectiveness is evaluated with a user study and two case studies.","cs.CV,cs.HC,cs.LG,stat.ML"
"Image-based 3D Object Reconstruction: State-of-the-Art and Trends in the Deep Learning Era. 3D reconstruction is a longstanding ill-posed problem, which has been
explored for decades by the computer vision, computer graphics, and machine
learning communities. Since 2015, image-based 3D reconstruction using
convolutional neural networks (CNN) has attracted increasing interest and
demonstrated an impressive performance. Given this new era of rapid evolution,
this article provides a comprehensive survey of the recent developments in this
field. We focus on the works which use deep learning techniques to estimate the
3D shape of generic objects either from a single or multiple RGB images. We
organize the literature based on the shape representations, the network
architectures, and the training mechanisms they use. While this survey is
intended for methods which reconstruct generic objects, we also review some of
the recent works which focus on specific object classes such as human body
shapes and faces. We provide an analysis and comparison of the performance of
some key papers, summarize some of the open problems in this field, and discuss
promising directions for future research.","cs.CG,cs.CV,cs.GR,cs.LG"
"Variable-lag Granger Causality for Time Series Analysis. Granger causality is a fundamental technique for causal inference in time
series data, commonly used in the social and biological sciences. Typical
operationalizations of Granger causality make a strong assumption that every
time point of the effect time series is influenced by a combination of other
time series with a fixed time delay. However, the assumption of the fixed time
delay does not hold in many applications, such as collective behavior,
financial markets, and many natural phenomena. To address this issue, we
develop variable-lag Granger causality, a generalization of Granger causality
that relaxes the assumption of the fixed time delay and allows causes to
influence effects with arbitrary time delays. In addition, we propose a method
for inferring variable-lag Granger causality relations. We demonstrate our
approach on an application for studying coordinated collective behavior and
show that it performs better than several existing methods in both simulated
and real-world datasets. Our approach can be applied in any domain of time
series analysis.","62-07,68T05,91-08,G.3; I.2.3; I.2.6; J.4,cs.LG,econ.EM,q-bio.QM,stat.ME,stat.ML"
"Predicting the direction of stock market prices using random forest. Predicting trends in stock market prices has been an area of interest for
researchers for many years due to its complex and dynamic nature. Intrinsic
volatility in stock market across the globe makes the task of prediction
challenging. Forecasting and diffusion modeling, although effective can't be
the panacea to the diverse range of problems encountered in prediction,
short-term or otherwise. Market risk, strongly correlated with forecasting
errors, needs to be minimized to ensure minimal risk in investment. The authors
propose to minimize forecasting error by treating the forecasting problem as a
classification problem, a popular suite of algorithms in Machine learning. In
this paper, we propose a novel way to minimize the risk of investment in stock
market by predicting the returns of a stock using a class of powerful machine
learning algorithms known as ensemble learning. Some of the technical
indicators such as Relative Strength Index (RSI), stochastic oscillator etc are
used as inputs to train our model. The learning model used is an ensemble of
multiple decision trees. The algorithm is shown to outperform existing algo-
rithms found in the literature. Out of Bag (OOB) error estimates have been
found to be encouraging. Key Words: Random Forest Classifier, stock price
forecasting, Exponential smoothing, feature extraction, OOB error and
convergence.","cs.CE,cs.LG"
"Carbon Emissions and Large Neural Network Training. The computation demand for machine learning (ML) has grown rapidly recently,
which comes with a number of costs. Estimating the energy cost helps measure
its environmental impact and finding greener strategies, yet it is challenging
without detailed information. We calculate the energy use and carbon footprint
of several recent large models-T5, Meena, GShard, Switch Transformer, and
GPT-3-and refine earlier estimates for the neural architecture search that
found Evolved Transformer. We highlight the following opportunities to improve
energy efficiency and CO2 equivalent emissions (CO2e): Large but sparsely
activated DNNs can consume <1/10th the energy of large, dense DNNs without
sacrificing accuracy despite using as many or even more parameters. Geographic
location matters for ML workload scheduling since the fraction of carbon-free
energy and resulting CO2e vary ~5X-10X, even within the same country and the
same organization. We are now optimizing where and when large models are
trained. Specific datacenter infrastructure matters, as Cloud datacenters can
be ~1.4-2X more energy efficient than typical datacenters, and the ML-oriented
accelerators inside them can be ~2-5X more effective than off-the-shelf
systems. Remarkably, the choice of DNN, datacenter, and processor can reduce
the carbon footprint up to ~100-1000X. These large factors also make
retroactive estimates of energy cost difficult. To avoid miscalculations, we
believe ML papers requiring large computational resources should make energy
consumption and CO2e explicit when practical. We are working to be more
transparent about energy use and CO2e in our future research. To help reduce
the carbon footprint of ML, we believe energy usage and CO2e should be a key
metric in evaluating models, and we are collaborating with MLPerf developers to
include energy usage during training and inference in this industry standard
benchmark.","cs.CY,cs.LG"
"Generic Primitive Detection in Point Clouds Using Novel Minimal Quadric Fits. We present a novel and effective method for detecting 3D primitives in
cluttered, unorganized point clouds, without axillary segmentation or type
specification. We consider the quadric surfaces for encapsulating the basic
building blocks of our environments - planes, spheres, ellipsoids, cones or
cylinders, in a unified fashion. Moreover, quadrics allow us to model higher
degree of freedom shapes, such as hyperboloids or paraboloids that could be
used in non-rigid settings.
  We begin by contributing two novel quadric fits targeting 3D point sets that
are endowed with tangent space information. Based upon the idea of aligning the
quadric gradients with the surface normals, our first formulation is exact and
requires as low as four oriented points. The second fit approximates the first,
and reduces the computational effort. We theoretically analyze these fits with
rigor, and give algebraic and geometric arguments. Next, by re-parameterizing
the solution, we devise a new local Hough voting scheme on the null-space
coefficients that is combined with RANSAC, reducing the complexity from
$O(N^4)$ to $O(N^3)$ (three points). To the best of our knowledge, this is the
first method capable of performing a generic cross-type multi-object primitive
detection in difficult scenes without segmentation. Our extensive qualitative
and quantitative results show that our method is efficient and flexible, as
well as being accurate.","cs.CG,cs.CV,cs.GR,cs.RO"
"Deep Reinforcement Learning with Quantum-inspired Experience Replay. In this paper, a novel training paradigm inspired by quantum computation is
proposed for deep reinforcement learning (DRL) with experience replay. In
contrast to traditional experience replay mechanism in DRL, the proposed deep
reinforcement learning with quantum-inspired experience replay (DRL-QER)
adaptively chooses experiences from the replay buffer according to the
complexity and the replayed times of each experience (also called transition),
to achieve a balance between exploration and exploitation. In DRL-QER,
transitions are first formulated in quantum representations, and then the
preparation operation and the depreciation operation are performed on the
transitions. In this progress, the preparation operation reflects the
relationship between the temporal difference errors (TD-errors) and the
importance of the experiences, while the depreciation operation is taken into
account to ensure the diversity of the transitions. The experimental results on
Atari 2600 games show that DRL-QER outperforms state-of-the-art algorithms such
as DRL-PER and DCRL on most of these games with improved training efficiency,
and is also applicable to such memory-based DRL approaches as double network
and dueling network.","cs.AI,cs.LG,quant-ph"
"Out-of-Core Surface Reconstruction via Global $TGV$ Minimization. We present an out-of-core variational approach for surface reconstruction
from a set of aligned depth maps. Input depth maps are supposed to be
reconstructed from regular photos or/and can be a representation of terrestrial
LIDAR point clouds. Our approach is based on surface reconstruction via total
generalized variation minimization ($TGV$) because of its strong
visibility-based noise-filtering properties and GPU-friendliness. Our main
contribution is an out-of-core OpenCL-accelerated adaptation of this numerical
algorithm which can handle arbitrarily large real-world scenes with scale
diversity.","I.4.8; I.3.5; C.2.4,cs.CV,cs.DC,cs.GR"
"Understanding the Basis of Graph Convolutional Neural Networks via an Intuitive Matched Filtering Approach. Graph Convolutional Neural Networks (GCNN) are becoming a preferred model for
data processing on irregular domains, yet their analysis and principles of
operation are rarely examined due to the black box nature of NNs. To this end,
we revisit the operation of GCNNs and show that their convolution layers
effectively perform matched filtering of input data with the chosen patterns
(features). This allows us to provide a unifying account of GCNNs through a
matched filter perspective, whereby the nonlinear ReLU and max-pooling layers
are also discussed within the matched filtering framework. This is followed by
a step-by-step guide on information propagation and learning in GCNNs. It is
also shown that standard CNNs and fully connected NNs can be obtained as a
special case of GCNNs. A carefully chosen numerical example guides the reader
through the various steps of GCNN operation and learning both visually and
numerically.","cs.AI,cs.IT,cs.LG,cs.NE,math.IT"
"Fine-grained Pattern Matching Over Streaming Time Series. Pattern matching of streaming time series with lower latency under limited
computing resource comes to a critical problem, especially as the growth of
Industry 4.0 and Industry Internet of Things. However, against traditional
single pattern matching problem, a pattern may contain multiple segments
representing different statistical properties or physical meanings for more
precise and expressive matching in real world. Hence, we formulate a new
problem, called ""fine-grained pattern matching"", which allows users to specify
varied granularities of matching deviation to different segments of a given
pattern, and fuzzy regions for adaptive breakpoints determination between
consecutive segments. In this paper, we propose a novel two-phase approach. In
the pruning phase, we introduce Equal-Length Block (ELB) representation
together with Block-Skipping Pruning (BSP) policy, which guarantees low cost
feature calculation, effective pruning and no false dismissals. In the
post-processing phase, a delta-function is proposed to enable us to conduct
exact matching in linear complexity. Extensive experiments are conducted to
evaluate on synthetic and real-world datasets, which illustrates that our
algorithm outperforms the brute-force method and MSM, a multi-step filter
mechanism over the multi-scaled representation.","cs.CV,cs.DB"
"Neural Jump Ordinary Differential Equations: Consistent Continuous-Time Prediction and Filtering. Combinations of neural ODEs with recurrent neural networks (RNN), like
GRU-ODE-Bayes or ODE-RNN are well suited to model irregularly observed time
series. While those models outperform existing discrete-time approaches, no
theoretical guarantees for their predictive capabilities are available.
Assuming that the irregularly-sampled time series data originates from a
continuous stochastic process, the $L^2$-optimal online prediction is the
conditional expectation given the currently available information. We introduce
the Neural Jump ODE (NJ-ODE) that provides a data-driven approach to learn,
continuously in time, the conditional expectation of a stochastic process. Our
approach models the conditional expectation between two observations with a
neural ODE and jumps whenever a new observation is made. We define a novel
training framework, which allows us to prove theoretical guarantees for the
first time. In particular, we show that the output of our model converges to
the $L^2$-optimal prediction. This can be interpreted as solution to a special
filtering problem. We provide experiments showing that the theoretical results
also hold empirically. Moreover, we experimentally show that our model
outperforms the baselines in more complex learning tasks and give comparisons
on real-world datasets.","cs.LG,math.PR,q-fin.CP,q-fin.ST,stat.ML"
"Reinforced Molecular Optimization with Neighborhood-Controlled Grammars. A major challenge in the pharmaceutical industry is to design novel molecules
with specific desired properties, especially when the property evaluation is
costly. Here, we propose MNCE-RL, a graph convolutional policy network for
molecular optimization with molecular neighborhood-controlled embedding
grammars through reinforcement learning. We extend the original
neighborhood-controlled embedding grammars to make them applicable to molecular
graph generation and design an efficient algorithm to infer grammatical
production rules from given molecules. The use of grammars guarantees the
validity of the generated molecular structures. By transforming molecular
graphs to parse trees with the inferred grammars, the molecular structure
generation task is modeled as a Markov decision process where a policy gradient
strategy is utilized. In a series of experiments, we demonstrate that our
approach achieves state-of-the-art performance in a diverse range of molecular
optimization tasks and exhibits significant superiority in optimizing molecular
properties with a limited number of property evaluations.","cs.LG,q-bio.BM"
"Compressing deep neural networks by matrix product operators. A deep neural network is a parametrization of a multilayer mapping of signals
in terms of many alternatively arranged linear and nonlinear transformations.
The linear transformations, which are generally used in the fully connected as
well as convolutional layers, contain most of the variational parameters that
are trained and stored. Compressing a deep neural network to reduce its number
of variational parameters but not its prediction power is an important but
challenging problem toward the establishment of an optimized scheme in training
efficiently these parameters and in lowering the risk of overfitting. Here we
show that this problem can be effectively solved by representing linear
transformations with matrix product operators (MPOs), which is a tensor network
originally proposed in physics to characterize the short-range entanglement in
one-dimensional quantum states. We have tested this approach in five typical
neural networks, including FC2, LeNet-5, VGG, ResNet, and DenseNet on two
widely used data sets, namely, MNIST and CIFAR-10, and found that this MPO
representation indeed sets up a faithful and efficient mapping between input
and output signals, which can keep or even improve the prediction accuracy with
a dramatically reduced number of parameters. Our method greatly simplifies the
representations in deep learning, and opens a possible route toward
establishing a framework of modern neural networks which might be simpler and
cheaper, but more efficient.","cs.CV,cs.LG,cs.NE,physics.comp-ph,quant-ph"
"GrADE: A graph based data-driven solver for time-dependent nonlinear partial differential equations. The physical world is governed by the laws of physics, often represented in
form of nonlinear partial differential equations (PDEs). Unfortunately,
solution of PDEs is non-trivial and often involves significant computational
time. With recent developments in the field of artificial intelligence and
machine learning, the solution of PDEs using neural network has emerged as a
domain with huge potential. However, most of the developments in this field are
based on either fully connected neural networks (FNN) or convolutional neural
networks (CNN). While FNN is computationally inefficient as the number of
network parameters can be potentially huge, CNN necessitates regular grid and
simpler domain. In this work, we propose a novel framework referred to as the
Graph Attention Differential Equation (GrADE) for solving time dependent
nonlinear PDEs. The proposed approach couples FNN, graph neural network, and
recently developed Neural ODE framework. The primary idea is to use graph
neural network for modeling the spatial domain, and Neural ODE for modeling the
temporal domain. The attention mechanism identifies important inputs/features
and assign more weightage to the same; this enhances the performance of the
proposed framework. Neural ODE, on the other hand, results in constant memory
cost and allows trading of numerical precision for speed. We also propose depth
refinement as an effective technique for training the proposed architecture in
lesser time with better accuracy. The effectiveness of the proposed framework
is illustrated using 1D and 2D Burgers' equations. Results obtained illustrate
the capability of the proposed framework in modeling PDE and its scalability to
larger domains without the need for retraining.","cs.LG,physics.comp-ph,stat.ML"
"Logic and the $2$-Simplicial Transformer. We introduce the $2$-simplicial Transformer, an extension of the Transformer
which includes a form of higher-dimensional attention generalising the
dot-product attention, and uses this attention to update entity representations
with tensor products of value vectors. We show that this architecture is a
useful inductive bias for logical reasoning in the context of deep
reinforcement learning.","cs.LG,cs.LO,stat.ML"
"MosAIc: Finding Artistic Connections across Culture with Conditional Image Retrieval. We introduce MosAIc, an interactive web app that allows users to find pairs
of semantically related artworks that span different cultures, media, and
millennia. To create this application, we introduce Conditional Image Retrieval
(CIR) which combines visual similarity search with user supplied filters or
""conditions"". This technique allows one to find pairs of similar images that
span distinct subsets of the image corpus. We provide a generic way to adapt
existing image retrieval data-structures to this new domain and provide
theoretical bounds on our approach's efficiency. To quantify the performance of
CIR systems, we introduce new datasets for evaluating CIR methods and show that
CIR performs non-parametric style transfer. Finally, we demonstrate that our
CIR data-structures can identify ""blind spots"" in Generative Adversarial
Networks (GAN) where they fail to properly model the true data distribution.","cs.CV,cs.GR,cs.IR,cs.LG,stat.ML"
"No Fear of Heterogeneity: Classifier Calibration for Federated Learning with Non-IID Data. A central challenge in training classification models in the real-world
federated system is learning with non-IID data. To cope with this, most of the
existing works involve enforcing regularization in local optimization or
improving the model aggregation scheme at the server. Other works also share
public datasets or synthesized samples to supplement the training of
under-represented classes or introduce a certain level of personalization.
Though effective, they lack a deep understanding of how the data heterogeneity
affects each layer of a deep classification model. In this paper, we bridge
this gap by performing an experimental analysis of the representations learned
by different layers. Our observations are surprising: (1) there exists a
greater bias in the classifier than other layers, and (2) the classification
performance can be significantly improved by post-calibrating the classifier
after federated training. Motivated by the above findings, we propose a novel
and simple algorithm called Classifier Calibration with Virtual Representations
(CCVR), which adjusts the classifier using virtual representations sampled from
an approximated gaussian mixture model. Experimental results demonstrate that
CCVR achieves state-of-the-art performance on popular federated learning
benchmarks including CIFAR-10, CIFAR-100, and CINIC-10. We hope that our simple
yet effective method can shed some light on the future research of federated
learning with non-IID data.","cs.CV,cs.DC,cs.LG,stat.ML"
"Visualizing Graph Neural Networks with CorGIE: Corresponding a Graph to Its Embedding. Graph neural networks (GNNs) are a class of powerful machine learning tools
that model node relations for making predictions of nodes or links. GNN
developers rely on quantitative metrics of the predictions to evaluate a GNN,
but similar to many other neural networks, it is difficult for them to
understand if the GNN truly learns characteristics of a graph as expected. We
propose an approach to corresponding an input graph to its node embedding (aka
latent space), a common component of GNNs that is later used for prediction. We
abstract the data and tasks, and develop an interactive multi-view interface
called CorGIE to instantiate the abstraction. As the key function in CorGIE, we
propose the K-hop graph layout to show topological neighbors in hops and their
clustering structure. To evaluate the functionality and usability of CorGIE, we
present how to use CorGIE in two usage scenarios, and conduct a case study with
two GNN experts.","cs.HC,cs.LG"
"Linear Frequency Principle Model to Understand the Absence of Overfitting in Neural Networks. Why heavily parameterized neural networks (NNs) do not overfit the data is an
important long standing open question. We propose a phenomenological model of
the NN training to explain this non-overfitting puzzle. Our linear frequency
principle (LFP) model accounts for a key dynamical feature of NNs: they learn
low frequencies first, irrespective of microscopic details. Theory based on our
LFP model shows that low frequency dominance of target functions is the key
condition for the non-overfitting of NNs and is verified by experiments.
Furthermore, through an ideal two-layer NN, we unravel how detailed microscopic
NN training dynamics statistically gives rise to a LFP model with quantitative
prediction power.","cs.LG,physics.data-an"
"Supervised Topological Maps. Controlling the internal representation space of a neural network is a
desirable feature because it allows to generate new data in a supervised
manner. In this paper we will show how this can be achieved while building a
low-dimensional mapping of the input stream, by deriving a generalized
algorithm starting from Self Organizing Maps (SOMs). SOMs are a kind of neural
network which can be trained with unsupervised learning to produce a
low-dimensional discretized mapping of the input space. They can be used for
the generation of new data through backward propagation of interpolations made
from the mapping grid. Unfortunately the final topology of the mapping space of
a SOM is not known before learning, so interpolating new data in a supervised
way is not an easy task. Here we will show a variation from the SOM algorithm
consisting in constraining the update of prototypes so that it is also a
function of the distance of its prototypes from extrinsically given targets in
the mapping space. We will demonstrate how such variants, that we will call
Supervised Topological Maps (STMs), allow for a supervised mapping where the
position of internal representations in the mapping space is determined by the
experimenter. Controlling the internal representation space in STMs reveals to
be an easier task than what is currently done using other algorithms such as
variational or adversarial autoencoders.","68T07,I.5.1; I.5.3,cs.LG,cs.NE,stat.ML"
"Learning Curves for Deep Neural Networks: A Gaussian Field Theory Perspective. In the past decade, deep neural networks (DNNs) came to the fore as the
leading machine learning algorithms for a variety of tasks. Their raise was
founded on market needs and engineering craftsmanship, the latter based more on
trial and error than on theory. While still far behind the application
forefront, the theoretical study of DNNs has recently made important
advancements in analyzing the highly over-parameterized regime where some exact
results have been obtained. Leveraging these ideas and adopting a more
physics-like approach, here we construct a versatile field-theory formalism for
supervised deep learning, involving renormalization group, Feynman diagrams and
replicas. In particular we show that our approach leads to highly accurate
predictions of learning curves of truly deep DNNs trained on polynomial
regression tasks and that these predictions can be used for efficient
hyper-parameter optimization. In addition, they explain how DNNs generalize
well despite being highly over-parameterized, this due to an entropic bias to
simple functions which, for the case of fully-connected DNNs with data sampled
on the hypersphere, are low order polynomials in the input vector. Being a
complex interacting system of artificial neurons, we believe that such tools
and methodologies borrowed from condensed matter physics would prove essential
for obtaining an accurate quantitative understanding of deep learning.","cond-mat.stat-mech,cs.LG,cs.NE,physics.data-an,stat.ML"
"Federated Feature Selection for Cyber-Physical Systems of Systems. Autonomous systems generate a huge amount of multimodal data that are
collected and processed on the Edge, in order to enable AI-based services. The
collected datasets are pre-processed in order to extract informative
attributes, called features, which are used to feed AI algorithms. Due to the
limited computational and communication resources of some CPS, like autonomous
vehicles, selecting the subset of relevant features from a dataset is of the
utmost importance, in order to improve the result achieved by learning methods
and to reduce computation and communication costs. Precisely, feature selection
is the candidate approach, which assumes that data contain a certain number of
redundant or irrelevant attributes that can be eliminated. The quality of our
methods is confirmed by the promising results achieved on two different data
sets. In this work, we propose, for the first time, a federated feature
selection method suitable for being executed in a distributed manner.
Precisely, our results show that a fleet of autonomous vehicles finds a
consensus on the optimal set of features that they exploit to reduce data
transmission up to 99% with negligible information loss.","cs.LG,cs.NI"
"Consistency of semi-supervised learning algorithms on graphs: Probit and one-hot methods. Graph-based semi-supervised learning is the problem of propagating labels
from a small number of labelled data points to a larger set of unlabelled data.
This paper is concerned with the consistency of optimization-based techniques
for such problems, in the limit where the labels have small noise and the
underlying unlabelled data is well clustered. We study graph-based probit for
binary classification, and a natural generalization of this method to
multi-class classification using one-hot encoding. The resulting objective
function to be optimized comprises the sum of a quadratic form defined through
a rational function of the graph Laplacian, involving only the unlabelled data,
and a fidelity term involving only the labelled data. The consistency analysis
sheds light on the choice of the rational function defining the optimization.","62H30,68Q87,68T10,91C20,cs.LG,cs.NA,math.NA,math.OC,stat.ML"
"Adaptive template systems: Data-driven feature selection for learning with persistence diagrams. Feature extraction from persistence diagrams, as a tool to enrich machine
learning techniques, has received increasing attention in recent years. In this
paper we explore an adaptive methodology to localize features in persistent
diagrams, which are then used in learning tasks. Specifically, we investigate
three algorithms, CDER, GMM and HDBSCAN, to obtain adaptive template
functions/features. Said features are evaluated in three classification
experiments with persistence diagrams. Namely, manifold, human shapes and
protein classification. The main conclusion of our analysis is that adaptive
template systems, as a feature extraction technique, yield competitive and
often superior results in the studied examples. Moreover, from the adaptive
algorithms here studied, CDER consistently provides the most reliable and
robust adaptive featurization.","55N99 (Secondary),55U99 (Primary) 68W05,cs.CG,cs.LG,eess.IV,math.AT,stat.ML"
"StyleNAS: An Empirical Study of Neural Architecture Search to Uncover Surprisingly Fast End-to-End Universal Style Transfer Networks. Neural Architecture Search (NAS) has been widely studied for designing
discriminative deep learning models such as image classification, object
detection, and semantic segmentation. As a large number of priors have been
obtained through the manual design of architectures in the fields, NAS is
usually considered as a supplement approach. In this paper, we have
significantly expanded the application areas of NAS by performing an empirical
study of NAS to search generative models, or specifically, auto-encoder based
universal style transfer, which lacks systematic exploration, if any, from the
architecture search aspect. In our work, we first designed a search space where
common operators for image style transfer such as VGG-based encoders, whitening
and coloring transforms (WCT), convolution kernels, instance normalization
operators, and skip connections were searched in a combinatorial approach. With
a simple yet effective parallel evolutionary NAS algorithm with multiple
objectives, we derived the first group of end-to-end deep networks for
universal photorealistic style transfer. Comparing to random search, a NAS
method that is gaining popularity recently, we demonstrated that carefully
designed search strategy leads to much better architecture design. Finally
compared to existing universal style transfer networks for photorealistic
rendering such as PhotoWCT that stacks multiple well-trained auto-encoders and
WCT transforms in a non-end-to-end manner, the architectures designed by
StyleNAS produce better style-transferred images with details preserving, using
a tiny number of operators/parameters, and enjoying around 500x inference time
speed-up.","cs.CV,cs.GR,cs.LG"
"Loss Tolerant Federated Learning. Federated learning has attracted attention in recent years for
collaboratively training data on distributed devices with privacy-preservation.
The limited network capacity of mobile and IoT devices has been seen as one of
the major challenges for cross-device federated learning. Recent solutions have
been focusing on threshold-based client selection schemes to guarantee the
communication efficiency. However, we find this approach can cause biased
client selection and results in deteriorated performance. Moreover, we find
that the challenge of network limit may be overstated in some cases and the
packet loss is not always harmful. In this paper, we explore the loss tolerant
federated learning (LT-FL) in terms of aggregation, fairness, and
personalization. We use ThrowRightAway (TRA) to accelerate the data uploading
for low-bandwidth-devices by intentionally ignoring some packet losses. The
results suggest that, with proper integration, TRA and other algorithms can
together guarantee the personalization and fairness performance in the face of
packet loss below a certain fraction (10%-30%).","cs.AI,cs.LG,cs.NI"
"On the Convergence of Reinforcement Learning in Nonlinear Continuous State Space Problems. We consider the problem of Reinforcement Learning for nonlinear stochastic
dynamical systems. We show that in the RL setting, there is an inherent ``Curse
of Variance"" in addition to Bellman's infamous ``Curse of Dimensionality"", in
particular, we show that the variance in the solution grows
factorial-exponentially in the order of the approximation. A fundamental
consequence is that this precludes the search for anything other than ``local""
feedback solutions in RL, in order to control the explosive variance growth,
and thus, ensure accuracy. We further show that the deterministic optimal
control has a perturbation structure, in that the higher order terms do not
affect the calculation of lower order terms, which can be utilized in RL to get
accurate local solutions.","cs.LG,cs.SY,eess.SY"
"Exact Sampling from Determinantal Point Processes. Determinantal point processes (DPPs) are an important concept in random
matrix theory and combinatorics. They have also recently attracted interest in
the study of numerical methods for machine learning, as they offer an elegant
""missing link"" between independent Monte Carlo sampling and deterministic
evaluation on regular grids, applicable to a general set of spaces. This is
helpful whenever an algorithm explores to reduce uncertainty, such as in active
learning, Bayesian optimization, reinforcement learning, and marginalization in
graphical models. To draw samples from a DPP in practice, existing literature
focuses on approximate schemes of low cost, or comparably inefficient exact
algorithms like rejection sampling. We point out that, for many settings of
relevance to machine learning, it is also possible to draw exact samples from
DPPs on continuous domains. We start from an intuitive example on the real
line, which is then generalized to multivariate real vector spaces. We also
compare to previously studied approximations, showing that exact sampling,
despite higher cost, can be preferable where precision is needed.","cs.LG,math.PR,stat.ML"
"MolCLR: Molecular Contrastive Learning of Representations via Graph Neural Networks. Molecular machine learning bears promise for efficient molecule property
prediction and drug discovery. However, due to the limited labeled data and the
giant chemical space, machine learning models trained via supervised learning
perform poorly in generalization. This greatly limits the applications of
machine learning methods for molecular design and discovery. In this work, we
present MolCLR: Molecular Contrastive Learning of Representations via Graph
Neural Networks (GNNs), a self-supervised learning framework for large
unlabeled molecule datasets. Specifically, we first build a molecular graph,
where each node represents an atom and each edge represents a chemical bond. A
GNN is then used to encode the molecule graph. We propose three novel molecule
graph augmentations: atom masking, bond deletion, and subgraph removal. A
contrastive estimator is utilized to maximize the agreement of different graph
augmentations from the same molecule. Experiments show that molecule
representations learned by MolCLR can be transferred to multiple downstream
molecular property prediction tasks. Our method thus achieves state-of-the-art
performance on many challenging datasets. We also prove the efficiency of our
proposed molecule graph augmentations on supervised molecular classification
tasks.","cs.LG,physics.chem-ph"
"Forecasting the outcome of spintronic experiments with Neural Ordinary Differential Equations. Deep learning has an increasing impact to assist research, allowing, for
example, the discovery of novel materials. Until now, however, these artificial
intelligence techniques have fallen short of discovering the full differential
equation of an experimental physical system. Here we show that a dynamical
neural network, trained on a minimal amount of data, can predict the behavior
of spintronic devices with high accuracy and an extremely efficient simulation
time, compared to the micromagnetic simulations that are usually employed to
model them. For this purpose, we re-frame the formalism of Neural Ordinary
Differential Equations (ODEs) to the constraints of spintronics: few measured
outputs, multiple inputs and internal parameters. We demonstrate with
Spin-Neural ODEs an acceleration factor over 200 compared to micromagnetic
simulations for a complex problem -- the simulation of a reservoir computer
made of magnetic skyrmions (20 minutes compared to three days). In a second
realization, we show that we can predict the noisy response of experimental
spintronic nano-oscillators to varying inputs after training Spin-Neural ODEs
on five milliseconds of their measured response to different excitations.
Spin-Neural ODE is a disruptive tool for developing spintronic applications in
complement to micromagnetic simulations, which are time-consuming and cannot
fit experiments when noise or imperfections are present. Spin-Neural ODE can
also be generalized to other electronic devices involving dynamics.","cond-mat.mes-hall,cond-mat.mtrl-sci,cs.LG,cs.NA,math.NA,physics.data-an"
"Compositional Federated Learning: Applications in Distributionally Robust Averaging and Meta Learning. In the paper, we propose an effective and efficient Compositional Federated
Learning (ComFedL) algorithm for solving a new compositional Federated Learning
(FL) framework, which frequently appears in many machine learning problems with
a hierarchical structure such as distributionally robust federated learning and
model-agnostic meta learning (MAML). Moreover, we study the convergence
analysis of our ComFedL algorithm under some mild conditions, and prove that it
achieves a fast convergence rate of $O(\frac{1}{\sqrt{T}})$, where $T$ denotes
the number of iteration. To the best of our knowledge, our algorithm is the
first work to bridge federated learning with composition stochastic
optimization. In particular, we first transform the distributionally robust FL
(i.e., a minimax optimization problem) into a simple composition optimization
problem by using KL divergence regularization. At the same time, we also first
transform the distribution-agnostic MAML problem (i.e., a minimax optimization
problem) into a simple composition optimization problem. Finally, we apply two
popular machine learning tasks, i.e., distributionally robust FL and MAML to
demonstrate the effectiveness of our algorithm.","cs.DC,cs.LG,math.OC"
"Repository for Reusing Artifacts of Artificial Neural Networks. Artificial Neural Networks (ANNs) replaced conventional software systems in
various domains such as machine translation, natural language processing, and
image processing. So, why do we need an repository for artificial neural
networks? Those systems are developed with labeled data and we have strong
dependencies between the data that is used for training and testing our
network. Another challenge is the data quality as well as reuse-ability. There
we are trying to apply concepts from classic software engineering that is not
limited to the model, while data and code haven't been dealt with mostly in
other projects. The first question that comes to mind might be, why don't we
use GitHub, a well known widely spread tool for reuse, for our issue. And the
reason why is that GitHub, although very good in its class is not developed for
machine learning appliances and focuses more on software reuse. In addition to
that GitHub does not allow to execute the code directly on the platform which
would be very convenient for collaborative work on one project.","cs.LG,cs.SE"
"Personalized Cross-Silo Federated Learning on Non-IID Data. Non-IID data present a tough challenge for federated learning. In this paper,
we explore a novel idea of facilitating pairwise collaborations between clients
with similar data. We propose FedAMP, a new method employing federated
attentive message passing to facilitate similar clients to collaborate more. We
establish the convergence of FedAMP for both convex and non-convex models, and
propose a heuristic method to further improve the performance of FedAMP when
clients adopt deep neural networks as personalized models. Our extensive
experiments on benchmark data sets demonstrate the superior performance of the
proposed methods.","cs.DC,cs.LG,stat.ML"
"Interaction Matters: A Note on Non-asymptotic Local Convergence of Generative Adversarial Networks. Motivated by the pursuit of a systematic computational and algorithmic
understanding of Generative Adversarial Networks (GANs), we present a simple
yet unified non-asymptotic local convergence theory for smooth two-player
games, which subsumes several discrete-time gradient-based saddle point
dynamics. The analysis reveals the surprising nature of the off-diagonal
interaction term as both a blessing and a curse. On the one hand, this
interaction term explains the origin of the slow-down effect in the convergence
of Simultaneous Gradient Ascent (SGA) to stable Nash equilibria. On the other
hand, for the unstable equilibria, exponential convergence can be proved thanks
to the interaction term, for four modified dynamics proposed to stabilize GAN
training: Optimistic Mirror Descent (OMD), Consensus Optimization (CO),
Implicit Updates (IU) and Predictive Method (PM). The analysis uncovers the
intimate connections among these stabilizing techniques, and provides detailed
characterization on the choice of learning rate. As a by-product, we present a
new analysis for OMD proposed in Daskalakis, Ilyas, Syrgkanis, and Zeng [2017]
with improved rates.","cs.GT,cs.LG,stat.ML"
"Pay Attention to Evolution: Time Series Forecasting with Deep Graph-Evolution Learning. Time-series forecasting is one of the most active research topics in
artificial intelligence. Applications in real-world time series should consider
two factors for achieving reliable predictions: modeling dynamic dependencies
among multiple variables and adjusting the model's intrinsic hyperparameters. A
still open gap in that literature is that statistical and ensemble learning
approaches systematically present lower predictive performance than deep
learning methods. They generally disregard the data sequence aspect entangled
with multivariate data represented in more than one time series. Conversely,
this work presents a novel neural network architecture for time-series
forecasting that combines the power of graph evolution with deep recurrent
learning on distinct data distributions; we named our method Recurrent Graph
Evolution Neural Network (ReGENN). The idea is to infer multiple multivariate
relationships between co-occurring time-series by assuming that the temporal
data depends not only on inner variables and intra-temporal relationships
(i.e., observations from itself) but also on outer variables and inter-temporal
relationships (i.e., observations from other-selves). An extensive set of
experiments was conducted comparing ReGENN with dozens of ensemble methods and
classical statistical ones, showing sound improvement of up to 64.87% over the
competing algorithms. Furthermore, we present an analysis of the intermediate
weights arising from ReGENN, showing that by looking at inter and
intra-temporal relationships simultaneously, time-series forecasting is majorly
improved if paying attention to how multiple multivariate data synchronously
evolve.","37M10,68T05,68T07,68T37,82C32,I.2; I.5; I.2.4; I.2.6; I.5.1,cs.AI,cs.LG,cs.NE,stat.ML"
"Efficient Video Summarization Framework using EEG and Eye-tracking Signals. This paper proposes an efficient video summarization framework that will give
a gist of the entire video in a few key-frames or video skims. Existing video
summarization frameworks are based on algorithms that utilize computer vision
low-level feature extraction or high-level domain level extraction. However,
being the ultimate user of the summarized video, humans remain the most
neglected aspect. Therefore, the proposed paper considers human's role in
summarization and introduces human visual attention-based summarization
techniques. To understand human attention behavior, we have designed and
performed experiments with human participants using electroencephalogram (EEG)
and eye-tracking technology. The EEG and eye-tracking data obtained from the
experimentation are processed simultaneously and used to segment frames
containing useful information from a considerable video volume. Thus, the frame
segmentation primarily relies on the cognitive judgments of human beings. Using
our approach, a video is summarized by 96.5% while maintaining higher precision
and high recall factors. The comparison with the state-of-the-art techniques
demonstrates that the proposed approach yields ceiling-level performance with
reduced computational cost in summarising the videos.","cs.CV,cs.HC,cs.NE,q-bio.NC"
"Discrete-to-Deep Supervised Policy Learning. Neural networks are effective function approximators, but hard to train in
the reinforcement learning (RL) context mainly because samples are correlated.
For years, scholars have got around this by employing experience replay or an
asynchronous parallel-agent system. This paper proposes Discrete-to-Deep
Supervised Policy Learning (D2D-SPL) for training neural networks in RL.
D2D-SPL discretises the continuous state space into discrete states and uses
actor-critic to learn a policy. It then selects from each discrete state an
input value and the action with the highest numerical preference as an
input/target pair. Finally it uses input/target pairs from all discrete states
to train a classifier. D2D-SPL uses a single agent, needs no experience replay
and learns much faster than state-of-the-art methods. We test our method with
two RL environments, the Cartpole and an aircraft manoeuvring simulator.","68T05,I.2.6,cs.LG,stat.ML"
"Topological Attention for Time Series Forecasting. The problem of (point) forecasting $ \textit{univariate} $ time series is
considered. Most approaches, ranging from traditional statistical methods to
recent learning-based techniques with neural networks, directly operate on raw
time series observations. As an extension, we study whether $\textit{local
topological properties}$, as captured via persistent homology, can serve as a
reliable signal that provides complementary information for learning to
forecast. To this end, we propose $\textit{topological attention}$, which
allows attending to local topological features within a time horizon of
historical data. Our approach easily integrates into existing end-to-end
trainable forecasting models, such as $\texttt{N-BEATS}$, and in combination
with the latter exhibits state-of-the-art performance on the large-scale M4
benchmark dataset of 100,000 diverse time series from different domains.
Ablation experiments, as well as a comparison to a broad range of forecasting
methods in a setting where only a single time series is available for training,
corroborate the beneficial nature of including local topological information
through an attention mechanism.","cs.LG,stat.ML"
"Sample-Efficient Reinforcement Learning for Linearly-Parameterized MDPs with a Generative Model. The curse of dimensionality is a widely known issue in reinforcement learning
(RL). In the tabular setting where the state space $\mathcal{S}$ and the action
space $\mathcal{A}$ are both finite, to obtain a nearly optimal policy with
sampling access to a generative model, the minimax optimal sample complexity
scales linearly with $|\mathcal{S}|\times|\mathcal{A}|$, which can be
prohibitively large when $\mathcal{S}$ or $\mathcal{A}$ is large. This paper
considers a Markov decision process (MDP) that admits a set of state-action
features, which can linearly express (or approximate) its probability
transition kernel. We show that a model-based approach (resp.$~$Q-learning)
provably learns an $\varepsilon$-optimal policy (resp.$~$Q-function) with high
probability as soon as the sample size exceeds the order of
$\frac{K}{(1-\gamma)^{3}\varepsilon^{2}}$
(resp.$~$$\frac{K}{(1-\gamma)^{4}\varepsilon^{2}}$), up to some logarithmic
factor. Here $K$ is the feature dimension and $\gamma\in(0,1)$ is the discount
factor of the MDP. Both sample complexity bounds are provably tight, and our
result for the model-based approach matches the minimax lower bound. Our
results show that for arbitrarily large-scale MDP, both the model-based
approach and Q-learning are sample-efficient when $K$ is relatively small, and
hence the title of this paper.","cs.IT,cs.LG,math.IT,math.OC,math.ST,stat.ML,stat.TH"
"DRUM: End-To-End Differentiable Rule Mining On Knowledge Graphs. In this paper, we study the problem of learning probabilistic logical rules
for inductive and interpretable link prediction. Despite the importance of
inductive link prediction, most previous works focused on transductive link
prediction and cannot manage previously unseen entities. Moreover, they are
black-box models that are not easily explainable for humans. We propose DRUM, a
scalable and differentiable approach for mining first-order logical rules from
knowledge graphs which resolves these problems. We motivate our method by
making a connection between learning confidence scores for each rule and
low-rank tensor approximation. DRUM uses bidirectional RNNs to share useful
information across the tasks of learning rules for different relations. We also
empirically demonstrate the efficiency of DRUM over existing rule mining
methods for inductive link prediction on a variety of benchmark datasets.","cs.LG,cs.LO,stat.ML"
"Learning in Matrix Games can be Arbitrarily Complex. A growing number of machine learning architectures, such as Generative
Adversarial Networks, rely on the design of games which implement a desired
functionality via a Nash equilibrium. In practice these games have an implicit
complexity (e.g. from underlying datasets and the deep networks used) that
makes directly computing a Nash equilibrium impractical or impossible. For this
reason, numerous learning algorithms have been developed with the goal of
iteratively converging to a Nash equilibrium. Unfortunately, the dynamics
generated by the learning process can be very intricate and instances of
training failure hard to interpret. In this paper we show that, in a strong
sense, this dynamic complexity is inherent to games. Specifically, we prove
that replicator dynamics, the continuous-time analogue of Multiplicative
Weights Update, even when applied in a very restricted class of games -- known
as finite matrix games -- is rich enough to be able to approximate arbitrary
dynamical systems. Our results are positive in the sense that they show the
nearly boundless dynamic modelling capabilities of current machine learning
practices, but also negative in implying that these capabilities may come at
the cost of interpretability. As a concrete example, we show how replicator
dynamics can effectively reproduce the well-known strange attractor of Lonrenz
dynamics (the ""butterfly effect"") while achieving no regret.","cs.GT,cs.LG,math.DS,nlin.CD"
"Learning Unsplit-field-based PML for the FDTD Method by Deep Differentiable Forest. Alternative unsplit-filed-based absorbing boundary condition (ABC)
computation approach for the finite-difference time-domain (FDTD) is
efficiently proposed based on the deep differentiable forest. The deep
differentiable forest (DDF) model is introduced to replace the conventional
perfectly matched layer (PML) ABC during the computation process of FDTD. The
field component data on the interface of traditional PML are adopted to train
the DDF-based PML model. DDF has the advantages of both trees and neural
networks. Its tree structure is easy to use and explain for the numerical PML
data. It has full differentiability like neural networks. DDF could be trained
by powerful techniques from deep learning. So compared to the traditional PML
implementation, the proposed method can greatly reduce the size of FDTD
physical domain and the calculation complexity of FDTD due to the novel model
which only involves the one-cell thickness of boundary layer. Numerical
simulations have been carried out to benchmark the performance of the proposed
approach. Numerical results illustrate that the proposed method can not only
easily replace the traditional PML, but also be integrated into the FDTD
computation process with satisfactory numerical accuracy and compatibility to
the FDTD.",cs.LG
"Global Neighbor Sampling for Mixed CPU-GPU Training on Giant Graphs. Graph neural networks (GNNs) are powerful tools for learning from graph data
and are widely used in various applications such as social network
recommendation, fraud detection, and graph search. The graphs in these
applications are typically large, usually containing hundreds of millions of
nodes. Training GNN models on such large graphs efficiently remains a big
challenge. Despite a number of sampling-based methods have been proposed to
enable mini-batch training on large graphs, these methods have not been proved
to work on truly industry-scale graphs, which require GPUs or mixed-CPU-GPU
training. The state-of-the-art sampling-based methods are usually not optimized
for these real-world hardware setups, in which data movement between CPUs and
GPUs is a bottleneck. To address this issue, we propose Global Neighborhood
Sampling that aims at training GNNs on giant graphs specifically for
mixed-CPU-GPU training. The algorithm samples a global cache of nodes
periodically for all mini-batches and stores them in GPUs. This global cache
allows in-GPU importance sampling of mini-batches, which drastically reduces
the number of nodes in a mini-batch, especially in the input layer, to reduce
data copy between CPU and GPU and mini-batch computation without compromising
the training convergence rate or model accuracy. We provide a highly efficient
implementation of this method and show that our implementation outperforms an
efficient node-wise neighbor sampling baseline by a factor of 2X-4X on giant
graphs. It outperforms an efficient implementation of LADIES with small layers
by a factor of 2X-14X while achieving much higher accuracy than LADIES.We also
theoretically analyze the proposed algorithm and show that with cached node
data of a proper size, it enjoys a comparable convergence rate as the
underlying node-wise sampling method.","cs.DC,cs.LG"
"Computing Valid p-values for Image Segmentation by Selective Inference. Image segmentation is one of the most fundamental tasks of computer vision.
In many practical applications, it is essential to properly evaluate the
reliability of individual segmentation results. In this study, we propose a
novel framework to provide the statistical significance of segmentation results
in the form of p-values. Specifically, we consider a statistical hypothesis
test for determining the difference between the object and the background
regions. This problem is challenging because the difference can be deceptively
large (called segmentation bias) due to the adaptation of the segmentation
algorithm to the data. To overcome this difficulty, we introduce a statistical
approach called selective inference, and develop a framework to compute valid
p-values in which the segmentation bias is properly accounted for. Although the
proposed framework is potentially applicable to various segmentation
algorithms, we focus in this paper on graph cut-based and threshold-based
segmentation algorithms, and develop two specific methods to compute valid
p-values for the segmentation results obtained by these algorithms. We prove
the theoretical validity of these two methods and demonstrate their
practicality by applying them to segmentation problems for medical images.","cs.CV,cs.LG,math.ST,stat.ML,stat.TH"
"Fair Representations by Compression. Organizations that collect and sell data face increasing scrutiny for the
discriminatory use of data. We propose a novel unsupervised approach to
transform data into a compressed binary representation independent of sensitive
attributes. We show that in an information bottleneck framework, a parsimonious
representation should filter out information related to sensitive attributes if
they are provided directly to the decoder. Empirical results show that the
proposed method, \textbf{FBC}, achieves state-of-the-art accuracy-fairness
trade-off. Explicit control of the entropy of the representation bit stream
allows the user to move smoothly and simultaneously along both rate-distortion
and rate-fairness curves. \end{abstract}","cs.CY,cs.LG"
"Robust Representation Learning via Perceptual Similarity Metrics. A fundamental challenge in artificial intelligence is learning useful
representations of data that yield good performance on a downstream task,
without overfitting to spurious input features. Extracting such task-relevant
predictive information is particularly difficult for real-world datasets. In
this work, we propose Contrastive Input Morphing (CIM), a representation
learning framework that learns input-space transformations of the data to
mitigate the effect of irrelevant input features on downstream performance. Our
method leverages a perceptual similarity metric via a triplet loss to ensure
that the transformation preserves task-relevant information.Empirically, we
demonstrate the efficacy of our approach on tasks which typically suffer from
the presence of spurious correlations: classification with nuisance
information, out-of-distribution generalization, and preservation of subgroup
accuracies. We additionally show that CIM is complementary to other mutual
information-based representation learning techniques, and demonstrate that it
improves the performance of variational information bottleneck (VIB) when used
together.","cs.CV,cs.LG"
"Topology-Aware Segmentation Using Discrete Morse Theory. In the segmentation of fine-scale structures from natural and biomedical
images, per-pixel accuracy is not the only metric of concern. Topological
correctness, such as vessel connectivity and membrane closure, is crucial for
downstream analysis tasks. In this paper, we propose a new approach to train
deep image segmentation networks for better topological accuracy. In
particular, leveraging the power of discrete Morse theory (DMT), we identify
global structures, including 1D skeletons and 2D patches, which are important
for topological accuracy. Trained with a novel loss based on these global
structures, the network performance is significantly improved especially near
topologically challenging locations (such as weak spots of connections and
membranes). On diverse datasets, our method achieves superior performance on
both the DICE score and topological metrics.","cs.CG,cs.CV"
"Image Captioning using Deep Stacked LSTMs, Contextual Word Embeddings and Data Augmentation. Image Captioning, or the automatic generation of descriptions for images, is
one of the core problems in Computer Vision and has seen considerable progress
using Deep Learning Techniques. We propose to use Inception-ResNet
Convolutional Neural Network as encoder to extract features from images,
Hierarchical Context based Word Embeddings for word representations and a Deep
Stacked Long Short Term Memory network as decoder, in addition to using Image
Data Augmentation to avoid over-fitting. For data Augmentation, we use
Horizontal and Vertical Flipping in addition to Perspective Transformations on
the images. We evaluate our proposed methods with two image captioning
frameworks- Encoder-Decoder and Soft Attention. Evaluation on widely used
metrics have shown that our approach leads to considerable improvement in model
performance.","cs.AI,cs.CV,cs.LG,cs.MM,cs.NE"
"Predicting student performance using data from an auto-grading system. As online auto-grading systems appear, information obtained from those
systems can potentially enable researchers to create predictive models to
predict student behaviour and performances. In the University of Waterloo, the
ECE 150 (Fundamentals of Programming) Instructional Team wants to get an
insight into how to allocate the limited teaching resources better to achieve
improved educational outcomes. Currently, the Instructional Team allocates
tutoring time in a reactive basis. They help students ""as-requested"". This
approach serves those students with the wherewithal to request help; however,
many of the students who are struggling do not reach out for assistance.
Therefore, we, as the Research Team, want to explore if we can determine
students which need help by looking into the data from our auto-grading system,
Marmoset.
  In this paper, we conducted experiments building decision-tree and
linear-regression models with various features extracted from the Marmoset
auto-grading system, including passing rate, testcase outcomes, number of
submissions and submission time intervals (the time interval between the
student's first reasonable submission and the deadline). For each feature, we
interpreted the result at the confusion matrix level. Specifically for
poor-performance students, we show that the linear-regression model using
submission time intervals performs the best among all models in terms of
Precision and F-Measure. We also show that for students who are misclassified
into poor-performance students, they have the lowest actual grades in the
linear-regression model among all models. In addition, we show that for the
midterm, the submission time interval of the last assignment before the midterm
predicts the midterm performance the most. However, for the final exam, the
midterm performance contributes the most on the final exam performance.","cs.CY,cs.LG"
"Modeling the Complex Dynamics and Changing Correlations of Epileptic Events. Patients with epilepsy can manifest short, sub-clinical epileptic ""bursts"" in
addition to full-blown clinical seizures. We believe the relationship between
these two classes of events---something not previously studied
quantitatively---could yield important insights into the nature and intrinsic
dynamics of seizures. A goal of our work is to parse these complex epileptic
events into distinct dynamic regimes. A challenge posed by the intracranial EEG
(iEEG) data we study is the fact that the number and placement of electrodes
can vary between patients. We develop a Bayesian nonparametric Markov switching
process that allows for (i) shared dynamic regimes between a variable number of
channels, (ii) asynchronous regime-switching, and (iii) an unknown dictionary
of dynamic regimes. We encode a sparse and changing set of dependencies between
the channels using a Markov-switching Gaussian graphical model for the
innovations process driving the channel dynamics and demonstrate the importance
of this model in parsing and out-of-sample predictions of iEEG data. We show
that our model produces intuitive state assignments that can help automate
clinical analysis of seizures and enable the comparison of sub-clinical bursts
and full clinical seizures.","q-bio.NC,stat.AP,stat.ML"
"ISP Distillation. Nowadays, many of the images captured are ""observed"" by machines only and not
by humans, for example, robots' or autonomous cars' cameras. High-level machine
vision models, such as object recognition, assume images are transformed to
some canonical image space by the camera ISP. However, the camera ISP is
optimized for producing visually pleasing images to human observers and not for
machines, thus, one may spare the ISP compute time and apply the vision models
directly to the raw data. Yet, it has been shown that training such models
directly on the RAW images results in a performance drop. To mitigate this drop
in performance (without the need to annotate RAW data), we use a dataset of RAW
and RGB image pairs, which can be easily acquired with no human labeling. We
then train a model that is applied directly to the RAW data by using knowledge
distillation such that the model predictions for RAW images will be aligned
with the predictions of an off-the-shelf pre-trained model for processed RGB
images. Our experiments show that our performance on RAW images is
significantly better than a model trained on labeled RAW images. It also
reasonably matches the predictions of a pre-trained model on processed RGB
images, while saving the ISP compute overhead.","cs.AI,cs.CV"
"3DStyleNet: Creating 3D Shapes with Geometric and Texture Style Variations. We propose a method to create plausible geometric and texture style
variations of 3D objects in the quest to democratize 3D content creation. Given
a pair of textured source and target objects, our method predicts a part-aware
affine transformation field that naturally warps the source shape to imitate
the overall geometric style of the target. In addition, the texture style of
the target is transferred to the warped source object with the help of a
multi-view differentiable renderer. Our model, 3DStyleNet, is composed of two
sub-networks trained in two stages. First, the geometric style network is
trained on a large set of untextured 3D shapes. Second, we jointly optimize our
geometric style network and a pre-trained image style transfer network with
losses defined over both the geometry and the rendering of the result. Given a
small set of high-quality textured objects, our method can create many novel
stylized shapes, resulting in effortless 3D content creation and style-ware
data augmentation. We showcase our approach qualitatively on 3D content
stylization, and provide user studies to validate the quality of our results.
In addition, our method can serve as a valuable tool to create 3D data
augmentations for computer vision tasks. Extensive quantitative analysis shows
that 3DStyleNet outperforms alternative data augmentation techniques for the
downstream task of single-image 3D reconstruction.","cs.AI,cs.CV,cs.GR"
"Surface Type Estimation from GPS Tracked Bicycle Activities. Road conditions affect both machine and human powered modes of
transportation. In the case of human powered transportation, poor road
conditions increase the work for the individual to travel. Previous estimates
for these parameters have used computationally expensive analysis of satellite
images. In this work, we use a computationally inexpensive and simple method by
using only GPS data from a human powered cyclist. By estimating if the road
taken by the user has high or low variations in their directional vector, we
classify if the user is on a paved road or on an unpaved trail. In order to do
this, three methods were adopted, changes in frequency of the direction of
slope in a given path segment, fitting segments of the path, and finding the
first derivative and the number of points of zero crossings of each segment.
Machine learning models such as support vector machines, K-nearest neighbors,
and decision trees were used for the classification of the path. We show in our
methods, the decision trees performed the best with an accuracy of 86\%.
Estimation of the type of surface can be used for many applications such as
understanding rolling resistance for power estimation estimation or building
exercise recommendation systems by user profiling as described in detail in the
paper.","cs.CV,cs.HC,cs.LG"
"Linear Last-iterate Convergence in Constrained Saddle-point Optimization. Optimistic Gradient Descent Ascent (OGDA) and Optimistic Multiplicative
Weights Update (OMWU) for saddle-point optimization have received growing
attention due to their favorable last-iterate convergence. However, their
behaviors for simple bilinear games over the probability simplex are still not
fully understood - previous analysis lacks explicit convergence rates, only
applies to an exponentially small learning rate, or requires additional
assumptions such as the uniqueness of the optimal solution. In this work, we
significantly expand the understanding of last-iterate convergence for OGDA and
OMWU in the constrained setting. Specifically, for OMWU in bilinear games over
the simplex, we show that when the equilibrium is unique, linear last-iterate
convergence is achieved with a learning rate whose value is set to a universal
constant, improving the result of (Daskalakis & Panageas, 2019b) under the same
assumption. We then significantly extend the results to more general objectives
and feasible sets for the projected OGDA algorithm, by introducing a sufficient
condition under which OGDA exhibits concrete last-iterate convergence rates
with a constant learning rate whose value only depends on the smoothness of the
objective function. We show that bilinear games over any polytope satisfy this
condition and OGDA converges exponentially fast even without the unique
equilibrium assumption. Our condition also holds for
strongly-convex-strongly-concave functions, recovering the result of (Hsieh et
al., 2019). Finally, we provide experimental results to further support our
theory.","cs.GT,cs.LG,stat.ML"
"Reformer: The Efficient Transformer. Large Transformer models routinely achieve state-of-the-art results on a
number of tasks but training these models can be prohibitively costly,
especially on long sequences. We introduce two techniques to improve the
efficiency of Transformers. For one, we replace dot-product attention by one
that uses locality-sensitive hashing, changing its complexity from O($L^2$) to
O($L\log L$), where $L$ is the length of the sequence. Furthermore, we use
reversible residual layers instead of the standard residuals, which allows
storing activations only once in the training process instead of $N$ times,
where $N$ is the number of layers. The resulting model, the Reformer, performs
on par with Transformer models while being much more memory-efficient and much
faster on long sequences.","cs.CL,cs.LG,stat.ML"
"Potential-Based Advice for Stochastic Policy Learning. This paper augments the reward received by a reinforcement learning agent
with potential functions in order to help the agent learn (possibly stochastic)
optimal policies. We show that a potential-based reward shaping scheme is able
to preserve optimality of stochastic policies, and demonstrate that the ability
of an agent to learn an optimal policy is not affected when this scheme is
augmented to soft Q-learning. We propose a method to impart potential based
advice schemes to policy gradient algorithms. An algorithm that considers an
advantage actor-critic architecture augmented with this scheme is proposed, and
we give guarantees on its convergence. Finally, we evaluate our approach on a
puddle-jump grid world with indistinguishable states, and the continuous state
and action mountain car environment from classical control. Our results
indicate that these schemes allow the agent to learn a stochastic optimal
policy faster and obtain a higher average reward.","cs.AI,cs.LG,cs.SY,eess.SY,stat.ML"
"Attributed Sequence Embedding. Mining tasks over sequential data, such as clickstreams and gene sequences,
require a careful design of embeddings usable by learning algorithms. Recent
research in feature learning has been extended to sequential data, where each
instance consists of a sequence of heterogeneous items with a variable length.
However, many real-world applications often involve attributed sequences, where
each instance is composed of both a sequence of categorical items and a set of
attributes. In this paper, we study this new problem of attributed sequence
embedding, where the goal is to learn the representations of attributed
sequences in an unsupervised fashion. This problem is core to many important
data mining tasks ranging from user behavior analysis to the clustering of gene
sequences. This problem is challenging due to the dependencies between
sequences and their associated attributes. We propose a deep multimodal
learning framework, called NAS, to produce embeddings of attributed sequences.
The embeddings are task independent and can be used on various mining tasks of
attributed sequences. We demonstrate the effectiveness of our embeddings of
attributed sequences in various unsupervised learning tasks on real-world
datasets.","cs.CL,cs.DB,cs.LG,stat.ML"
"Fundamental Limits of Deep Graph Convolutional Networks. Graph convolutional networks (GCNs) are a widely used method for graph
representation learning. To elucidate the capabilities and limitations of GCNs,
we investigate their power, as a function of their number of layers, to
distinguish between different random graph models (corresponding to different
class-conditional distributions in a classification problem) on the basis of
the embeddings of their sample graphs. In particular, the graph models that we
consider arise from graphons, which are the most general possible
parameterizations of infinite exchangeable graph models and which are the
central objects of study in the theory of dense graph limits. We give a precise
characterization of the set of pairs of graphons that are indistinguishable by
a GCN with nonlinear activation functions coming from a certain broad class if
its depth is at least logarithmic in the size of the sample graph. This
characterization is in terms of a degree profile closeness property. Outside
this class, a very simple GCN architecture suffices for distinguishability. We
then exhibit a concrete, infinite class of graphons arising from stochastic
block models that are well-separated in terms of cut distance and are
indistinguishable by a GCN. These results theoretically match empirical
observations of several prior works. To prove our results, we exploit a
connection to random walks on graphs. Finally, we give empirical results on
synthetic and real graph classification datasets, indicating that
indistinguishable graph distributions arise in practice.","cs.IT,cs.LG,math.IT,math.PR,stat.ML"
"Learning Explainable Interventions to Mitigate HIV Transmission in Sex Workers Across Five States in India. Female sex workers(FSWs) are one of the most vulnerable and stigmatized
groups in society. As a result, they often suffer from a lack of quality access
to care. Grassroot organizations engaged in improving health services are often
faced with the challenge of improving the effectiveness of interventions due to
complex influences. This work combines structure learning, discriminative
modeling, and grass-root level expertise of designing interventions across five
different Indian states to discover the influence of non-obvious factors for
improving safe-sex practices in FSWs. A bootstrapped, ensemble-averaged
Bayesian Network structure was learned to quantify the factors that could
maximize condom usage as revealed from the model. A discriminative model was
then constructed using XgBoost and random forest in order to predict condom use
behavior The best model achieved 83% sensitivity, 99% specificity, and 99% area
under the precision-recall curve for the prediction. Both generative and
discriminative modeling approaches revealed that financial literacy training
was the primary influence and predictor of condom use in FSWs. These insights
have led to a currently ongoing field trial for assessing the real-world
utility of this approach. Our work highlights the potential of explainable
models for transparent discovery and prioritization of anti-HIV interventions
in female sex workers in a resource-limited setting.","cs.CY,cs.LG"
"Feature Learning in Infinite-Width Neural Networks. As its width tends to infinity, a deep neural network's behavior under
gradient descent can become simplified and predictable (e.g. given by the
Neural Tangent Kernel (NTK)), if it is parametrized appropriately (e.g. the NTK
parametrization). However, we show that the standard and NTK parametrizations
of a neural network do not admit infinite-width limits that can learn features,
which is crucial for pretraining and transfer learning such as with BERT. We
propose simple modifications to the standard parametrization to allow for
feature learning in the limit. Using the *Tensor Programs* technique, we derive
explicit formulas for such limits. On Word2Vec and few-shot learning on
Omniglot via MAML, two canonical tasks that rely crucially on feature learning,
we compute these limits exactly. We find that they outperform both NTK
baselines and finite-width networks, with the latter approaching the
infinite-width feature learning performance as width increases.
  More generally, we classify a natural space of neural network
parametrizations that generalizes standard, NTK, and Mean Field
parametrizations. We show 1) any parametrization in this space either admits
feature learning or has an infinite-width training dynamics given by kernel
gradient descent, but not both; 2) any such infinite-width limit can be
computed using the Tensor Programs technique. Code for our experiments can be
found at github.com/edwardjhu/TP4.","cond-mat.dis-nn,cs.LG,cs.NE"
"Vision-Based Navigation II: Error Analysis for a Navigation Algorithm based on Optical-Flow and a Digital Terrain Map. The paper deals with the error analysis of a navigation algorithm that uses
as input a sequence of images acquired by a moving camera and a Digital Terrain
Map (DTM) of the region been imaged by the camera during the motion. The main
sources of error are more or less straightforward to identify: camera
resolution, structure of the observed terrain and DTM accuracy, field of view
and camera trajectory. After characterizing and modeling these error sources in
the framework of the CDTM algorithm, a closed form expression for their effect
on the pose and motion errors of the camera can be found. The analytic
expression provides a priori measurements for the accuracy in terms of the
parameters mentioned above.","68T45,E.5; E.4; E.2; H.1.1; F.1.1; F.1.3,cs.AI,cs.CV"
"Cross-conformal e-prediction. This note discusses a simple modification of cross-conformal prediction
inspired by recent work on e-values. The precursor of conformal prediction
developed in the 1990s by Gammerman, Vapnik, and Vovk was also based on
e-values and is called conformal e-prediction in this note. Replacing e-values
by p-values led to conformal prediction, which has important advantages over
conformal e-prediction without obvious disadvantages. The situation with
cross-conformal prediction is, however, different: whereas for cross-conformal
prediction validity is only an empirical fact (and can be broken with excessive
randomization), this note draws the reader's attention to the obvious fact that
cross-conformal e-prediction enjoys a guaranteed property of validity.","68T05,cs.LG,stat.ML"
"CoPhy-PGNN: Learning Physics-guided Neural Networks with Competing Loss Functions for Solving Eigenvalue Problems. Physics-guided Neural Networks (PGNNs) represent an emerging class of neural
networks that are trained using physics-guided (PG) loss functions (capturing
violations in network outputs with known physics), along with the supervision
contained in data. Existing work in PGNNs have demonstrated the efficacy of
adding single PG loss functions in the neural network objectives, using
constant trade-off parameters, to ensure better generalizability. However, in
the presence of multiple physics loss functions with competing gradient
directions, there is a need to adaptively tune the contribution of competing PG
loss functions during the course of training to arrive at generalizable
solutions. We demonstrate the presence of competing PG losses in the generic
neural network problem of solving for the lowest (or highest) eigenvector of a
physics-based eigenvalue equation, common to many scientific problems. We
present a novel approach to handle competing PG losses and demonstrate its
efficacy in learning generalizable solutions in two motivating applications of
quantum mechanics and electromagnetic propagation. All the code and data used
in this work is available at https://github.com/jayroxis/Cophy-PGNN.","cs.LG,physics.comp-ph,quant-ph,stat.ML"
"Improving Gibbs Sampler Scan Quality with DoGS. The pairwise influence matrix of Dobrushin has long been used as an
analytical tool to bound the rate of convergence of Gibbs sampling. In this
work, we use Dobrushin influence as the basis of a practical tool to certify
and efficiently improve the quality of a discrete Gibbs sampler. Our
Dobrushin-optimized Gibbs samplers (DoGS) offer customized variable selection
orders for a given sampling budget and variable subset of interest, explicit
bounds on total variation distance to stationarity, and certifiable
improvements over the standard systematic and uniform random scan Gibbs
samplers. In our experiments with joint image segmentation and object
recognition, Markov chain Monte Carlo maximum likelihood estimation, and Ising
model inference, DoGS consistently deliver higher-quality inferences with
significantly smaller sampling budgets than standard Gibbs samplers.","cs.LG,math.PR,stat.ME,stat.ML"
"Single-Photon Image Classification. Quantum computing-based machine learning mainly focuses on quantum computing
hardware that is experimentally challenging to realize due to requiring quantum
gates that operate at very low temperature. Instead, we demonstrate the
existence of a lower performance and much lower effort island on the
accuracy-vs-qubits graph that may well be experimentally accessible with room
temperature optics. This high temperature ""quantum computing toy model"" is
nevertheless interesting to study as it allows rather accessible explanations
of key concepts in quantum computing, in particular interference, entanglement,
and the measurement process.
  We specifically study the problem of classifying an example from the MNIST
and Fashion-MNIST datasets, subject to the constraint that we have to make a
prediction after the detection of the very first photon that passed a
coherently illuminated filter showing the example. Whereas a classical set-up
in which a photon is detected after falling on one of the $28\times 28$ image
pixels is limited to a (maximum likelihood estimation) accuracy of $21.27\%$
for MNIST, respectively $18.27\%$ for Fashion-MNIST, we show that the
theoretically achievable accuracy when exploiting inference by optically
transforming the quantum state of the photon is at least $41.27\%$ for MNIST,
respectively $36.14\%$ for Fashion-MNIST.
  We show in detail how to train the corresponding transformation with
TensorFlow and also explain how this example can serve as a teaching tool for
the measurement process in quantum mechanics.","cs.LG,quant-ph,stat.ML"
"Equivariant Graph Neural Networks for 3D Macromolecular Structure. Representing and reasoning about 3D structures of macromolecules is emerging
as a distinct challenge in machine learning. Here, we extend recent work on
geometric vector perceptrons and apply equivariant graph neural networks to a
wide range of tasks from structural biology. Our method outperforms all
reference architectures on three out of eight tasks in the ATOM3D benchmark, is
tied for first on two others, and is competitive with equivariant networks
using higher-order representations and spherical harmonic convolutions. In
addition, we demonstrate that transfer learning can further improve performance
on certain downstream tasks. Code is available at
https://github.com/drorlab/gvp-pytorch.","cs.LG,q-bio.BM"
"Missing Slice Recovery for Tensors Using a Low-rank Model in Embedded Space. Let us consider a case where all of the elements in some continuous slices
are missing in tensor data.
  In this case, the nuclear-norm and total variation regularization methods
usually fail to recover the missing elements.
  The key problem is capturing some delay/shift-invariant structure.
  In this study, we consider a low-rank model in an embedded space of a tensor.
  For this purpose, we extend a delay embedding for a time series to a
""multi-way delay-embedding transform"" for a tensor, which takes a given
incomplete tensor as the input and outputs a higher-order incomplete Hankel
tensor.
  The higher-order tensor is then recovered by Tucker-based low-rank tensor
factorization.
  Finally, an estimated tensor can be obtained by using the inverse multi-way
delay embedding transform of the recovered higher-order tensor.
  Our experiments showed that the proposed method successfully recovered
missing slices for some color images and functional magnetic resonance images.","cs.CV,cs.DS"
"DeepCEL0 for 2D Single Molecule Localization in Fluorescence Microscopy. In fluorescence microscopy, Single Molecule Localization Microscopy (SMLM)
techniques aim at localizing with high precision high density fluorescent
molecules by stochastically activating and imaging small subsets of blinking
emitters. Super Resolution (SR) plays an important role in this field since it
allows to go beyond the intrinsic light diffraction limit. In this work, we
propose a deep learning-based algorithm for precise molecule localization of
high density frames acquired by SMLM techniques whose $\ell_{2}$-based loss
function is regularized by positivity and $\ell_{0}$-based constraints. The
$\ell_{0}$ is relaxed through its Continuous Exact $\ell_{0}$ (CEL0)
counterpart. The arising approach, named DeepCEL0, is parameter-free, more
flexible, faster and provides more precise molecule localization maps if
compared to the other state-of-the-art methods. We validate our approach on
both simulated and real fluorescence microscopy data.","65F22,65K10,65R30,65Z05,68T07,68U10,G.1; I.2; I.4; J.3,cs.LG,cs.NA,eess.IV,math.NA"
"Joint convolutional neural pyramid for depth map super-resolution. High-resolution depth map can be inferred from a low-resolution one with the
guidance of an additional high-resolution texture map of the same scene.
Recently, deep neural networks with large receptive fields are shown to benefit
applications such as image completion. Our insight is that super resolution is
similar to image completion, where only parts of the depth values are precisely
known. In this paper, we present a joint convolutional neural pyramid model
with large receptive fields for joint depth map super-resolution. Our model
consists of three sub-networks, two convolutional neural pyramids concatenated
by a normal convolutional neural network. The convolutional neural pyramids
extract information from large receptive fields of the depth map and guidance
map, while the convolutional neural network effectively transfers useful
structures of the guidance image to the depth image. Experimental results show
that our model outperforms existing state-of-the-art algorithms not only on
data pairs of RGB/depth images, but also on other data pairs like
color/saliency and color-scribbles/colorized images.","cs.CV,cs.GR"
"Deep Predictive Coding Networks for Video Prediction and Unsupervised Learning. While great strides have been made in using deep learning algorithms to solve
supervised learning tasks, the problem of unsupervised learning - leveraging
unlabeled examples to learn about the structure of a domain - remains a
difficult unsolved challenge. Here, we explore prediction of future frames in a
video sequence as an unsupervised learning rule for learning about the
structure of the visual world. We describe a predictive neural network
(""PredNet"") architecture that is inspired by the concept of ""predictive coding""
from the neuroscience literature. These networks learn to predict future frames
in a video sequence, with each layer in the network making local predictions
and only forwarding deviations from those predictions to subsequent network
layers. We show that these networks are able to robustly learn to predict the
movement of synthetic (rendered) objects, and that in doing so, the networks
learn internal representations that are useful for decoding latent object
parameters (e.g. pose) that support object recognition with fewer training
views. We also show that these networks can scale to complex natural image
streams (car-mounted camera videos), capturing key aspects of both egocentric
movement and the movement of objects in the visual scene, and the
representation learned in this setting is useful for estimating the steering
angle. Altogether, these results suggest that prediction represents a powerful
framework for unsupervised learning, allowing for implicit learning of object
and scene structure.","cs.AI,cs.CV,cs.LG,cs.NE,q-bio.NC"
"Semi-supervised learning of images with strong rotational disorder: assembling nanoparticle libraries. The proliferation of optical, electron, and scanning probe microscopies gives
rise to large volumes of imaging data of objects as diversified as cells,
bacteria, pollen, to nanoparticles and atoms and molecules. In most cases, the
experimental data streams contain images having arbitrary rotations and
translations within the image. At the same time, for many cases, small amounts
of labeled data are available in the form of prior published results, image
collections, and catalogs, or even theoretical models. Here we develop an
approach that allows generalizing from a small subset of labeled data with a
weak orientational disorder to a large unlabeled dataset with a much stronger
orientational (and positional) disorder, i.e., it performs a classification of
image data given a small number of examples even in the presence of a
distribution shift between the labeled and unlabeled parts. This approach is
based on the semi-supervised rotationally invariant variational autoencoder
(ss-rVAE) model consisting of the encoder-decoder ""block"" that learns a
rotationally (and translationally) invariant continuous latent representation
of data and a classifier that encodes data into a finite number of discrete
classes. The classifier part of the trained ss-rVAE inherits the rotational
(and translational) invariances and can be deployed independently of the other
parts of the model. The performance of the ss-rVAE is illustrated using the
synthetic data sets with known factors of variation. We further demonstrate its
application for experimental data sets of nanoparticles, creating nanoparticle
libraries and disentangling the representations defining the physical factors
of variation in the data. The code reproducing the results is available at
https://github.com/ziatdinovmax/Semi-Supervised-VAE-nanoparticles.","cond-mat.dis-nn,cond-mat.mtrl-sci,cs.LG,physics.data-an"
"TreeGen -- a Monte Carlo generator for data frames. The typical problem in Data Science is creating a structure that encodes the
occurrence frequency of unique elements in rows and relations between different
rows of a data frame. We present the probability tree abstract data structure,
an extension of the decision tree, that facilitates more than two choices with
assigned probabilities. Such a tree represents statistical relations between
different rows of the data frame. The Probability Tree algorithmic structure is
supplied with the Generator module that is a Monte Carlo generator that
traverses through the tree. These two components are implemented in TreeGen
Python package. The package can be used in increasing data multiplicity,
compressing data preserving its statistical information, constructing
hierarchical models, exploring data, and in feature extraction.","cs.LG,stat.CO"
"Learning normal form autoencoders for data-driven discovery of universal,parameter-dependent governing equations. Complex systems manifest a small number of instabilities and bifurcations
that are canonical in nature, resulting in universal pattern forming
characteristics as a function of some parametric dependence. Such parametric
instabilities are mathematically characterized by their universal un-foldings,
or normal form dynamics, whereby a parsimonious model can be used to represent
the dynamics. Although center manifold theory guarantees the existence of such
low-dimensional normal forms, finding them has remained a long standing
challenge. In this work, we introduce deep learning autoencoders to discover
coordinate transformations that capture the underlying parametric dependence of
a dynamical system in terms of its canonical normal form, allowing for a simple
representation of the parametric dependence and bifurcation structure. The
autoencoder constrains the latent variable to adhere to a given normal form,
thus allowing it to learn the appropriate coordinate transformation. We
demonstrate the method on a number of example problems, showing that it can
capture a diverse set of normal forms associated with Hopf, pitchfork,
transcritical and/or saddle node bifurcations. This method shows how normal
forms can be leveraged as canonical and universal building blocks in deep
learning approaches for model discovery and reduced-order modeling.","37G05,cs.LG,math.DS"
"Lifted Model Checking for Relational MDPs. Model checking has been developed for verifying the behaviour of systems with
stochastic and non-deterministic behavior. It is used to provide guarantees
about such systems. While most model checking methods focus on propositional
models, various probabilistic planning and reinforcement frameworks deal with
relational domains, for instance, STRIPS planning and relational Markov
Decision Processes. Using propositional model checking in relational settings
requires one to ground the model, which leads to the well known state explosion
problem and intractability. We present pCTL-REBEL, a lifted model checking
approach for verifying pCTL properties on relational MDPs. It extends REBEL,
the relational Bellman update operator, which is a lifted value iteration
approach for model-based relational reinforcement learning, toward relational
model-checking. PCTL-REBEL is lifted, which means that rather than grounding,
the model exploits symmetries and reasons at an abstract relational level.
Theoretically, we show that the pCTL model checking approach is decidable for
relational MDPs even for possibly infinite domains provided that the states
have a bounded size. Practically, we contribute algorithms and an
implementation of lifted relational model checking, and we show that the lifted
approach improves the scalability of the model checking approach.","cs.AI,cs.LG,cs.LO"
"Attending to Emotional Narratives. Attention mechanisms in deep neural networks have achieved excellent
performance on sequence-prediction tasks. Here, we show that these
recently-proposed attention-based mechanisms---in particular, the Transformer
with its parallelizable self-attention layers, and the Memory Fusion Network
with attention across modalities and time---also generalize well to multimodal
time-series emotion recognition. Using a recently-introduced dataset of
emotional autobiographical narratives, we adapt and apply these two attention
mechanisms to predict emotional valence over time. Our models perform extremely
well, in some cases reaching a performance comparable with human raters. We end
with a discussion of the implications of attention mechanisms to affective
computing.","cs.CL,cs.LG,stat.ML"
"Using massive health insurance claims data to predict very high-cost claimants: a machine learning approach. Due to escalating healthcare costs, accurately predicting which patients will
incur high costs is an important task for payers and providers of healthcare.
High-cost claimants (HiCCs) are patients who have annual costs above
$\$250,000$ and who represent just 0.16% of the insured population but
currently account for 9% of all healthcare costs. In this study, we aimed to
develop a high-performance algorithm to predict HiCCs to inform a novel care
management system. Using health insurance claims from 48 million people and
augmented with census data, we applied machine learning to train binary
classification models to calculate the personal risk of HiCC. To train the
models, we developed a platform starting with 6,006 variables across all
clinical and demographic dimensions and constructed over one hundred candidate
models. The best model achieved an area under the receiver operating
characteristic curve of 91.2%. The model exceeds the highest published
performance (84%) and remains high for patients with no prior history of
high-cost status (89%), who have less than a full year of enrollment (87%), or
lack pharmacy claims data (88%). It attains an area under the precision-recall
curve of 23.1%, and precision of 74% at a threshold of 0.99. A care management
program enrolling 500 people with the highest HiCC risk is expected to treat
199 true HiCCs and generate a net savings of $\$7.3$ million per year. Our
results demonstrate that high-performing predictive models can be constructed
using claims data and publicly available data alone, even for rare high-cost
claimants exceeding $\$250,000$. Our model demonstrates the transformational
power of machine learning and artificial intelligence in care management, which
would allow healthcare payers and providers to introduce the next generation of
care management programs.","I.2.6,J.3,J.3; I.2.6,cs.LG,stat.ML"
"StrobeNet: Category-Level Multiview Reconstruction of Articulated Objects. We present StrobeNet, a method for category-level 3D reconstruction of
articulating objects from one or more unposed RGB images. Reconstructing
general articulating object categories % has important applications, but is
challenging since objects can have wide variation in shape, articulation,
appearance and topology. We address this by building on the idea of
category-level articulation canonicalization -- mapping observations to a
canonical articulation which enables correspondence-free multiview aggregation.
Our end-to-end trainable neural network estimates feature-enriched canonical 3D
point clouds, articulation joints, and part segmentation from one or more
unposed images of an object. These intermediate estimates are used to generate
a final implicit 3D reconstruction.Our approach reconstructs objects even when
they are observed in different articulations in images with large baselines,
and animation of reconstructed shapes. Quantitative and qualitative evaluations
on different object categories show that our method is able to achieve high
reconstruction accuracy, especially as more views are added.","cs.CG,cs.CV"
"Shape-driven Coordinate Ordering for Star Glyph Sets via Reinforcement Learning. We present a neural optimization model trained with reinforcement learning to
solve the coordinate ordering problem for sets of star glyphs. Given a set of
star glyphs associated to multiple class labels, we propose to use shape
context descriptors to measure the perceptual distance between pairs of glyphs,
and use the derived silhouette coefficient to measure the perception of class
separability within the entire set. To find the optimal coordinate order for
the given set, we train a neural network using reinforcement learning to reward
orderings with high silhouette coefficients. The network consists of an encoder
and a decoder with an attention mechanism. The encoder employs a recurrent
neural network (RNN) to encode input shape and class information, while the
decoder together with the attention mechanism employs another RNN to output a
sequence with the new coordinate order. In addition, we introduce a neural
network to efficiently estimate the similarity between shape context
descriptors, which allows to speed up the computation of silhouette
coefficients and thus the training of the axis ordering network. Two user
studies demonstrate that the orders provided by our method are preferred by
users for perceiving class separation. We tested our model on different
settings to show its robustness and generalization abilities and demonstrate
that it allows to order input sets with unseen data size, data dimension, or
number of classes. We also demonstrate that our model can be adapted to
coordinate ordering of other types of plots such as RadViz by replacing the
proposed shape-aware silhouette coefficient with the corresponding quality
metric to guide network training.","cs.CV,cs.GR,cs.HC"
"MSP: An FPGA-Specific Mixed-Scheme, Multi-Precision Deep Neural Network Quantization Framework. With the tremendous success of deep learning, there exists imminent need to
deploy deep learning models onto edge devices. To tackle the limited computing
and storage resources in edge devices, model compression techniques have been
widely used to trim deep neural network (DNN) models for on-device inference
execution. This paper targets the commonly used FPGA (field programmable gate
array) devices as the hardware platforms for DNN edge computing. We focus on
the DNN quantization as the main model compression technique, since DNN
quantization has been of great importance for the implementations of DNN models
on the hardware platforms. The novelty of this work comes in twofold: (i) We
propose a mixed-scheme DNN quantization method that incorporates both the
linear and non-linear number systems for quantization, with the aim to boost
the utilization of the heterogeneous computing resources, i.e., LUTs (look up
tables) and DSPs (digital signal processors) on an FPGA. Note that all the
existing (single-scheme) quantization methods can only utilize one type of
resources (either LUTs or DSPs for the MAC (multiply-accumulate) operations in
deep learning computations. (ii) We use a quantization method that supports
multiple precisions along the intra-layer dimension, while the existing
quantization methods apply multi-precision quantization along the inter-layer
dimension. The intra-layer multi-precision method can uniform the hardware
configurations for different layers to reduce computation overhead and at the
same time preserve the model accuracy as the inter-layer approach.","68T07,cs.CV,cs.LG"
"Collaborative Large-Scale Dense 3D Reconstruction with Online Inter-Agent Pose Optimisation. Reconstructing dense, volumetric models of real-world 3D scenes is important
for many tasks, but capturing large scenes can take significant time, and the
risk of transient changes to the scene goes up as the capture time increases.
These are good reasons to want instead to capture several smaller sub-scenes
that can be joined to make the whole scene. Achieving this has traditionally
been difficult: joining sub-scenes that may never have been viewed from the
same angle requires a high-quality camera relocaliser that can cope with novel
poses, and tracking drift in each sub-scene can prevent them from being joined
to make a consistent overall scene. Recent advances, however, have
significantly improved our ability to capture medium-sized sub-scenes with
little to no tracking drift: real-time globally consistent reconstruction
systems can close loops and re-integrate the scene surface on the fly, whilst
new visual-inertial odometry approaches can significantly reduce tracking drift
during live reconstruction. Moreover, high-quality regression forest-based
relocalisers have recently been made more practical by the introduction of a
method to allow them to be trained and used online. In this paper, we leverage
these advances to present what to our knowledge is the first system to allow
multiple users to collaborate interactively to reconstruct dense, voxel-based
models of whole buildings using only consumer-grade hardware, a task that has
traditionally been both time-consuming and dependent on the availability of
specialised hardware. Using our system, an entire house or lab can be
reconstructed in under half an hour and at a far lower cost than was previously
possible.","68T45,cs.CV"
"Physicist's Journeys Through the AI World - A Topical Review. There is no royal road to unsupervised learning. Artificial Intelligence (AI), defined in its most simple form, is a
technological tool that makes machines intelligent. Since learning is at the
core of intelligence, machine learning poses itself as a core sub-field of AI.
Then there comes a subclass of machine learning, known as deep learning, to
address the limitations of their predecessors. AI has generally acquired its
prominence over the past few years due to its considerable progress in various
fields. AI has vastly invaded the realm of research. This has led physicists to
attentively direct their research towards implementing AI tools. Their central
aim has been to gain better understanding and enrich their intuition. This
review article is meant to supplement the previously presented efforts to
bridge the gap between AI and physics, and take a serious step forward to
filter out the ""Babelian"" clashes brought about from such gabs. This
necessitates first to have fundamental knowledge about common AI tools. To this
end, the review's primary focus shall be on deep learning models called
artificial neural networks. They are deep learning models which train
themselves through different learning processes. It discusses also the concept
of Markov decision processes. Finally, shortcut to the main goal, the review
thoroughly examines how these neural networks are capable to construct a
physical theory describing some observations without applying any previous
physical knowledge.","cond-mat.dis-nn,cs.LG,physics.comp-ph,quant-ph,stat.ML"
"Intention Propagation for Multi-agent Reinforcement Learning. A hallmark of an AI agent is to mimic human beings to understand and interact
with others. In this paper, we propose a collaborative multi-agent
reinforcement learning algorithm to learn a \emph{joint} policy through the
interactions over agents. To make a joint decision over the group, each agent
makes an initial decision and tells its policy to its neighbors. Then each
agent modifies its own policy properly based on received messages and spreads
out its plan. As this intention propagation procedure goes on, we prove that it
converges to a mean-field approximation of the joint policy with the framework
of neural embedded probabilistic inference. We evaluate our algorithm on
several large scale challenging tasks and demonstrate that it outperforms
previous state-of-the-arts.","cs.LG,cs.MA,stat.ML"
"AdaLead: A simple and robust adaptive greedy search algorithm for sequence design. Efficient design of biological sequences will have a great impact across many
industrial and healthcare domains. However, discovering improved sequences
requires solving a difficult optimization problem. Traditionally, this
challenge was approached by biologists through a model-free method known as
""directed evolution"", the iterative process of random mutation and selection.
As the ability to build models that capture the sequence-to-function map
improves, such models can be used as oracles to screen sequences before running
experiments. In recent years, interest in better algorithms that effectively
use such oracles to outperform model-free approaches has intensified. These
span from approaches based on Bayesian Optimization, to regularized generative
models and adaptations of reinforcement learning. In this work, we implement an
open-source Fitness Landscape EXploration Sandbox (FLEXS:
github.com/samsinai/FLEXS) environment to test and evaluate these algorithms
based on their optimality, consistency, and robustness. Using FLEXS, we develop
an easy-to-implement, scalable, and robust evolutionary greedy algorithm
(AdaLead). Despite its simplicity, we show that AdaLead is a remarkably strong
benchmark that out-competes more complex state of the art approaches in a
variety of biologically motivated sequence design challenges.","cs.LG,math.OC,q-bio.BM,q-bio.QM"
"A Robust Approach for Securing Audio Classification Against Adversarial Attacks. Adversarial audio attacks can be considered as a small perturbation
unperceptive to human ears that is intentionally added to the audio signal and
causes a machine learning model to make mistakes. This poses a security concern
about the safety of machine learning models since the adversarial attacks can
fool such models toward the wrong predictions. In this paper we first review
some strong adversarial attacks that may affect both audio signals and their 2D
representations and evaluate the resiliency of the most common machine learning
model, namely deep learning models and support vector machines (SVM) trained on
2D audio representations such as short time Fourier transform (STFT), discrete
wavelet transform (DWT) and cross recurrent plot (CRP) against several
state-of-the-art adversarial attacks. Next, we propose a novel approach based
on pre-processed DWT representation of audio signals and SVM to secure audio
systems against adversarial attacks. The proposed architecture has several
preprocessing modules for generating and enhancing spectrograms including
dimension reduction and smoothing. We extract features from small patches of
the spectrograms using speeded up robust feature (SURF) algorithm which are
further used to generate a codebook using the K-Means++ algorithm. Finally,
codewords are used to train a SVM on the codebook of the SURF-generated
vectors. All these steps yield to a novel approach for audio classification
that provides a good trade-off between accuracy and resilience. Experimental
results on three environmental sound datasets show the competitive performance
of proposed approach compared to the deep neural networks both in terms of
accuracy and robustness against strong adversarial attacks.","cs.CR,cs.LG,cs.SD,eess.AS,stat.ML"
"Molecular machine learning with conformer ensembles. Virtual screening can accelerate drug discovery by identifying promising
candidates for experimental evaluation. Machine learning is a powerful method
for screening, as it can learn complex structure-property relationships from
experimental data and make rapid predictions over virtual libraries. Molecules
inherently exist as a three-dimensional ensemble and their biological action
typically occurs through supramolecular recognition. However, most deep
learning approaches to molecular property prediction use a 2D graph
representation as input, and in some cases a single 3D conformation. Here we
investigate how the 3D information of multiple conformers, traditionally known
as 4D information in the cheminformatics community, can improve molecular
property prediction in deep learning models. We introduce multiple deep
learning models that expand upon key architectures such as ChemProp and Schnet,
adding elements such as multiple-conformer inputs and conformer attention. We
then benchmark the performance trade-offs of these models on 2D, 3D and 4D
representations in the prediction of drug activity using a large training set
of geometrically resolved molecules. The new architectures perform
significantly better than 2D models, but their performance is often just as
strong with a single conformer as with many. We also find that 4D deep learning
models learn interpretable attention weights for each conformer.","cs.LG,physics.chem-ph"
"Improving Image Captioning with Conditional Generative Adversarial Nets. In this paper, we propose a novel
conditional-generative-adversarial-nets-based image captioning framework as an
extension of traditional reinforcement-learning (RL)-based encoder-decoder
architecture. To deal with the inconsistent evaluation problem among different
objective language metrics, we are motivated to design some ""discriminator""
networks to automatically and progressively determine whether generated caption
is human described or machine generated. Two kinds of discriminator
architectures (CNN and RNN-based structures) are introduced since each has its
own advantages. The proposed algorithm is generic so that it can enhance any
existing RL-based image captioning framework and we show that the conventional
RL training method is just a special case of our approach. Empirically, we show
consistent improvements over all language evaluation metrics for different
state-of-the-art image captioning models. In addition, the well-trained
discriminators can also be viewed as objective image captioning evaluators","68T45,cs.CV,cs.LG"
"Ensemble Slice Sampling: Parallel, black-box and gradient-free inference for correlated & multimodal distributions. Slice Sampling has emerged as a powerful Markov Chain Monte Carlo algorithm
that adapts to the characteristics of the target distribution with minimal
hand-tuning. However, Slice Sampling's performance is highly sensitive to the
user-specified initial length scale hyperparameter and the method generally
struggles with poorly scaled or strongly correlated distributions. This paper
introduces Ensemble Slice Sampling (ESS), a new class of algorithms that
bypasses such difficulties by adaptively tuning the initial length scale and
utilising an ensemble of parallel walkers in order to efficiently handle strong
correlations between parameters. These affine-invariant algorithms are trivial
to construct, require no hand-tuning, and can easily be implemented in parallel
computing environments. Empirical tests show that Ensemble Slice Sampling can
improve efficiency by more than an order of magnitude compared to conventional
MCMC methods on a broad range of highly correlated target distributions. In
cases of strongly multimodal target distributions, Ensemble Slice Sampling can
sample efficiently even in high dimensions. We argue that the parallel,
black-box and gradient-free nature of the method renders it ideal for use in
scientific fields such as physics, astrophysics and cosmology which are
dominated by a wide variety of computationally expensive and non-differentiable
models.","astro-ph.CO,astro-ph.IM,cs.LG,stat.CO,stat.ML"
"Back to Basics: Deep Reinforcement Learning in Traffic Signal Control. In this paper we revisit some of the fundamental premises for a reinforcement
learning (RL) approach to self-learning traffic lights. We propose RLight, a
combination of choices that offers robust performance and good generalization
to unseen traffic flows. In particular, our main contributions are threefold:
our lightweight and cluster-aware state representation leads to improved
performance; we reformulate the MDP such that it skips redundant timesteps of
yellow light, speeding up learning by 30%; and we investigate the action space
and provide insight into the difference in performance between acyclic and
cyclic phase transitions. Additionally, we provide insights into the
generalisation of the methods to unseen traffic. Evaluations using the
real-world Hangzhou traffic dataset show that RLight outperforms
state-of-the-art rule-based and deep reinforcement learning algorithms,
demonstrating the potential of RL-based methods to improve urban traffic flows.","I.2.6,cs.LG"
"Explainability and Adversarial Robustness for RNNs. Recurrent Neural Networks (RNNs) yield attractive properties for constructing
Intrusion Detection Systems (IDSs) for network data. With the rise of
ubiquitous Machine Learning (ML) systems, malicious actors have been catching
up quickly to find new ways to exploit ML vulnerabilities for profit. Recently
developed adversarial ML techniques focus on computer vision and their
applicability to network traffic is not straightforward: Network packets expose
fewer features than an image, are sequential and impose several constraints on
their features.
  We show that despite these completely different characteristics, adversarial
samples can be generated reliably for RNNs. To understand a classifier's
potential for misclassification, we extend existing explainability techniques
and propose new ones, suitable particularly for sequential data. Applying them
shows that already the first packets of a communication flow are of crucial
importance and are likely to be targeted by attackers. Feature importance
methods show that even relatively unimportant features can be effectively
abused to generate adversarial samples. Since traditional evaluation metrics
such as accuracy are not sufficient for quantifying the adversarial threat, we
propose the Adversarial Robustness Score (ARS) for comparing IDSs, capturing a
common notion of adversarial robustness, and show that an adversarial training
procedure can significantly and successfully reduce the attack surface.","cs.CR,cs.LG,cs.NI,stat.ML"
"Learning to Embed Categorical Features without Embedding Tables for Recommendation. Embedding learning of categorical features (e.g. user/item IDs) is at the
core of various recommendation models including matrix factorization and neural
collaborative filtering. The standard approach creates an embedding table where
each row represents a dedicated embedding vector for every unique feature
value. However, this method fails to efficiently handle high-cardinality
features and unseen feature values (e.g. new video ID) that are prevalent in
real-world recommendation systems. In this paper, we propose an alternative
embedding framework Deep Hash Embedding (DHE), replacing embedding tables by a
deep embedding network to compute embeddings on the fly. DHE first encodes the
feature value to a unique identifier vector with multiple hashing functions and
transformations, and then applies a DNN to convert the identifier vector to an
embedding. The encoding module is deterministic, non-learnable, and free of
storage, while the embedding network is updated during the training time to
learn embedding generation. Empirical results show that DHE achieves comparable
AUC against the standard one-hot full embedding, with smaller model sizes. Our
work sheds light on the design of DNN-based alternative embedding schemes for
categorical features without using embedding table lookup.","cs.IR,cs.LG"
"Strategically Efficient Exploration in Competitive Multi-agent Reinforcement Learning. High sample complexity remains a barrier to the application of reinforcement
learning (RL), particularly in multi-agent systems. A large body of work has
demonstrated that exploration mechanisms based on the principle of optimism
under uncertainty can significantly improve the sample efficiency of RL in
single agent tasks. This work seeks to understand the role of optimistic
exploration in non-cooperative multi-agent settings. We will show that, in
zero-sum games, optimistic exploration can cause the learner to waste time
sampling parts of the state space that are irrelevant to strategic play, as
they can only be reached through cooperation between both players. To address
this issue, we introduce a formal notion of strategically efficient exploration
in Markov games, and use this to develop two strategically efficient learning
algorithms for finite Markov games. We demonstrate that these methods can be
significantly more sample efficient than their optimistic counterparts.","68T05,I.2.6,cs.AI,cs.LG,cs.MA"
"Simple Transparent Adversarial Examples. There has been a rise in the use of Machine Learning as a Service (MLaaS)
Vision APIs as they offer multiple services including pre-built models and
algorithms, which otherwise take a huge amount of resources if built from
scratch. As these APIs get deployed for high-stakes applications, it's very
important that they are robust to different manipulations. Recent works have
only focused on typical adversarial attacks when evaluating the robustness of
vision APIs. We propose two new aspects of adversarial image generation methods
and evaluate them on the robustness of Google Cloud Vision API's optical
character recognition service and object detection APIs deployed in real-world
settings such as sightengine.com, picpurify.com, Google Cloud Vision API, and
Microsoft Azure's Computer Vision API. Specifically, we go beyond the
conventional small-noise adversarial attacks and introduce secret embedding and
transparent adversarial examples as a simpler way to evaluate robustness. These
methods are so straightforward that even non-specialists can craft such
attacks. As a result, they pose a serious threat where APIs are used for
high-stakes applications. Our transparent adversarial examples successfully
evade state-of-the art object detections APIs such as Azure Cloud Vision
(attack success rate 52%) and Google Cloud Vision (attack success rate 36%).
90% of the images have a secret embedded text that successfully fools the
vision of time-limited humans but is detected by Google Cloud Vision API's
optical character recognition. Complementing to current research, our results
provide simple but unconventional methods on robustness evaluation.","cs.AI,cs.CR,cs.CV,cs.LG"
"Automatic Source Code Summarization with Extended Tree-LSTM. Neural machine translation models are used to automatically generate a
document from given source code since this can be regarded as a machine
translation task. Source code summarization is one of the components for
automatic document generation, which generates a summary in natural language
from given source code. This suggests that techniques used in neural machine
translation, such as Long Short-Term Memory (LSTM), can be used for source code
summarization. However, there is a considerable difference between source code
and natural language: Source code is essentially {\em structured}, having loops
and conditional branching, etc. Therefore, there is some obstacle to apply
known machine translation models to source code.
  Abstract syntax trees (ASTs) capture these structural properties and play an
important role in recent machine learning studies on source code. Tree-LSTM is
proposed as a generalization of LSTMs for tree-structured data. However, there
is a critical issue when applying it to ASTs: It cannot handle a tree that
contains nodes having an arbitrary number of children and their order
simultaneously, which ASTs generally have such nodes. To address this issue, we
propose an extension of Tree-LSTM, which we call \emph{Multi-way Tree-LSTM} and
apply it for source code summarization. As a result of computational
experiments, our proposal achieved better results when compared with several
state-of-the-art techniques.","cs.LG,cs.SE,stat.ML"
"On the One-sided Convergence of Adam-type Algorithms in Non-convex Non-concave Min-max Optimization. Adam-type methods, the extension of adaptive gradient methods, have shown
great performance in the training of both supervised and unsupervised machine
learning models. In particular, Adam-type optimizers have been widely used
empirically as the default tool for training generative adversarial networks
(GANs). On the theory side, however, despite the existence of theoretical
results showing the efficiency of Adam-type methods in minimization problems,
the reason of their wonderful performance still remains absent in GAN's
training. In existing works, the fast convergence has long been considered as
one of the most important reasons and multiple works have been proposed to give
a theoretical guarantee of the convergence to a critical point of min-max
optimization algorithms under certain assumptions. In this paper, we firstly
argue empirically that in GAN's training, Adam does not converge to a critical
point even upon successful training: Only the generator is converging while the
discriminator's gradient norm remains high throughout the training. We name
this one-sided convergence. Then we bridge the gap between experiments and
theory by showing that Adam-type algorithms provably converge to a one-sided
first order stationary points in min-max optimization problems under the
one-sided MVI condition. We also empirically verify that such one-sided MVI
condition is satisfied for standard GANs after trained over standard data sets.
To the best of our knowledge, this is the very first result which provides an
empirical observation and a strict theoretical guarantee on the one-sided
convergence of Adam-type algorithms in min-max optimization.","cs.LG,math.OC,stat.ML"
"Restrained Generative Adversarial Network against Overfitting in Numeric Data Augmentation. In recent studies, Generative Adversarial Network (GAN) is one of the popular
schemes to augment the image dataset. However, in our study we find the
generator G in the GAN fails to generate numerical data in lower-dimensional
spaces, and we address overfitting in the generation. By analyzing the Directed
Graphical Model (DGM), we propose a theoretical restraint, independence on the
loss function, to suppress the overfitting. Practically, as the Statically
Restrained GAN (SRGAN) and Dynamically Restrained GAN (DRGAN), two frameworks
are proposed to employ the theoretical restraint to the network structure. In
the static structure, we predefined a pair of particular network topologies of
G and D as the restraint, and quantify such restraint by the interpretable
metric Similarity of the Restraint (SR). While for DRGAN we design an
adjustable dropout module for the restraint function. In the widely carried out
20 group experiments, on four public numerical class imbalance datasets and
five classifiers, the static and dynamic methods together produce the best
augmentation results of 19 from 20; and both two methods simultaneously
generate 14 of 20 groups of the top-2 best, proving the effectiveness and
feasibility of the theoretical restraints.","cs.LG,cs.NI"
"The watershed concept and its use in segmentation : a brief history. The watershed is one of the most used tools in image segmentation. We present
how its concept is born and developed over time. Its implementation as an
algorithm or a hardwired device evolved together with the technology which
allowed it. We present also how it is used in practice, first together with
markers, and later introduced in a multiscale framework, in order to produce
not a unique partition but a complete hierarchy.","05C85,68U10,cs.CV"
"Unsupervised machine learning to analyse city logistics through Twitter. City Logistics is characterized by multiple stakeholders that often have
different views of such a complex system. From a public policy perspective,
identifying stakeholders, issues and trends is a daunting challenge, only
partially addressed by traditional observation systems. Nowadays, social media
is one of the biggest channels of public expression and is often used to
communicate opinions and content related to City Logistics. The idea of this
research is that analysing social media content could help in understanding the
public perception of City logistics. This paper offers a methodology for
collecting content from Twitter and implementing Machine Learning techniques
(Unsupervised Learning and Natural Language Processing), to perform content and
sentiment analysis. The proposed methodology is applied to more than 110 000
tweets containing City Logistics key-terms. Results allowed the building of an
Interest Map of concepts and a Sentiment Analysis to determine if City
Logistics entries are positive, negative or neutral.","cs.CY,cs.LG,cs.SI,stat.ML"
"SleepTransformer: Automatic Sleep Staging with Interpretability and Uncertainty Quantification. Black-box skepticism is one of the main hindrances impeding
deep-learning-based automatic sleep scoring from being used in clinical
environments. Towards interpretability, this work proposes a
sequence-to-sequence sleep-staging model, namely SleepTransformer. It is based
on the transformer backbone whose self-attention scores offer interpretability
of the model's decisions at both the epoch and sequence level. At the epoch
level, the attention scores can be encoded as a heat map to highlight
sleep-relevant features captured from the input EEG signal. At the sequence
level, the attention scores are visualized as the influence of different
neighboring epochs in an input sequence (i.e. the context) to recognition of a
target epoch, mimicking the way manual scoring is done by human experts. We
further propose a simple yet efficient method to quantify uncertainty in the
model's decisions. The method, which is based on entropy, can serve as a metric
for deferring low-confidence epochs to a human expert for further inspection.
Additionally, we demonstrate that the proposed SleepTransformer outperforms
existing methods at a lower computational cost and achieves state-of-the-art
performance on two experimental databases of different sizes.","cs.LG,eess.SP"
"Quantile-Quantile Embedding for Distribution Transformation and Manifold Embedding with Ability to Choose the Embedding Distribution. We propose a new embedding method, named Quantile-Quantile Embedding (QQE),
for distribution transformation and manifold embedding with the ability to
choose the embedding distribution. QQE, which uses the concept of
quantile-quantile plot from visual statistical tests, can transform the
distribution of data to any theoretical desired distribution or empirical
reference sample. Moreover, QQE gives the user a choice of embedding
distribution in embedding the manifold of data into the low dimensional
embedding space. It can also be used for modifying the embedding distribution
of other dimensionality reduction methods, such as PCA, t-SNE, and deep metric
learning, for better representation or visualization of data. We propose QQE in
both unsupervised and supervised forms. QQE can also transform a distribution
to either an exact reference distribution or its shape. We show that QQE allows
for better discrimination of classes in some cases. Our experiments on
different synthetic and image datasets show the effectiveness of the proposed
embedding method.","cs.CV,cs.LG,stat.CO,stat.ML"
"Meta-Graph: Few Shot Link Prediction via Meta Learning. We consider the task of few shot link prediction on graphs. The goal is to
learn from a distribution over graphs so that a model is able to quickly infer
missing edges in a new graph after a small amount of training. We show that
current link prediction methods are generally ill-equipped to handle this task.
They cannot effectively transfer learned knowledge from one graph to another
and are unable to effectively learn from sparse samples of edges. To address
this challenge, we introduce a new gradient-based meta learning framework,
Meta-Graph. Our framework leverages higher-order gradients along with a learned
graph signature function that conditionally generates a graph neural network
initialization. Using a novel set of few shot link prediction benchmarks, we
show that Meta-Graph can learn to quickly adapt to a new graph using only a
small sample of true edges, enabling not only fast adaptation but also improved
results at convergence.","cs.LG,cs.SI,stat.ML"
"A Bayesian approach for structure learning in oscillating regulatory networks. Oscillations lie at the core of many biological processes, from the cell
cycle, to circadian oscillations and developmental processes. Time-keeping
mechanisms are essential to enable organisms to adapt to varying conditions in
environmental cycles, from day/night to seasonal. Transcriptional regulatory
networks are one of the mechanisms behind these biological oscillations.
However, while identifying cyclically expressed genes from time series
measurements is relatively easy, determining the structure of the interaction
network underpinning the oscillation is a far more challenging problem. Here,
we explicitly leverage the oscillatory nature of the transcriptional signals
and present a method for reconstructing network interactions tailored to this
special but important class of genetic circuits. Our method is based on
projecting the signal onto a set of oscillatory basis functions using a
Discrete Fourier Transform. We build a Bayesian Hierarchical model within a
frequency domain linear model in order to enforce sparsity and incorporate
prior knowledge about the network structure. Experiments on real and simulated
data show that the method can lead to substantial improvements over competing
approaches if the oscillatory assumption is met, and remains competitive also
in cases it is not.","q-bio.QM,stat.ML"
"Informative Policy Representations in Multi-Agent Reinforcement Learning via Joint-Action Distributions. In multi-agent reinforcement learning, the inherent non-stationarity of the
environment caused by other agents' actions posed significant difficulties for
an agent to learn a good policy independently. One way to deal with
non-stationarity is agent modeling, by which the agent takes into consideration
the influence of other agents' policies. Most existing work relies on
predicting other agents' actions or goals, or discriminating between their
policies. However, such modeling fails to capture the similarities and
differences between policies simultaneously and thus cannot provide useful
information when generalizing to unseen policies. To address this, we propose a
general method to learn representations of other agents' policies via the
joint-action distributions sampled in interactions. The similarities and
differences between policies are naturally captured by the policy distance
inferred from the joint-action distributions and deliberately reflected in the
learned representations. Agents conditioned on the policy representations can
well generalize to unseen agents. We empirically demonstrate that our method
outperforms existing work in multi-agent tasks when facing unseen agents.","cs.AI,cs.LG,cs.MA"
"Automatic learning of gait signatures for people identification. This work targets people identification in video based on the way they walk
(i.e. gait). While classical methods typically derive gait signatures from
sequences of binary silhouettes, in this work we explore the use of
convolutional neural networks (CNN) for learning high-level descriptors from
low-level motion features (i.e. optical flow components). We carry out a
thorough experimental evaluation of the proposed CNN architecture on the
challenging TUM-GAID dataset. The experimental results indicate that using
spatio-temporal cuboids of optical flow as input data for CNN allows to obtain
state-of-the-art results on the gait task with an image resolution eight times
lower than the previously reported results (i.e. 80x60 pixels).","cs.AI,cs.CV"
"V-SlowFast Network for Efficient Visual Sound Separation. The objective of this paper is to perform visual sound separation: i) we
study visual sound separation on spectrograms of different temporal
resolutions; ii) we propose a new light yet efficient three-stream framework
V-SlowFast that operates on Visual frame, Slow spectrogram, and Fast
spectrogram. The Slow spectrogram captures the coarse temporal resolution while
the Fast spectrogram contains the fine-grained temporal resolution; iii) we
introduce two contrastive objectives to encourage the network to learn
discriminative visual features for separating sounds; iv) we propose an
audio-visual global attention module for audio and visual feature fusion; v)
the introduced V-SlowFast model outperforms previous state-of-the-art in
single-frame based visual sound separation on small- and large-scale datasets:
MUSIC-21, AVE, and VGG-Sound. We also propose a small V-SlowFast architecture
variant, which achieves 74.2% reduction in the number of model parameters and
81.4% reduction in GMACs compared to the previous multi-stage models. Project
page: https://ly-zhu.github.io/V-SlowFast","cs.CV,cs.SD,eess.AS"
"Deep tree-ensembles for multi-output prediction. Recently, deep neural networks have expanded the state-of-art in various
scientific fields and provided solutions to long standing problems across
multiple application domains. Nevertheless, they also suffer from weaknesses
since their optimal performance depends on massive amounts of training data and
the tuning of an extended number of parameters. As a countermeasure, some
deep-forest methods have been recently proposed, as efficient and low-scale
solutions. Despite that, these approaches simply employ label classification
probabilities as induced features and primarily focus on traditional
classification and regression tasks, leaving multi-output prediction
under-explored. Moreover, recent work has demonstrated that tree-embeddings are
highly representative, especially in structured output prediction. In this
direction, we propose a novel deep tree-ensemble (DTE) model, where every layer
enriches the original feature set with a representation learning component
based on tree-embeddings. In this paper, we specifically focus on two
structured output prediction tasks, namely multi-label classification and
multi-target regression. We conducted experiments using multiple benchmark
datasets and the obtained results confirm that our method provides superior
results to state-of-the-art methods in both tasks.","cs.LG,stat.ML"
"Edge Contraction Pooling for Graph Neural Networks. Graph Neural Network (GNN) research has concentrated on improving
convolutional layers, with little attention paid to developing graph pooling
layers. Yet pooling layers can enable GNNs to reason over abstracted groups of
nodes instead of single nodes. To close this gap, we propose a graph pooling
layer relying on the notion of edge contraction: EdgePool learns a localized
and sparse hard pooling transform. We show that EdgePool outperforms
alternative pooling methods, can be easily integrated into most GNN models, and
improves performance on both node and graph classification.","cs.LG,stat.ML"
"Structure-Property Maps with Kernel Principal Covariates Regression. Data analyses based on linear methods constitute the simplest, most robust,
and transparent approaches to the automatic processing of large amounts of data
for building supervised or unsupervised machine learning models. Principal
covariates regression (PCovR) is an underappreciated method that interpolates
between principal component analysis and linear regression, and can be used to
conveniently reveal structure-property relations in terms of
simple-to-interpret, low-dimensional maps. Here we provide a pedagogic overview
of these data analysis schemes, including the use of the kernel trick to
introduce an element of non-linearity, while maintaining most of the
convenience and the simplicity of linear approaches. We then introduce a
kernelized version of PCovR and a sparsified extension, and demonstrate the
performance of this approach in revealing and predicting structure-property
relations in chemistry and materials science, showing a variety of examples
including elemental carbon, porous silicate frameworks, organic molecules,
amino acid conformers, and molecular materials.","cond-mat.mtrl-sci,cs.LG,physics.chem-ph,stat.ML"
"Medical Image Super-Resolution Using a Generative Adversarial Network. During the growing popularity of electronic medical records, electronic
medical record (EMR) data has exploded increasingly. It is very meaningful to
retrieve high quality EMR in mass data. In this paper, an EMR value network
with retrieval function is constructed by taking stroke disease as the research
object. It mainly includes: 1) It establishes the electronic medical record
database and corresponding stroke knowledge graph. 2) The strategy of
similarity measurement is included three parts(patients' chief complaint,
pathology results and medical images). Patients' chief complaints are text
data, mainly describing patients' symptoms and expressed in words or phrases,
and patients' chief complaints are input in independent tick of various
symptoms. The data of the pathology results is a structured and digitized
expression, so the input method is the same as the patient's chief complaint;
Image similarity adopts content-based image retrieval(CBIR) technology. 3) The
analytic hierarchy process (AHP) is used to establish the weights of the three
types of data and then synthesize them into an indicator. The accuracy rate of
similarity in top 5 was more than 85\% based on EMR database with more 200
stroke records using leave-one-out method. It will be the good tool for
assistant diagnosis and doctor training, as good quality records are colleted
into the databases, like Doctor Watson, in the future.","cs.CV,cs.NA,math.NA"
"Accelerated Large Batch Optimization of BERT Pretraining in 54 minutes. BERT has recently attracted a lot of attention in natural language
understanding (NLU) and achieved state-of-the-art results in various NLU tasks.
However, its success requires large deep neural networks and huge amount of
data, which result in long training time and impede development progress. Using
stochastic gradient methods with large mini-batch has been advocated as an
efficient tool to reduce the training time. Along this line of research, LAMB
is a prominent example that reduces the training time of BERT from 3 days to 76
minutes on a TPUv3 Pod. In this paper, we propose an accelerated gradient
method called LANS to improve the efficiency of using large mini-batches for
training. As the learning rate is theoretically upper bounded by the inverse of
the Lipschitz constant of the function, one cannot always reduce the number of
optimization iterations by selecting a larger learning rate. In order to use
larger mini-batch size without accuracy loss, we develop a new learning rate
scheduler that overcomes the difficulty of using large learning rate. Using the
proposed LANS method and the learning rate scheme, we scaled up the mini-batch
sizes to 96K and 33K in phases 1 and 2 of BERT pretraining, respectively. It
takes 54 minutes on 192 AWS EC2 P3dn.24xlarge instances to achieve a target F1
score of 90.5 or higher on SQuAD v1.1, achieving the fastest BERT training time
in the cloud.","cs.CL,cs.DC,cs.LG,stat.ML"
"Distributed Algorithms for Linearly-Solvable Optimal Control in Networked Multi-Agent Systems. Distributed algorithms for both discrete-time and continuous-time linearly
solvable optimal control (LSOC) problems of networked multi-agent systems
(MASs) are investigated in this paper. A distributed framework is proposed to
partition the optimal control problem of a networked MAS into several local
optimal control problems in factorial subsystems, such that each (central)
agent behaves optimally to minimize the joint cost function of a subsystem that
comprises a central agent and its neighboring agents, and the local control
actions (policies) only rely on the knowledge of local observations. Under this
framework, we not only preserve the correlations between neighboring agents,
but moderate the communication and computational complexities by decentralizing
the sampling and computational processes over the network. For discrete-time
systems modeled by Markov decision processes, the joint Bellman equation of
each subsystem is transformed into a system of linear equations and solved
using parallel programming. For continuous-time systems modeled by It\^o
diffusion processes, the joint optimality equation of each subsystem is
converted into a linear partial differential equation, whose solution is
approximated by a path integral formulation and a sample-efficient relative
entropy policy search algorithm, respectively. The learned control policies are
generalized to solve the unlearned tasks by resorting to the compositionality
principle, and illustrative examples of cooperative UAV teams are provided to
verify the effectiveness and advantages of these algorithms.","cs.LG,cs.MA,cs.RO,cs.SY,eess.SY,math.OC"
"Federated Learning for Industrial Internet of Things in Future Industries. The Industrial Internet of Things (IIoT) offers promising opportunities to
transform the operation of industrial systems and becomes a key enabler for
future industries. Recently, artificial intelligence (AI) has been widely
utilized for realizing intelligent IIoT applications where AI techniques
require centralized data collection and processing. However, this is not always
feasible in realistic scenarios due to the high scalability of modern IIoT
networks and growing industrial data confidentiality. Federated Learning (FL),
as an emerging collaborative AI approach, is particularly attractive for
intelligent IIoT networks by coordinating multiple IIoT devices and machines to
perform AI training at the network edge while helping protect user privacy. In
this article, we provide a detailed overview and discussions of the emerging
applications of FL in key IIoT services and applications. A case study is also
provided to demonstrate the feasibility of FL in IIoT. Finally, we highlight a
range of interesting open research topics that need to be addressed for the
full realization of FL-IIoT in industries.","cs.LG,eess.SP"
"Evaluating Protein Transfer Learning with TAPE. Protein modeling is an increasingly popular area of machine learning
research. Semi-supervised learning has emerged as an important paradigm in
protein modeling due to the high cost of acquiring supervised protein labels,
but the current literature is fragmented when it comes to datasets and
standardized evaluation techniques. To facilitate progress in this field, we
introduce the Tasks Assessing Protein Embeddings (TAPE), a set of five
biologically relevant semi-supervised learning tasks spread across different
domains of protein biology. We curate tasks into specific training, validation,
and test splits to ensure that each task tests biologically relevant
generalization that transfers to real-life scenarios. We benchmark a range of
approaches to semi-supervised protein representation learning, which span
recent work as well as canonical sequence learning techniques. We find that
self-supervised pretraining is helpful for almost all models on all tasks, more
than doubling performance in some cases. Despite this increase, in several
cases features learned by self-supervised pretraining still lag behind features
extracted by state-of-the-art non-neural techniques. This gap in performance
suggests a huge opportunity for innovative architecture design and improved
modeling paradigms that better capture the signal in biological sequences. TAPE
will help the machine learning community focus effort on scientifically
relevant problems. Toward this end, all data and code used to run these
experiments are available at https://github.com/songlab-cal/tape.","cs.LG,q-bio.BM,stat.ML"
"Gradient Surgery for Multi-Task Learning. While deep learning and deep reinforcement learning (RL) systems have
demonstrated impressive results in domains such as image classification, game
playing, and robotic control, data efficiency remains a major challenge.
Multi-task learning has emerged as a promising approach for sharing structure
across multiple tasks to enable more efficient learning. However, the
multi-task setting presents a number of optimization challenges, making it
difficult to realize large efficiency gains compared to learning tasks
independently. The reasons why multi-task learning is so challenging compared
to single-task learning are not fully understood. In this work, we identify a
set of three conditions of the multi-task optimization landscape that cause
detrimental gradient interference, and develop a simple yet general approach
for avoiding such interference between task gradients. We propose a form of
gradient surgery that projects a task's gradient onto the normal plane of the
gradient of any other task that has a conflicting gradient. On a series of
challenging multi-task supervised and multi-task RL problems, this approach
leads to substantial gains in efficiency and performance. Further, it is
model-agnostic and can be combined with previously-proposed multi-task
architectures for enhanced performance.","cs.CV,cs.LG,cs.RO,stat.ML"
"EPOpt: Learning Robust Neural Network Policies Using Model Ensembles. Sample complexity and safety are major challenges when learning policies with
reinforcement learning for real-world tasks, especially when the policies are
represented using rich function approximators like deep neural networks.
Model-based methods where the real-world target domain is approximated using a
simulated source domain provide an avenue to tackle the above challenges by
augmenting real data with simulated data. However, discrepancies between the
simulated source domain and the target domain pose a challenge for simulated
training. We introduce the EPOpt algorithm, which uses an ensemble of simulated
source domains and a form of adversarial training to learn policies that are
robust and generalize to a broad range of possible target domains, including
unmodeled effects. Further, the probability distribution over source domains in
the ensemble can be adapted using data from target domain and approximate
Bayesian methods, to progressively make it a better approximation. Thus,
learning on a model ensemble, along with source domain adaptation, provides the
benefit of both robustness and learning/adaptation.","cs.AI,cs.LG,cs.RO"
"Texture Fuzzy Segmentation using Skew Divergence Adaptive Affinity Functions. Digital image segmentation is the process of assigning distinct labels to
different objects in a digital image, and the fuzzy segmentation algorithm has
been successfully used in the segmentation of images from a wide variety of
sources. However, the traditional fuzzy segmentation algorithm fails to segment
objects that are characterized by textures whose patterns cannot be
successfully described by simple statistics computed over a very restricted
area. In this paper, we propose an extension of the fuzzy segmentation
algorithm that uses adaptive textural affinity functions to perform the
segmentation of such objects on bidimensional images. The adaptive affinity
functions compute their appropriate neighborhood size as they compute the
texture descriptors surrounding the seed spels (spatial elements), according to
the characteristics of the texture being processed. The algorithm then segments
the image with an appropriate neighborhood for each object. We performed
experiments on mosaic images that were composed using images from the Brodatz
database, and compared our results with the ones produced by a recently
published texture segmentation algorithm, showing the applicability of our
method.","cs.AI,cs.CV,cs.GR"
"Single-Photon Image Classification. Quantum computing-based machine learning mainly focuses on quantum computing
hardware that is experimentally challenging to realize due to requiring quantum
gates that operate at very low temperature. Instead, we demonstrate the
existence of a lower performance and much lower effort island on the
accuracy-vs-qubits graph that may well be experimentally accessible with room
temperature optics. This high temperature ""quantum computing toy model"" is
nevertheless interesting to study as it allows rather accessible explanations
of key concepts in quantum computing, in particular interference, entanglement,
and the measurement process.
  We specifically study the problem of classifying an example from the MNIST
and Fashion-MNIST datasets, subject to the constraint that we have to make a
prediction after the detection of the very first photon that passed a
coherently illuminated filter showing the example. Whereas a classical set-up
in which a photon is detected after falling on one of the $28\times 28$ image
pixels is limited to a (maximum likelihood estimation) accuracy of $21.27\%$
for MNIST, respectively $18.27\%$ for Fashion-MNIST, we show that the
theoretically achievable accuracy when exploiting inference by optically
transforming the quantum state of the photon is at least $41.27\%$ for MNIST,
respectively $36.14\%$ for Fashion-MNIST.
  We show in detail how to train the corresponding transformation with
TensorFlow and also explain how this example can serve as a teaching tool for
the measurement process in quantum mechanics.","cs.LG,quant-ph,stat.ML"
"Sequential View Synthesis with Transformer. This paper addresses the problem of novel view synthesis by means of neural
rendering, where we are interested in predicting the novel view at an arbitrary
camera pose based on a given set of input images from other viewpoints. Using
the known query pose and input poses, we create an ordered set of observations
that leads to the target view. Thus, the problem of single novel view synthesis
is reformulated as a sequential view prediction task. In this paper, the
proposed Transformer-based Generative Query Network (T-GQN) extends the
neural-rendering methods by adding two new concepts. First, we use multi-view
attention learning between context images to obtain multiple implicit scene
representations. Second, we introduce a sequential rendering decoder to predict
an image sequence, including the target view, based on the learned
representations. Finally, we evaluate our model on various challenging datasets
and demonstrate that our model not only gives consistent predictions but also
doesn't require any retraining for finetuning.","cs.CV,cs.LG,eess.IV"
"Learning Efficient Photometric Feature Transform for Multi-view Stereo. We present a novel framework to learn to convert the perpixel photometric
information at each view into spatially distinctive and view-invariant
low-level features, which can be plugged into existing multi-view stereo
pipeline for enhanced 3D reconstruction. Both the illumination conditions
during acquisition and the subsequent per-pixel feature transform can be
jointly optimized in a differentiable fashion. Our framework automatically
adapts to and makes efficient use of the geometric information available in
different forms of input data. High-quality 3D reconstructions of a variety of
challenging objects are demonstrated on the data captured with an illumination
multiplexing device, as well as a point light. Our results compare favorably
with state-of-the-art techniques.","cs.CV,cs.GR"
"Vision Transformer for Learning Driving Policies in Complex Multi-Agent Environments. Driving in a complex urban environment is a difficult task that requires a
complex decision policy. In order to make informed decisions, one needs to gain
an understanding of the long-range context and the importance of other
vehicles. In this work, we propose to use Vision Transformer (ViT) to learn a
driving policy in urban settings with birds-eye-view (BEV) input images. The
ViT network learns the global context of the scene more effectively than with
earlier proposed Convolutional Neural Networks (ConvNets). Furthermore, ViT's
attention mechanism helps to learn an attention map for the scene which allows
the ego car to determine which surrounding cars are important to its next
decision. We demonstrate that a DQN agent with a ViT backbone outperforms
baseline algorithms with ConvNet backbones pre-trained in various ways. In
particular, the proposed method helps reinforcement learning algorithms to
learn faster, with increased performance and less data than baselines.","cs.AI,cs.LG,cs.MA,cs.RO"
"FAT-DeepFFM: Field Attentive Deep Field-aware Factorization Machine. Click through rate (CTR) estimation is a fundamental task in personalized
advertising and recommender systems. Recent years have witnessed the success of
both the deep learning based model and attention mechanism in various tasks in
computer vision (CV) and natural language processing (NLP). How to combine the
attention mechanism with deep CTR model is a promising direction because it may
ensemble the advantages of both sides. Although some CTR model such as
Attentional Factorization Machine (AFM) has been proposed to model the weight
of second order interaction features, we posit the evaluation of feature
importance before explicit feature interaction procedure is also important for
CTR prediction tasks because the model can learn to selectively highlight the
informative features and suppress less useful ones if the task has many input
features. In this paper, we propose a new neural CTR model named Field
Attentive Deep Field-aware Factorization Machine (FAT-DeepFFM) by combining the
Deep Field-aware Factorization Machine (DeepFFM) with Compose-Excitation
network (CENet) field attention mechanism which is proposed by us as an
enhanced version of Squeeze-Excitation network (SENet) to highlight the feature
importance. We conduct extensive experiments on two real-world datasets and the
experiment results show that FAT-DeepFFM achieves the best performance and
obtains different improvements over the state-of-the-art methods. We also
compare two kinds of attention mechanisms (attention before explicit feature
interaction vs. attention after explicit feature interaction) and demonstrate
that the former one outperforms the latter one significantly.","cs.IR,cs.LG"
"Costs and Benefits of Wasserstein Fair Regression. Real-world applications of machine learning tools in high-stakes domains are
often regulated to be fair, in the sense that the predicted target should
satisfy some quantitative notion of parity with respect to a protected
attribute. However, the exact tradeoff between fairness and accuracy with a
real-valued target is not clear. In this paper, we characterize the inherent
tradeoff between statistical parity and accuracy in the regression setting by
providing a lower bound on the error of any fair regressor. Our lower bound is
sharp, algorithm-independent, and admits a simple interpretation: when the
moments of the target differ between groups, any fair algorithm has to make a
large error on at least one of the groups. We further extend this result to
give a lower bound on the joint error of any (approximately) fair algorithm,
using the Wasserstein distance to measure the quality of the approximation. On
the upside, we establish the first connection between individual fairness,
accuracy parity, and the Wasserstein distance by showing that if a regressor is
individually fair, it also approximately verifies the accuracy parity, where
the gap is given by the Wasserstein distance between the two groups. Inspired
by our theoretical results, we develop a practical algorithm for fair
regression through the lens of representation learning, and conduct experiments
on a real-world dataset to corroborate our findings.","cs.AI,cs.CY,cs.LG,stat.ML"
"Deeply Self-Supervised Contour Embedded Neural Network Applied to Liver Segmentation. Objective: Herein, a neural network-based liver segmentation algorithm is
proposed, and its performance was evaluated using abdominal computed tomography
(CT) images. Methods: A fully convolutional network was developed to overcome
the volumetric image segmentation problem. To guide a neural network to
accurately delineate a target liver object, the network was deeply supervised
by applying the adaptive self-supervision scheme to derive the essential
contour, which acted as a complement with the global shape. The discriminative
contour, shape, and deep features were internally merged for the segmentation
results. Results and Conclusion: 160 abdominal CT images were used for training
and validation. The quantitative evaluation of the proposed network was
performed through an eight-fold cross-validation. The result showed that the
method, which uses the contour feature, segmented the liver more accurately
than the state-of-the-art with a 2.13% improvement in the dice score.
Significance: In this study, a new framework was introduced to guide a neural
network and learn complementary contour features. The proposed neural network
demonstrates that the guided contour features can significantly improve the
performance of the segmentation task.","68U10,cs.CV"
"Provenance Graph Kernel. Provenance is a record that describes how entities, activities, and agents
have influenced a piece of data; it is commonly represented as graphs with
relevant labels on both their nodes and edges. With the growing adoption of
provenance in a wide range of application domains, users are increasingly
confronted with an abundance of graph data, which may prove challenging to
process. Graph kernels, on the other hand, have been successfully used to
efficiently analyse graphs. In this paper, we introduce a novel graph kernel
called provenance kernel, which is inspired by and tailored for provenance
data. It decomposes a provenance graph into tree-patterns rooted at a given
node and considers the labels of edges and nodes up to a certain distance from
the root. We employ provenance kernels to classify provenance graphs from three
application domains. Our evaluation shows that they perform well in terms of
classification accuracy and yield competitive results when compared against
existing graph kernel methods and the provenance network analytics method while
more efficient in computing time. Moreover, the provenance types used by
provenance kernels also help improve the explainability of predictive models
built on them.","I.2.6,cs.AI,cs.DB,cs.LG"
"Explainable Reinforcement Learning Through a Causal Lens. Prevalent theories in cognitive science propose that humans understand and
represent the knowledge of the world through causal relationships. In making
sense of the world, we build causal models in our mind to encode cause-effect
relations of events and use these to explain why new events happen. In this
paper, we use causal models to derive causal explanations of behaviour of
reinforcement learning agents. We present an approach that learns a structural
causal model during reinforcement learning and encodes causal relationships
between variables of interest. This model is then used to generate explanations
of behaviour based on counterfactual analysis of the causal model. We report on
a study with 120 participants who observe agents playing a real-time strategy
game (Starcraft II) and then receive explanations of the agents' behaviour. We
investigated: 1) participants' understanding gained by explanations through
task prediction; 2) explanation satisfaction and 3) trust. Our results show
that causal model explanations perform better on these measures compared to two
other baseline explanation models.","cs.AI,cs.HC,cs.LG,stat.ML"
"Cheating Detection Pipeline for Online Interviews and Exams. Remote examination and job interviews have gained popularity and become
indispensable because of both pandemics and the advantage of remote working
circumstances. Most companies and academic institutions utilize these systems
for their recruitment processes and also for online exams. However, one of the
critical problems of the remote examination systems is conducting the exams in
a reliable environment. In this work, we present a cheating analysis pipeline
for online interviews and exams. The system only requires a video of the
candidate, which is recorded during the exam. Then cheating detection pipeline
is employed to detect another person, electronic device usage, and candidate
absence status. The pipeline consists of face detection, face recognition,
object detection, and face tracking algorithms. To evaluate the performance of
the pipeline we collected a private video dataset. The video dataset includes
both cheating activities and clean videos. Ultimately, our pipeline presents an
efficient and fast guideline to detect and analyze cheating activities in an
online interview and exam video.","cs.AI,cs.CV,cs.HC,cs.LG,cs.MM"
"Flying Guide Dog: Walkable Path Discovery for the Visually Impaired Utilizing Drones and Transformer-based Semantic Segmentation. Lacking the ability to sense ambient environments effectively, blind and
visually impaired people (BVIP) face difficulty in walking outdoors, especially
in urban areas. Therefore, tools for assisting BVIP are of great importance. In
this paper, we propose a novel ""flying guide dog"" prototype for BVIP assistance
using drone and street view semantic segmentation. Based on the walkable areas
extracted from the segmentation prediction, the drone can adjust its movement
automatically and thus lead the user to walk along the walkable path. By
recognizing the color of pedestrian traffic lights, our prototype can help the
user to cross a street safely. Furthermore, we introduce a new dataset named
Pedestrian and Vehicle Traffic Lights (PVTL), which is dedicated to traffic
light recognition. The result of our user study in real-world scenarios shows
that our prototype is effective and easy to use, providing new insight into
BVIP assistance.","cs.CV,cs.HC,cs.RO,eess.IV"
"MonoPerfCap: Human Performance Capture from Monocular Video. We present the first marker-less approach for temporally coherent 3D
performance capture of a human with general clothing from monocular video. Our
approach reconstructs articulated human skeleton motion as well as medium-scale
non-rigid surface deformations in general scenes. Human performance capture is
a challenging problem due to the large range of articulation, potentially fast
motion, and considerable non-rigid deformations, even from multi-view data.
Reconstruction from monocular video alone is drastically more challenging,
since strong occlusions and the inherent depth ambiguity lead to a highly
ill-posed reconstruction problem. We tackle these challenges by a novel
approach that employs sparse 2D and 3D human pose detections from a
convolutional neural network using a batch-based pose estimation strategy.
Joint recovery of per-batch motion allows to resolve the ambiguities of the
monocular reconstruction problem based on a low dimensional trajectory
subspace. In addition, we propose refinement of the surface geometry based on
fully automatically extracted silhouettes to enable medium-scale non-rigid
alignment. We demonstrate state-of-the-art performance capture results that
enable exciting applications such as video editing and free viewpoint video,
previously infeasible from monocular video. Our qualitative and quantitative
evaluation demonstrates that our approach significantly outperforms previous
monocular methods in terms of accuracy, robustness and scene complexity that
can be handled.","cs.CV,cs.GR"
"DeepPhys: Video-Based Physiological Measurement Using Convolutional Attention Networks. Non-contact video-based physiological measurement has many applications in
health care and human-computer interaction. Practical applications require
measurements to be accurate even in the presence of large head rotations. We
propose the first end-to-end system for video-based measurement of heart and
breathing rate using a deep convolutional network. The system features a new
motion representation based on a skin reflection model and a new attention
mechanism using appearance information to guide motion estimation, both of
which enable robust measurement under heterogeneous lighting and major motions.
Our approach significantly outperforms all current state-of-the-art methods on
both RGB and infrared video datasets. Furthermore, it allows spatial-temporal
distributions of physiological signals to be visualized via the attention
mechanism.","cs.CV,cs.HC"
"An Exploration of 2D and 3D Deep Learning Techniques for Cardiac MR Image Segmentation. Accurate segmentation of the heart is an important step towards evaluating
cardiac function. In this paper, we present a fully automated framework for
segmentation of the left (LV) and right (RV) ventricular cavities and the
myocardium (Myo) on short-axis cardiac MR images. We investigate various 2D and
3D convolutional neural network architectures for this task. We investigate the
suitability of various state-of-the art 2D and 3D convolutional neural network
architectures, as well as slight modifications thereof, for this task.
Experiments were performed on the ACDC 2017 challenge training dataset
comprising cardiac MR images of 100 patients, where manual reference
segmentations were made available for end-diastolic (ED) and end-systolic (ES)
frames. We find that processing the images in a slice-by-slice fashion using 2D
networks is beneficial due to a relatively large slice thickness. However, the
exact network architecture only plays a minor role. We report mean Dice
coefficients of $0.950$ (LV), $0.893$ (RV), and $0.899$ (Myo), respectively
with an average evaluation time of 1.1 seconds per volume on a modern GPU.",cs.CV
"ETA Prediction with Graph Neural Networks in Google Maps. Travel-time prediction constitutes a task of high importance in
transportation networks, with web mapping services like Google Maps regularly
serving vast quantities of travel time queries from users and enterprises
alike. Further, such a task requires accounting for complex spatiotemporal
interactions (modelling both the topological properties of the road network and
anticipating events -- such as rush hours -- that may occur in the future).
Hence, it is an ideal target for graph representation learning at scale. Here
we present a graph neural network estimator for estimated time of arrival (ETA)
which we have deployed in production at Google Maps. While our main
architecture consists of standard GNN building blocks, we further detail the
usage of training schedule methods such as MetaGradients in order to make our
model robust and production-ready. We also provide prescriptive studies:
ablating on various architectural decisions and training regimes, and
qualitative analyses on real-world situations where our model provides a
competitive edge. Our GNN proved powerful when deployed, significantly reducing
negative ETA outcomes in several regions compared to the previous production
baseline (40+% in cities like Sydney).","cs.AI,cs.LG,cs.SI"
"Introducing Symmetries to Black Box Meta Reinforcement Learning. Meta reinforcement learning (RL) attempts to discover new RL algorithms
automatically from environment interaction. In so-called black-box approaches,
the policy and the learning algorithm are jointly represented by a single
neural network. These methods are very flexible, but they tend to underperform
in terms of generalisation to new, unseen environments. In this paper, we
explore the role of symmetries in meta-generalisation. We show that a recent
successful meta RL approach that meta-learns an objective for
backpropagation-based learning exhibits certain symmetries (specifically the
reuse of the learning rule, and invariance to input and output permutations)
that are not present in typical black-box meta RL systems. We hypothesise that
these symmetries can play an important role in meta-generalisation. Building
off recent work in black-box supervised meta learning, we develop a black-box
meta RL system that exhibits these same symmetries. We show through careful
experimentation that incorporating these symmetries can lead to algorithms with
a greater ability to generalise to unseen action & observation spaces, tasks,
and environments.","cs.AI,cs.LG,cs.NE,stat.ML"
"On the Opportunities and Risks of Foundation Models. AI is undergoing a paradigm shift with the rise of models (e.g., BERT,
DALL-E, GPT-3) that are trained on broad data at scale and are adaptable to a
wide range of downstream tasks. We call these models foundation models to
underscore their critically central yet incomplete character. This report
provides a thorough account of the opportunities and risks of foundation
models, ranging from their capabilities (e.g., language, vision, robotics,
reasoning, human interaction) and technical principles(e.g., model
architectures, training procedures, data, systems, security, evaluation,
theory) to their applications (e.g., law, healthcare, education) and societal
impact (e.g., inequity, misuse, economic and environmental impact, legal and
ethical considerations). Though foundation models are based on standard deep
learning and transfer learning, their scale results in new emergent
capabilities,and their effectiveness across so many tasks incentivizes
homogenization. Homogenization provides powerful leverage but demands caution,
as the defects of the foundation model are inherited by all the adapted models
downstream. Despite the impending widespread deployment of foundation models,
we currently lack a clear understanding of how they work, when they fail, and
what they are even capable of due to their emergent properties. To tackle these
questions, we believe much of the critical research on foundation models will
require deep interdisciplinary collaboration commensurate with their
fundamentally sociotechnical nature.","cs.AI,cs.CY,cs.LG"
"On Locality of Local Explanation Models. Shapley values provide model agnostic feature attributions for model outcome
at a particular instance by simulating feature absence under a global
population distribution. The use of a global population can lead to potentially
misleading results when local model behaviour is of interest. Hence we consider
the formulation of neighbourhood reference distributions that improve the local
interpretability of Shapley values. By doing so, we find that the
Nadaraya-Watson estimator, a well-studied kernel regressor, can be expressed as
a self-normalised importance sampling estimator. Empirically, we observe that
Neighbourhood Shapley values identify meaningful sparse feature relevance
attributions that provide insight into local model behaviour, complimenting
conventional Shapley analysis. They also increase on-manifold explainability
and robustness to the construction of adversarial classifiers.","cs.LG,stat.CO,stat.ME,stat.ML"
"Automated Theorem Proving in Intuitionistic Propositional Logic by Deep Reinforcement Learning. The problem-solving in automated theorem proving (ATP) can be interpreted as
a search problem where the prover constructs a proof tree step by step. In this
paper, we propose a deep reinforcement learning algorithm for proof search in
intuitionistic propositional logic. The most significant challenge in the
application of deep learning to the ATP is the absence of large, public theorem
database. We, however, overcame this issue by applying a novel data
augmentation procedure at each iteration of the reinforcement learning. We also
improve the efficiency of the algorithm by representing the syntactic structure
of formulas by a novel compact graph representation. Using the large volume of
augmented data, we train highly accurate graph neural networks that approximate
the value function for the set of the syntactic structures of formulas. Our
method is also cost-efficient in terms of computational time. We will show that
our prover outperforms Coq's $\texttt{tauto}$ tactic, a prover based on
human-engineered heuristics. Within the specified time limit, our prover solved
84% of the theorems in a benchmark library, while $\texttt{tauto}$ was able to
solve only 52%.","cs.AI,cs.LG,cs.LO,stat.ML"
"Decimated Framelet System on Graphs and Fast G-Framelet Transforms. Graph representation learning has many real-world applications, from
super-resolution imaging, 3D computer vision to drug repurposing, protein
classification, social networks analysis. An adequate representation of graph
data is vital to the learning performance of a statistical or machine learning
model for graph-structured data. In this paper, we propose a novel multiscale
representation system for graph data, called decimated framelets, which form a
localized tight frame on the graph. The decimated framelet system allows
storage of the graph data representation on a coarse-grained chain and
processes the graph data at multi scales where at each scale, the data is
stored at a subgraph. Based on this, we then establish decimated G-framelet
transforms for the decomposition and reconstruction of the graph data at multi
resolutions via a constructive data-driven filter bank. The graph framelets are
built on a chain-based orthonormal basis that supports fast graph Fourier
transforms. From this, we give a fast algorithm for the decimated G-framelet
transforms, or FGT, that has linear computational complexity O(N) for a graph
of size N. The theory of decimated framelets and FGT is verified with numerical
examples for random graphs. The effectiveness is demonstrated by real-world
applications, including multiresolution analysis for traffic network, and graph
neural networks for graph classification tasks.","cs.LG,cs.NA,math.NA"
"Generating equilibrium molecules with deep neural networks. Discovery of atomistic systems with desirable properties is a major challenge
in chemistry and material science. Here we introduce a novel, autoregressive,
convolutional deep neural network architecture that generates molecular
equilibrium structures by sequentially placing atoms in three-dimensional
space. The model estimates the joint probability over molecular configurations
with tractable conditional probabilities which only depend on distances between
atoms and their nuclear charges. It combines concepts from state-of-the-art
atomistic neural networks with auto-regressive generative models for images and
speech. We demonstrate that the architecture is capable of generating molecules
close to equilibrium for constitutional isomers of C$_7$O$_2$H$_{10}$.","cs.LG,physics.chem-ph,stat.ML"
"Joint Policy Search for Multi-agent Collaboration with Imperfect Information. To learn good joint policies for multi-agent collaboration with imperfect
information remains a fundamental challenge. While for two-player zero-sum
games, coordinate-ascent approaches (optimizing one agent's policy at a time,
e.g., self-play) work with guarantees, in multi-agent cooperative setting they
often converge to sub-optimal Nash equilibrium. On the other hand, directly
modeling joint policy changes in imperfect information game is nontrivial due
to complicated interplay of policies (e.g., upstream updates affect downstream
state reachability). In this paper, we show global changes of game values can
be decomposed to policy changes localized at each information set, with a novel
term named policy-change density. Based on this, we propose Joint Policy
Search(JPS) that iteratively improves joint policies of collaborative agents in
imperfect information games, without re-evaluating the entire game. On
multi-agent collaborative tabular games, JPS is proven to never worsen
performance and can improve solutions provided by unilateral approaches (e.g,
CFR), outperforming algorithms designed for collaborative policy learning (e.g.
BAD). Furthermore, for real-world games, JPS has an online form that naturally
links with gradient updates. We test it to Contract Bridge, a 4-player
imperfect-information game where a team of $2$ collaborates to compete against
the other. In its bidding phase, players bid in turn to find a good contract
through a limited information channel. Based on a strong baseline agent that
bids competitive bridge purely through domain-agnostic self-play, JPS improves
collaboration of team players and outperforms WBridge5, a championship-winning
software, by $+0.63$ IMPs (International Matching Points) per board over 1k
games, substantially better than previous SoTA ($+0.41$ IMPs/b) under
Double-Dummy evaluation.","cs.AI,cs.GT,cs.LG,cs.MA,stat.ML"
"Selecting Optimal Trace Clustering Pipelines with AutoML. Trace clustering has been extensively used to preprocess event logs. By
grouping similar behavior, these techniques guide the identification of
sub-logs, producing more understandable models and conformance analytics.
Nevertheless, little attention has been posed to the relationship between event
log properties and clustering quality. In this work, we propose an Automatic
Machine Learning (AutoML) framework to recommend the most suitable pipeline for
trace clustering given an event log, which encompasses the encoding method,
clustering algorithm, and its hyperparameters. Our experiments were conducted
using a thousand event logs, four encoding techniques, and three clustering
methods. Results indicate that our framework sheds light on the trace
clustering problem and can assist users in choosing the best pipeline
considering their scenario.","cs.IR,cs.LG,cs.SE"
"Code Generation as a Dual Task of Code Summarization. Code summarization (CS) and code generation (CG) are two crucial tasks in the
field of automatic software development. Various neural network-based
approaches are proposed to solve these two tasks separately. However, there
exists a specific intuitive correlation between CS and CG, which have not been
exploited in previous work. In this paper, we apply the relations between two
tasks to improve the performance of both tasks. In other words, exploiting the
duality between the two tasks, we propose a dual training framework to train
the two tasks simultaneously. In this framework, we consider the dualities on
probability and attention weights, and design corresponding regularization
terms to constrain the duality. We evaluate our approach on two datasets
collected from GitHub, and experimental results show that our dual framework
can improve the performance of CS and CG tasks over baselines.","cs.AI,cs.LG,cs.SE"
"A novel approach for modelling and classifying sit-to-stand kinematics using inertial sensors. Sit-to-stand transitions are an important part of activities of daily living
and play a key role in functional mobility in humans. The sit-to-stand movement
is often affected in older adults due to frailty and in patients with motor
impairments such as Parkinson's disease leading to falls. Studying kinematics
of sit-to-stand transitions can provide insight in assessment, monitoring and
developing rehabilitation strategies for the affected populations. We propose a
three-segment body model for estimating sit-to-stand kinematics using only two
wearable inertial sensors, placed on the shank and back. Reducing the number of
sensors to two instead of one per body segment facilitates monitoring and
classifying movements over extended periods, making it more comfortable to wear
while reducing the power requirements of sensors. We applied this model on 10
younger healthy adults (YH), 12 older healthy adults (OH) and 12 people with
Parkinson's disease (PwP). We have achieved this by incorporating unique
sit-to-stand classification technique using unsupervised learning in the model
based reconstruction of angular kinematics using extended Kalman filter. Our
proposed model showed that it was possible to successfully estimate thigh
kinematics despite not measuring the thigh motion with inertial sensor. We
classified sit-to-stand transitions, sitting and standing states with the
accuracies of 98.67%, 94.20% and 91.41% for YH, OH and PwP respectively. We
have proposed a novel integrated approach of modelling and classification for
estimating the body kinematics during sit-to-stand motion and successfully
applied it on YH, OH and PwP groups.","cs.LG,cs.RO,eess.SP"
"Representation Learning and Recovery in the ReLU Model. Rectified linear units, or ReLUs, have become the preferred activation
function for artificial neural networks. In this paper we consider two basic
learning problems assuming that the underlying data follow a generative model
based on a ReLU-network -- a neural network with ReLU activations. As a
primarily theoretical study, we limit ourselves to a single-layer network. The
first problem we study corresponds to dictionary-learning in the presence of
nonlinearity (modeled by the ReLU functions). Given a set of observation
vectors $\mathbf{y}^i \in \mathbb{R}^d, i =1, 2, \dots , n$, we aim to recover
$d\times k$ matrix $A$ and the latent vectors $\{\mathbf{c}^i\} \subset
\mathbb{R}^k$ under the model $\mathbf{y}^i = \mathrm{ReLU}(A\mathbf{c}^i
+\mathbf{b})$, where $\mathbf{b}\in \mathbb{R}^d$ is a random bias. We show
that it is possible to recover the column space of $A$ within an error of
$O(d)$ (in Frobenius norm) under certain conditions on the probability
distribution of $\mathbf{b}$.
  The second problem we consider is that of robust recovery of the signal in
the presence of outliers, i.e., large but sparse noise. In this setting we are
interested in recovering the latent vector $\mathbf{c}$ from its noisy
nonlinear sketches of the form $\mathbf{v} = \mathrm{ReLU}(A\mathbf{c}) +
\mathbf{e}+\mathbf{w}$, where $\mathbf{e} \in \mathbb{R}^d$ denotes the
outliers with sparsity $s$ and $\mathbf{w} \in \mathbb{R}^d$ denote the dense
but small noise. This line of work has recently been studied (Soltanolkotabi,
2017) without the presence of outliers. For this problem, we show that a
generalized LASSO algorithm is able to recover the signal $\mathbf{c} \in
\mathbb{R}^k$ within an $\ell_2$ error of $O(\sqrt{\frac{(k+s)\log d}{d}})$
when $A$ is a random Gaussian matrix.","cs.IT,cs.LG,math.IT,stat.ML"
"Decoupled Data Based Approach for Learning to Control Nonlinear Dynamical Systems. This paper addresses the problem of learning the optimal control policy for a
nonlinear stochastic dynamical system with continuous state space, continuous
action space and unknown dynamics. This class of problems are typically
addressed in stochastic adaptive control and reinforcement learning literature
using model-based and model-free approaches respectively. Both methods rely on
solving a dynamic programming problem, either directly or indirectly, for
finding the optimal closed loop control policy. The inherent `curse of
dimensionality' associated with dynamic programming method makes these
approaches also computationally difficult.
  This paper proposes a novel decoupled data-based control (D2C) algorithm that
addresses this problem using a decoupled, `open loop - closed loop', approach.
First, an open-loop deterministic trajectory optimization problem is solved
using a black-box simulation model of the dynamical system. Then, a closed loop
control is developed around this open loop trajectory by linearization of the
dynamics about this nominal trajectory. By virtue of linearization, a linear
quadratic regulator based algorithm can be used for this closed loop control.
We show that the performance of D2C algorithm is approximately optimal.
Moreover, simulation performance suggests significant reduction in training
time compared to other state of the art algorithms.","cs.LG,cs.RO,cs.SY,stat.ML"
"AutoFL: Enabling Heterogeneity-Aware Energy Efficient Federated Learning. Federated learning enables a cluster of decentralized mobile devices at the
edge to collaboratively train a shared machine learning model, while keeping
all the raw training samples on device. This decentralized training approach is
demonstrated as a practical solution to mitigate the risk of privacy leakage.
However, enabling efficient FL deployment at the edge is challenging because of
non-IID training data distribution, wide system heterogeneity and
stochastic-varying runtime effects in the field. This paper jointly optimizes
time-to-convergence and energy efficiency of state-of-the-art FL use cases by
taking into account the stochastic nature of edge execution. We propose AutoFL
by tailor-designing a reinforcement learning algorithm that learns and
determines which K participant devices and per-device execution targets for
each FL model aggregation round in the presence of stochastic runtime variance,
system and data heterogeneity. By considering the unique characteristics of FL
edge deployment judiciously, AutoFL achieves 3.6 times faster model convergence
time and 4.7 and 5.2 times higher energy efficiency for local clients and
globally over the cluster of K participants, respectively.","cs.DC,cs.LG"
"NPAS: A Compiler-aware Framework of Unified Network Pruning and Architecture Search for Beyond Real-Time Mobile Acceleration. With the increasing demand to efficiently deploy DNNs on mobile edge devices,
it becomes much more important to reduce unnecessary computation and increase
the execution speed. Prior methods towards this goal, including model
compression and network architecture search (NAS), are largely performed
independently and do not fully consider compiler-level optimizations which is a
must-do for mobile acceleration. In this work, we first propose (i) a general
category of fine-grained structured pruning applicable to various DNN layers,
and (ii) a comprehensive, compiler automatic code generation framework
supporting different DNNs and different pruning schemes, which bridge the gap
of model compression and NAS. We further propose NPAS, a compiler-aware unified
network pruning, and architecture search. To deal with large search space, we
propose a meta-modeling procedure based on reinforcement learning with fast
evaluation and Bayesian optimization, ensuring the total number of training
epochs comparable with representative NAS frameworks. Our framework achieves
6.7ms, 5.9ms, 3.9ms ImageNet inference times with 78.2%, 75% (MobileNet-V3
level), and 71% (MobileNet-V2 level) Top-1 accuracy respectively on an
off-the-shelf mobile phone, consistently outperforming prior work.","cs.AI,cs.CV,cs.LG,cs.NE"
"Splitting Methods for Convex Clustering. Clustering is a fundamental problem in many scientific applications. Standard
methods such as $k$-means, Gaussian mixture models, and hierarchical
clustering, however, are beset by local minima, which are sometimes drastically
suboptimal. Recently introduced convex relaxations of $k$-means and
hierarchical clustering shrink cluster centroids toward one another and ensure
a unique global minimizer. In this work we present two splitting methods for
solving the convex clustering problem. The first is an instance of the
alternating direction method of multipliers (ADMM); the second is an instance
of the alternating minimization algorithm (AMA). In contrast to previously
considered algorithms, our ADMM and AMA formulations provide simple and unified
frameworks for solving the convex clustering problem under the previously
studied norms and open the door to potentially novel norms. We demonstrate the
performance of our algorithm on both simulated and real data examples. While
the differences between the two algorithms appear to be minor on the surface,
complexity analysis and numerical experiments show AMA to be significantly more
efficient.","62H30,90C25,90C90,math.NA,math.OC,stat.CO,stat.ML"
"LesionSeg: Semantic segmentation of skin lesions using Deep Convolutional Neural Network. We present a method for skin lesion segmentation for the ISIC 2017 Skin
Lesion Segmentation Challenge. Our approach is based on a Fully Convolutional
Network architecture which is trained end to end, from scratch, on a limited
dataset. Our semantic segmentation architecture utilizes several recent
innovations in particularly in the combined use of (i) use of atrous
convolutions to increase the effective field of view of the network's receptive
field without increasing the number of parameters, (ii) the use of
network-in-network $1\times1$ convolution layers to add capacity to the network
and (iii) state-of-art super-resolution upsampling of predictions using
subpixel CNN layers. We reported a mean IOU score of 0.642 on the validation
set provided by the organisers.","cs.AI,cs.CV,cs.NE"
"Emotional Expression Classification using Time-Series Kernels. Estimation of facial expressions, as spatio-temporal processes, can take
advantage of kernel methods if one considers facial landmark positions and
their motion in 3D space. We applied support vector classification with kernels
derived from dynamic time-warping similarity measures. We achieved over 99%
accuracy - measured by area under ROC curve - using only the 'motion pattern'
of the PCA compressed representation of the marker point vector, the so-called
shape parameters. Beyond the classification of full motion patterns, several
expressions were recognized with over 90% accuracy in as few as 5-6 frames from
their onset, about 200 milliseconds.","37M10,46E22,62H30,65D19,68T10,68U10,G.3; I.2.10; I.4; I.5.4,cs.CV,cs.LG,stat.ML"
"Partial FC: Training 10 Million Identities on a Single Machine. Face recognition has been an active and vital topic among computer vision
community for a long time. Previous researches mainly focus on loss functions
used for facial feature extraction network, among which the improvements of
softmax-based loss functions greatly promote the performance of face
recognition. However, the contradiction between the drastically increasing
number of face identities and the shortage of GPU memories is gradually
becoming irreconcilable. In this paper, we thoroughly analyze the optimization
goal of softmax-based loss functions and the difficulty of training massive
identities. We find that the importance of negative classes in softmax function
in face representation learning is not as high as we previously thought. The
experiment demonstrates no loss of accuracy when training with only 10\%
randomly sampled classes for the softmax-based loss functions, compared with
training with full classes using state-of-the-art models on mainstream
benchmarks. We also implement a very efficient distributed sampling algorithm,
taking into account model accuracy and training efficiency, which uses only
eight NVIDIA RTX2080Ti to complete classification tasks with tens of millions
of identities. The code of this paper has been made available
https://github.com/deepinsight/insightface/tree/master/recognition/partial_fc.","cs.CV,cs.DC"
"Etalumis: Bringing Probabilistic Programming to Scientific Simulators at Scale. Probabilistic programming languages (PPLs) are receiving widespread attention
for performing Bayesian inference in complex generative models. However,
applications to science remain limited because of the impracticability of
rewriting complex scientific simulators in a PPL, the computational cost of
inference, and the lack of scalable implementations. To address these, we
present a novel PPL framework that couples directly to existing scientific
simulators through a cross-platform probabilistic execution protocol and
provides Markov chain Monte Carlo (MCMC) and deep-learning-based inference
compilation (IC) engines for tractable inference. To guide IC inference, we
perform distributed training of a dynamic 3DCNN--LSTM architecture with a
PyTorch-MPI-based framework on 1,024 32-core CPU nodes of the Cori
supercomputer with a global minibatch size of 128k: achieving a performance of
450 Tflop/s through enhancements to PyTorch. We demonstrate a Large Hadron
Collider (LHC) use-case with the C++ Sherpa simulator and achieve the
largest-scale posterior inference in a Turing-complete PPL.","62P35,68T05,68T37,G.3; I.2.6; J.2,cs.LG,cs.PF,stat.ML"
"Safety Verification of Model Based Reinforcement Learning Controllers. Model-based reinforcement learning (RL) has emerged as a promising tool for
developing controllers for real world systems (e.g., robotics, autonomous
driving, etc.). However, real systems often have constraints imposed on their
state space which must be satisfied to ensure the safety of the system and its
environment. Developing a verification tool for RL algorithms is challenging
because the non-linear structure of neural networks impedes analytical
verification of such models or controllers. To this end, we present a novel
safety verification framework for model-based RL controllers using reachable
set analysis. The proposed frame-work can efficiently handle models and
controllers which are represented using neural networks. Additionally, if a
controller fails to satisfy the safety constraints in general, the proposed
framework can also be used to identify the subset of initial states from which
the controller can be safely executed.","cs.LG,cs.RO,cs.SY,eess.SY"
"A Junction Tree Framework for Undirected Graphical Model Selection. An undirected graphical model is a joint probability distribution defined on
an undirected graph G*, where the vertices in the graph index a collection of
random variables and the edges encode conditional independence relationships
among random variables. The undirected graphical model selection (UGMS) problem
is to estimate the graph G* given observations drawn from the undirected
graphical model. This paper proposes a framework for decomposing the UGMS
problem into multiple subproblems over clusters and subsets of the separators
in a junction tree. The junction tree is constructed using a graph that
contains a superset of the edges in G*. We highlight three main properties of
using junction trees for UGMS. First, different regularization parameters or
different UGMS algorithms can be used to learn different parts of the graph.
This is possible since the subproblems we identify can be solved independently
of each other. Second, under certain conditions, a junction tree based UGMS
algorithm can produce consistent results with fewer observations than the usual
requirements of existing algorithms. Third, both our theoretical and
experimental results show that the junction tree framework does a significantly
better job at finding the weakest edges in a graph than existing methods. This
property is a consequence of both the first and second properties. Finally, we
note that our framework is independent of the choice of the UGMS algorithm and
can be used as a wrapper around standard UGMS algorithms for more accurate
graph estimation.","cs.AI,cs.IT,math.IT,stat.ML"
"Fast Planar Correlation Clustering for Image Segmentation. We describe a new optimization scheme for finding high-quality correlation
clusterings in planar graphs that uses weighted perfect matching as a
subroutine. Our method provides lower-bounds on the energy of the optimal
correlation clustering that are typically fast to compute and tight in
practice. We demonstrate our algorithm on the problem of image segmentation
where this approach outperforms existing global optimization techniques in
minimizing the objective and is competitive with the state of the art in
producing high-quality segmentations.","cs.CV,cs.DS,cs.LG,stat.ML"
"Fully Decentralized Multi-Agent Reinforcement Learning with Networked Agents. We consider the problem of \emph{fully decentralized} multi-agent
reinforcement learning (MARL), where the agents are located at the nodes of a
time-varying communication network. Specifically, we assume that the reward
functions of the agents might correspond to different tasks, and are only known
to the corresponding agent. Moreover, each agent makes individual decisions
based on both the information observed locally and the messages received from
its neighbors over the network. Within this setting, the collective goal of the
agents is to maximize the globally averaged return over the network through
exchanging information with their neighbors. To this end, we propose two
decentralized actor-critic algorithms with function approximation, which are
applicable to large-scale MARL problems where both the number of states and the
number of agents are massively large. Under the decentralized structure, the
actor step is performed individually by each agent with no need to infer the
policies of others. For the critic step, we propose a consensus update via
communication over the network. Our algorithms are fully incremental and can be
implemented in an online fashion. Convergence analyses of the algorithms are
provided when the value functions are approximated within the class of linear
functions. Extensive simulation results with both linear and nonlinear function
approximations are presented to validate the proposed algorithms. Our work
appears to be the first study of fully decentralized MARL algorithms for
networked agents with function approximation, with provable convergence
guarantees.","cs.AI,cs.LG,cs.MA,math.OC,stat.ML"
"Evolving Context-Aware Recommender Systems With Users in Mind. A context-aware recommender system (CARS) applies sensing and analysis of
user context to provide personalized services. The contextual information can
be driven from sensors in order to improve the accuracy of the recommendations.
Yet, generating accurate recommendations is not enough to constitute a useful
system from the users' perspective, since certain contextual information may
cause different issues, such as draining the user's battery, privacy issues,
and more. Adding high-dimensional contextual information may increase both the
dimensionality and sparsity of the model. Previous studies suggest reducing the
amount of contextual information by selecting the most suitable contextual
information using a domain knowledge. Another solution is compressing it into a
denser latent space, thus disrupting the ability to explain the recommendation
item to the user, and damaging users' trust. In this paper we present an
approach for selecting low-dimensional subsets of the contextual information
and incorporating them explicitly within CARS. Specifically, we present a novel
feature-selection algorithm, based on genetic algorithms (GA), that outperforms
SOTA dimensional-reduction CARS algorithms, improves the accuracy and the
explainability of the recommendations, and allows for controlling user aspects,
such as privacy and battery consumption. Furthermore, we exploit the top
subsets that are generated along the evolutionary process, by learning multiple
deep context-aware models and applying a stacking technique on them, thus
improving the accuracy while remaining at the explicit space. We evaluated our
approach on two high-dimensional context-aware datasets driven from
smartphones. An empirical analysis of our results validates that our proposed
approach outperforms SOTA CARS models while improving transparency and
explainability to the user.","cs.IR,cs.LG,cs.SI,stat.ML"
"Estimating Predictive Uncertainty Under Program Data Distribution Shift. Deep learning (DL) techniques have achieved great success in predictive
accuracy in a variety of tasks, but deep neural networks (DNNs) are shown to
produce highly overconfident scores for even abnormal samples. Well-defined
uncertainty indicates whether a model's output should (or should not) be
trusted and thus becomes critical in real-world scenarios which typically
involves shifted input distributions due to many factors. Existing uncertainty
approaches assume that testing samples from a different data distribution would
induce unreliable model predictions thus have higher uncertainty scores. They
quantify model uncertainty by calibrating DL model's confidence of a given
input and evaluate the effectiveness in computer vision (CV) and natural
language processing (NLP)-related tasks. However, their methodologies'
reliability may be compromised under programming tasks due to difference in
data representations and shift patterns. In this paper, we first define three
different types of distribution shift in program data and build a large-scale
shifted Java dataset. We implement two common programming language tasks on our
dataset to study the effect of each distribution shift on DL model performance.
We also propose a large-scale benchmark of existing state-of-the-art predictive
uncertainty on programming tasks and investigate their effectiveness under data
distribution shift. Experiments show that program distribution shift does
degrade the DL model performance to varying degrees and that existing
uncertainty methods all present certain limitations in quantifying uncertainty
on program dataset.","68T37,I.2.5; G.4,cs.LG,cs.SE"
"Deep CNN-based Speech Balloon Detection and Segmentation for Comic Books. We develop a method for the automated detection and segmentation of speech
balloons in comic books, including their carrier and tails. Our method is based
on a deep convolutional neural network that was trained on annotated pages of
the Graphic Narrative Corpus. More precisely, we are using a fully
convolutional network approach inspired by the U-Net architecture, combined
with a VGG-16 based encoder. The trained model delivers state-of-the-art
performance with an F1-score of over 0.94. Qualitative results suggest that
wiggly tails, curved corners, and even illusory contours do not pose a major
problem. Furthermore, the model has learned to distinguish speech balloons from
captions. We compare our model to earlier results and discuss some possible
applications.","68T45 (Primary) 68T05,91E30 (Secondary),cs.CV,cs.LG,q-bio.NC"
"Graph-based Incident Aggregation for Large-Scale Online Service Systems. As online service systems continue to grow in terms of complexity and volume,
how service incidents are managed will significantly impact company revenue and
user trust. Due to the cascading effect, cloud failures often come with an
overwhelming number of incidents from dependent services and devices. To pursue
efficient incident management, related incidents should be quickly aggregated
to narrow down the problem scope. To this end, in this paper, we propose GRLIA,
an incident aggregation framework based on graph representation learning over
the cascading graph of cloud failures. A representation vector is learned for
each unique type of incident in an unsupervised and unified manner, which is
able to simultaneously encode the topological and temporal correlations among
incidents. Thus, it can be easily employed for online incident aggregation. In
particular, to learn the correlations more accurately, we try to recover the
complete scope of failures' cascading impact by leveraging fine-grained system
monitoring data, i.e., Key Performance Indicators (KPIs). The proposed
framework is evaluated with real-world incident data collected from a
large-scale online service system of Huawei Cloud. The experimental results
demonstrate that GRLIA is effective and outperforms existing methods.
Furthermore, our framework has been successfully deployed in industrial
practice.","cs.LG,cs.SE"
"Variational Quantum Circuits for Deep Reinforcement Learning. The state-of-the-art machine learning approaches are based on classical von
Neumann computing architectures and have been widely used in many industrial
and academic domains. With the recent development of quantum computing,
researchers and tech-giants have attempted new quantum circuits for machine
learning tasks. However, the existing quantum computing platforms are hard to
simulate classical deep learning models or problems because of the
intractability of deep quantum circuits. Thus, it is necessary to design
feasible quantum algorithms for quantum machine learning for noisy intermediate
scale quantum (NISQ) devices. This work explores variational quantum circuits
for deep reinforcement learning. Specifically, we reshape classical deep
reinforcement learning algorithms like experience replay and target network
into a representation of variational quantum circuits. Moreover, we use a
quantum information encoding scheme to reduce the number of model parameters
compared to classical neural networks. To the best of our knowledge, this work
is the first proof-of-principle demonstration of variational quantum circuits
to approximate the deep $Q$-value function for decision-making and
policy-selection reinforcement learning with experience replay and target
network. Besides, our variational quantum circuits can be deployed in many
near-term NISQ machines.","cs.AI,cs.LG,quant-ph,stat.ML"
"What Doubling Tricks Can and Can't Do for Multi-Armed Bandits. An online reinforcement learning algorithm is anytime if it does not need to
know in advance the horizon T of the experiment. A well-known technique to
obtain an anytime algorithm from any non-anytime algorithm is the ""Doubling
Trick"". In the context of adversarial or stochastic multi-armed bandits, the
performance of an algorithm is measured by its regret, and we study two
families of sequences of growing horizons (geometric and exponential) to
generalize previously known results that certain doubling tricks can be used to
conserve certain regret bounds. In a broad setting, we prove that a geometric
doubling trick can be used to conserve (minimax) bounds in $R\_T = O(\sqrt{T})$
but cannot conserve (distribution-dependent) bounds in $R\_T = O(\log T)$. We
give insights as to why exponential doubling tricks may be better, as they
conserve bounds in $R\_T = O(\log T)$, and are close to conserving bounds in
$R\_T = O(\sqrt{T})$.","cs.LG,math.ST,stat.ML,stat.TH"
"Vehicle Local Position Estimation System. In this paper, a robust vehicle local position estimation with the help of
single camera sensor and GPS is presented. A modified Inverse Perspective
Mapping, illuminant Invariant techniques and object detection based approach is
used to localize the vehicle in the road. Vehicles current lane, its position
from road boundary and other cars are used to define its local position. For
this purpose Lane markings are detected using a Laplacian edge feature, robust
to shadowing. Effect of shadowing and extra sun light are removed using Lab
color space and illuminant invariant techniques. Lanes are assumed to be as
parabolic model and fitted using robust RANSAC. This method can reliably detect
all lanes of the road, estimate lane departure angle and local position of
vehicle relative to lanes, road boundary and other cars. Different type of
obstacle like pedestrians, vehicles are detected using HOG feature based
deformable part model.","68T45,cs.CV"
"Statistical-mechanical analysis of pre-training and fine tuning in deep learning. In this paper, we present a statistical-mechanical analysis of deep learning.
We elucidate some of the essential components of deep learning---pre-training
by unsupervised learning and fine tuning by supervised learning. We formulate
the extraction of features from the training data as a margin criterion in a
high-dimensional feature-vector space. The self-organized classifier is then
supplied with small amounts of labelled data, as in deep learning. Although we
employ a simple single-layer perceptron model, rather than directly analyzing a
multi-layer neural network, we find a nontrivial phase transition that is
dependent on the number of unlabelled data in the generalization error of the
resultant classifier. In this sense, we evaluate the efficacy of the
unsupervised learning component of deep learning. The analysis is performed by
the replica method, which is a sophisticated tool in statistical mechanics. We
validate our result in the manner of deep learning, using a simple iterative
algorithm to learn the weight vector on the basis of belief propagation.","cond-mat.dis-nn,cond-mat.stat-mech,cs.AI,cs.LG,stat.ML"
"Anomaly Detection and Sampling Cost Control via Hierarchical GANs. Anomaly detection incurs certain sampling and sensing costs and therefore it
is of great importance to strike a balance between the detection accuracy and
these costs. In this work, we study anomaly detection by considering the
detection of threshold crossings in a stochastic time series without the
knowledge of its statistics. To reduce the sampling cost in this detection
process, we propose the use of hierarchical generative adversarial networks
(GANs) to perform nonuniform sampling. In order to improve the detection
accuracy and reduce the delay in detection, we introduce a buffer zone in the
operation of the proposed GAN-based detector. In the experiments, we analyze
the performance of the proposed hierarchical GAN detector considering the
metrics of detection delay, miss rates, average cost of error, and sampling
ratio. We identify the tradeoffs in the performance as the buffer zone sizes
and the number of GAN levels in the hierarchy vary. We also compare the
performance with that of a sampling policy that approximately minimizes the sum
of average costs of sampling and error given the parameters of the stochastic
process. We demonstrate that the proposed GAN-based detector can have
significant performance improvements in terms of detection delay and average
cost of error with a larger buffer zone but at the cost of increased sampling
rates.","cs.IT,cs.LG,math.IT,stat.ML"
"QVMix and QVMix-Max: Extending the Deep Quality-Value Family of Algorithms to Cooperative Multi-Agent Reinforcement Learning. This paper introduces four new algorithms that can be used for tackling
multi-agent reinforcement learning (MARL) problems occurring in cooperative
settings. All algorithms are based on the Deep Quality-Value (DQV) family of
algorithms, a set of techniques that have proven to be successful when dealing
with single-agent reinforcement learning problems (SARL). The key idea of DQV
algorithms is to jointly learn an approximation of the state-value function
$V$, alongside an approximation of the state-action value function $Q$. We
follow this principle and generalise these algorithms by introducing two fully
decentralised MARL algorithms (IQV and IQV-Max) and two algorithms that are
based on the centralised training with decentralised execution training
paradigm (QVMix and QVMix-Max). We compare our algorithms with state-of-the-art
MARL techniques on the popular StarCraft Multi-Agent Challenge (SMAC)
environment. We show competitive results when QVMix and QVMix-Max are compared
to well-known MARL techniques such as QMIX and MAVEN and show that QVMix can
even outperform them on some of the tested environments, being the algorithm
which performs best overall. We hypothesise that this is due to the fact that
QVMix suffers less from the overestimation bias of the $Q$ function.","cs.AI,cs.LG,cs.MA"
"Probabilistic framework for solving Visual Dialog. In this paper, we propose a probabilistic framework for solving the task of
`Visual Dialog'. Solving this task requires reasoning and understanding of
visual modality, language modality, and common sense knowledge to answer.
Various architectures have been proposed to solve this task by variants of
multi-modal deep learning techniques that combine visual and language
representations. However, we believe that it is crucial to understand and
analyze the sources of uncertainty for solving this task. Our approach allows
for estimating uncertainty and also aids a diverse generation of answers. The
proposed approach is obtained through a probabilistic representation module
that provides us with representations for image, question and conversation
history, a module that ensures that diverse latent representations for
candidate answers are obtained given the probabilistic representations and an
uncertainty representation module that chooses the appropriate answer that
minimizes uncertainty. We thoroughly evaluate the model with a detailed
ablation analysis, comparison with state of the art and visualization of the
uncertainty that aids in the understanding of the method. Using the proposed
probabilistic framework, we thus obtain an improved visual dialog system that
is also more explainable.","cs.CL,cs.CV,cs.LG,cs.MM"
"Single and Multi-Agent Deep Reinforcement Learning for AI-Enabled Wireless Networks: A Tutorial. Deep Reinforcement Learning (DRL) has recently witnessed significant advances
that have led to multiple successes in solving sequential decision-making
problems in various domains, particularly in wireless communications. The
future sixth-generation (6G) networks are expected to provide scalable,
low-latency, ultra-reliable services empowered by the application of
data-driven Artificial Intelligence (AI). The key enabling technologies of
future 6G networks, such as intelligent meta-surfaces, aerial networks, and AI
at the edge, involve more than one agent which motivates the importance of
multi-agent learning techniques. Furthermore, cooperation is central to
establishing self-organizing, self-sustaining, and decentralized networks. In
this context, this tutorial focuses on the role of DRL with an emphasis on deep
Multi-Agent Reinforcement Learning (MARL) for AI-enabled 6G networks. The first
part of this paper will present a clear overview of the mathematical frameworks
for single-agent RL and MARL. The main idea of this work is to motivate the
application of RL beyond the model-free perspective which was extensively
adopted in recent years. Thus, we provide a selective description of RL
algorithms such as Model-Based RL (MBRL) and cooperative MARL and we highlight
their potential applications in 6G wireless networks. Finally, we overview the
state-of-the-art of MARL in fields such as Mobile Edge Computing (MEC),
Unmanned Aerial Vehicles (UAV) networks, and cell-free massive MIMO, and
identify promising future research directions. We expect this tutorial to
stimulate more research endeavors to build scalable and decentralized systems
based on MARL.","cs.LG,cs.NI"
"Computing Graph Descriptors on Edge Streams. Graph feature extraction is a fundamental task in graphs analytics. Using
feature vectors (graph descriptors) in tandem with data mining algorithms that
operate on Euclidean data, one can solve problems such as classification,
clustering, and anomaly detection on graph-structured data. This idea has
proved fruitful in the past, with spectral-based graph descriptors providing
state-of-the-art classification accuracy on benchmark datasets. However, these
algorithms do not scale to large graphs since: 1) they require storing the
entire graph in memory, and 2) the end-user has no control over the algorithm's
runtime. In this paper, we present single-pass streaming algorithms to
approximate structural features of graphs (counts of subgraphs of order $k \geq
4$). Operating on edge streams allows us to avoid keeping the entire graph in
memory, and controlling the sample size enables us to control the time taken by
the algorithm. We demonstrate the efficacy of our descriptors by analyzing the
approximation error, classification accuracy, and scalability to massive
graphs. Our experiments showcase the effect of the sample size on approximation
error and predictive accuracy. The proposed descriptors are applicable on
graphs with millions of edges within minutes and outperform the
state-of-the-art descriptors in classification accuracy.","cs.AI,cs.DS,cs.LG"
"A survey of Monte Carlo methods for noisy and costly densities with application to reinforcement learning. This survey gives an overview of Monte Carlo methodologies using surrogate
models, for dealing with densities which are intractable, costly, and/or noisy.
This type of problem can be found in numerous real-world scenarios, including
stochastic optimization and reinforcement learning, where each evaluation of a
density function may incur some computationally-expensive or even physical
(real-world activity) cost, likely to give different results each time. The
surrogate model does not incur this cost, but there are important trade-offs
and considerations involved in the choice and design of such methodologies. We
classify the different methodologies into three main classes and describe
specific instances of algorithms under a unified notation. A modular scheme
which encompasses the considered methods is also presented. A range of
application scenarios is discussed, with special attention to the
likelihood-free setting and reinforcement learning. Several numerical
comparisons are also provided.","cs.LG,stat.CO,stat.ML"
"Theoretical Foundations of t-SNE for Visualizing High-Dimensional Clustered Data. This study investigates the theoretical foundations of t-distributed
stochastic neighbor embedding (t-SNE), a popular nonlinear dimension reduction
and data visualization method. A novel theoretical framework for the analysis
of t-SNE based on the gradient descent approach is presented. For the early
exaggeration stage of t-SNE, we show its asymptotic equivalence to a power
iteration based on the underlying graph Laplacian, characterize its limiting
behavior, and uncover its deep connection to Laplacian spectral clustering, and
fundamental principles including early stopping as implicit regularization. The
results explain the intrinsic mechanism and the empirical benefits of such a
computational strategy. For the embedding stage of t-SNE, we characterize the
kinematics of the low-dimensional map throughout the iterations, and identify
an amplification phase, featuring the intercluster repulsion and the expansive
behavior of the low-dimensional map. The general theory explains the fast
convergence rate and the exceptional empirical performance of t-SNE for
visualizing clustered data, brings forth the interpretations of the t-SNE
output, and provides theoretical guidance for selecting tuning parameters in
various applications.","cs.LG,math.ST,stat.ML,stat.TH"
"Towards Robust Deep Reinforcement Learning for Traffic Signal Control: Demand Surges, Incidents and Sensor Failures. Reinforcement learning (RL) constitutes a promising solution for alleviating
the problem of traffic congestion. In particular, deep RL algorithms have been
shown to produce adaptive traffic signal controllers that outperform
conventional systems. However, in order to be reliable in highly dynamic urban
areas, such controllers need to be robust with the respect to a series of
exogenous sources of uncertainty. In this paper, we develop an open-source
callback-based framework for promoting the flexible evaluation of different
deep RL configurations under a traffic simulation environment. With this
framework, we investigate how deep RL-based adaptive traffic controllers
perform under different scenarios, namely under demand surges caused by special
events, capacity reductions from incidents and sensor failures. We extract
several key insights for the development of robust deep RL algorithms for
traffic control and propose concrete designs to mitigate the impact of the
considered exogenous uncertainties.","cs.LG,cs.SY,stat.ML"
"Learning to Fuse Music Genres with Generative Adversarial Dual Learning. FusionGAN is a novel genre fusion framework for music generation that
integrates the strengths of generative adversarial networks and dual learning.
In particular, the proposed method offers a dual learning extension that can
effectively integrate the styles of the given domains. To efficiently quantify
the difference among diverse domains and avoid the vanishing gradient issue,
FusionGAN provides a Wasserstein based metric to approximate the distance
between the target domain and the existing domains. Adopting the Wasserstein
distance, a new domain is created by combining the patterns of the existing
domains using adversarial learning. Experimental results on public music
datasets demonstrated that our approach could effectively merge two genres.","cs.AI,cs.LG,cs.MM,cs.SD,eess.AS"
"Quantum-inspired Machine Learning on high-energy physics data. Tensor Networks, a numerical tool originally designed for simulating quantum
many-body systems, have recently been applied to solve Machine Learning
problems. Exploiting a tree tensor network, we apply a quantum-inspired machine
learning technique to a very important and challenging big data problem in high
energy physics: the analysis and classification of data produced by the Large
Hadron Collider at CERN. In particular, we present how to effectively classify
so-called b-jets, jets originating from b-quarks from proton-proton collisions
in the LHCb experiment, and how to interpret the classification results. We
exploit the Tensor Network approach to select important features and adapt the
network geometry based on information acquired in the learning process.
Finally, we show how to adapt the tree tensor network to achieve optimal
precision or fast response in time without the need of repeating the learning
process. These results pave the way to the implementation of high-frequency
real-time applications, a key ingredient needed among others for current and
future LHCb event classification able to trigger events at the tens of MHz
scale.","cond-mat.stat-mech,cs.LG,hep-ex,physics.data-an,quant-ph,stat.ML"
"Learning Q-network for Active Information Acquisition. In this paper, we propose a novel Reinforcement Learning approach for solving
the Active Information Acquisition problem, which requires an agent to choose a
sequence of actions in order to acquire information about a process of interest
using on-board sensors. The classic challenges in the information acquisition
problem are the dependence of a planning algorithm on known models and the
difficulty of computing information-theoretic cost functions over arbitrary
distributions. In contrast, the proposed framework of reinforcement learning
does not require any knowledge on models and alleviates the problems during an
extended training stage. It results in policies that are efficient to execute
online and applicable for real-time control of robotic systems. Furthermore,
the state-of-the-art planning methods are typically restricted to short
horizons, which may become problematic with local minima. Reinforcement
learning naturally handles the issue of planning horizon in information
problems as it maximizes a discounted sum of rewards over a long finite or
infinite time horizon. We discuss the potential benefits of the proposed
framework and compare the performance of the novel algorithm to an existing
information acquisition method for multi-target tracking scenarios.","cs.LG,cs.RO,stat.ML"
"Free-Form Image Inpainting with Gated Convolution. We present a generative image inpainting system to complete images with
free-form mask and guidance. The system is based on gated convolutions learned
from millions of images without additional labelling efforts. The proposed
gated convolution solves the issue of vanilla convolution that treats all input
pixels as valid ones, generalizes partial convolution by providing a learnable
dynamic feature selection mechanism for each channel at each spatial location
across all layers. Moreover, as free-form masks may appear anywhere in images
with any shape, global and local GANs designed for a single rectangular mask
are not applicable. Thus, we also present a patch-based GAN loss, named
SN-PatchGAN, by applying spectral-normalized discriminator on dense image
patches. SN-PatchGAN is simple in formulation, fast and stable in training.
Results on automatic image inpainting and user-guided extension demonstrate
that our system generates higher-quality and more flexible results than
previous methods. Our system helps user quickly remove distracting objects,
modify image layouts, clear watermarks and edit faces. Code, demo and models
are available at: https://github.com/JiahuiYu/generative_inpainting","cs.CV,cs.GR,cs.LG"
"CogTree: Cognition Tree Loss for Unbiased Scene Graph Generation. Scene graphs are semantic abstraction of images that encourage visual
understanding and reasoning. However, the performance of Scene Graph Generation
(SGG) is unsatisfactory when faced with biased data in real-world scenarios.
Conventional debiasing research mainly studies from the view of balancing data
distribution or learning unbiased models and representations, ignoring the
correlations among the biased classes. In this work, we analyze this problem
from a novel cognition perspective: automatically building a hierarchical
cognitive structure from the biased predictions and navigating that hierarchy
to locate the relationships, making the tail relationships receive more
attention in a coarse-to-fine mode. To this end, we propose a novel debiasing
Cognition Tree (CogTree) loss for unbiased SGG. We first build a cognitive
structure CogTree to organize the relationships based on the prediction of a
biased SGG model. The CogTree distinguishes remarkably different relationships
at first and then focuses on a small portion of easily confused ones. Then, we
propose a debiasing loss specially for this cognitive structure, which supports
coarse-to-fine distinction for the correct relationships. The loss is
model-agnostic and consistently boosting the performance of several
state-of-the-art models. The code is available at:
https://github.com/CYVincent/Scene-Graph-Transformer-CogTree.","cs.CL,cs.CV,cs.MM"
"Reinforced Data Sampling for Model Diversification. With the rising number of machine learning competitions, the world has
witnessed an exciting race for the best algorithms. However, the involved data
selection process may fundamentally suffer from evidence ambiguity and concept
drift issues, thereby possibly leading to deleterious effects on the
performance of various models. This paper proposes a new Reinforced Data
Sampling (RDS) method to learn how to sample data adequately on the search for
useful models and insights. We formulate the optimisation problem of model
diversification $\delta{-div}$ in data sampling to maximise learning potentials
and optimum allocation by injecting model diversity. This work advocates the
employment of diverse base learners as value functions such as neural networks,
decision trees, or logistic regressions to reinforce the selection process of
data subsets with multi-modal belief. We introduce different ensemble reward
mechanisms, including soft voting and stochastic choice to approximate optimal
sampling policy. The evaluation conducted on four datasets evidently highlights
the benefits of using RDS method over traditional sampling approaches. Our
experimental results suggest that the trainable sampling for model
diversification is useful for competition organisers, researchers, or even
starters to pursue full potentials of various machine learning tasks such as
classification and regression. The source code is available at
https://github.com/probeu/RDS.","cs.LG,cs.NA,math.NA,stat.ML"
"Challenges and approaches to time-series forecasting in data center telemetry: A Survey. Time-series forecasting has been an important research domain for so many
years. Its applications include ECG predictions, sales forecasting, weather
conditions, even COVID-19 spread predictions. These applications have motivated
many researchers to figure out an optimal forecasting approach, but the
modeling approach also changes as the application domain changes. This work has
focused on reviewing different forecasting approaches for telemetry data
predictions collected at data centers. Forecasting of telemetry data is a
critical feature of network and data center management products. However, there
are multiple options of forecasting approaches that range from a simple linear
statistical model to high capacity deep learning architectures. In this paper,
we attempted to summarize and evaluate the performance of well known time
series forecasting techniques. We hope that this evaluation provides a
comprehensive summary to innovate in forecasting approaches for telemetry data.","cs.AI,cs.LG,cs.NI"
"DeepGS: Deep Representation Learning of Graphs and Sequences for Drug-Target Binding Affinity Prediction. Accurately predicting drug-target binding affinity (DTA) in silico is a key
task in drug discovery. Most of the conventional DTA prediction methods are
simulation-based, which rely heavily on domain knowledge or the assumption of
having the 3D structure of the targets, which are often difficult to obtain.
Meanwhile, traditional machine learning-based methods apply various features
and descriptors, and simply depend on the similarities between drug-target
pairs. Recently, with the increasing amount of affinity data available and the
success of deep representation learning models on various domains, deep
learning techniques have been applied to DTA prediction. However, these methods
consider either label/one-hot encodings or the topological structure of
molecules, without considering the local chemical context of amino acids and
SMILES sequences. Motivated by this, we propose a novel end-to-end learning
framework, called DeepGS, which uses deep neural networks to extract the local
chemical context from amino acids and SMILES sequences, as well as the
molecular structure from the drugs. To assist the operations on the symbolic
data, we propose to use advanced embedding techniques (i.e., Smi2Vec and
Prot2Vec) to encode the amino acids and SMILES sequences to a distributed
representation. Meanwhile, we suggest a new molecular structure modeling
approach that works well under our framework. We have conducted extensive
experiments to compare our proposed method with state-of-the-art models
including KronRLS, SimBoost, DeepDTA and DeepCPI. Extensive experimental
results demonstrate the superiorities and competitiveness of DeepGS.","cs.LG,q-bio.QM"
"Identifying Pairs in Simulated Bio-Medical Time-Series. The paper presents a time-series-based classification approach to identify
similarities in pairs of simulated human-generated patterns. An example for a
pattern is a time-series representing a heart rate during a specific
time-range, wherein the time-series is a sequence of data points that represent
the changes in the heart rate values. A bio-medical simulator system was
developed to acquire a collection of 7,871 price patterns of financial
instruments. The financial instruments traded in real-time on three American
stock exchanges, NASDAQ, NYSE, and AMEX, simulate bio-medical measurements. The
system simulates a human in which each price pattern represents one bio-medical
sensor. Data provided during trading hours from the stock exchanges allowed
real-time classification. Classification is based on new machine learning
techniques: self-labeling, which allows the application of supervised learning
methods on unlabeled time-series and similarity ranking, which applied on a
decision tree learning algorithm to classify time-series regardless of type and
quantity.","cs.CE,cs.LG"
"Lipreading using Temporal Convolutional Networks. Lip-reading has attracted a lot of research attention lately thanks to
advances in deep learning. The current state-of-the-art model for recognition
of isolated words in-the-wild consists of a residual network and Bidirectional
Gated Recurrent Unit (BGRU) layers. In this work, we address the limitations of
this model and we propose changes which further improve its performance.
Firstly, the BGRU layers are replaced with Temporal Convolutional Networks
(TCN). Secondly, we greatly simplify the training procedure, which allows us to
train the model in one single stage. Thirdly, we show that the current
state-of-the-art methodology produces models that do not generalize well to
variations on the sequence length, and we addresses this issue by proposing a
variable-length augmentation. We present results on the largest
publicly-available datasets for isolated word recognition in English and
Mandarin, LRW and LRW1000, respectively. Our proposed model results in an
absolute improvement of 1.2% and 3.2%, respectively, in these datasets which is
the new state-of-the-art performance.","cs.CV,cs.SD,eess.AS"
"Learning Spatio-Temporal Features with Two-Stream Deep 3D CNNs for Lipreading. We focus on the word-level visual lipreading, which requires recognizing the
word being spoken, given only the video but not the audio. State-of-the-art
methods explore the use of end-to-end neural networks, including a shallow (up
to three layers) 3D convolutional neural network (CNN) + a deep 2D CNN (e.g.,
ResNet) as the front-end to extract visual features, and a recurrent neural
network (e.g., bidirectional LSTM) as the back-end for classification. In this
work, we propose to replace the shallow 3D CNNs + deep 2D CNNs front-end with
recent successful deep 3D CNNs --- two-stream (i.e., grayscale video and
optical flow streams) I3D. We evaluate different combinations of front-end and
back-end modules with the grayscale video and optical flow inputs on the LRW
dataset. The experiments show that, compared to the shallow 3D CNNs + deep 2D
CNNs front-end, the deep 3D CNNs front-end with pre-training on the large-scale
image and video datasets (e.g., ImageNet and Kinetics) can improve the
classification accuracy. Also, we demonstrate that using the optical flow input
alone can achieve comparable performance as using the grayscale video as input.
Moreover, the two-stream network using both the grayscale video and optical
flow inputs can further improve the performance. Overall, our two-stream I3D
front-end with a Bi-LSTM back-end results in an absolute improvement of 5.3%
over the previous art on the LRW dataset.","cs.CV,cs.MM"
"MaAST: Map Attention with Semantic Transformersfor Efficient Visual Navigation. Visual navigation for autonomous agents is a core task in the fields of
computer vision and robotics. Learning-based methods, such as deep
reinforcement learning, have the potential to outperform the classical
solutions developed for this task; however, they come at a significantly
increased computational load. Through this work, we design a novel approach
that focuses on performing better or comparable to the existing learning-based
solutions but under a clear time/computational budget. To this end, we propose
a method to encode vital scene semantics such as traversable paths, unexplored
areas, and observed scene objects -- alongside raw visual streams such as RGB,
depth, and semantic segmentation masks -- into a semantically informed,
top-down egocentric map representation. Further, to enable the effective use of
this information, we introduce a novel 2-D map attention mechanism, based on
the successful multi-layer Transformer networks. We conduct experiments on 3-D
reconstructed indoor PointGoal visual navigation and demonstrate the
effectiveness of our approach. We show that by using our novel attention schema
and auxiliary rewards to better utilize scene semantics, we outperform multiple
baselines trained with only raw inputs or implicit semantic information while
operating with an 80% decrease in the agent's experience.","cs.CV,cs.RO"
"Knowledge Graph Driven Approach to Represent Video Streams for Spatiotemporal Event Pattern Matching in Complex Event Processing. Complex Event Processing (CEP) is an event processing paradigm to perform
real-time analytics over streaming data and match high-level event patterns.
Presently, CEP is limited to process structured data stream. Video streams are
complicated due to their unstructured data model and limit CEP systems to
perform matching over them. This work introduces a graph-based structure for
continuous evolving video streams, which enables the CEP system to query
complex video event patterns. We propose the Video Event Knowledge Graph
(VEKG), a graph driven representation of video data. VEKG models video objects
as nodes and their relationship interaction as edges over time and space. It
creates a semantic knowledge representation of video data derived from the
detection of high-level semantic concepts from the video using an ensemble of
deep learning models. A CEP-based state optimization - VEKG-Time Aggregated
Graph (VEKG-TAG) is proposed over VEKG representation for faster event
detection. VEKG-TAG is a spatiotemporal graph aggregation method that provides
a summarized view of the VEKG graph over a given time length. We defined a set
of nine event pattern rules for two domains (Activity Recognition and Traffic
Management), which act as a query and applied over VEKG graphs to discover
complex event patterns. To show the efficacy of our approach, we performed
extensive experiments over 801 video clips across 10 datasets. The proposed
VEKG approach was compared with other state-of-the-art methods and was able to
detect complex event patterns over videos with F-Score ranging from 0.44 to
0.90. In the given experiments, the optimized VEKG-TAG was able to reduce 99%
and 93% of VEKG nodes and edges, respectively, with 5.19X faster search time,
achieving sub-second median latency of 4-20 milliseconds.","cs.AI,cs.CV,cs.DB,cs.MM"
"Explainable $k$-Means and $k$-Medians Clustering. Clustering is a popular form of unsupervised learning for geometric data.
Unfortunately, many clustering algorithms lead to cluster assignments that are
hard to explain, partially because they depend on all the features of the data
in a complicated way. To improve interpretability, we consider using a small
decision tree to partition a data set into clusters, so that clusters can be
characterized in a straightforward manner. We study this problem from a
theoretical viewpoint, measuring cluster quality by the $k$-means and
$k$-medians objectives: Must there exist a tree-induced clustering whose cost
is comparable to that of the best unconstrained clustering, and if so, how can
it be found? In terms of negative results, we show, first, that popular
top-down decision tree algorithms may lead to clusterings with arbitrarily
large cost, and second, that any tree-induced clustering must in general incur
an $\Omega(\log k)$ approximation factor compared to the optimal clustering. On
the positive side, we design an efficient algorithm that produces explainable
clusters using a tree with $k$ leaves. For two means/medians, we show that a
single threshold cut suffices to achieve a constant factor approximation, and
we give nearly-matching lower bounds. For general $k \geq 2$, our algorithm is
an $O(k)$ approximation to the optimal $k$-medians and an $O(k^2)$
approximation to the optimal $k$-means. Prior to our work, no algorithms were
known with provable guarantees independent of dimension and input size.","cs.CG,cs.DS,cs.LG,stat.ML"
"Partial success in closing the gap between human and machine vision. A few years ago, the first CNN surpassed human performance on ImageNet.
However, it soon became clear that machines lack robustness on more challenging
test cases, a major obstacle towards deploying machines ""in the wild"" and
towards obtaining better computational models of human visual perception. Here
we ask: Are we making progress in closing the gap between human and machine
vision? To answer this question, we tested human observers on a broad range of
out-of-distribution (OOD) datasets, adding the ""missing human baseline"" by
recording 85,120 psychophysical trials across 90 participants. We then
investigated a range of promising machine learning developments that crucially
deviate from standard supervised CNNs along three axes: objective function
(self-supervised, adversarially trained, CLIP language-image training),
architecture (e.g. vision transformers), and dataset size (ranging from 1M to
1B). Our findings are threefold. (1.) The longstanding robustness gap between
humans and CNNs is closing, with the best models now matching or exceeding
human performance on most OOD datasets. (2.) There is still a substantial
image-level consistency gap, meaning that humans make different errors than
models. In contrast, most models systematically agree in their categorisation
errors, even substantially different ones like contrastive self-supervised vs.
standard supervised models. (3.) In many cases, human-to-model consistency
improves when training dataset size is increased by one to three orders of
magnitude. Our results give reason for cautious optimism: While there is still
much room for improvement, the behavioural difference between human and machine
vision is narrowing. In order to measure future progress, 17 OOD datasets with
image-level human behavioural data are provided as a benchmark here:
https://github.com/bethgelab/model-vs-human/","cs.AI,cs.CV,cs.LG,q-bio.NC"
"Discovering Sparse Interpretable Dynamics from Partial Observations. Identifying the governing equations of a nonlinear dynamical system is key to
both understanding the physical features of the system and constructing an
accurate model of the dynamics that generalizes well beyond the available data.
We propose a machine learning framework for discovering these governing
equations using only partial observations, combining an encoder for state
reconstruction with a sparse symbolic model. Our tests show that this method
can successfully reconstruct the full system state and identify the underlying
dynamics for a variety of ODE and PDE systems.","cs.LG,physics.comp-ph,physics.data-an"
"Small Object Detection Based on Modified FSSD and Model Compression. Small objects have relatively low resolution, the unobvious visual features
which are difficult to be extracted, so the existing object detection methods
cannot effectively detect small objects, and the detection speed and stability
are poor. Thus, this paper proposes a small object detection algorithm based on
FSSD, meanwhile, in order to reduce the computational cost and storage space,
pruning is carried out to achieve model compression. Firstly, the semantic
information contained in the features of different layers can be used to detect
different scale objects, and the feature fusion method is improved to obtain
more information beneficial to small objects; secondly, batch normalization
layer is introduced to accelerate the training of neural network and make the
model sparse; finally, the model is pruned by scaling factor to get the
corresponding compressed model. The experimental results show that the average
accuracy (mAP) of the algorithm can reach 80.4% on PASCAL VOC and the speed is
59.5 FPS on GTX1080ti. After pruning, the compressed model can reach 79.9% mAP,
and 79.5 FPS in detection speed. On MS COCO, the best detection accuracy (APs)
is 12.1%, and the overall detection accuracy is 49.8% AP when IoU is 0.5. The
algorithm can not only improve the detection accuracy of small objects, but
also greatly improves the detection speed, which reaches a balance between
speed and accuracy.","cs.CV,cs.MM"
"Image Segmentation to Distinguish Between Overlapping Human Chromosomes. In medicine, visualizing chromosomes is important for medical diagnostics,
drug development, and biomedical research. Unfortunately, chromosomes often
overlap and it is necessary to identify and distinguish between the overlapping
chromosomes. A segmentation solution that is fast and automated will enable
scaling of cost effective medicine and biomedical research. We apply neural
network-based image segmentation to the problem of distinguishing between
partially overlapping DNA chromosomes. A convolutional neural network is
customized for this problem. The results achieved intersection over union (IOU)
scores of 94.7% for the overlapping region and 88-94% on the non-overlapping
chromosome regions.","cs.CV,cs.LG,q-bio.QM,stat.ML"
"Deep Fluids: A Generative Network for Parameterized Fluid Simulations. This paper presents a novel generative model to synthesize fluid simulations
from a set of reduced parameters. A convolutional neural network is trained on
a collection of discrete, parameterizable fluid simulation velocity fields. Due
to the capability of deep learning architectures to learn representative
features of the data, our generative model is able to accurately approximate
the training data set, while providing plausible interpolated in-betweens. The
proposed generative model is optimized for fluids by a novel loss function that
guarantees divergence-free velocity fields at all times. In addition, we
demonstrate that we can handle complex parameterizations in reduced spaces, and
advance simulations in time by integrating in the latent space with a second
network. Our method models a wide variety of fluid behaviors, thus enabling
applications such as fast construction of simulations, interpolation of fluids
with different parameters, time re-sampling, latent space simulations, and
compression of fluid simulation data. Reconstructed velocity fields are
generated up to 700x faster than re-simulating the data with the underlying CPU
solver, while achieving compression rates of up to 1300x.","cs.GR,cs.LG,physics.comp-ph,physics.flu-dyn,stat.ML"
"An Analysis of Unsupervised Pre-training in Light of Recent Advances. Convolutional neural networks perform well on object recognition because of a
number of recent advances: rectified linear units (ReLUs), data augmentation,
dropout, and large labelled datasets. Unsupervised data has been proposed as
another way to improve performance. Unfortunately, unsupervised pre-training is
not used by state-of-the-art methods leading to the following question: Is
unsupervised pre-training still useful given recent advances? If so, when? We
answer this in three parts: we 1) develop an unsupervised method that
incorporates ReLUs and recent unsupervised regularization techniques, 2)
analyze the benefits of unsupervised pre-training compared to data augmentation
and dropout on CIFAR-10 while varying the ratio of unsupervised to supervised
samples, 3) verify our findings on STL-10. We discover unsupervised
pre-training, as expected, helps when the ratio of unsupervised to supervised
samples is high, and surprisingly, hurts when the ratio is low. We also use
unsupervised pre-training with additional color augmentation to achieve near
state-of-the-art performance on STL-10.","cs.CV,cs.LG,cs.NE"
"Multi-View Frame Reconstruction with Conditional GAN. Multi-view frame reconstruction is an important problem particularly when
multiple frames are missing and past and future frames within the camera are
far apart from the missing ones. Realistic coherent frames can still be
reconstructed using corresponding frames from other overlapping cameras. We
propose an adversarial approach to learn the spatio-temporal representation of
the missing frame using conditional Generative Adversarial Network (cGAN). The
conditional input to each cGAN is the preceding or following frames within the
camera or the corresponding frames in other overlapping cameras, all of which
are merged together using a weighted average. Representations learned from
frames within the camera are given more weight compared to the ones learned
from other cameras when they are close to the missing frames and vice versa.
Experiments on two challenging datasets demonstrate that our framework produces
comparable results with the state-of-the-art reconstruction method in a single
camera and achieves promising performance in multi-camera scenario.","cs.CV,cs.MM"
"Efficient algorithms for decision tree cross-validation. Cross-validation is a useful and generally applicable technique often
employed in machine learning, including decision tree induction. An important
disadvantage of straightforward implementation of the technique is its
computational overhead. In this paper we show that, for decision trees, the
computational overhead of cross-validation can be reduced significantly by
integrating the cross-validation with the normal decision tree induction
process. We discuss how existing decision tree algorithms can be adapted to
this aim, and provide an analysis of the speedups these adaptations may yield.
The analysis is supported by experimental results.","I.2.6,cs.LG"
"Binary Classification as a Phase Separation Process. We propose a new binary classification model called Phase Separation Binary
Classifier (PSBC). It consists of a discretization of a nonlinear
reaction-diffusion equation coupled with an Ordinary Differential Equation, and
is inspired by fluids behavior, namely, on how binary fluids phase separate.
Thus, parameters and hyperparameters have physical meaning, whose effects are
studied in several different scenarios.
  PSBC's equations can be seen as a dynamical system whose coefficients are
trainable weights, with a similar architecture to that of a Recurrent Neural
Network. As such, forward propagation amounts to an initial value problem.
Boundary conditions are also present, bearing similarity with figure padding
techniques in Computer Vision. Model compression is exploited in several ways,
with weight sharing taking place both across and within layers.
  The model is tested on pairs of digits of the classical MNIST database. An
associated multiclass classifier is also constructed using a combination of
Ensemble Learning and one versus one techniques. It is also shown how the PSBC
can be combined with other methods - like aggregation and PCA - in order to
construct better binary classifiers. The role of boundary conditions and
viscosity is thoroughly studied in the case of digits ``0'' and ``1''.","35B50,35K57,37N30,65M06,I.5,cs.LG,cs.NA,math.DS,math.NA,math.OC,stat.ML"
"Predicting Human Decision Making in Psychological Tasks with Recurrent Neural Networks. Unlike traditional time series, the action sequences of human decision making
usually involve many cognitive processes such as beliefs, desires, intentions
and theory of mind, i.e. what others are thinking. This makes predicting human
decision making challenging to be treated agnostically to the underlying
psychological mechanisms. We propose to use a recurrent neural network
architecture based on long short-term memory networks (LSTM) to predict the
time series of the actions taken by the human subjects at each step of their
decision making, the first application of such methods in this research domain.
We trained our prediction networks on the behavioral data from several
published psychological experiments of human decision making, and demonstrated
a clear advantage over the state-of-the-art methods in predicting human
decision making trajectories in both single-agent scenarios such as Iowa
Gambling Task and multi-agent scenarios such as Iterated Prisoner's Dilemma.","cs.AI,cs.LG,q-bio.NC"
"Image Augmentation Is All You Need: Regularizing Deep Reinforcement Learning from Pixels. We propose a simple data augmentation technique that can be applied to
standard model-free reinforcement learning algorithms, enabling robust learning
directly from pixels without the need for auxiliary losses or pre-training. The
approach leverages input perturbations commonly used in computer vision tasks
to regularize the value function. Existing model-free approaches, such as Soft
Actor-Critic (SAC), are not able to train deep networks effectively from image
pixels. However, the addition of our augmentation method dramatically improves
SAC's performance, enabling it to reach state-of-the-art performance on the
DeepMind control suite, surpassing model-based (Dreamer, PlaNet, and SLAC)
methods and recently proposed contrastive learning (CURL). Our approach can be
combined with any model-free reinforcement learning algorithm, requiring only
minor modifications. An implementation can be found at
https://sites.google.com/view/data-regularized-q.","cs.CV,cs.LG,eess.IV,stat.ML"
"WildMix Dataset and Spectro-Temporal Transformer Model for Monoaural Audio Source Separation. Monoaural audio source separation is a challenging research area in machine
learning. In this area, a mixture containing multiple audio sources is given,
and a model is expected to disentangle the mixture into isolated atomic
sources. In this paper, we first introduce a challenging new dataset for
monoaural source separation called WildMix. WildMix is designed with the goal
of extending the boundaries of source separation beyond what previous datasets
in this area would allow. It contains diverse in-the-wild recordings from 25
different sound classes, combined with each other using arbitrary composition
policies. Source separation often requires modeling long-range dependencies in
both temporal and spectral domains. To this end, we introduce a novel
trasnformer-based model called Spectro-Temporal Transformer (STT). STT utilizes
a specialized encoder, called Spectro-Temporal Encoder (STE). STE highlights
temporal and spectral components of sources within a mixture, using a
self-attention mechanism. It subsequently disentangles them in a hierarchical
manner. In our experiments, STT swiftly outperforms various previous baselines
for monoaural source separation on the challenging WildMix dataset.","cs.LG,cs.SD,eess.AS,stat.ML"
"BrainNNExplainer: An Interpretable Graph Neural Network Framework for Brain Network based Disease Analysis. Interpretable brain network models for disease prediction are of great value
for the advancement of neuroscience. GNNs are promising to model complicated
network data, but they are prone to overfitting and suffer from poor
interpretability, which prevents their usage in decision-critical scenarios
like healthcare. To bridge this gap, we propose BrainNNExplainer, an
interpretable GNN framework for brain network analysis. It is mainly composed
of two jointly learned modules: a backbone prediction model that is
specifically designed for brain networks and an explanation generator that
highlights disease-specific prominent brain network connections. Extensive
experimental results with visualizations on two challenging disease prediction
datasets demonstrate the unique interpretability and outstanding performance of
BrainNNExplainer.","68T07,68T20,68T45,I.2.6; I.2.10; J.3,cs.CV,cs.LG,eess.IV,q-bio.NC"
"CodeReef: an open platform for portable MLOps, reusable automation actions and reproducible benchmarking. We present CodeReef - an open platform to share all the components necessary
to enable cross-platform MLOps (MLSysOps), i.e. automating the deployment of ML
models across diverse systems in the most efficient way. We also introduce the
CodeReef solution - a way to package and share models as non-virtualized,
portable, customizable and reproducible archive files. Such ML packages include
JSON meta description of models with all dependencies, Python APIs, CLI actions
and portable workflows necessary to automatically build, benchmark, test and
customize models across diverse platforms, AI frameworks, libraries, compilers
and datasets. We demonstrate several CodeReef solutions to automatically build,
run and measure object detection based on SSD-Mobilenets, TensorFlow and COCO
dataset from the latest MLPerf inference benchmark across a wide range of
platforms from Raspberry Pi, Android phones and IoT devices to data centers.
Our long-term goal is to help researchers share their new techniques as
production-ready packages along with research papers to participate in
collaborative and reproducible benchmarking, compare the different
ML/software/hardware stacks and select the most efficient ones on a Pareto
frontier using online CodeReef dashboards.","cs.LG,cs.SE,stat.ML"
"Cross-modal Recurrent Models for Weight Objective Prediction from Multimodal Time-series Data. We analyse multimodal time-series data corresponding to weight, sleep and
steps measurements. We focus on predicting whether a user will successfully
achieve his/her weight objective. For this, we design several deep long
short-term memory (LSTM) architectures, including a novel cross-modal LSTM
(X-LSTM), and demonstrate their superiority over baseline approaches. The
X-LSTM improves parameter efficiency by processing each modality separately and
allowing for information flow between them by way of recurrent
cross-connections. We present a general hyperparameter optimisation technique
for X-LSTMs, which allows us to significantly improve on the LSTM and a prior
state-of-the-art cross-modal approach, using a comparable number of parameters.
Finally, we visualise the model's predictions, revealing implications about
latent variables in this task.","cs.AI,cs.LG,q-bio.QM,stat.ML"
"Multi-label Classification of User Reactions in Online News. The increase in the number of Internet users and the strong interaction
brought by Web 2.0 made the Opinion Mining an important task in the area of
natural language processing. Although several methods are capable of performing
this task, few use multi-label classification, where there is a group of true
labels for each example. This type of classification is useful for situations
where the opinions are analyzed from the perspective of the reader, this
happens because each person can have different interpretations and opinions on
the same subject. This paper discuss the efficiency of problem transformation
methods combined with different classification algorithms for the task of
multi-label classification of reactions in news texts. To do that, extensive
tests were carried out on two news corpora written in Brazilian Portuguese
annotated with reactions. A new corpus called BFRC-PT is presented. In the
tests performed, the highest number of correct predictions was obtained with
the Classifier Chains method combined with the Random Forest algorithm. When
considering the class distribution, the best results were obtained with the
Binary Relevance method combined with the LSTM and Random Forest algorithms.","cs.IR,cs.LG,stat.ML"
"Towards the Localisation of Lesions in Diabetic Retinopathy. Convolutional Neural Networks (CNNs) have successfully been used to classify
diabetic retinopathy (DR) fundus images in recent times. However, deeper
representations in CNNs may capture higher-level semantics at the expense of
spatial resolution. To make predictions usable for ophthalmologists, we use a
post-attention technique called Gradient-weighted Class Activation Mapping
(Grad-CAM) on the penultimate layer of deep learning models to produce coarse
localisation maps on DR fundus images. This is to help identify discriminative
regions in the images, consequently providing evidence for ophthalmologists to
make a diagnosis and potentially save lives by early diagnosis. Specifically,
this study uses pre-trained weights from four state-of-the-art deep learning
models to produce and compare localisation maps of DR fundus images. The models
used include VGG16, ResNet50, InceptionV3, and InceptionResNetV2. We find that
InceptionV3 achieves the best performance with a test classification accuracy
of 96.07%, and localise lesions better and faster than the other models.","68T07,I.4,cs.CV,stat.ML"
"Modularity Matters: Learning Invariant Relational Reasoning Tasks. We focus on two supervised visual reasoning tasks whose labels encode a
semantic relational rule between two or more objects in an image: the MNIST
Parity task and the colorized Pentomino task. The objects in the images undergo
random translation, scaling, rotation and coloring transformations. Thus these
tasks involve invariant relational reasoning. We report uneven performance of
various deep CNN models on these two tasks. For the MNIST Parity task, we
report that the VGG19 model soundly outperforms a family of ResNet models.
Moreover, the family of ResNet models exhibits a general sensitivity to random
initialization for the MNIST Parity task. For the colorized Pentomino task, now
both the VGG19 and ResNet models exhibit sluggish optimization and very poor
test generalization, hovering around 30% test error. The CNN we tested all
learn hierarchies of fully distributed features and thus encode the distributed
representation prior. We are motivated by a hypothesis from cognitive
neuroscience which posits that the human visual cortex is modularized, and this
allows the visual cortex to learn higher order invariances. To this end, we
consider a modularized variant of the ResNet model, referred to as a Residual
Mixture Network (ResMixNet) which employs a mixture-of-experts architecture to
interleave distributed representations with more specialized, modular
representations. We show that very shallow ResMixNets are capable of learning
each of the two tasks well, attaining less than 2% and 1% test error on the
MNIST Parity and the colorized Pentomino tasks respectively. Most importantly,
the ResMixNet models are extremely parameter efficient: generalizing better
than various non-modular CNNs that have over 10x the number of parameters.
These experimental results support the hypothesis that modularity is a robust
prior for learning invariant relational reasoning.","cs.LG,cs.NE,q-bio.NC,stat.ML"
"Temporal Gaussian Process Regression in Logarithmic Time. The aim of this article is to present a novel parallelization method for
temporal Gaussian process (GP) regression problems. The method allows for
solving GP regression problems in logarithmic O(log N) time, where N is the
number of time steps. Our approach uses the state-space representation of GPs
which in its original form allows for linear O(N) time GP regression by
leveraging the Kalman filtering and smoothing methods. By using a recently
proposed parallelization method for Bayesian filters and smoothers, we are able
to reduce the linear computational complexity of the temporal GP regression
problems into logarithmic span complexity. This ensures logarithmic time
complexity when run on parallel hardware such as a graphics processing unit
(GPU). We experimentally demonstrate the computational benefits on simulated
and real datasets via our open-source implementation leveraging the GPflow
framework.","cs.LG,stat.CO,stat.ME"
"BiasedWalk: Biased Sampling for Representation Learning on Graphs. Network embedding algorithms are able to learn latent feature representations
of nodes, transforming networks into lower dimensional vector representations.
Typical key applications, which have effectively been addressed using network
embeddings, include link prediction, multilabel classification and community
detection. In this paper, we propose BiasedWalk, a scalable, unsupervised
feature learning algorithm that is based on biased random walks to sample
context information about each node in the network. Our random-walk based
sampling can behave as Breath-First-Search (BFS) and Depth-First-Search (DFS)
samplings with the goal to capture homophily and role equivalence between the
nodes in the network. We have performed a detailed experimental evaluation
comparing the performance of the proposed algorithm against various baseline
methods, on several datasets and learning tasks. The experiment results show
that the proposed method outperforms the baseline ones in most of the tasks and
datasets.","cs.LG,cs.SI,stat.ML"
"Approximation Methods for Partially Observed Markov Decision Processes (POMDPs). POMDPs are useful models for systems where the true underlying state is not
known completely to an outside observer; the outside observer incompletely
knows the true state of the system, and observes a noisy version of the true
system state. When the number of system states is large in a POMDP that often
necessitates the use of approximation methods to obtain near optimal solutions
for control. This survey is centered around the origins, theory, and
approximations of finite-state POMDPs. In order to understand POMDPs, it is
required to have an understanding of finite-state Markov Decision Processes
(MDPs) in \autoref{mdp} and Hidden Markov Models (HMMs) in \autoref{hmm}. For
this background theory, I provide only essential details on MDPs and HMMs and
leave longer expositions to textbook treatments before diving into the main
topics of POMDPs. Once the required background is covered, the POMDP is
introduced in \autoref{pomdp}. The origins of the POMDP are explained in the
classical papers section \autoref{classical}. Once the high computational
requirements are understood from the exact methodological point of view, the
main approximation methods are surveyed in \autoref{approximations}. Then, I
end the survey with some new research directions in \autoref{conclusion}.","cs.LG,cs.SY,eess.SY"
"EXTRACT: Strong Examples from Weakly-Labeled Sensor Data. Thanks to the rise of wearable and connected devices, sensor-generated time
series comprise a large and growing fraction of the world's data.
Unfortunately, extracting value from this data can be challenging, since
sensors report low-level signals (e.g., acceleration), not the high-level
events that are typically of interest (e.g., gestures). We introduce a
technique to bridge this gap by automatically extracting examples of real-world
events in low-level data, given only a rough estimate of when these events have
taken place.
  By identifying sets of features that repeat in the same temporal arrangement,
we isolate examples of such diverse events as human actions, power consumption
patterns, and spoken words with up to 96% precision and recall. Our method is
fast enough to run in real time and assumes only minimal knowledge of which
variables are relevant or the lengths of events. Our evaluation uses numerous
publicly available datasets and over 1 million samples of manually labeled
sensor data.","cs.DB,cs.LG,stat.ML"
"A Benchmarking Environment for Reinforcement Learning Based Task Oriented Dialogue Management. Dialogue assistants are rapidly becoming an indispensable daily aid. To avoid
the significant effort needed to hand-craft the required dialogue flow, the
Dialogue Management (DM) module can be cast as a continuous Markov Decision
Process (MDP) and trained through Reinforcement Learning (RL). Several RL
models have been investigated over recent years. However, the lack of a common
benchmarking framework makes it difficult to perform a fair comparison between
different models and their capability to generalise to different environments.
Therefore, this paper proposes a set of challenging simulated environments for
dialogue model development and evaluation. To provide some baselines, we
investigate a number of representative parametric algorithms, namely deep
reinforcement learning algorithms - DQN, A2C and Natural Actor-Critic and
compare them to a non-parametric model, GP-SARSA. Both the environments and
policy models are implemented using the publicly available PyDial toolkit and
released on-line, in order to establish a testbed framework for further
experiments and to facilitate experimental reproducibility.","cs.CL,cs.NE,stat.ML"
"Grammars and reinforcement learning for molecule optimization. We seek to automate the design of molecules based on specific chemical
properties. Our primary contributions are a simpler method for generating
SMILES strings guaranteed to be chemically valid, using a combination of a new
context-free grammar for SMILES and additional masking logic; and casting the
molecular property optimization as a reinforcement learning problem,
specifically best-of-batch policy gradient applied to a Transformer model
architecture. This approach uses substantially fewer model steps per atom than
earlier approaches, thus enabling generation of larger molecules, and beats
previous state-of-the art baselines by a significant margin. Applying
reinforcement learning to a combination of a custom context-free grammar with
additional masking to enforce non-local constraints is applicable to any
optimization of a graph structure under a mixture of local and nonlocal
constraints.","cs.LG,physics.chem-ph,stat.ML"
"Statistical feature embedding for heart sound classification. Cardiovascular Disease (CVD) is considered as one of the principal causes of
death in the world. Over recent years, this field of study has attracted
researchers' attention to investigate heart sounds' patterns for disease
diagnostics. In this study, an approach is proposed for normal/abnormal heart
sound classification on the Physionet challenge 2016 dataset. For the first
time, a fixed-length feature vector; called i-vector; is extracted from each
heart sound using Mel Frequency Cepstral Coefficient (MFCC) features.
Afterwards, Principal Component Analysis (PCA) transform and Variational
Autoencoder (VAE) are applied on the i-vector to achieve dimension reduction.
Eventually, the reduced size vector is fed to Gaussian Mixture Models (GMMs)
and Support Vector Machine (SVM) for classification purpose. Experimental
results demonstrate the proposed method could achieve a performance improvement
of 16% based on Modified Accuracy (MAcc) compared with the baseline system on
the Physoinet dataset.","cs.LG,cs.SD,eess.AS,stat.ML"
"Compare and Reweight: Distinctive Image Captioning Using Similar Images Sets. A wide range of image captioning models has been developed, achieving
significant improvement based on popular metrics, such as BLEU, CIDEr, and
SPICE. However, although the generated captions can accurately describe the
image, they are generic for similar images and lack distinctiveness, i.e.,
cannot properly describe the uniqueness of each image. In this paper, we aim to
improve the distinctiveness of image captions through training with sets of
similar images. First, we propose a distinctiveness metric -- between-set CIDEr
(CIDErBtw) to evaluate the distinctiveness of a caption with respect to those
of similar images. Our metric shows that the human annotations of each image
are not equivalent based on distinctiveness. Thus we propose several new
training strategies to encourage the distinctiveness of the generated caption
for each image, which are based on using CIDErBtw in a weighted loss function
or as a reinforcement learning reward. Finally, extensive experiments are
conducted, showing that our proposed approach significantly improves both
distinctiveness (as measured by CIDErBtw and retrieval metrics) and accuracy
(e.g., as measured by CIDEr) for a wide variety of image captioning baselines.
These results are further confirmed through a user study.","cs.CL,cs.CV,cs.LG"
"An attention-based multi-resolution model for prostate whole slide imageclassification and localization. Histology review is often used as the `gold standard' for disease diagnosis.
Computer aided diagnosis tools can potentially help improve current pathology
workflows by reducing examination time and interobserver variability. Previous
work in cancer grading has focused mainly on classifying pre-defined regions of
interest (ROIs), or relied on large amounts of fine-grained labels. In this
paper, we propose a two-stage attention-based multiple instance learning model
for slide-level cancer grading and weakly-supervised ROI detection and
demonstrate its use in prostate cancer. Compared with existing Gleason
classification models, our model goes a step further by utilizing visualized
saliency maps to select informative tiles for fine-grained grade
classification. The model was primarily developed on a large-scale whole slide
dataset consisting of 3,521 prostate biopsy slides with only slide-level labels
from 718 patients. The model achieved state-of-the-art performance for prostate
cancer grading with an accuracy of 85.11\% for classifying benign, low-grade
(Gleason grade 3+3 or 3+4), and high-grade (Gleason grade 4+3 or higher) slides
on an independent test set.","cs.CV,eess.IV"
"Symmetry-adapted generation of 3d point sets for the targeted discovery of molecules. Deep learning has proven to yield fast and accurate predictions of
quantum-chemical properties to accelerate the discovery of novel molecules and
materials. As an exhaustive exploration of the vast chemical space is still
infeasible, we require generative models that guide our search towards systems
with desired properties. While graph-based models have previously been
proposed, they are restricted by a lack of spatial information such that they
are unable to recognize spatial isomerism and non-bonded interactions. Here, we
introduce a generative neural network for 3d point sets that respects the
rotational invariance of the targeted structures. We apply it to the generation
of molecules and demonstrate its ability to approximate the distribution of
equilibrium structures using spatial metrics as well as established measures
from chemoinformatics. As our model is able to capture the complex relationship
between 3d geometry and electronic properties, we bias the distribution of the
generator towards molecules with a small HOMO-LUMO gap - an important property
for the design of organic solar cells.","cs.LG,physics.chem-ph,physics.comp-ph,stat.ML"
"Cohomology of Cryo-Electron Microscopy. The goal of cryo-electron microscopy (EM) is to reconstruct the 3-dimensional
structure of a molecule from a collection of its 2-dimensional projected
images. In this article, we show that the basic premise of cryo-EM --- patching
together 2-dimensional projections to reconstruct a 3-dimensional object --- is
naturally one of Cech cohomology with SO(2)-coefficients. We deduce that every
cryo-EM reconstruction problem corresponds to an oriented circle bundle on a
simplicial complex, allowing us to classify cryo-EM problems via principal
bundles. In practice, the 2-dimensional images are noisy and a main task in
cryo-EM is to denoise them. We will see how the aforementioned insights can be
used towards this end.","44A12,46M20,55R35,68U10,92E10,94A08,cs.CV,math.AT"
"Improved Structural Discovery and Representation Learning of Multi-Agent Data. Central to all machine learning algorithms is data representation. For
multi-agent systems, selecting a representation which adequately captures the
interactions among agents is challenging due to the latent group structure
which tends to vary depending on context. However, in multi-agent systems with
strong group structure, we can simultaneously learn this structure and map a
set of agents to a consistently ordered representation for further learning. In
this paper, we present a dynamic alignment method which provides a robust
ordering of structured multi-agent data enabling representation learning to
occur in a fraction of the time of previous methods. We demonstrate the value
of this approach using a large amount of soccer tracking data from a
professional league.","cs.LG,cs.MA,stat.ML"
"Token Shift Transformer for Video Classification. Transformer achieves remarkable successes in understanding 1 and
2-dimensional signals (e.g., NLP and Image Content Understanding). As a
potential alternative to convolutional neural networks, it shares merits of
strong interpretability, high discriminative power on hyper-scale data, and
flexibility in processing varying length inputs. However, its encoders
naturally contain computational intensive operations such as pair-wise
self-attention, incurring heavy computational burden when being applied on the
complex 3-dimensional video signals.
  This paper presents Token Shift Module (i.e., TokShift), a novel,
zero-parameter, zero-FLOPs operator, for modeling temporal relations within
each transformer encoder. Specifically, the TokShift barely temporally shifts
partial [Class] token features back-and-forth across adjacent frames. Then, we
densely plug the module into each encoder of a plain 2D vision transformer for
learning 3D video representation. It is worth noticing that our TokShift
transformer is a pure convolutional-free video transformer pilot with
computational efficiency for video understanding. Experiments on standard
benchmarks verify its robustness, effectiveness, and efficiency. Particularly,
with input clips of 8/12 frames, the TokShift transformer achieves SOTA
precision: 79.83%/80.40% on the Kinetics-400, 66.56% on EGTEA-Gaze+, and 96.80%
on UCF-101 datasets, comparable or better than existing SOTA convolutional
counterparts. Our code is open-sourced in:
https://github.com/VideoNetworks/TokShift-Transformer.","cs.CV,cs.MM"
"End-to-End 3D Point Cloud Learning for Registration Task Using Virtual Correspondences. 3D Point cloud registration is still a very challenging topic due to the
difficulty in finding the rigid transformation between two point clouds with
partial correspondences, and it's even harder in the absence of any initial
estimation information. In this paper, we present an end-to-end deep-learning
based approach to resolve the point cloud registration problem. Firstly, the
revised LPD-Net is introduced to extract features and aggregate them with the
graph network. Secondly, the self-attention mechanism is utilized to enhance
the structure information in the point cloud and the cross-attention mechanism
is designed to enhance the corresponding information between the two input
point clouds. Based on which, the virtual corresponding points can be generated
by a soft pointer based method, and finally, the point cloud registration
problem can be solved by implementing the SVD method. Comparison results in
ModelNet40 dataset validate that the proposed approach reaches the
state-of-the-art in point cloud registration tasks and experiment resutls in
KITTI dataset validate the effectiveness of the proposed approach in real
applications.Our source code is available at
\url{https://github.com/qiaozhijian/VCR-Net.git}","cs.CV,cs.RO"
"Exponential Convergence of Gradient Methods in Concave Network Zero-sum Games. Motivated by Generative Adversarial Networks, we study the computation of
Nash equilibrium in concave network zero-sum games (NZSGs), a multiplayer
generalization of two-player zero-sum games first proposed with linear payoffs.
Extending previous results, we show that various game theoretic properties of
convex-concave two-player zero-sum games are preserved in this generalization.
We then generalize last iterate convergence results obtained previously in
two-player zero-sum games. We analyze convergence rates when players update
their strategies using Gradient Ascent, and its variant, Optimistic Gradient
Ascent, showing last iterate convergence in three settings -- when the payoffs
of players are linear, strongly concave and Lipschitz, and strongly concave and
smooth. We provide experimental results that support these theoretical
findings.","cs.GT,cs.LG,math.OC,stat.ML"
"Rule Extraction in Unsupervised Anomaly Detection for Model Explainability: Application to OneClass SVM. OneClass SVM is a popular method for unsupervised anomaly detection. As many
other methods, it suffers from the black box problem: it is difficult to
justify, in an intuitive and simple manner, why the decision frontier is
identifying data points as anomalous or non anomalous. Such type of problem is
being widely addressed for supervised models. However, it is still an uncharted
area for unsupervised learning. In this paper, we evaluate several rule
extraction techniques over OneClass SVM models, as well as present alternative
designs for some of those algorithms. Together with that, we propose algorithms
to compute metrics related with eXplainable Artificial Intelligence (XAI)
regarding the ""comprehensibility"", ""representativeness"", ""stability"" and
""diversity"" of the extracted rules. We evaluate our proposals with different
datasets, including real-world data coming from industry. With this, our
proposal contributes to extend XAI techniques to unsupervised machine learning
models.","I.2.6,I.2.6; I.5.4,I.5.4,cs.AI,cs.LG"
"On a novel training algorithm for sequence-to-sequence predictive recurrent networks. Neural networks mapping sequences to sequences (seq2seq) lead to significant
progress in machine translation and speech recognition. Their traditional
architecture includes two recurrent networks (RNs) followed by a linear
predictor. In this manuscript we perform analysis of a corresponding algorithm
and show that the parameters of the RNs of the well trained predictive network
are not independent of each other. Their dependence can be used to
significantly improve the network effectiveness. The traditional seq2seq
algorithms require short term memory of a size proportional to the predicted
sequence length. This requirement is quite difficult to implement in a
neuroscience context. We present a novel memoryless algorithm for seq2seq
predictive networks and compare it to the traditional one in the context of
time series prediction. We show that the new algorithm is more robust and makes
predictions with higher accuracy than the traditional one.","37N30,68T07,cs.LG"
"The Right to Talk: An Audio-Visual Transformer Approach. Turn-taking has played an essential role in structuring the regulation of a
conversation. The task of identifying the main speaker (who is properly taking
his/her turn of speaking) and the interrupters (who are interrupting or
reacting to the main speaker's utterances) remains a challenging task. Although
some prior methods have partially addressed this task, there still remain some
limitations. Firstly, a direct association of Audio and Visual features may
limit the correlations to be extracted due to different modalities. Secondly,
the relationship across temporal segments helping to maintain the consistency
of localization, separation, and conversation contexts is not effectively
exploited. Finally, the interactions between speakers that usually contain the
tracking and anticipatory decisions about the transition to a new speaker are
usually ignored. Therefore, this work introduces a new Audio-Visual Transformer
approach to the problem of localization and highlighting the main speaker in
both audio and visual channels of a multi-speaker conversation video in the
wild. The proposed method exploits different types of correlations presented in
both visual and audio signals. The temporal audio-visual relationships across
spatial-temporal space are anticipated and optimized via the self-attention
mechanism in a Transformerstructure. Moreover, a newly collected dataset is
introduced for the main speaker detection. To the best of our knowledge, it is
one of the first studies that is able to automatically localize and highlight
the main speaker in both visual and audio channels in multi-speaker
conversation videos.","cs.CV,cs.SD,eess.AS"
"Emulating dynamic non-linear simulators using Gaussian processes. The dynamic emulation of non-linear deterministic computer codes where the
output is a time series, possibly multivariate, is examined. Such computer
models simulate the evolution of some real-world phenomenon over time, for
example models of the climate or the functioning of the human brain. The models
we are interested in are highly non-linear and exhibit tipping points,
bifurcations and chaotic behaviour. However, each simulation run could be too
time-consuming to perform analyses that require many runs, including
quantifying the variation in model output with respect to changes in the
inputs. Therefore, Gaussian process emulators are used to approximate the
output of the code. To do this, the flow map of the system under study is
emulated over a short time period. Then, it is used in an iterative way to
predict the whole time series. A number of ways are proposed to take into
account the uncertainty of inputs to the emulators, after fixed initial
conditions, and the correlation between them through the time series. The
methodology is illustrated with two examples: the highly non-linear dynamical
systems described by the Lorenz and Van der Pol equations. In both cases, the
predictive performance is relatively high and the measure of uncertainty
provided by the method reflects the extent of predictability in each system.","math.DS,stat.AP,stat.ML"
"Near-Optimal Explainable $k$-Means for All Dimensions. Many clustering algorithms are guided by certain cost functions such as the
widely-used $k$-means cost. These algorithms divide data points into clusters
with often complicated boundaries, creating difficulties in explaining the
clustering decision. In a recent work, Dasgupta, Frost, Moshkovitz, and
Rashtchian (ICML'20) introduced explainable clustering, where the cluster
boundaries are axis-parallel hyperplanes and the clustering is obtained by
applying a decision tree to the data. The central question here is: how much
does the explainability constraint increase the value of the cost function?
  Given $d$-dimensional data points, we show an efficient algorithm that finds
an explainable clustering whose $k$-means cost is at most $k^{1 -
2/d}\mathrm{poly}(d\log k)$ times the minimum cost achievable by a clustering
without the explainability constraint, assuming $k,d\ge 2$. Combining this with
an independent work by Makarychev and Shan (ICML'21), we get an improved bound
of $k^{1 - 2/d}\mathrm{polylog}(k)$, which we show is optimal for every choice
of $k,d\ge 2$ up to a poly-logarithmic factor in $k$. For $d = 2$ in
particular, we show an $O(\log k\log\log k)$ bound, improving exponentially
over the previous best bound of $\widetilde O(k)$.","cs.CG,cs.DS,cs.LG,stat.ML"
"Gabor Wavelets in Image Processing. This work shows the use of a two-dimensional Gabor wavelets in image
processing. Convolution with such a two-dimensional wavelet can be separated
into two series of one-dimensional ones. The key idea of this work is to
utilize a Gabor wavelet as a multiscale partial differential operator of a
given order. Gabor wavelets are used here to detect edges, corners and blobs. A
performance of such an interest point detector is compared to detectors
utilizing a Haar wavelet and a derivative of a Gaussian function. The proposed
approach may be useful when a fast implementation of the Gabor transform is
available or when the transform is already precomputed.","cs.CV,cs.GR,cs.MM"
"Robust Deep Neural Networks Inspired by Fuzzy Logic. Deep neural networks have achieved impressive performance and become the
de-facto standard in many tasks. However, troubling phenomena such as
adversarial and fooling examples suggest that the generalization they make is
flawed. I argue that among the roots of the phenomena are two geometric
properties of common deep learning architectures: their distributed nature and
the connectedness of their decision regions. As a remedy, I propose new
architectures inspired by fuzzy logic that combine several alternative design
elements. Through experiments on MNIST and CIFAR-10, the new models are shown
to be more local, better at rejecting noise samples, and more robust against
adversarial examples. Ablation analyses reveal behaviors on adversarial
examples that cannot be explained by the linearity hypothesis but are
consistent with the hypothesis that logic-inspired traits create more robust
models.","cs.LG,cs.LO,stat.ML"
"Inclusion of Domain-Knowledge into GNNs using Mode-Directed Inverse Entailment. We present a general technique for constructing Graph Neural Networks (GNNs)
capable of using multi-relational domain knowledge. The technique is based on
mode-directed inverse entailment (MDIE) developed in Inductive Logic
Programming (ILP). Given a data instance $e$ and background knowledge $B$, MDIE
identifies a most-specific logical formula $\bot_B(e)$ that contains all the
relational information in $B$ that is related to $e$. We represent $\bot_B(e)$
by a ""bottom-graph"" that can be converted into a form suitable for GNN
implementations. This transformation allows a principled way of incorporating
generic background knowledge into GNNs: we use the term `BotGNN' for this form
of graph neural networks. For several GNN variants, using real-world datasets
with substantial background knowledge, we show that BotGNNs perform
significantly better than both GNNs without background knowledge and a recently
proposed simplified technique for including domain knowledge into GNNs. We also
provide experimental evidence comparing BotGNNs favourably to multi-layer
perceptrons (MLPs) that use features representing a ""propositionalised"" form of
the background knowledge; and BotGNNs to a standard ILP based on the use of
most-specific clauses. Taken together, these results point to BotGNNs as
capable of combining the computational efficacy of GNNs with the
representational versatility of ILP.","68T05,68T07,68T27,68T30,I.2.6,cs.AI,cs.LG,cs.NE"
"Metric Pose Estimation for Human-Machine Interaction Using Monocular Vision. The rapid growth of collaborative robotics in production requires new
automation technologies that take human and machine equally into account. In
this work, we describe a monocular camera based system to detect human-machine
interactions from a bird's-eye perspective. Our system predicts poses of humans
and robots from a single wide-angle color image. Even though our approach works
on 2D color input, we lift the majority of detections to a metric 3D space. Our
system merges pose information with predefined virtual sensors to coordinate
human-machine interactions. We demonstrate the advantages of our system in
three use cases.","cs.CV,cs.HC,cs.RO"
"Neural Operator: Learning Maps Between Function Spaces. The classical development of neural networks has primarily focused on
learning mappings between finite dimensional Euclidean spaces or finite sets.
We propose a generalization of neural networks tailored to learn operators
mapping between infinite dimensional function spaces. We formulate the
approximation of operators by composition of a class of linear integral
operators and nonlinear activation functions, so that the composed operator can
approximate complex nonlinear operators. We prove a universal approximation
theorem for our construction. Furthermore, we introduce four classes of
operator parameterizations: graph-based operators, low-rank operators,
multipole graph-based operators, and Fourier operators and describe efficient
algorithms for computing with each one. The proposed neural operators are
resolution-invariant: they share the same network parameters between different
discretizations of the underlying function spaces and can be used for zero-shot
super-resolutions. Numerically, the proposed models show superior performance
compared to existing machine learning based methodologies on Burgers' equation,
Darcy flow, and the Navier-Stokes equation, while being several order of
magnitude faster compared to conventional PDE solvers.","cs.LG,cs.NA,math.NA"
"Discovery of Physics and Characterization of Microstructure from Data with Bayesian Hidden Physics Models. There has been a surge in the interest of using machine learning techniques
to assist in the scientific process of formulating knowledge to explain
observational data. We demonstrate the use of Bayesian Hidden Physics Models to
first uncover the physics governing the propagation of acoustic impulses in
metallic specimens using data obtained from a pristine sample. We then use the
learned physics to characterize the microstructure of a separate specimen with
a surface-breaking crack flaw. Remarkably, we find that the physics learned
from the first specimen allows us to understand the backscattering observed in
the latter sample, a qualitative feature that is wholly absent from the
specimen from which the physics were inferred. The backscattering is explained
through inhomogeneities of a latent spatial field that can be recognized as the
speed of sound in the media.","cs.CE,cs.LG"
"Lifelong Learning of Graph Neural Networks for Open-World Node Classification. Graph neural networks (GNNs) have emerged as the standard method for numerous
tasks on graph-structured data such as node classification. However, real-world
graphs are often evolving over time and even new classes may arise. We model
these challenges as an instance of lifelong learning, in which a learner faces
a sequence of tasks and may take over knowledge acquired in past tasks. Such
knowledge may be stored explicitly as historic data or implicitly within model
parameters. In this work, we systematically analyze the influence of implicit
and explicit knowledge. Therefore, we present an incremental training method
for lifelong learning on graphs and introduce a new measure based on
$k$-neighborhood time differences to address variances in the historic data. We
apply our training method to five representative GNN architectures and evaluate
them on three new lifelong node classification datasets. Our results show that
no more than 50% of the GNN's receptive field is necessary to retain at least
95% accuracy compared to training over the complete history of the graph data.
Furthermore, our experiments confirm that implicit knowledge becomes more
important when fewer explicit knowledge is available.","cs.LG,cs.SI,stat.ML"
"Non-Local Robust Quaternion Matrix Completion for Large-Scale Color Images and Videos Inpainting. The image nonlocal self-similarity (NSS) prior refers to the fact that a
local patch often has many nonlocal similar patches to it across the image. In
this paper we apply such NSS prior to enhance the robust quaternion matrix
completion (QMC) method and significantly improve the inpainting performance. A
patch group based NSS prior learning scheme is proposed to learn explicit NSS
models from natural color images. The NSS-based QMC algorithm computes an
optimal low-rank approximation to the high-rank color image, resulting in high
PSNR and SSIM measures and particularly the better visual quality. A new joint
NSS-base QMC method is also presented to solve the color video inpainting
problem based quaternion tensor representation. The numerical experiments on
large-scale color images and videos indicate the advantages of NSS-based QMC
over the state-of-the-art methods.","65F55,G.1.3,cs.CV,cs.NA,math.NA"
"ModelDiff: Testing-Based DNN Similarity Comparison for Model Reuse Detection. The knowledge of a deep learning model may be transferred to a student model,
leading to intellectual property infringement or vulnerability propagation.
Detecting such knowledge reuse is nontrivial because the suspect models may not
be white-box accessible and/or may serve different tasks. In this paper, we
propose ModelDiff, a testing-based approach to deep learning model similarity
comparison. Instead of directly comparing the weights, activations, or outputs
of two models, we compare their behavioral patterns on the same set of test
inputs. Specifically, the behavioral pattern of a model is represented as a
decision distance vector (DDV), in which each element is the distance between
the model's reactions to a pair of inputs. The knowledge similarity between two
models is measured with the cosine similarity between their DDVs. To evaluate
ModelDiff, we created a benchmark that contains 144 pairs of models that cover
most popular model reuse methods, including transfer learning, model
compression, and model stealing. Our method achieved 91.7% correctness on the
benchmark, which demonstrates the effectiveness of using ModelDiff for model
reuse detection. A study on mobile deep learning apps has shown the feasibility
of ModelDiff on real-world models.","cs.AI,cs.LG,cs.SE"
"Comparing Natural Language Processing Techniques for Alzheimer's Dementia Prediction in Spontaneous Speech. Alzheimer's Dementia (AD) is an incurable, debilitating, and progressive
neurodegenerative condition that affects cognitive function. Early diagnosis is
important as therapeutics can delay progression and give those diagnosed vital
time. Developing models that analyse spontaneous speech could eventually
provide an efficient diagnostic modality for earlier diagnosis of AD. The
Alzheimer's Dementia Recognition through Spontaneous Speech task offers
acoustically pre-processed and balanced datasets for the classification and
prediction of AD and associated phenotypes through the modelling of spontaneous
speech. We exclusively analyse the supplied textual transcripts of the
spontaneous speech dataset, building and comparing performance across numerous
models for the classification of AD vs controls and the prediction of Mental
Mini State Exam scores. We rigorously train and evaluate Support Vector
Machines (SVMs), Gradient Boosting Decision Trees (GBDT), and Conditional
Random Fields (CRFs) alongside deep learning Transformer based models. We find
our top performing models to be a simple Term Frequency-Inverse Document
Frequency (TF-IDF) vectoriser as input into a SVM model and a pre-trained
Transformer based model `DistilBERT' when used as an embedding layer into
simple linear models. We demonstrate test set scores of 0.81-0.82 across
classification metrics and a RMSE of 4.58.","cs.CL,cs.LG,cs.SD,eess.AS,stat.ML"
"An Adaptive Deep Learning Framework for Day-ahead Forecasting of Photovoltaic Power Generation. Accurate forecasts of photovoltaic power generation (PVPG) are essential to
optimize operations between energy supply and demand. Recently, the propagation
of sensors and smart meters has produced an enormous volume of data, which
supports the development of data based PVPG forecasting. Although emerging deep
learning (DL) models, such as the long short-term memory (LSTM) model, based on
historical data, have provided effective solutions for PVPG forecasting with
great successes, these models utilize offline learning. As a result, DL models
cannot take advantage of the opportunity to learn from newly-arrived data, and
are unable to handle concept drift caused by installing extra PV units and
unforeseen PV unit failures. Consequently, to improve day-ahead PVPG
forecasting accuracy, as well as eliminate the impacts of concept drift, this
paper proposes an adaptive LSTM (AD-LSTM) model, which is a DL framework that
can not only acquire general knowledge from historical data, but also
dynamically learn specific knowledge from newly-arrived data. A two-phase
adaptive learning strategy (TP-ALS) is integrated into AD-LSTM, and a sliding
window (SDWIN) algorithm is proposed, to detect concept drift in PV systems.
Multiple datasets from PV systems are utilized to assess the feasibility and
effectiveness of the proposed approaches. The developed AD-LSTM model
demonstrates greater forecasting capability than the offline LSTM model,
particularly in the presence of concept drift. Additionally, the proposed
AD-LSTM model also achieves superior performance in terms of day-ahead PVPG
forecasting compared to other traditional machine learning models and
statistical models in the literature.","cs.AI,cs.LG,physics.comp-ph,physics.data-an"
"Ultrasound Image Representation Learning by Modeling Sonographer Visual Attention. Image representations are commonly learned from class labels, which are a
simplistic approximation of human image understanding. In this paper we
demonstrate that transferable representations of images can be learned without
manual annotations by modeling human visual attention. The basis of our
analyses is a unique gaze tracking dataset of sonographers performing routine
clinical fetal anomaly screenings. Models of sonographer visual attention are
learned by training a convolutional neural network (CNN) to predict gaze on
ultrasound video frames through visual saliency prediction or gaze-point
regression. We evaluate the transferability of the learned representations to
the task of ultrasound standard plane detection in two contexts. Firstly, we
perform transfer learning by fine-tuning the CNN with a limited number of
labeled standard plane images. We find that fine-tuning the saliency predictor
is superior to training from random initialization, with an average F1-score
improvement of 9.6% overall and 15.3% for the cardiac planes. Secondly, we
train a simple softmax regression on the feature activations of each CNN layer
in order to evaluate the representations independently of transfer learning
hyper-parameters. We find that the attention models derive strong
representations, approaching the precision of a fully-supervised baseline model
for all but the last layer.","68T45,I.2.10,cs.CV,cs.LG,cs.NE"
"Industrial object, machine part and defect recognition towards fully automated industrial monitoring employing deep learning. The case of multilevel VGG19. Modern industry requires modern solutions for monitoring the automatic
production of goods. Smart monitoring of the functionality of the mechanical
parts of technology systems or machines is mandatory for a fully automatic
production process. Although Deep Learning has been advancing, allowing for
real-time object detection and other tasks, little has been investigated about
the effectiveness of specially designed Convolutional Neural Networks for
defect detection and industrial object recognition. In the particular study, we
employed six publically available industrial-related datasets containing defect
materials and industrial tools or engine parts, aiming to develop a specialized
model for pattern recognition. Motivated by the recent success of the Virtual
Geometry Group (VGG) network, we propose a modified version of it, called
Multipath VGG19, which allows for more local and global feature extraction,
while the extra features are fused via concatenation. The experiments verified
the effectiveness of MVGG19 over the traditional VGG19. Specifically, top
classification performance was achieved in five of the six image datasets,
while the average classification improvement was 6.95%.","cs.CV,eess.IV"
"EAGER: Embedding-Assisted Entity Resolution for Knowledge Graphs. Entity Resolution (ER) is a constitutional part for integrating different
knowledge graphs in order to identify entities referring to the same real-world
object. A promising approach is the use of graph embeddings for ER in order to
determine the similarity of entities based on the similarity of their graph
neighborhood. The similarity computations for such embeddings translates to
calculating the distance between them in the embedding space which is
comparatively simple. However, previous work has shown that the use of graph
embeddings alone is not sufficient to achieve high ER quality. We therefore
propose a more comprehensive ER approach for knowledge graphs called EAGER
(Embedding-Assisted Knowledge Graph Entity Resolution) to flexibly utilize both
the similarity of graph embeddings and attribute values within a supervised
machine learning approach. We evaluate our approach on 23 benchmark datasets
with differently sized and structured knowledge graphs and use hypothesis tests
to ensure statistical significance of our results. Furthermore we compare our
approach with state-of-the-art ER solutions, where our approach yields
competitive results for table-oriented ER problems and shallow knowledge graphs
but much better results for deeper knowledge graphs.","I.2.4; I.2.6,cs.DB,cs.LG"
"On the Transferability of VAE Embeddings using Relational Knowledge with Semi-Supervision. We propose a new model for relational VAE semi-supervision capable of
balancing disentanglement and low complexity modelling of relations with
different symbolic properties. We compare the relative benefits of
relation-decoder complexity and latent space structure on both inductive and
transductive transfer learning. Our results depict a complex picture where
enforcing structure on semi-supervised representations can greatly improve
zero-shot transductive transfer, but may be less favourable or even impact
negatively the capacity for inductive transfer.","cs.AI,cs.LG"
"Predicting Sequences of Traversed Nodes in Graphs using Network Models with Multiple Higher Orders. We propose a novel sequence prediction method for sequential data capturing
node traversals in graphs. Our method builds on a statistical modelling
framework that combines multiple higher-order network models into a single
multi-order model. We develop a technique to fit such multi-order models in
empirical sequential data and to select the optimal maximum order. Our
framework facilitates both next-element and full sequence prediction given a
sequence-prefix of any length. We evaluate our model based on six empirical
data sets containing sequences from website navigation as well as public
transport systems. The results show that our method out-performs
state-of-the-art algorithms for next-element prediction. We further demonstrate
the accuracy of our method during out-of-sample sequence prediction and
validate that our method can scale to data sets with millions of sequences.","cs.IT,cs.LG,cs.SI,math.IT,physics.data-an,stat.ML"
"Inference with Aggregate Data: An Optimal Transport Approach. We consider inference (filtering) problems over probabilistic graphical
models with aggregate data generated by a large population of individuals. We
propose a new efficient belief propagation type algorithm over tree-structured
graphs with polynomial computational complexity as well as a global convergence
guarantee. This is in contrast to previous methods that either exhibit
prohibitive complexity as the population grows or do not guarantee convergence.
Our method is based on optimal transport, or more specifically, multi-marginal
optimal transport theory. In particular, we consider an inference problem with
aggregate observations, that can be seen as a structured multi-marginal optimal
transport problem where the cost function decomposes according to the
underlying graph. Consequently, the celebrated Sinkhorn/iterative scaling
algorithm for multi-marginal optimal transport can be leveraged together with
the standard belief propagation algorithm to establish an efficient inference
scheme which we call Sinkhorn belief propagation (SBP). We further specialize
the SBP algorithm to cases associated with hidden Markov models due to their
significance in control and estimation. We demonstrate the performance of our
algorithm on applications such as inferring population flow from aggregate
observations. We also show that in the special case where the observations are
generated by a single individual, our algorithm naturally reduces to the
standard belief propagation algorithm.","62F15,62M05,93E10,cs.LG,cs.SY,eess.SY,stat.ML"
"Robust Multi-object Matching via Iterative Reweighting of the Graph Connection Laplacian. We propose an efficient and robust iterative solution to the multi-object
matching problem. We first clarify serious limitations of current methods as
well as the inappropriateness of the standard iteratively reweighted least
squares procedure. In view of these limitations, we suggest a novel and more
reliable iterative reweighting strategy that incorporates information from
higher-order neighborhoods by exploiting the graph connection Laplacian. We
demonstrate the superior performance of our procedure over state-of-the-art
methods using both synthetic and real datasets.","65C20,68Q87,90C10,90C17,90C26,G.1.6; I.4.0,cs.CV,cs.NA,math.NA,math.PR,stat.ML"
"PySS3: A Python package implementing a novel text classifier with visualization tools for Explainable AI. A recently introduced text classifier, called SS3, has obtained
state-of-the-art performance on the CLEF's eRisk tasks. SS3 was created to deal
with risk detection over text streams and, therefore, not only supports
incremental training and classification but also can visually explain its
rationale. However, little attention has been paid to the potential use of SS3
as a general classifier. We believe this could be due to the unavailability of
an open-source implementation of SS3. In this work, we introduce PySS3, a
package that implements SS3 and also comes with visualization tools that allow
researchers to deploy robust, explainable, and trusty machine learning models
for text classification.","cs.AI,cs.IR,cs.LG,cs.SE,stat.ML"
"Sparse-Push: Communication- & Energy-Efficient Decentralized Distributed Learning over Directed & Time-Varying Graphs with non-IID Datasets. Current deep learning (DL) systems rely on a centralized computing paradigm
which limits the amount of available training data, increases system latency,
and adds privacy and security constraints. On-device learning, enabled by
decentralized and distributed training of DL models over peer-to-peer
wirelessly connected edge devices, not only alleviate the above limitations but
also enable next-gen applications that need DL models to continuously interact
and learn from their environment. However, this necessitates the development of
novel training algorithms that train DL models over time-varying and directed
peer-to-peer graph structures while minimizing the amount of communication
between the devices and also being resilient to non-IID data distributions. In
this work we propose, Sparse-Push, a communication efficient decentralized
distributed training algorithm that supports training over peer-to-peer,
directed, and time-varying graph topologies. The proposed algorithm enables
466x reduction in communication with only 1% degradation in performance when
training various DL models such as ResNet-20 and VGG11 over the CIFAR-10
dataset. Further, we demonstrate how communication compression can lead to
significant performance degradation in-case of non-IID datasets, and propose
Skew-Compensated Sparse Push algorithm that recovers this performance drop
while maintaining similar levels of communication compression.","cs.AI,cs.CV,cs.DC,cs.LG"
"Multi-level Multimodal Common Semantic Space for Image-Phrase Grounding. We address the problem of phrase grounding by lear ing a multi-level common
semantic space shared by the textual and visual modalities. We exploit multiple
levels of feature maps of a Deep Convolutional Neural Network, as well as
contextualized word and sentence embeddings extracted from a character-based
language model. Following dedicated non-linear mappings for visual features at
each level, word, and sentence embeddings, we obtain multiple instantiations of
our common semantic space in which comparisons between any target text and the
visual content is performed with cosine similarity. We guide the model by a
multi-level multimodal attention mechanism which outputs attended visual
features at each level. The best level is chosen to be compared with text
content for maximizing the pertinence scores of image-sentence pairs of the
ground truth. Experiments conducted on three publicly available datasets show
significant performance gains (20%-60% relative) over the state-of-the-art in
phrase localization and set a new performance record on those datasets. We
provide a detailed ablation study to show the contribution of each element of
our approach and release our code on GitHub.","cs.CL,cs.CV,cs.LG,eess.IV"
"Learning Interpretable Representations of Entanglement in Quantum Optics Experiments using Deep Generative Models. Quantum physics experiments produce interesting phenomena such as
interference or entanglement, which is a core property of numerous future
quantum technologies. The complex relationship between a quantum experiment's
structure and its entanglement properties is essential to fundamental research
in quantum optics but is difficult to intuitively understand. We present the
first deep generative model of quantum optics experiments where a variational
autoencoder (QOVAE) is trained on a dataset of experimental setups. In a series
of computational experiments, we investigate the learned representation of the
QOVAE and its internal understanding of the quantum optics world. We
demonstrate that the QOVAE learns an intrepretable representation of quantum
optics experiments and the relationship between experiment structure and
entanglement. We show the QOVAE is able to generate novel experiments for
highly entangled quantum states with specific distributions that match its
training data. Importantly, we are able to fully interpret how the QOVAE
structures its latent space, finding curious patterns that we can entirely
explain in terms of quantum physics. The results demonstrate how we can
successfully use and understand the internal representations of deep generative
models in a complex scientific domain. The QOVAE and the insights from our
investigations can be immediately applied to other physical systems throughout
fundamental scientific research.","cs.LG,quant-ph"
"Finite-Sample Analysis For Decentralized Batch Multi-Agent Reinforcement Learning With Networked Agents. Despite the increasing interest in multi-agent reinforcement learning (MARL)
in multiple communities, understanding its theoretical foundation has long been
recognized as a challenging problem. In this work, we address this problem by
providing a finite-sample analysis for decentralized batch MARL with networked
agents. Specifically, we consider two decentralized MARL settings, where teams
of agents are connected by time-varying communication networks, and either
collaborate or compete in a zero-sum game setting, without any central
controller. These settings cover many conventional MARL settings in the
literature. For both settings, we develop batch MARL algorithms that can be
implemented in a decentralized fashion, and quantify the finite-sample errors
of the estimated action-value functions. Our error analysis captures how the
function class, the number of samples within each iteration, and the number of
iterations determine the statistical accuracy of the proposed algorithms. Our
results, compared to the finite-sample bounds for single-agent RL, involve
additional error terms caused by decentralized computation, which is inherent
in our decentralized MARL setting. This work appears to be the first
finite-sample analysis for batch MARL, a step towards rigorous theoretical
understanding of general MARL algorithms in the finite-sample regime.","cs.AI,cs.LG,cs.MA,stat.ML"
"Quantum Algorithms for Structured Prediction. We introduce two quantum algorithms for solving structured prediction
problems. We first show that a stochastic gradient descent that uses the
quantum minimum finding algorithm and takes its probabilistic failure into
account solves the structured prediction problem with a runtime that scales
with the square root of the size of the label space, and in $\widetilde
O\left(1/\epsilon\right)$ with respect to the precision, $\epsilon$, of the
solution. Motivated by robust inference techniques in machine learning, we then
introduce another quantum algorithm that solves a smooth approximation of the
structured prediction problem with a similar quantum speedup in the size of the
label space and a similar scaling in the precision parameter. In doing so, we
analyze a variant of stochastic gradient descent for convex optimization in the
presence of an additive error in the calculation of the gradients, and show
that its convergence rate does not deteriorate if the additive errors are of
the order $O(\sqrt\epsilon)$. This algorithm uses quantum Gibbs sampling at
temperature $\Omega (\epsilon)$ as a subroutine. Based on these theoretical
observations, we propose a method for using quantum Gibbs samplers to combine
feedforward neural networks with probabilistic graphical models for quantum
machine learning. Our numerical results using Monte Carlo simulations on an
image tagging task demonstrate the benefit of the approach.","cs.CC,cs.LG,math.OC,quant-ph,stat.ML"
"A Generalized Asymmetric Dual-front Model for Active Contours and Image Segmentation. The Voronoi diagram-based dual-front active contour models are known as a
powerful and efficient way for addressing the image segmentation and domain
partitioning problems. In the basic formulation of the dual-front models, the
evolving contours can be considered as the interfaces of adjacent Voronoi
regions. Among these dual-front models, a crucial ingredient is regarded as the
geodesic metrics by which the geodesic distances and the corresponding Voronoi
diagram can be estimated. In this paper, we introduce a type of asymmetric
quadratic metrics dual-front model. The metrics considered are built by the
integration of the image features and a vector field derived from the evolving
contours. The use of the asymmetry enhancement can reduce the risk of contour
shortcut or leakage problems especially when the initial contours are far away
from the target boundaries or the images have complicated intensity
distributions. Moreover, the proposed dual-front model can be applied for image
segmentation in conjunction with various region-based homogeneity terms. The
numerical experiments on both synthetic and real images show that the proposed
dual-front model indeed achieves encouraging results.","cs.CG,cs.CV"
"A fast noise filtering algorithm for time series prediction using recurrent neural networks. Recent research demonstrate that prediction of time series by recurrent
neural networks (RNNs) based on the noisy input generates a smooth anticipated
trajectory. We examine the internal dynamics of RNNs and establish a set of
conditions required for such behavior. Based on this analysis we propose a new
approximate algorithm and show that it significantly speeds up the predictive
process without loss of accuracy.","37N30,68T07,cs.LG,math.DS,stat.ML"
"Probabilistic Color Constancy. In this paper, we propose a novel unsupervised color constancy method, called
Probabilistic Color Constancy (PCC). We define a framework for estimating the
illumination of a scene by weighting the contribution of different image
regions using a graph-based representation of the image. To estimate the weight
of each (super-)pixel, we rely on two assumptions: (Super-)pixels with similar
colors contribute similarly and darker (super-)pixels contribute less. The
resulting system has one global optimum solution. The proposed method achieves
competitive performance, compared to the state-of-the-art, on INTEL-TAU
dataset.","cs.CV,eess.IV"
"Performance Based Evaluation of Various Machine Learning Classification Techniques for Chronic Kidney Disease Diagnosis. Areas where Artificial Intelligence (AI) & related fields are finding their
applications are increasing day by day, moving from core areas of computer
science they are finding their applications in various other domains.In recent
times Machine Learning i.e. a sub-domain of AI has been widely used in order to
assist medical experts and doctors in the prediction, diagnosis and prognosis
of various diseases and other medical disorders. In this manuscript the authors
applied various machine learning algorithms to a problem in the domain of
medical diagnosis and analyzed their efficiency in predicting the results. The
problem selected for the study is the diagnosis of the Chronic Kidney
Disease.The dataset used for the study consists of 400 instances and 24
attributes. The authors evaluated 12 classification techniques by applying them
to the Chronic Kidney Disease data. In order to calculate efficiency, results
of the prediction by candidate methods were compared with the actual medical
results of the subject.The various metrics used for performance evaluation are
predictive accuracy, precision, sensitivity and specificity. The results
indicate that decision-tree performed best with nearly the accuracy of 98.6%,
sensitivity of 0.9720, precision of 1 and specificity of 1.","cs.AI,cs.CY,cs.LG"
"Correspondence Learning via Linearly-invariant Embedding. In this paper, we propose a fully differentiable pipeline for estimating
accurate dense correspondences between 3D point clouds. The proposed pipeline
is an extension and a generalization of the functional maps framework. However,
instead of using the Laplace-Beltrami eigenfunctions as done in virtually all
previous works in this domain, we demonstrate that learning the basis from data
can both improve robustness and lead to better accuracy in challenging
settings. We interpret the basis as a learned embedding into a higher
dimensional space. Following the functional map paradigm the optimal
transformation in this embedding space must be linear and we propose a separate
architecture aimed at estimating the transformation by learning optimal
descriptor functions. This leads to the first end-to-end trainable functional
map-based correspondence approach in which both the basis and the descriptors
are learned from data. Interestingly, we also observe that learning a
\emph{canonical} embedding leads to worse results, suggesting that leaving an
extra linear degree of freedom to the embedding network gives it more
robustness, thereby also shedding light onto the success of previous methods.
Finally, we demonstrate that our approach achieves state-of-the-art results in
challenging non-rigid 3D point cloud correspondence applications.","cs.CG,cs.CV,cs.LG"
"Dynamic Visualization and Fast Computation for Convex Clustering via Algorithmic Regularization. Convex clustering is a promising new approach to the classical problem of
clustering, combining strong performance in empirical studies with rigorous
theoretical foundations. Despite these advantages, convex clustering has not
been widely adopted, due to its computationally intensive nature and its lack
of compelling visualizations. To address these impediments, we introduce
Algorithmic Regularization, an innovative technique for obtaining high-quality
estimates of regularization paths using an iterative one-step approximation
scheme. We justify our approach with a novel theoretical result, guaranteeing
global convergence of the approximate path to the exact solution under
easily-checked non-data-dependent assumptions. The application of algorithmic
regularization to convex clustering yields the Convex Clustering via
Algorithmic Regularization Paths (CARP) algorithm for computing the clustering
solution path. On example data sets from genomics and text analysis, CARP
delivers over a 100-fold speed-up over existing methods, while attaining a
finer approximation grid than standard methods. Furthermore, CARP enables
improved visualization of clustering solutions: the fine solution grid returned
by CARP can be used to construct a convex clustering-based dendrogram, as well
as forming the basis of a dynamic path-wise visualization based on modern web
technologies. Our methods are implemented in the open-source R package
clustRviz, available at https://github.com/DataSlingers/clustRviz.","cs.LG,stat.CO,stat.ME,stat.ML"
"The Whole Is Greater Than the Sum of Its Nonrigid Parts. According to Aristotle, a philosopher in Ancient Greece, ""the whole is
greater than the sum of its parts"". This observation was adopted to explain
human perception by the Gestalt psychology school of thought in the twentieth
century. Here, we claim that observing part of an object which was previously
acquired as a whole, one could deal with both partial matching and shape
completion in a holistic manner. More specifically, given the geometry of a
full, articulated object in a given pose, as well as a partial scan of the same
object in a different pose, we address the problem of matching the part to the
whole while simultaneously reconstructing the new pose from its partial
observation. Our approach is data-driven, and takes the form of a Siamese
autoencoder without the requirement of a consistent vertex labeling at
inference time; as such, it can be used on unorganized point clouds as well as
on triangle meshes. We demonstrate the practical effectiveness of our model in
the applications of single-view deformable shape completion and dense shape
correspondence, both on synthetic and real-world geometric data, where we
outperform prior work on these tasks by a large margin.","I.4.5,cs.CG,cs.CV,cs.LG"
"Mapping of Sparse 3D Data using Alternating Projection. We propose a novel technique to register sparse 3D scans in the absence of
texture. While existing methods such as KinectFusion or Iterative Closest
Points (ICP) heavily rely on dense point clouds, this task is particularly
challenging under sparse conditions without RGB data. Sparse texture-less data
does not come with high-quality boundary signal, and this prohibits the use of
correspondences from corners, junctions, or boundary lines. Moreover, in the
case of sparse data, it is incorrect to assume that the same point will be
captured in two consecutive scans. We take a different approach and first
re-parameterize the point-cloud using a large number of line segments. In this
re-parameterized data, there exists a large number of line intersection (and
not correspondence) constraints that allow us to solve the registration task.
We propose the use of a two-step alternating projection algorithm by
formulating the registration as the simultaneous satisfaction of intersection
and rigidity constraints. The proposed approach outperforms other top-scoring
algorithms on both Kinect and LiDAR datasets. In Kinect, we can use 100X
downsampled sparse data and still outperform competing methods operating on
full-resolution data.","cs.CV,cs.RO"
"Semi-supervised Sparse Representation with Graph Regularization for Image Classification. Image classification is a challenging problem for computer in reality. Large
numbers of methods can achieve satisfying performances with sufficient labeled
images. However, labeled images are still highly limited for certain image
classification tasks. Instead, lots of unlabeled images are available and easy
to be obtained. Therefore, making full use of the available unlabeled data can
be a potential way to further improve the performance of current image
classification methods. In this paper, we propose a discriminative
semi-supervised sparse representation algorithm for image classification. In
the algorithm, the classification process is combined with the sparse coding to
learn a data-driven linear classifier. To obtain discriminative predictions,
the predicted labels are regularized with three graphs, i.e., the global
manifold structure graph, the within-class graph and the between-classes graph.
The constructed graphs are able to extract structure information included in
both the labeled and unlabeled data. Moreover, the proposed method is extended
to a kernel version for dealing with data that cannot be linearly classified.
Accordingly, efficient algorithms are developed to solve the corresponding
optimization problems. Experimental results on several challenging databases
demonstrate that the proposed algorithm achieves excellent performances
compared with related popular methods.","cs.CV,cs.LG,math.OC"
"Multi-agent Reinforcement Learning Accelerated MCMC on Multiscale Inversion Problem. In this work, we propose a multi-agent actor-critic reinforcement learning
(RL) algorithm to accelerate the multi-level Monte Carlo Markov Chain (MCMC)
sampling algorithms. The policies (actors) of the agents are used to generate
the proposal in the MCMC steps; and the critic, which is centralized, is in
charge of estimating the long term reward. We verify our proposed algorithm by
solving an inverse problem with multiple scales. There are several difficulties
in the implementation of this problem by using traditional MCMC sampling.
Firstly, the computation of the posterior distribution involves evaluating the
forward solver, which is very time consuming for a problem with heterogeneous.
We hence propose to use the multi-level algorithm. More precisely, we use the
generalized multiscale finite element method (GMsFEM) as the forward solver in
evaluating a posterior distribution in the multi-level rejection procedure.
Secondly, it is hard to find a function which can generate samplings which are
meaningful. To solve this issue, we learn an RL policy as the proposal
generator. Our experiments show that the proposed method significantly improves
the sampling process","cs.LG,cs.NA,math.NA"
"Subgroup Generalization and Fairness of Graph Neural Networks. Despite enormous successful applications of graph neural networks (GNNs)
recently, theoretical understandings of their generalization ability,
especially for node-level tasks where data are not independent and
identically-distributed (IID), have been sparse. The theoretical investigation
of the generalization performance is beneficial for understanding fundamental
issues (such as fairness) of GNN models and designing better learning methods.
In this paper, we present a novel PAC-Bayesian analysis for GNNs under a
non-IID semi-supervised learning setup. Moreover, we analyze the generalization
performances on different subgroups of unlabeled nodes, which allows us to
further study an accuracy-(dis)parity-style (un)fairness of GNNs from a
theoretical perspective. Under reasonable assumptions, we demonstrate that the
distance between a test subgroup and the training set can be a key factor
affecting the GNN performance on that subgroup, which calls special attention
to the training node selection for fair learning. Experiments across multiple
GNN models and datasets support our theoretical results.","cs.AI,cs.LG"
"False Detection (Positives and Negatives) in Object Detection. Object detection is a very important function of visual perception systems.
Since the early days of classical object detection based on HOG to modern deep
learning based detectors, object detection has improved in accuracy. Two stage
detectors usually have higher accuracy than single stage ones. Both types of
detectors use some form of quantization of the search space of rectangular
regions of image. There are far more of the quantized elements than true
objects. The way these bounding boxes are filtered out possibly results in the
false positive and false negatives. This empirical experimental study explores
ways of reducing false positives and negatives with labelled data.. In the
process also discovered insufficient labelling in Openimage 2019 Object
Detection dataset.","cs.CV,cs.LG,cs.NE"
"Anomaly Detection for Network Connection Logs. We leverage a streaming architecture based on ELK, Spark and Hadoop in order
to collect, store, and analyse database connection logs in near real-time. The
proposed system investigates outliers using unsupervised learning; widely
adopted clustering and classification algorithms for log data, highlighting the
subtle variances in each model by visualisation of outliers. Arriving at a
novel solution to evaluate untagged, unfiltered connection logs, we propose an
approach that can be extrapolated to a generalised system of analysing
connection logs across a large infrastructure comprising thousands of
individual nodes and generating hundreds of lines in logs per second.","cs.DC,cs.LG,stat.ML"
"Deep Reinforcement Learning for Time Scheduling in RF-Powered Backscatter Cognitive Radio Networks. In an RF-powered backscatter cognitive radio network, multiple secondary
users communicate with a secondary gateway by backscattering or harvesting
energy and actively transmitting their data depending on the primary channel
state. To coordinate the transmission of multiple secondary transmitters, the
secondary gateway needs to schedule the backscattering time, energy harvesting
time, and transmission time among them. However, under the dynamics of the
primary channel and the uncertainty of the energy state of the secondary
transmitters, it is challenging for the gateway to find a time scheduling
mechanism which maximizes the total throughput. In this paper, we propose to
use the deep reinforcement learning algorithm to derive an optimal time
scheduling policy for the gateway. Specifically, to deal with the problem with
large state and action spaces, we adopt a Double Deep-Q Network (DDQN) that
enables the gateway to learn the optimal policy. The simulation results clearly
show that the proposed deep reinforcement learning algorithm outperforms
non-learning schemes in terms of network throughput.","cs.GT,cs.LG,cs.NI,eess.SP"
"Interpreting recurrent neural networks behaviour via excitable network attractors. Introduction: Machine learning provides fundamental tools both for scientific
research and for the development of technologies with significant impact on
society. It provides methods that facilitate the discovery of regularities in
data and that give predictions without explicit knowledge of the rules
governing a system. However, a price is paid for exploiting such flexibility:
machine learning methods are typically black-boxes where it is difficult to
fully understand what the machine is doing or how it is operating. This poses
constraints on the applicability and explainability of such methods. Methods:
Our research aims to open the black-box of recurrent neural networks, an
important family of neural networks used for processing sequential data. We
propose a novel methodology that provides a mechanistic interpretation of
behaviour when solving a computational task. Our methodology uses mathematical
constructs called excitable network attractors, which are invariant sets in
phase space composed of stable attractors and excitable connections between
them. Results and Discussion: As the behaviour of recurrent neural networks
depends both on training and on inputs to the system, we introduce an algorithm
to extract network attractors directly from the trajectory of a neural network
while solving tasks. Simulations conducted on a controlled benchmark task
confirm the relevance of these attractors for interpreting the behaviour of
recurrent neural networks, at least for tasks that involve learning a finite
number of stable states and transitions between them.","cs.LG,math.DS,stat.ML"
"Using Collective Intelligence to Route Internet Traffic. A COllective INtelligence (COIN) is a set of interacting reinforcement
learning (RL) algorithms designed in an automated fashion so that their
collective behavior optimizes a global utility function. We summarize the
theory of COINs, then present experiments using that theory to design COINs to
control internet traffic routing. These experiments indicate that COINs
outperform all previously investigated RL-based, shortest path routing
algorithms.","I.2.6; I.2.11,adap-org,cond-mat.stat-mech,cs.DC,cs.LG,cs.NI,nlin.AO"
"Convolutional Drift Networks for Video Classification. Analyzing spatio-temporal data like video is a challenging task that requires
processing visual and temporal information effectively. Convolutional Neural
Networks have shown promise as baseline fixed feature extractors through
transfer learning, a technique that helps minimize the training cost on visual
information. Temporal information is often handled using hand-crafted features
or Recurrent Neural Networks, but this can be overly specific or prohibitively
complex. Building a fully trainable system that can efficiently analyze
spatio-temporal data without hand-crafted features or complex training is an
open challenge. We present a new neural network architecture to address this
challenge, the Convolutional Drift Network (CDN). Our CDN architecture combines
the visual feature extraction power of deep Convolutional Neural Networks with
the intrinsically efficient temporal processing provided by Reservoir
Computing. In this introductory paper on the CDN, we provide a very simple
baseline implementation tested on two egocentric (first-person) video activity
datasets.We achieve video-level activity classification results on-par with
state-of-the art methods. Notably, performance on this complex spatio-temporal
task was produced by only training a single feed-forward layer in the CDN.","cs.CV,cs.NE,eess.IV"
"Full-Glow: Fully conditional Glow for more realistic image generation. Autonomous agents, such as driverless cars, require large amounts of labeled
visual data for their training. A viable approach for acquiring such data is
training a generative model with collected real data, and then augmenting the
collected real dataset with synthetic images from the model, generated with
control of the scene layout and ground truth labeling. In this paper we propose
Full-Glow, a fully conditional Glow-based architecture for generating plausible
and realistic images of novel street scenes given a semantic segmentation map
indicating the scene layout. Benchmark comparisons show our model to outperform
recent works in terms of the semantic segmentation performance of a pretrained
PSPNet. This indicates that images from our model are, to a higher degree than
from other models, similar to real images of the same kinds of scenes and
objects, making them suitable as training data for a visual semantic
segmentation or object recognition system.","68T07,I.4.0; I.2.9; I.2.6; G.3; I.3.3,cs.CV,cs.LG"
"Predicting Visual Importance Across Graphic Design Types. This paper introduces a Unified Model of Saliency and Importance (UMSI),
which learns to predict visual importance in input graphic designs, and
saliency in natural images, along with a new dataset and applications. Previous
methods for predicting saliency or visual importance are trained individually
on specialized datasets, making them limited in application and leading to poor
generalization on novel image classes, while requiring a user to know which
model to apply to which input. UMSI is a deep learning-based model
simultaneously trained on images from different design classes, including
posters, infographics, mobile UIs, as well as natural images, and includes an
automatic classification module to classify the input. This allows the model to
work more effectively without requiring a user to label the input. We also
introduce Imp1k, a new dataset of designs annotated with importance
information. We demonstrate two new design interfaces that use importance
prediction, including a tool for adjusting the relative importance of design
elements, and a tool for reflowing designs to new aspect ratios while
preserving visual importance. The model, code, and importance dataset are
available at https://predimportance.mit.edu .","cs.CV,cs.GR,cs.HC,eess.IV"
"A Kernel for Multi-Parameter Persistent Homology. Topological data analysis and its main method, persistent homology, provide a
toolkit for computing topological information of high-dimensional and noisy
data sets. Kernels for one-parameter persistent homology have been established
to connect persistent homology with machine learning techniques. We contribute
a kernel construction for multi-parameter persistence by integrating a
one-parameter kernel weighted along straight lines. We prove that our kernel is
stable and efficiently computable, which establishes a theoretical connection
between topological data analysis and machine learning for multivariate data
analysis.","55N99,68T10,cs.CG,cs.LG,math.AT,stat.ML"
"Predicting Depth from Semantic Segmentation using Game Engine Dataset. Depth perception is fundamental for robots to understand the surrounding
environment. As the view of cognitive neuroscience, visual depth perception
methods are divided into three categories, namely binocular, active, and
pictorial. The first two categories have been studied for decades in detail.
However, research for the exploration of the third category is still in its
infancy and has got momentum by the advent of deep learning methods in recent
years. In cognitive neuroscience, it is known that pictorial depth perception
mechanisms are dependent on the perception of seen objects. Inspired by this
fact, in this thesis, we investigated the relation of perception of objects and
depth estimation convolutional neural networks. For this purpose, we developed
new network structures based on a simple depth estimation network that only
used a single image at its input. Our proposed structures use both an image and
a semantic label of the image as their input. We used semantic labels as the
output of object perception. The obtained results of performance comparison
between the developed network and original network showed that our novel
structures can improve the performance of depth estimation by 52\% of relative
error of distance in the examined cases. Most of the experimental studies were
carried out on synthetic datasets that were generated by game engines to
isolate the performance comparison from the effect of inaccurate depth and
semantic labels of non-synthetic datasets. It is shown that particular
synthetic datasets may be used for training of depth networks in cases that an
appropriate dataset is not available. Furthermore, we showed that in these
cases, usage of semantic labels improves the robustness of the network against
domain shift from synthetic training data to non-synthetic test data.","68T45,I.4.6; I.4.8,cs.CV,cs.LG,cs.RO"
"Graph Neural Network Architecture Search for Molecular Property Prediction. Predicting the properties of a molecule from its structure is a challenging
task. Recently, deep learning methods have improved the state of the art for
this task because of their ability to learn useful features from the given
data. By treating molecule structure as graphs, where atoms and bonds are
modeled as nodes and edges, graph neural networks (GNNs) have been widely used
to predict molecular properties. However, the design and development of GNNs
for a given data set rely on labor-intensive design and tuning of the network
architectures. Neural architecture search (NAS) is a promising approach to
discover high-performing neural network architectures automatically. To that
end, we develop an NAS approach to automate the design and development of GNNs
for molecular property prediction. Specifically, we focus on automated
development of message-passing neural networks (MPNNs) to predict the molecular
properties of small molecules in quantum mechanics and physical chemistry data
sets from the MoleculeNet benchmark. We demonstrate the superiority of the
automatically discovered MPNNs by comparing them with manually designed GNNs
from the MoleculeNet benchmark. We study the relative importance of the choices
in the MPNN search space, demonstrating that customizing the architecture is
critical to enhancing performance in molecular property prediction and that the
proposed approach can perform customization automatically with minimal manual
effort.","cs.LG,q-bio.BM,stat.ML"
"Skew-Fit: State-Covering Self-Supervised Reinforcement Learning. Autonomous agents that must exhibit flexible and broad capabilities will need
to be equipped with large repertoires of skills. Defining each skill with a
manually-designed reward function limits this repertoire and imposes a manual
engineering burden. Self-supervised agents that set their own goals can
automate this process, but designing appropriate goal setting objectives can be
difficult, and often involves heuristic design decisions. In this paper, we
propose a formal exploration objective for goal-reaching policies that
maximizes state coverage. We show that this objective is equivalent to
maximizing goal reaching performance together with the entropy of the goal
distribution, where goals correspond to full state observations. To instantiate
this principle, we present an algorithm called Skew-Fit for learning a
maximum-entropy goal distributions. We prove that, under regularity conditions,
Skew-Fit converges to a uniform distribution over the set of valid states, even
when we do not know this set beforehand. Our experiments show that combining
Skew-Fit for learning goal distributions with existing goal-reaching methods
outperforms a variety of prior methods on open-sourced visual goal-reaching
tasks. Moreover, we demonstrate that Skew-Fit enables a real-world robot to
learn to open a door, entirely from scratch, from pixels, and without any
manually-designed reward function.","cs.AI,cs.LG,cs.RO,stat.ML"
"PAC-learning bounded tree-width Graphical Models. We show that the class of strongly connected graphical models with treewidth
at most k can be properly efficiently PAC-learnt with respect to the
Kullback-Leibler Divergence. Previous approaches to this problem, such as those
of Chow ([1]), and Ho gen ([7]) have shown that this class is PAC-learnable by
reducing it to a combinatorial optimization problem. However, for k > 1, this
problem is NP-complete ([15]), and so unless P=NP, these approaches will take
exponential amounts of time. Our approach differs significantly from these, in
that it first attempts to find approximate conditional independencies by
solving (polynomially many) submodular optimization problems, and then using a
dynamic programming formulation to combine the approximate conditional
independence information to derive a graphical model with underlying graph of
the tree-width specified. This gives us an efficient (polynomial time in the
number of random variables) PAC-learning algorithm which requires only
polynomial number of samples of the true distribution, and only polynomial
running time.","cs.DS,cs.LG,stat.ML"
"Learning Mobile CNN Feature Extraction Toward Fast Computation of Visual Object Tracking. In this paper, we construct a lightweight, high-precision and high-speed
object tracking using a trained CNN. Conventional methods with trained CNNs use
VGG16 network which requires powerful computational resources. Therefore, there
is a problem that it is difficult to apply in low computation resources
environments. To solve this problem, we use MobileNetV3, which is a CNN for
mobile terminals.Based on Feature Map Selection Tracking, we propose a new
architecture that extracts effective features of MobileNet for object tracking.
The architecture requires no online learning but only offline learning. In
addition, by using features of objects other than tracking target, the features
of tracking target are extracted more efficiently. We measure the tracking
accuracy with Visual Tracker Benchmark and confirm that the proposed method can
perform high-precision and high-speed calculation even in low computation
resource environments.","68T05,68T07,68T10,cs.CV,cs.GR,cs.LG"
"SQL for SRL: Structure Learning Inside a Database System. The position we advocate in this paper is that relational algebra can provide
a unified language for both representing and computing with
statistical-relational objects, much as linear algebra does for traditional
single-table machine learning. Relational algebra is implemented in the
Structured Query Language (SQL), which is the basis of relational database
management systems. To support our position, we have developed the FACTORBASE
system, which uses SQL as a high-level scripting language for
statistical-relational learning of a graphical model structure. The design
philosophy of FACTORBASE is to manage statistical models as first-class
citizens inside a database. Our implementation shows how our SQL constructs in
FACTORBASE facilitate fast, modular, and reliable program development.
Empirical evidence from six benchmark databases indicates that leveraging
database system capabilities achieves scalable model structure learning.","H.2.8; H.2.4,cs.DB,cs.LG"
"Boilerplate Removal using a Neural Sequence Labeling Model. The extraction of main content from web pages is an important task for
numerous applications, ranging from usability aspects, like reader views for
news articles in web browsers, to information retrieval or natural language
processing. Existing approaches are lacking as they rely on large amounts of
hand-crafted features for classification. This results in models that are
tailored to a specific distribution of web pages, e.g. from a certain time
frame, but lack in generalization power. We propose a neural sequence labeling
model that does not rely on any hand-crafted features but takes only the HTML
tags and words that appear in a web page as input. This allows us to present a
browser extension which highlights the content of arbitrary web pages directly
within the browser using our model. In addition, we create a new, more current
dataset to show that our model is able to adapt to changes in the structure of
web pages and outperform the state-of-the-art model.","cs.IR,cs.LG,stat.ML"
"Learning Restricted Boltzmann Machines with Sparse Latent Variables. Restricted Boltzmann Machines (RBMs) are a common family of undirected
graphical models with latent variables. An RBM is described by a bipartite
graph, with all observed variables in one layer and all latent variables in the
other. We consider the task of learning an RBM given samples generated
according to it. The best algorithms for this task currently have time
complexity $\tilde{O}(n^2)$ for ferromagnetic RBMs (i.e., with attractive
potentials) but $\tilde{O}(n^d)$ for general RBMs, where $n$ is the number of
observed variables and $d$ is the maximum degree of a latent variable. Let the
MRF neighborhood of an observed variable be its neighborhood in the Markov
Random Field of the marginal distribution of the observed variables. In this
paper, we give an algorithm for learning general RBMs with time complexity
$\tilde{O}(n^{2^s+1})$, where $s$ is the maximum number of latent variables
connected to the MRF neighborhood of an observed variable. This is an
improvement when $s < \log_2 (d-1)$, which corresponds to RBMs with sparse
latent variables. Furthermore, we give a version of this learning algorithm
that recovers a model with small prediction error and whose sample complexity
is independent of the minimum potential in the Markov Random Field of the
observed variables. This is of interest because the sample complexity of
current algorithms scales with the inverse of the minimum potential, which
cannot be controlled in terms of natural properties of the RBM.","cs.DS,cs.IT,cs.LG,math.IT,stat.ML"
"Chan-Vese Reformulation for Selective Image Segmentation. Selective segmentation involves incorporating user input to partition an
image into foreground and background, by discriminating between objects of a
similar type. Typically, such methods involve introducing additional
constraints to generic segmentation approaches. However, we show that this is
often inconsistent with respect to common assumptions about the image. The
proposed method introduces a new fitting term that is more useful in practice
than the Chan-Vese framework. In particular, the idea is to define a term that
allows for the background to consist of multiple regions of inhomogeneity. We
provide comparitive experimental results to alternative approaches to
demonstrate the advantages of the proposed method, broadening the possible
application of these methods.","cs.CV,cs.NA,math.NA"
"Stochastic Gradient MCMC for Nonlinear State Space Models. State space models (SSMs) provide a flexible framework for modeling complex
time series via a latent stochastic process. Inference for nonlinear,
non-Gaussian SSMs is often tackled with particle methods that do not scale well
to long time series. The challenge is two-fold: not only do computations scale
linearly with time, as in the linear case, but particle filters additionally
suffer from increasing particle degeneracy with longer series. Stochastic
gradient MCMC methods have been developed to scale inference for hidden Markov
models (HMMs) and linear SSMs using buffered stochastic gradient estimates to
account for temporal dependencies. We extend these stochastic gradient
estimators to nonlinear SSMs using particle methods. We present error bounds
that account for both buffering error and particle error in the case of
nonlinear SSMs that are log-concave in the latent process. We evaluate our
proposed particle buffered stochastic gradient using SGMCMC for inference on
both long sequential synthetic and minute-resolution financial returns data,
demonstrating the importance of this class of methods.","cs.LG,stat.CO,stat.ML"
"Deep Q-Learning for Nash Equilibria: Nash-DQN. Model-free learning for multi-agent stochastic games is an active area of
research. Existing reinforcement learning algorithms, however, are often
restricted to zero-sum games, and are applicable only in small state-action
spaces or other simplified settings. Here, we develop a new data efficient
Deep-Q-learning methodology for model-free learning of Nash equilibria for
general-sum stochastic games. The algorithm uses a local linear-quadratic
expansion of the stochastic game, which leads to analytically solvable optimal
actions. The expansion is parametrized by deep neural networks to give it
sufficient flexibility to learn the environment without the need to experience
all state-action pairs. We study symmetry properties of the algorithm stemming
from label-invariant stochastic games and as a proof of concept, apply our
algorithm to learning optimal trading strategies in competitive electronic
markets.","cs.GT,cs.LG,q-fin.CP,stat.ML"
"Self-Supervised Learning of Generative Spin-Glasses with Normalizing Flows. Spin-glasses are universal models that can capture complex behavior of
many-body systems at the interface of statistical physics and computer science
including discrete optimization, inference in graphical models, and automated
reasoning. Computing the underlying structure and dynamics of such complex
systems is extremely difficult due to the combinatorial explosion of their
state space. Here, we develop deep generative continuous spin-glass
distributions with normalizing flows to model correlations in generic discrete
problems. We use a self-supervised learning paradigm by automatically
generating the data from the spin-glass itself. We demonstrate that key
physical and computational properties of the spin-glass phase can be
successfully learned, including multi-modal steady-state distributions and
topological structures among metastable states. Remarkably, we observe that the
learning itself corresponds to a spin-glass phase transition within the layers
of the trained normalizing flows. The inverse normalizing flows learns to
perform reversible multi-scale coarse-graining operations which are very
different from the typical irreversible renormalization group techniques.","cond-mat.dis-nn,cs.LG,quant-ph,stat.ML"
"Federated Graph Learning -- A Position Paper. Graph neural networks (GNN) have been successful in many fields, and derived
various researches and applications in real industries. However, in some
privacy sensitive scenarios (like finance, healthcare), training a GNN model
centrally faces challenges due to the distributed data silos. Federated
learning (FL) is a an emerging technique that can collaboratively train a
shared model while keeping the data decentralized, which is a rational solution
for distributed GNN training. We term it as federated graph learning (FGL).
Although FGL has received increasing attention recently, the definition and
challenges of FGL is still up in the air. In this position paper, we present a
categorization to clarify it. Considering how graph data are distributed among
clients, we propose four types of FGL: inter-graph FL, intra-graph FL and
graph-structured FL, where intra-graph is further divided into horizontal and
vertical FGL. For each type of FGL, we make a detailed discussion about the
formulation and applications, and propose some potential challenges.","cs.DC,cs.LG,cs.NI"
"Simultaneously Learning Vision and Feature-based Control Policies for Real-world Ball-in-a-Cup. We present a method for fast training of vision based control policies on
real robots. The key idea behind our method is to perform multi-task
Reinforcement Learning with auxiliary tasks that differ not only in the reward
to be optimized but also in the state-space in which they operate. In
particular, we allow auxiliary task policies to utilize task features that are
available only at training-time. This allows for fast learning of auxiliary
policies, which subsequently generate good data for training the main,
vision-based control policies. This method can be seen as an extension of the
Scheduled Auxiliary Control (SAC-X) framework. We demonstrate the efficacy of
our method by using both a simulated and real-world Ball-in-a-Cup game
controlled by a robot arm. In simulation, our approach leads to significant
learning speed-ups when compared to standard SAC-X. On the real robot we show
that the task can be learned from-scratch, i.e., with no transfer from
simulation and no imitation learning. Videos of our learned policies running on
the real robot can be found at
https://sites.google.com/view/rss-2019-sawyer-bic/.","cs.LG,cs.RO,stat.ML"
"Learning Inner-Group Relations on Point Clouds. The prevalence of relation networks in computer vision is in stark contrast
to underexplored point-based methods. In this paper, we explore the
possibilities of local relation operators and survey their feasibility. We
propose a scalable and efficient module, called group relation aggregator. The
module computes a feature of a group based on the aggregation of the features
of the inner-group points weighted by geometric relations and semantic
relations. We adopt this module to design our RPNet. We further verify the
expandability of RPNet, in terms of both depth and width, on the tasks of
classification and segmentation. Surprisingly, empirical results show that
wider RPNet fits for classification, while deeper RPNet works better on
segmentation. RPNet achieves state-of-the-art for classification and
segmentation on challenging benchmarks. We also compare our local aggregator
with PointNet++, with around 30% parameters and 50% computation saving.
Finally, we conduct experiments to reveal the robustness of RPNet with regard
to rigid transformation and noises.","cs.AI,cs.CV,cs.GR,cs.LG,cs.RO"
"Study of Efficient Technique Based On 2D Tsallis Entropy For Image Thresholding. Thresholding is an important task in image processing. It is a main tool in
pattern recognition, image segmentation, edge detection and scene analysis. In
this paper, we present a new thresholding technique based on two-dimensional
Tsallis entropy. The two-dimensional Tsallis entropy was obtained from the
twodimensional histogram which was determined by using the gray value of the
pixels and the local average gray value of the pixels, the work it was applied
a generalized entropy formalism that represents a recent development in
statistical mechanics. The effectiveness of the proposed method is demonstrated
by using examples from the real-world and synthetic images. The performance
evaluation of the proposed technique in terms of the quality of the thresholded
images are presented. Experimental results demonstrate that the proposed method
achieve better result than the Shannon method.","68U10,cs.CV"
"Monte Carlo Gradient Estimation in Machine Learning. This paper is a broad and accessible survey of the methods we have at our
disposal for Monte Carlo gradient estimation in machine learning and across the
statistical sciences: the problem of computing the gradient of an expectation
of a function with respect to parameters defining the distribution that is
integrated; the problem of sensitivity analysis. In machine learning research,
this gradient problem lies at the core of many learning problems, in
supervised, unsupervised and reinforcement learning. We will generally seek to
rewrite such gradients in a form that allows for Monte Carlo estimation,
allowing them to be easily and efficiently used and analysed. We explore three
strategies--the pathwise, score function, and measure-valued gradient
estimators--exploring their historical development, derivation, and underlying
assumptions. We describe their use in other fields, show how they are related
and can be combined, and expand on their possible generalisations. Wherever
Monte Carlo gradient estimators have been derived and deployed in the past,
important advances have followed. A deeper and more widely-held understanding
of this problem will lead to further advances, and it is these advances that we
wish to support.","cs.LG,math.OC,stat.ML"
"Composition of kernel and acquisition functions for High Dimensional Bayesian Optimization. Bayesian Optimization has become the reference method for the global
optimization of black box, expensive and possibly noisy functions. Bayesian
Op-timization learns a probabilistic model about the objective function,
usually a Gaussian Process, and builds, depending on its mean and variance, an
acquisition function whose optimizer yields the new evaluation point, leading
to update the probabilistic surrogate model. Despite its sample efficiency,
Bayesian Optimiza-tion does not scale well with the dimensions of the problem.
The optimization of the acquisition function has received less attention
because its computational cost is usually considered negligible compared to
that of the evaluation of the objec-tive function. Its efficient optimization
is often inhibited, particularly in high di-mensional problems, by multiple
extrema. In this paper we leverage the addition-ality of the objective function
into mapping both the kernel and the acquisition function of the Bayesian
Optimization in lower dimensional subspaces. This ap-proach makes more
efficient the learning/updating of the probabilistic surrogate model and allows
an efficient optimization of the acquisition function. Experi-mental results
are presented for real-life application, that is the control of pumps in urban
water distribution systems.","cs.LG,math.OC,stat.ML"
"Effects of Sampling Methods on Prediction Quality. The Case of Classifying Land Cover Using Decision Trees. Clever sampling methods can be used to improve the handling of big data and
increase its usefulness. The subject of this study is remote sensing,
specifically airborne laser scanning point clouds representing different
classes of ground cover. The aim is to derive a supervised learning model for
the classification using CARTs. In order to measure the effect of different
sampling methods on the classification accuracy, various experiments with
varying types of sampling methods, sample sizes, and accuracy metrics have been
designed. Numerical results for a subset of a large surveying project covering
the lower Rhine area in Germany are shown. General conclusions regarding
sampling design are drawn and presented.","cs.LG,stat.AP,stat.ML"
"NormalGAN: Learning Detailed 3D Human from a Single RGB-D Image. We propose NormalGAN, a fast adversarial learning-based method to reconstruct
the complete and detailed 3D human from a single RGB-D image. Given a single
front-view RGB-D image, NormalGAN performs two steps: front-view RGB-D
rectification and back-view RGBD inference. The final model was then generated
by simply combining the front-view and back-view RGB-D information. However,
inferring backview RGB-D image with high-quality geometric details and
plausible texture is not trivial. Our key observation is: Normal maps generally
encode much more information of 3D surface details than RGB and depth images.
Therefore, learning geometric details from normal maps is superior than other
representations. In NormalGAN, an adversarial learning framework conditioned by
normal maps is introduced, which is used to not only improve the front-view
depth denoising performance, but also infer the back-view depth image with
surprisingly geometric details. Moreover, for texture recovery, we remove
shading information from the front-view RGB image based on the refined normal
map, which further improves the quality of the back-view color inference.
Results and experiments on both testing data set and real captured data
demonstrate the superior performance of our approach. Given a consumer RGB-D
sensor, NormalGAN can generate the complete and detailed 3D human
reconstruction results in 20 fps, which further enables convenient interactive
experiences in telepresence, AR/VR and gaming scenarios.","68T07,I.4.5,cs.CV"
"Delving Deeper into the Decoder for Video Captioning. Video captioning is an advanced multi-modal task which aims to describe a
video clip using a natural language sentence. The encoder-decoder framework is
the most popular paradigm for this task in recent years. However, there exist
some problems in the decoder of a video captioning model. We make a thorough
investigation into the decoder and adopt three techniques to improve the
performance of the model. First of all, a combination of variational dropout
and layer normalization is embedded into a recurrent unit to alleviate the
problem of overfitting. Secondly, a new online method is proposed to evaluate
the performance of a model on a validation set so as to select the best
checkpoint for testing. Finally, a new training strategy called professional
learning is proposed which uses the strengths of a captioning model and
bypasses its weaknesses. It is demonstrated in the experiments on Microsoft
Research Video Description Corpus (MSVD) and MSR-Video to Text (MSR-VTT)
datasets that our model has achieved the best results evaluated by BLEU, CIDEr,
METEOR and ROUGE-L metrics with significant gains of up to 18% on MSVD and 3.5%
on MSR-VTT compared with the previous state-of-the-art models.","68T45,68T50,I.2.10; I.2.7,cs.CL,cs.CV"
"HOMRS: High Order Metamorphic Relations Selector for Deep Neural Networks. Deep Neural Networks (DNN) applications are increasingly becoming a part of
our everyday life, from medical applications to autonomous cars. Traditional
validation of DNN relies on accuracy measures, however, the existence of
adversarial examples has highlighted the limitations of these accuracy
measures, raising concerns especially when DNN are integrated into
safety-critical systems. In this paper, we present HOMRS, an approach to boost
metamorphic testing by automatically building a small optimized set of high
order metamorphic relations from an initial set of elementary metamorphic
relations. HOMRS' backbone is a multi-objective search; it exploits ideas drawn
from traditional systems testing such as code coverage, test case, and path
diversity. We applied HOMRS to LeNet5 DNN with MNIST dataset and we report
evidence that it builds a small but effective set of high order transformations
achieving a 95% kill ratio. Five raters manually labeled a pool of images
before and after high order transformation; Fleiss' Kappa and statistical tests
confirmed that they are metamorphic properties. HOMRS built-in relations are
also effective to confront adversarial or out-of-distribution examples; HOMRS
detected 92% of randomly sampled out-of-distribution images. HOMRS
transformations are also suitable for online real-time use.","cs.LG,cs.SE"
"Guiding Policies with Language via Meta-Learning. Behavioral skills or policies for autonomous agents are conventionally
learned from reward functions, via reinforcement learning, or from
demonstrations, via imitation learning. However, both modes of task
specification have their disadvantages: reward functions require manual
engineering, while demonstrations require a human expert to be able to actually
perform the task in order to generate the demonstration. Instruction following
from natural language instructions provides an appealing alternative: in the
same way that we can specify goals to other humans simply by speaking or
writing, we would like to be able to specify tasks for our machines. However, a
single instruction may be insufficient to fully communicate our intent or, even
if it is, may be insufficient for an autonomous agent to actually understand
how to perform the desired task. In this work, we propose an interactive
formulation of the task specification problem, where iterative language
corrections are provided to an autonomous agent, guiding it in acquiring the
desired skill. Our proposed language-guided policy learning algorithm can
integrate an instruction and a sequence of corrections to acquire new skills
very quickly. In our experiments, we show that this method can enable a policy
to follow instructions and corrections for simulated navigation and
manipulation tasks, substantially outperforming direct, non-interactive
instruction following.","cs.AI,cs.CL,cs.HC,cs.LG"
"Do Compressed Representations Generalize Better?. One of the most studied problems in machine learning is finding reasonable
constraints that guarantee the generalization of a learning algorithm. These
constraints are usually expressed as some simplicity assumptions on the target.
For instance, in the Vapnik-Chervonenkis (VC) theory the space of possible
hypotheses is considered to have a limited VC dimension. In this paper, the
constraint on the entropy $H(X)$ of the input variable $X$ is studied as a
simplicity assumption. It is proven that the sample complexity to achieve an
$\epsilon$-$\delta$ Probably Approximately Correct (PAC) hypothesis is bounded
by $\frac{2^{
\left.6H(X)\middle/\epsilon\right.}+\log{\frac{1}{\delta}}}{\epsilon^2}$ which
is sharp up to the $\frac{1}{\epsilon^2}$ factor. Morever, it is shown that if
a feature learning process is employed to learn the compressed representation
from the dataset, this bound no longer exists. These findings have important
implications on the Information Bottleneck (IB) theory which had been utilized
to explain the generalization power of Deep Neural Networks (DNNs), but its
applicability for this purpose is currently under debate by researchers. In
particular, this is a rigorous proof for the previous heuristic that compressed
representations are exponentially easier to be learned. However, our analysis
pinpoints two factors preventing the IB, in its current form, to be applicable
in studying neural networks. Firstly, the exponential dependence of sample
complexity on $\frac{1}{\epsilon}$, which can lead to a dramatic effect on the
bounds in practical applications when $\epsilon$ is small. Secondly, our
analysis reveals that arguments based on input compression are inherently
insufficient to explain generalization of methods like DNNs in which the
features are also learned using available data.","cs.AI,cs.IT,cs.LG,math.IT,stat.ML"
"FedScale: Benchmarking Model and System Performance of Federated Learning. We present FedScale, a diverse set of challenging and realistic benchmark
datasets to facilitate scalable, comprehensive, and reproducible federated
learning (FL) research. FedScale datasets are large-scale, encompassing a
diverse range of important FL tasks, such as image classification, object
detection, language modeling, speech recognition, and reinforcement learning.
For each dataset, we provide a unified evaluation protocol using realistic data
splits and evaluation metrics. To meet the pressing need for reproducing
realistic FL at scale, we have also built an efficient evaluation platform to
simplify and standardize the process of FL experimental setup and model
evaluation. Our evaluation platform provides flexible APIs to implement new FL
algorithms and includes new execution backends with minimal developer efforts.
Finally, we perform indepth benchmark experiments on these datasets. Our
experiments suggest fruitful opportunities in heterogeneity-aware
co-optimizations of the system and statistical efficiency under realistic FL
characteristics. FedScale is open-source with permissive licenses and actively
maintained,1 and we welcome feedback and contributions from the community.","cs.AI,cs.DC,cs.LG,cs.PF"
"SleepTransformer: Automatic Sleep Staging with Interpretability and Uncertainty Quantification. Black-box skepticism is one of the main hindrances impeding
deep-learning-based automatic sleep scoring from being used in clinical
environments. Towards interpretability, this work proposes a
sequence-to-sequence sleep-staging model, namely SleepTransformer. It is based
on the transformer backbone whose self-attention scores offer interpretability
of the model's decisions at both the epoch and sequence level. At the epoch
level, the attention scores can be encoded as a heat map to highlight
sleep-relevant features captured from the input EEG signal. At the sequence
level, the attention scores are visualized as the influence of different
neighboring epochs in an input sequence (i.e. the context) to recognition of a
target epoch, mimicking the way manual scoring is done by human experts. We
further propose a simple yet efficient method to quantify uncertainty in the
model's decisions. The method, which is based on entropy, can serve as a metric
for deferring low-confidence epochs to a human expert for further inspection.
Additionally, we demonstrate that the proposed SleepTransformer outperforms
existing methods at a lower computational cost and achieves state-of-the-art
performance on two experimental databases of different sizes.","cs.LG,eess.SP"
"Hierarchical Protein Function Prediction with Tail-GNNs. Protein function prediction may be framed as predicting subgraphs (with
certain closure properties) of a directed acyclic graph describing the
hierarchy of protein functions. Graph neural networks (GNNs), with their
built-in inductive bias for relational data, are hence naturally suited for
this task. However, in contrast with most GNN applications, the graph is not
related to the input, but to the label space. Accordingly, we propose
Tail-GNNs, neural networks which naturally compose with the output space of any
neural network for multi-task prediction, to provide relationally-reinforced
labels. For protein function prediction, we combine a Tail-GNN with a dilated
convolutional network which learns representations of the protein sequence,
making significant improvement in F_1 score and demonstrating the ability of
Tail-GNNs to learn useful representations of labels and exploit them in
real-world problem solving.","cs.LG,q-bio.BM,stat.ML"
"Image-Text Multi-Modal Representation Learning by Adversarial Backpropagation. We present novel method for image-text multi-modal representation learning.
In our knowledge, this work is the first approach of applying adversarial
learning concept to multi-modal learning and not exploiting image-text pair
information to learn multi-modal feature. We only use category information in
contrast with most previous methods using image-text pair information for
multi-modal embedding. In this paper, we show that multi-modal feature can be
achieved without image-text pair information and our method makes more similar
distribution with image and text in multi-modal feature space than other
methods which use image-text pair information. And we show our multi-modal
feature has universal semantic information, even though it was trained for
category prediction. Our model is end-to-end backpropagation, intuitive and
easily extended to other multi-modal learning work.","cs.CL,cs.CV,cs.LG"
"Federated Learning of Molecular Properties in a Heterogeneous Setting. Chemistry research has both high material and computational costs to conduct
experiments. Institutions thus consider chemical data to be valuable and there
have been few efforts to construct large public datasets for machine learning.
Another challenge is that different intuitions are interested in different
classes of molecules, creating heterogeneous data that cannot be easily joined
by conventional distributed training. In this work, we introduce federated
heterogeneous molecular learning to address these challenges. Federated
learning allows end-users to build a global model collaboratively while
preserving the training data distributed over isolated clients. Due to the lack
of related research, we first simulate a federated heterogeneous benchmark
called FedChem. FedChem is constructed by jointly performing scaffold splitting
and Latent Dirichlet Allocation on existing datasets. Our results on FedChem
show that significant learning challenges arise when working with heterogeneous
molecules. We then propose a method to alleviate the problem, namely Federated
Learning by Instance reweighTing (FLIT). FLIT can align the local training
across heterogeneous clients by improving the performance for uncertain
samples. Comprehensive experiments conducted on our new benchmark FedChem
validate the advantages of this method over other federated learning schemes.
FedChem should enable a new type of collaboration for improving AI in chemistry
that mitigates concerns about valuable chemical data.","cs.LG,physics.chem-ph"
"Toward Quantifying Ambiguities in Artistic Images. It has long been hypothesized that perceptual ambiguities play an important
role in aesthetic experience: a work with some ambiguity engages a viewer more
than one that does not. However, current frameworks for testing this theory are
limited by the availability of stimuli and data collection methods. This paper
presents an approach to measuring the perceptual ambiguity of a collection of
images. Crowdworkers are asked to describe image content, after different
viewing durations. Experiments are performed using images created with
Generative Adversarial Networks, using the Artbreeder website. We show that
text processing of viewer responses can provide a fine-grained way to measure
and describe image ambiguities.","I.4.0; I.3.8,cs.CV,cs.GR,cs.HC"
"Hierarchical internal representation of spectral features in deep convolutional networks trained for EEG decoding. Recently, there is increasing interest and research on the interpretability
of machine learning models, for example how they transform and internally
represent EEG signals in Brain-Computer Interface (BCI) applications. This can
help to understand the limits of the model and how it may be improved, in
addition to possibly provide insight about the data itself. Schirrmeister et
al. (2017) have recently reported promising results for EEG decoding with deep
convolutional neural networks (ConvNets) trained in an end-to-end manner and,
with a causal visualization approach, showed that they learn to use spectral
amplitude changes in the input. In this study, we investigate how ConvNets
represent spectral features through the sequence of intermediate stages of the
network. We show higher sensitivity to EEG phase features at earlier stages and
higher sensitivity to EEG amplitude features at later stages. Intriguingly, we
observed a specialization of individual stages of the network to the classical
EEG frequency bands alpha, beta, and high gamma. Furthermore, we find first
evidence that particularly in the last convolutional layer, the network learns
to detect more complex oscillatory patterns beyond spectral phase and
amplitude, reminiscent of the representation of complex visual features in
later layers of ConvNets in computer vision tasks. Our findings thus provide
insights into how ConvNets hierarchically represent spectral EEG features in
their intermediate layers and suggest that ConvNets can exploit and might help
to better understand the compositional structure of EEG time series.","cs.LG,q-bio.NC,stat.ML"
"Graph Laplacian mixture model. Graph learning methods have recently been receiving increasing interest as
means to infer structure in datasets. Most of the recent approaches focus on
different relationships between a graph and data sample distributions, mostly
in settings where all available data relate to the same graph. This is,
however, not always the case, as data is often available in mixed form,
yielding the need for methods that are able to cope with mixture data and learn
multiple graphs. We propose a novel generative model that represents a
collection of distinct data which naturally live on different graphs. We assume
the mapping of data to graphs is not known and investigate the problem of
jointly clustering a set of data and learning a graph for each of the clusters.
Experiments demonstrate promising performance in data clustering and multiple
graph inference, and show desirable properties in terms of interpretability and
coping with high dimensionality on weather and traffic data, as well as digit
classification.","cs.LG,cs.SI,stat.ML"
"Distributed Power Control for Large Energy Harvesting Networks: A Multi-Agent Deep Reinforcement Learning Approach. In this paper, we develop a multi-agent reinforcement learning (MARL)
framework to obtain online power control policies for a large energy harvesting
(EH) multiple access channel, when only causal information about the EH process
and wireless channel is available. In the proposed framework, we model the
online power control problem as a discrete-time mean-field game (MFG), and
analytically show that the MFG has a unique stationary solution. Next, we
leverage the fictitious play property of the mean-field games, and the deep
reinforcement learning technique to learn the stationary solution of the game,
in a completely distributed fashion. We analytically show that the proposed
procedure converges to the unique stationary solution of the MFG. This, in
turn, ensures that the optimal policies can be learned in a completely
distributed fashion. In order to benchmark the performance of the distributed
policies, we also develop a deep neural network (DNN) based centralized as well
as distributed online power control schemes. Our simulation results show the
efficacy of the proposed power control policies. In particular, the DNN based
centralized power control policies provide a very good performance for large EH
networks for which the design of optimal policies is intractable using the
conventional methods such as Markov decision processes. Further, performance of
both the distributed policies is close to the throughput achieved by the
centralized policies.","cs.AI,cs.IT,cs.LG,math.IT,stat.ML"
"Neural Closure Models for Dynamical Systems. Complex dynamical systems are used for predictions in many domains. Because
of computational costs, models are truncated, coarsened, or aggregated. As the
neglected and unresolved terms become important, the utility of model
predictions diminishes. We develop a novel, versatile, and rigorous methodology
to learn non-Markovian closure parameterizations for known-physics/low-fidelity
models using data from high-fidelity simulations. The new ""neural closure
models"" augment low-fidelity models with neural delay differential equations
(nDDEs), motivated by the Mori-Zwanzig formulation and the inherent delays in
complex dynamical systems. We demonstrate that neural closures efficiently
account for truncated modes in reduced-order-models, capture the effects of
subgrid-scale processes in coarse models, and augment the simplification of
complex biological and physical-biogeochemical models. We find that using
non-Markovian over Markovian closures improves long-term prediction accuracy
and requires smaller networks. We derive adjoint equations and network
architectures needed to efficiently implement the new discrete and distributed
nDDEs, for any time-integration schemes and allowing nonuniformly-spaced
temporal training data. The performance of discrete over distributed delays in
closure models is explained using information theory, and we find an optimal
amount of past information for a specified architecture. Finally, we analyze
computational complexity and explain the limited additional cost due to neural
closure models.","34A99,68T01 (Primary) 37M05,86-08 (Secondary),J.2; I.2.m,cs.LG,math.DS,physics.flu-dyn"
"Bridging the Gap Between Spectral and Spatial Domains in Graph Neural Networks. This paper aims at revisiting Graph Convolutional Neural Networks by bridging
the gap between spectral and spatial design of graph convolutions. We
theoretically demonstrate some equivalence of the graph convolution process
regardless it is designed in the spatial or the spectral domain. The obtained
general framework allows to lead a spectral analysis of the most popular
ConvGNNs, explaining their performance and showing their limits. Moreover, the
proposed framework is used to design new convolutions in spectral domain with a
custom frequency profile while applying them in the spatial domain. We also
propose a generalization of the depthwise separable convolution framework for
graph convolutional networks, what allows to decrease the total number of
trainable parameters by keeping the capacity of the model. To the best of our
knowledge, such a framework has never been used in the GNNs literature. Our
proposals are evaluated on both transductive and inductive graph learning
problems. Obtained results show the relevance of the proposed method and
provide one of the first experimental evidence of transferability of spectral
filter coefficients from one graph to another. Our source codes are publicly
available at: https://github.com/balcilar/Spectral-Designed-Graph-Convolutions","68T05,I.5.2,cs.LG,stat.ML"
"Information-Bottleneck-Based Behavior Representation Learning for Multi-agent Reinforcement learning. In multi-agent deep reinforcement learning, extracting sufficient and compact
information of other agents is critical to attain efficient convergence and
scalability of an algorithm. In canonical frameworks, distilling of such
information is often done in an implicit and uninterpretable manner, or
explicitly with cost functions not able to reflect the relationship between
information compression and utility in representation. In this paper, we
present Information-Bottleneck-based Other agents' behavior Representation
learning for Multi-agent reinforcement learning (IBORM) to explicitly seek
low-dimensional mapping encoder through which a compact and informative
representation relevant to other agents' behaviors is established. IBORM
leverages the information bottleneck principle to compress observation
information, while retaining sufficient information relevant to other agents'
behaviors used for cooperation decision. Empirical results have demonstrated
that IBORM delivers the fastest convergence rate and the best performance of
the learned policies, as compared with implicit behavior representation
learning and explicit behavior representation learning without explicitly
considering information compression and utility.","cs.IT,cs.LG,cs.MA,math.IT"
"The Cyborg Astrobiologist: First Field Experience. We present results from the first geological field tests of the `Cyborg
Astrobiologist', which is a wearable computer and video camcorder system that
we are using to test and train a computer-vision system towards having some of
the autonomous decision-making capabilities of a field-geologist and
field-astrobiologist. The Cyborg Astrobiologist platform has thus far been used
for testing and development of these algorithms and systems: robotic
acquisition of quasi-mosaics of images, real-time image segmentation, and
real-time determination of interesting points in the image mosaics. The
hardware and software systems function reliably, and the computer-vision
algorithms are adequate for the first field tests. In addition to the
proof-of-concept aspect of these field tests, the main result of these field
tests is the enumeration of those issues that we can improve in the future,
including: first, detection and accounting for shadows caused by 3D jagged
edges in the outcrop; second, reincorporation of more sophisticated
texture-analysis algorithms into the system; third, creation of hardware and
software capabilities to control the camera's zoom lens in an intelligent
manner; and fourth, development of algorithms for interpretation of complex
geological scenery. Nonetheless, despite these technical inadequacies, this
Cyborg Astrobiologist system, consisting of a camera-equipped wearable-computer
and its computer-vision algorithms, has demonstrated its ability of finding
genuinely interesting points in real-time in the geological scenery, and then
gathering more information about these interest points in an automated manner.","I.4.8; I.4.6; I.4.0; I.2.9; I.2.10; J.2.; I.5.5; I.5.4; I.4.9,astro-ph,cs.AI,cs.CE,cs.CV,cs.HC,cs.RO,cs.SE,q-bio.NC"
"DREAM: Deep Regret minimization with Advantage baselines and Model-free learning. We introduce DREAM, a deep reinforcement learning algorithm that finds
optimal strategies in imperfect-information games with multiple agents.
Formally, DREAM converges to a Nash Equilibrium in two-player zero-sum games
and to an extensive-form coarse correlated equilibrium in all other games. Our
primary innovation is an effective algorithm that, in contrast to other
regret-based deep learning algorithms, does not require access to a perfect
simulator of the game to achieve good performance. We show that DREAM
empirically achieves state-of-the-art performance among model-free algorithms
in popular benchmark games, and is even competitive with algorithms that do use
a perfect simulator.","cs.GT,cs.LG,stat.ML"
"A New Framework for Retinex based Color Image Enhancement using Particle Swarm Optimization. A new approach for tuning the parameters of MultiScale Retinex (MSR) based
color image enhancement algorithm using a popular optimization method, namely,
Particle Swarm Optimization (PSO) is presented in this paper. The image
enhancement using MSR scheme heavily depends on parameters such as Gaussian
surround space constant, number of scales, gain and offset etc. Selection of
these parameters, empirically and its application to MSR scheme to produce
inevitable results are the major blemishes. The method presented here results
in huge savings of computation time as well as improvement in the visual
quality of an image, since the PSO exploited maximizes the MSR parameters. The
objective of PSO is to validate the visual quality of the enhanced image
iteratively using an effective objective criterion based on entropy and edge
information of an image. The PSO method of parameter optimization of MSR scheme
achieves a very good quality of reconstructed images, far better than that
possible with the other existing methods. Finally, the quality of the enhanced
color images obtained by the proposed method are evaluated using novel metric,
namely, Wavelet Energy (WE). The experimental results presented show that color
images enhanced using the proposed scheme are clearer, more vivid and
efficient.","68T45,H.2.0,cs.CV"
"Graph-based calibration transfer. The problem of transferring calibrations from a primary to a secondary
instrument, i.e. calibration transfer (CT), has been a matter of considerable
research in chemometrics over the past decades. Current state-of-the-art (SoA)
methods like (piecewise) direct standardization perform well when suitable
transfer standards are available. However, stable calibration standards that
share similar (spectral) features with the calibration samples are not always
available. Towards enabling CT with arbitrary calibration standards, we propose
a novel CT technique that employs manifold regularization of the partial least
squares (PLS) objective. In particular, our method enforces that calibration
standards, measured on primary and secondary instruments, have (nearly)
invariant projections in the latent variable space of the primary calibration
model. Thereby, our approach implicitly removes inter-device variation in the
predictive directions of X which is in contrast to most state-of-the-art
techniques that employ explicit pre-processing of the input data. We test our
approach on the well-known corn benchmark data set employing the NBS glass
standard spectra for instrument standardization and compare the results with
current SoA methods.","cs.LG,stat.ME,stat.ML"
"Entangled q-Convolutional Neural Nets. We introduce a machine learning model, the q-CNN model, sharing key features
with convolutional neural networks and admitting a tensor network description.
As examples, we apply q-CNN to the MNIST and Fashion MNIST classification
tasks. We explain how the network associates a quantum state to each
classification label, and study the entanglement structure of these network
states. In both our experiments on the MNIST and Fashion-MNIST datasets, we
observe a distinct increase in both the left/right as well as the up/down
bipartition entanglement entropy during training as the network learns the fine
features of the data. More generally, we observe a universal negative
correlation between the value of the entanglement entropy and the value of the
cost function, suggesting that the network needs to learn the entanglement
structure in order the perform the task accurately. This supports the
possibility of exploiting the entanglement structure as a guide to design the
machine learning algorithm suitable for given tasks.","cs.LG,quant-ph,stat.ML"
"Neural Control Variates. We propose neural control variates (NCV) for unbiased variance reduction in
parametric Monte Carlo integration. So far, the core challenge of applying the
method of control variates has been finding a good approximation of the
integrand that is cheap to integrate. We show that a set of neural networks can
face that challenge: a normalizing flow that approximates the shape of the
integrand and another neural network that infers the solution of the integral
equation. We also propose to leverage a neural importance sampler to estimate
the difference between the original integrand and the learned control variate.
To optimize the resulting parametric estimator, we derive a theoretically
optimal, variance-minimizing loss function, and propose an alternative,
composite loss for stable online training in practice. When applied to light
transport simulation, neural control variates are capable of matching the
state-of-the-art performance of other unbiased approaches, while providing
means to develop more performant, practical solutions. Specifically, we show
that the learned light-field approximation is of sufficient quality for
high-order bounces, allowing us to omit the error correction and thereby
dramatically reduce the noise at the cost of negligible visible bias.","cs.GR,cs.LG,stat.ML"
"Neural Architecture Search in operational context: a remote sensing case-study. Deep learning has become in recent years a cornerstone tool fueling key
innovations in the industry, such as autonomous driving. To attain good
performances, the neural network architecture used for a given application must
be chosen with care. These architectures are often handcrafted and therefore
prone to human biases and sub-optimal selection. Neural Architecture Search
(NAS) is a framework introduced to mitigate such risks by jointly optimizing
the network architectures and its weights. Albeit its novelty, it was applied
on complex tasks with significant results - e.g. semantic image segmentation.
In this technical paper, we aim to evaluate its ability to tackle a challenging
operational task: semantic segmentation of objects of interest in satellite
imagery. Designing a NAS framework is not trivial and has strong dependencies
to hardware constraints. We therefore motivate our NAS approach selection and
provide corresponding implementation details. We also present novel ideas to
carry out other such use-case studies.","cs.CV,cs.NE"
"Optimal transport for vector Gaussian mixture models. Vector Gaussian mixture models form an important special subset of
vector-valued distributions. Any physical entity that can mutate or transit
among alternative manifestations distributed in a given space falls into this
category. A key example is color imagery. In this note, we vectorize the
Gaussian mixture model and study different optimal mass transport related
problems for such models. The benefits of using vector Gaussian mixture for
optimal mass transport include computational efficiency and the ability to
preserve structure.","cs.LG,math.OC,math.PR,stat.ML"
"Probability Link Models with Symmetric Information Divergence. This paper introduces link functions for transforming one probability
distribution to another such that the Kullback-Leibler and R\'enyi divergences
between the two distributions are symmetric. Two general classes of link models
are proposed. The first model links two survival functions and is applicable to
models such as the proportional odds and change point, which are used in
survival analysis and reliability modeling. A prototype application involving
the proportional odds model demonstrates advantages of symmetric divergence
measures over asymmetric measures for assessing the efficacy of features and
for model averaging purposes. The advantages include providing unique ranks for
models and unique information weights for model averaging with one-half as much
computation requirement of asymmetric divergences. The second model links two
cumulative probability distribution functions. This model produces a
generalized location model which are continuous counterparts of the binary
probability models such as probit and logit models. Examples include the
generalized probit and logit models which have appeared in the survival
analysis literature, and a generalized Laplace model and a generalized
Student-$t$ model, which are survival time models corresponding to the
respective binary probability models. Lastly, extensions to symmetric
divergence between survival functions and conditions for copula dependence
information are presented.","cs.IT,cs.LG,math.IT,math.ST,stat.ML,stat.TH"
"Heterogeneous Noisy Short Signal Camouflage in Multi-Domain Environment Decision-Making. Data transmission between two or more digital devices in industry and
government demands secure and agile technology. Digital information
distribution often requires deployment of Internet of Things (IoT) devices and
Data Fusion techniques which have also gained popularity in both, civilian and
military environments, such as, emergence of Smart Cities and Internet of
Battlefield Things (IoBT). This usually requires capturing and consolidating
data from multiple sources. Because datasets do not necessarily originate from
identical sensors, fused data typically results in a complex Big Data problem.
Due to potentially sensitive nature of IoT datasets, Blockchain technology is
used to facilitate secure sharing of IoT datasets, which allows digital
information to be distributed, but not copied. However, blockchain has several
limitations related to complexity, scalability, and excessive energy
consumption. We propose an approach to hide information (sensor signal) by
transforming it to an image or an audio signal. In one of the latest attempts
to the military modernization, we investigate sensor fusion approach by
investigating the challenges of enabling an intelligent identification and
detection operation and demonstrates the feasibility of the proposed Deep
Learning and Anomaly Detection models that can support future application for
specific hand gesture alert system from wearable devices.","cs.IT,cs.LG,math.IT,stat.AP"
"Understanding Structural Vulnerability in Graph Convolutional Networks. Recent studies have shown that Graph Convolutional Networks (GCNs) are
vulnerable to adversarial attacks on the graph structure. Although multiple
works have been proposed to improve their robustness against such structural
adversarial attacks, the reasons for the success of the attacks remain unclear.
In this work, we theoretically and empirically demonstrate that structural
adversarial examples can be attributed to the non-robust aggregation scheme
(i.e., the weighted mean) of GCNs. Specifically, our analysis takes advantage
of the breakdown point which can quantitatively measure the robustness of
aggregation schemes. The key insight is that weighted mean, as the basic design
of GCNs, has a low breakdown point and its output can be dramatically changed
by injecting a single edge. We show that adopting the aggregation scheme with a
high breakdown point (e.g., median or trimmed mean) could significantly enhance
the robustness of GCNs against structural attacks. Extensive experiments on
four real-world datasets demonstrate that such a simple but effective method
achieves the best robustness performance compared to state-of-the-art models.","cs.CR,cs.LG"
"Differentiable Reasoning on Large Knowledge Bases and Natural Language. Reasoning with knowledge expressed in natural language and Knowledge Bases
(KBs) is a major challenge for Artificial Intelligence, with applications in
machine reading, dialogue, and question answering. General neural architectures
that jointly learn representations and transformations of text are very
data-inefficient, and it is hard to analyse their reasoning process. These
issues are addressed by end-to-end differentiable reasoning systems such as
Neural Theorem Provers (NTPs), although they can only be used with small-scale
symbolic KBs. In this paper we first propose Greedy NTPs (GNTPs), an extension
to NTPs addressing their complexity and scalability limitations, thus making
them applicable to real-world datasets. This result is achieved by dynamically
constructing the computation graph of NTPs and including only the most
promising proof paths during inference, thus obtaining orders of magnitude more
efficient models. Then, we propose a novel approach for jointly reasoning over
KBs and textual mentions, by embedding logic facts and natural language
sentences in a shared embedding space. We show that GNTPs perform on par with
NTPs at a fraction of their cost while achieving competitive link prediction
results on large datasets, providing explanations for predictions, and inducing
interpretable models. Source code, datasets, and supplementary material are
available online at https://github.com/uclnlp/gntp.","cs.CL,cs.LG,cs.LO"
"Hierarchically Compositional Tasks and Deep Convolutional Networks. The main success stories of deep learning, starting with ImageNet, depend on
deep convolutional networks, which on certain tasks perform significantly
better than traditional shallow classifiers, such as support vector machines,
and also better than deep fully connected networks; but what is so special
about deep convolutional networks? Recent results in approximation theory
proved an exponential advantage of deep convolutional networks with or without
shared weights in approximating functions with hierarchical locality in their
compositional structure. More recently, the hierarchical structure was proved
to be hard to learn from data, suggesting that it is a powerful prior embedded
in the architecture of the network. These mathematical results, however, do not
say which real-life tasks correspond to input-output functions with
hierarchical locality. To evaluate this, we consider a set of visual tasks
where we disrupt the local organization of images via ""deterministic
scrambling"" to later perform a visual task on these images structurally-altered
in the same way for training and testing. For object recognition we find, as
expected, that scrambling does not affect the performance of shallow or deep
fully connected networks contrary to the out-performance of convolutional
networks. Not all tasks involving images are however affected. Texture
perception and global color estimation are much less sensitive to deterministic
scrambling showing that the underlying functions corresponding to these tasks
are not hierarchically local; and also counter-intuitively showing that these
tasks are better approximated by networks that are not deep (texture) nor
convolutional (color). Altogether, these results shed light into the importance
of matching a network architecture with its embedded prior of the task to be
learned.","cs.LG,eess.IV,q-bio.NC,stat.ML"
"From Game-theoretic Multi-agent Log Linear Learning to Reinforcement Learning. The main focus of this paper is on enhancement of two types of game-theoretic
learning algorithms: log-linear learning and reinforcement learning. The
standard analysis of log-linear learning needs a highly structured environment,
i.e. strong assumptions about the game from an implementation perspective. In
this paper, we introduce a variant of log-linear learning that provides
asymptotic guarantees while relaxing the structural assumptions to include
synchronous updates and limitations in information available to the players. On
the other hand, model-free reinforcement learning is able to perform even under
weaker assumptions on players' knowledge about the environment and other
players' strategies. We propose a reinforcement algorithm that uses a
double-aggregation scheme in order to deepen players' insight about the
environment and constant learning step-size which achieves a higher convergence
rate. Numerical experiments are conducted to verify each algorithm's robustness
and performance.","cs.LG,cs.MA"
"Backdoor Attacks Against Deep Learning Systems in the Physical World. Backdoor attacks embed hidden malicious behaviors into deep learning models,
which only activate and cause misclassifications on model inputs containing a
specific trigger. Existing works on backdoor attacks and defenses, however,
mostly focus on digital attacks that use digitally generated patterns as
triggers. A critical question remains unanswered: can backdoor attacks succeed
using physical objects as triggers, thus making them a credible threat against
deep learning systems in the real world? We conduct a detailed empirical study
to explore this question for facial recognition, a critical deep learning task.
Using seven physical objects as triggers, we collect a custom dataset of 3205
images of ten volunteers and use it to study the feasibility of physical
backdoor attacks under a variety of real-world conditions. Our study reveals
two key findings. First, physical backdoor attacks can be highly successful if
they are carefully configured to overcome the constraints imposed by physical
objects. In particular, the placement of successful triggers is largely
constrained by the target model's dependence on key facial features. Second,
four of today's state-of-the-art defenses against (digital) backdoors are
ineffective against physical backdoors, because the use of physical objects
breaks core assumptions used to construct these defenses. Our study confirms
that (physical) backdoor attacks are not a hypothetical phenomenon but rather
pose a serious real-world threat to critical classification tasks. We need new
and more robust defenses against backdoors in the physical world.","cs.CR,cs.CV,cs.LG"
"Beyond Low-frequency Information in Graph Convolutional Networks. Graph neural networks (GNNs) have been proven to be effective in various
network-related tasks. Most existing GNNs usually exploit the low-frequency
signals of node features, which gives rise to one fundamental question: is the
low-frequency information all we need in the real world applications? In this
paper, we first present an experimental investigation assessing the roles of
low-frequency and high-frequency signals, where the results clearly show that
exploring low-frequency signal only is distant from learning an effective node
representation in different scenarios. How can we adaptively learn more
information beyond low-frequency information in GNNs? A well-informed answer
can help GNNs enhance the adaptability. We tackle this challenge and propose a
novel Frequency Adaptation Graph Convolutional Networks (FAGCN) with a
self-gating mechanism, which can adaptively integrate different signals in the
process of message passing. For a deeper understanding, we theoretically
analyze the roles of low-frequency signals and high-frequency signals on
learning node representations, which further explains why FAGCN can perform
well on different types of networks. Extensive experiments on six real-world
networks validate that FAGCN not only alleviates the over-smoothing problem,
but also has advantages over the state-of-the-arts.","cs.LG,cs.SI"
"Model Extraction Warning in MLaaS Paradigm. Cloud vendors are increasingly offering machine learning services as part of
their platform and services portfolios. These services enable the deployment of
machine learning models on the cloud that are offered on a pay-per-query basis
to application developers and end users. However recent work has shown that the
hosted models are susceptible to extraction attacks. Adversaries may launch
queries to steal the model and compromise future query payments or privacy of
the training data. In this work, we present a cloud-based extraction monitor
that can quantify the extraction status of models by observing the query and
response streams of both individual and colluding adversarial users. We present
a novel technique that uses information gain to measure the model learning rate
by users with increasing number of queries. Additionally, we present an
alternate technique that maintains intelligent query summaries to measure the
learning rate relative to the coverage of the input feature space in the
presence of collusion. Both these approaches have low computational overhead
and can easily be offered as services to model owners to warn them of possible
extraction attacks from adversaries. We present performance results for these
approaches for decision tree models deployed on BigML MLaaS platform, using
open source datasets and different adversarial attack strategies.","cs.CR,cs.DC,cs.LG"
"Scalable Optimal Transport in High Dimensions for Graph Distances, Embedding Alignment, and More. The current best practice for computing optimal transport (OT) is via entropy
regularization and Sinkhorn iterations. This algorithm runs in quadratic time
as it requires the full pairwise cost matrix, which is prohibitively expensive
for large sets of objects. In this work we propose two effective log-linear
time approximations of the cost matrix: First, a sparse approximation based on
locality-sensitive hashing (LSH) and, second, a Nystr\""om approximation with
LSH-based sparse corrections, which we call locally corrected Nystr\""om (LCN).
These approximations enable general log-linear time algorithms for
entropy-regularized OT that perform well even for the complex, high-dimensional
spaces common in deep learning. We analyse these approximations theoretically
and evaluate them experimentally both directly and end-to-end as a component
for real-world applications. Using our approximations for unsupervised word
embedding alignment enables us to speed up a state-of-the-art method by a
factor of 3 while also improving the accuracy by 3.1 percentage points without
any additional model changes. For graph distance regression we propose the
graph transport network (GTN), which combines graph neural networks (GNNs) with
enhanced Sinkhorn. GTN outcompetes previous models by 48% and still scales
log-linearly in the number of nodes.","cs.CL,cs.DS,cs.LG,cs.SI,stat.ML"
"Elements of Sequential Monte Carlo. A core problem in statistics and probabilistic machine learning is to compute
probability distributions and expectations. This is the fundamental problem of
Bayesian statistics and machine learning, which frames all inference as
expectations with respect to the posterior distribution. The key challenge is
to approximate these intractable expectations. In this tutorial, we review
sequential Monte Carlo (SMC), a random-sampling-based class of methods for
approximate inference. First, we explain the basics of SMC, discuss practical
issues, and review theoretical results. We then examine two of the main user
design choices: the proposal distributions and the so called intermediate
target distributions. We review recent results on how variational inference and
amortization can be used to learn efficient proposals and target distributions.
Next, we discuss the SMC estimate of the normalizing constant, how this can be
used for pseudo-marginal inference and inference evaluation. Throughout the
tutorial we illustrate the use of SMC on various models commonly used in
machine learning, such as stochastic recurrent neural networks, probabilistic
graphical models, and probabilistic programs.","cs.LG,stat.CO,stat.ML"
"Memorizing without overfitting: Bias, variance, and interpolation in over-parameterized models. The bias-variance trade-off is a central concept in supervised learning. In
classical statistics, increasing the complexity of a model (e.g., number of
parameters) reduces bias but also increases variance. Until recently, it was
commonly believed that optimal performance is achieved at intermediate model
complexities which strike a balance between bias and variance. Modern Deep
Learning methods flout this dogma, achieving state-of-the-art performance using
""over-parameterized models"" where the number of fit parameters is large enough
to perfectly fit the training data. As a result, understanding bias and
variance in over-parameterized models has emerged as a fundamental problem in
machine learning. Here, we use methods from statistical physics to derive
analytic expressions for bias and variance in two minimal models of
over-parameterization (linear regression and two-layer neural networks with
nonlinear data distributions), allowing us to disentangle properties stemming
from the model architecture and random sampling of data. In both models,
increasing the number of fit parameters leads to a phase transition where the
training error goes to zero and the test error diverges as a result of the
variance (while the bias remains finite). Beyond this threshold in the
interpolation regime, the training error remains zero while the test error
decreases. We also show that in contrast with classical intuition,
over-parameterized models can overfit even in the absence of noise and exhibit
bias even if the student and teacher models match. We synthesize these results
to construct a holistic understanding of generalization error and the
bias-variance trade-off in over-parameterized models and relate our results to
random matrix theory.","cond-mat.dis-nn,cs.LG,stat.ML"
"Learning Power Control for Cellular Systems with Heterogeneous Graph Neural Network. Optimizing power control in multi-cell cellular networks with deep learning
enables such a non-convex problem to be implemented in real-time. When channels
are time-varying, the deep neural networks (DNNs) need to be re-trained
frequently, which calls for low training complexity. To reduce the number of
training samples and the size of DNN required to achieve good performance, a
promising approach is to embed the DNNs with priori knowledge. Since cellular
networks can be modelled as a graph, it is natural to employ graph neural
networks (GNNs) for learning, which exhibit permutation invariance (PI) and
equivalence (PE) properties. Unlike the homogeneous GNNs that have been used
for wireless problems, whose outputs are invariant or equivalent to arbitrary
permutations of vertexes, heterogeneous GNNs (HetGNNs), which are more
appropriate to model cellular networks, are only invariant or equivalent to
some permutations. If the PI or PE properties of the HetGNN do not match the
property of the task to be learned, the performance degrades dramatically. In
this paper, we show that the power control policy has a combination of
different PI and PE properties, and existing HetGNN does not satisfy these
properties. We then design a parameter sharing scheme for HetGNN such that the
learned relationship satisfies the desired properties. Simulation results show
that the sample complexity and the size of designed GNN for learning the
optimal power control policy in multi-user multi-cell networks are much lower
than the existing DNNs, when achieving the same sum rate loss from the
numerically obtained solutions.","cs.LG,cs.SY,eess.SY"
"An Inverse QSAR Method Based on Linear Regression and Integer Programming. Recently a novel framework has been proposed for designing the molecular
structure of chemical compounds using both artificial neural networks (ANNs)
and mixed integer linear programming (MILP). In the framework, we first define
a feature vector $f(C)$ of a chemical graph $C$ and construct an ANN that maps
$x=f(C)$ to a predicted value $\eta(x)$ of a chemical property $\pi$ to $C$.
After this, we formulate an MILP that simulates the computation process of
$f(C)$ from $C$ and that of $\eta(x)$ from $x$. Given a target value $y^*$ of
the chemical property $\pi$, we infer a chemical graph $C^\dagger$ such that
$\eta(f(C^\dagger))=y^*$ by solving the MILP. In this paper, we use linear
regression to construct a prediction function $\eta$ instead of ANNs. For this,
we derive an MILP formulation that simulates the computation process of a
prediction function by linear regression. The results of computational
experiments suggest our method can infer chemical graphs with around up to 50
non-hydrogen atoms.","cs.LG,math.OC,q-bio.BM"
"Smart Irrigation IoT Solution using Transfer Learning for Neural Networks. In this paper we develop a reliable system for smart irrigation of
greenhouses using artificial neural networks, and an IoT architecture. Our
solution uses four sensors in different layers of soil to predict future
moisture. Using a dataset we collected by running experiments on different
soils, we show high performance of neural networks compared to existing
alternative method of support vector regression. To reduce the processing power
of neural network for the IoT edge devices, we propose using transfer learning.
Transfer learning also speeds up training performance with small amount of
training data, and allows integrating climate sensors to a pre-trained model,
which are the other two challenges of smart irrigation of greenhouses. Our
proposed IoT architecture shows a complete solution for smart irrigation.",cs.LG
"A Multi-Scale Tensor Network Architecture for Classification and Regression. We present an algorithm for supervised learning using tensor networks,
employing a step of preprocessing the data by coarse-graining through a
sequence of wavelet transformations. We represent these transformations as a
set of tensor network layers identical to those in a multi-scale entanglement
renormalization ansatz (MERA) tensor network, and perform supervised learning
and regression tasks through a model based on a matrix product state (MPS)
tensor network acting on the coarse-grained data. Because the entire model
consists of tensor contractions (apart from the initial non-linear feature
map), we can adaptively fine-grain the optimized MPS model backwards through
the layers with essentially no loss in performance. The MPS itself is trained
using an adaptive algorithm based on the density matrix renormalization group
(DMRG) algorithm. We test our methods by performing a classification task on
audio data and a regression task on temperature time-series data, studying the
dependence of training accuracy on the number of coarse-graining layers and
showing how fine-graining through the network may be used to initialize models
with access to finer-scale features.","cs.LG,quant-ph,stat.ML"
"Fast Global Convergence of Natural Policy Gradient Methods with Entropy Regularization. Natural policy gradient (NPG) methods are among the most widely used policy
optimization algorithms in contemporary reinforcement learning. This class of
methods is often applied in conjunction with entropy regularization -- an
algorithmic scheme that encourages exploration -- and is closely related to
soft policy iteration and trust region policy optimization. Despite the
empirical success, the theoretical underpinnings for NPG methods remain limited
even for the tabular setting. This paper develops $\textit{non-asymptotic}$
convergence guarantees for entropy-regularized NPG methods under softmax
parameterization, focusing on discounted Markov decision processes (MDPs).
Assuming access to exact policy evaluation, we demonstrate that the algorithm
converges linearly -- or even quadratically once it enters a local region
around the optimal policy -- when computing optimal value functions of the
regularized MDP. Moreover, the algorithm is provably stable vis-\`a-vis
inexactness of policy evaluation. Our convergence results accommodate a wide
range of learning rates, and shed light upon the role of entropy regularization
in enabling fast convergence.","cs.IT,cs.LG,math.IT,math.OC,stat.ML"
"Deep-learning based discovery of partial differential equations in integral form from sparse and noisy data. Data-driven discovery of partial differential equations (PDEs) has attracted
increasing attention in recent years. Although significant progress has been
made, certain unresolved issues remain. For example, for PDEs with high-order
derivatives, the performance of existing methods is unsatisfactory, especially
when the data are sparse and noisy. It is also difficult to discover
heterogeneous parametric PDEs where heterogeneous parameters are embedded in
the partial differential operators. In this work, a new framework combining
deep-learning and integral form is proposed to handle the above-mentioned
problems simultaneously, and improve the accuracy and stability of PDE
discovery. In the framework, a deep neural network is firstly trained with
observation data to generate meta-data and calculate derivatives. Then, a
unified integral form is defined, and the genetic algorithm is employed to
discover the best structure. Finally, the value of parameters is calculated,
and whether the parameters are constants or variables is identified. Numerical
experiments proved that our proposed algorithm is more robust to noise and more
accurate compared with existing methods due to the utilization of integral
form. Our proposed algorithm is also able to discover PDEs with high-order
derivatives or heterogeneous parameters accurately with sparse and noisy data.","cs.LG,cs.NA,math.NA,physics.comp-ph,stat.ML"
"Efficient Global-Local Memory for Real-time Instrument Segmentation of Robotic Surgical Video. Performing a real-time and accurate instrument segmentation from videos is of
great significance for improving the performance of robotic-assisted surgery.
We identify two important clues for surgical instrument perception, including
local temporal dependency from adjacent frames and global semantic correlation
in long-range duration. However, most existing works perform segmentation
purely using visual cues in a single frame. Optical flow is just used to model
the motion between only two frames and brings heavy computational cost. We
propose a novel dual-memory network (DMNet) to wisely relate both global and
local spatio-temporal knowledge to augment the current features, boosting the
segmentation performance and retaining the real-time prediction capability. We
propose, on the one hand, an efficient local memory by taking the complementary
advantages of convolutional LSTM and non-local mechanisms towards the relating
reception field. On the other hand, we develop an active global memory to
gather the global semantic correlation in long temporal range to current one,
in which we gather the most informative frames derived from model uncertainty
and frame similarity. We have extensively validated our method on two public
benchmark surgical video datasets. Experimental results demonstrate that our
method largely outperforms the state-of-the-art works on segmentation accuracy
while maintaining a real-time speed.","cs.CV,eess.IV"
"Training Object Detectors on Synthetic Images Containing Reflecting Materials. One of the grand challenges of deep learning is the requirement to obtain
large labeled training data sets. While synthesized data sets can be used to
overcome this challenge, it is important that these data sets close the reality
gap, i.e., a model trained on synthetic image data is able to generalize to
real images. Whereas, the reality gap can be considered bridged in several
application scenarios, training on synthesized images containing reflecting
materials requires further research. Since the appearance of objects with
reflecting materials is dominated by the surrounding environment, this
interaction needs to be considered during training data generation. Therefore,
within this paper we examine the effect of reflecting materials in the context
of synthetic image generation for training object detectors. We investigate the
influence of rendering approach used for image synthesis, the effect of domain
randomization, as well as the amount of used training data. To be able to
compare our results to the state-of-the-art, we focus on indoor scenes as they
have been investigated extensively. Within this scenario, bathroom furniture is
a natural choice for objects with reflecting materials, for which we report our
findings on real and synthetic testing data.","cs.CV,cs.GR,stat.ML"
"Active Screening for Recurrent Diseases: A Reinforcement Learning Approach. Active screening is a common approach in controlling the spread of recurring
infectious diseases such as tuberculosis and influenza. In this approach,
health workers periodically select a subset of population for screening.
However, given the limited number of health workers, only a small subset of the
population can be visited in any given time period. Given the recurrent nature
of the disease and rapid spreading, the goal is to minimize the number of
infections over a long time horizon. Active screening can be formalized as a
sequential combinatorial optimization over the network of people and their
connections. The main computational challenges in this formalization arise from
i) the combinatorial nature of the problem, ii) the need of sequential planning
and iii) the uncertainties in the infectiousness states of the population.
  Previous works on active screening fail to scale to large time horizon while
fully considering the future effect of current interventions. In this paper, we
propose a novel reinforcement learning (RL) approach based on Deep Q-Networks
(DQN), with several innovative adaptations that are designed to address the
above challenges. First, we use graph convolutional networks (GCNs) to
represent the Q-function that exploit the node correlations of the underlying
contact network. Second, to avoid solving a combinatorial optimization problem
in each time period, we decompose the node set selection as a sub-sequence of
decisions, and further design a two-level RL framework that solves the problem
in a hierarchical way. Finally, to speed-up the slow convergence of RL which
arises from reward sparseness, we incorporate ideas from curriculum learning
into our hierarchical RL approach. We evaluate our RL algorithm on several
real-world networks.","cs.LG,cs.MA"
"DynamicViT: Efficient Vision Transformers with Dynamic Token Sparsification. Attention is sparse in vision transformers. We observe the final prediction
in vision transformers is only based on a subset of most informative tokens,
which is sufficient for accurate image recognition. Based on this observation,
we propose a dynamic token sparsification framework to prune redundant tokens
progressively and dynamically based on the input. Specifically, we devise a
lightweight prediction module to estimate the importance score of each token
given the current features. The module is added to different layers to prune
redundant tokens hierarchically. To optimize the prediction module in an
end-to-end manner, we propose an attention masking strategy to differentiably
prune a token by blocking its interactions with other tokens. Benefiting from
the nature of self-attention, the unstructured sparse tokens are still hardware
friendly, which makes our framework easy to achieve actual speed-up. By
hierarchically pruning 66% of the input tokens, our method greatly reduces
31%~37% FLOPs and improves the throughput by over 40% while the drop of
accuracy is within 0.5% for various vision transformers. Equipped with the
dynamic token sparsification framework, DynamicViT models can achieve very
competitive complexity/accuracy trade-offs compared to state-of-the-art CNNs
and vision transformers on ImageNet. Code is available at
https://github.com/raoyongming/DynamicViT","cs.AI,cs.CV,cs.LG"
"Deferred Neural Rendering: Image Synthesis using Neural Textures. The modern computer graphics pipeline can synthesize images at remarkable
visual quality; however, it requires well-defined, high-quality 3D content as
input. In this work, we explore the use of imperfect 3D content, for instance,
obtained from photo-metric reconstructions with noisy and incomplete surface
geometry, while still aiming to produce photo-realistic (re-)renderings. To
address this challenging problem, we introduce Deferred Neural Rendering, a new
paradigm for image synthesis that combines the traditional graphics pipeline
with learnable components. Specifically, we propose Neural Textures, which are
learned feature maps that are trained as part of the scene capture process.
Similar to traditional textures, neural textures are stored as maps on top of
3D mesh proxies; however, the high-dimensional feature maps contain
significantly more information, which can be interpreted by our new deferred
neural rendering pipeline. Both neural textures and deferred neural renderer
are trained end-to-end, enabling us to synthesize photo-realistic images even
when the original 3D content was imperfect. In contrast to traditional,
black-box 2D generative neural networks, our 3D representation gives us
explicit control over the generated output, and allows for a wide range of
application domains. For instance, we can synthesize temporally-consistent
video re-renderings of recorded 3D scenes as our representation is inherently
embedded in 3D space. This way, neural textures can be utilized to coherently
re-render or manipulate existing video content in both static and dynamic
environments at real-time rates. We show the effectiveness of our approach in
several experiments on novel view synthesis, scene editing, and facial
reenactment, and compare to state-of-the-art approaches that leverage the
standard graphics pipeline as well as conventional generative neural networks.","cs.CV,cs.GR"
"End-to-End Dense Video Captioning with Parallel Decoding. Dense video captioning aims to generate multiple associated captions with
their temporal locations from the video. Previous methods follow a
sophisticated ""localize-then-describe"" scheme, which heavily relies on numerous
hand-crafted components. In this paper, we proposed a simple yet effective
framework for end-to-end dense video captioning with parallel decoding (PDVC),
by formulating the dense caption generation as a set prediction task. In
practice, through stacking a newly proposed event counter on the top of a
transformer decoder, the PDVC precisely segments the video into a number of
event pieces under the holistic understanding of the video content, which
effectively increases the coherence and readability of predicted captions.
Compared with prior arts, the PDVC has several appealing advantages: (1)
Without relying on heuristic non-maximum suppression or a recurrent event
sequence selection network to remove redundancy, PDVC directly produces an
event set with an appropriate size; (2) In contrast to adopting the two-stage
scheme, we feed the enhanced representations of event queries into the
localization head and caption head in parallel, making these two sub-tasks
deeply interrelated and mutually promoted through the optimization; (3) Without
bells and whistles, extensive experiments on ActivityNet Captions and YouCook2
show that PDVC is capable of producing high-quality captioning results,
surpassing the state-of-the-art two-stage methods when its localization
accuracy is on par with them. Code is available at
https://github.com/ttengwang/PDVC.","68T45,I.4.9; I.5.4,cs.CV"
"Token Shift Transformer for Video Classification. Transformer achieves remarkable successes in understanding 1 and
2-dimensional signals (e.g., NLP and Image Content Understanding). As a
potential alternative to convolutional neural networks, it shares merits of
strong interpretability, high discriminative power on hyper-scale data, and
flexibility in processing varying length inputs. However, its encoders
naturally contain computational intensive operations such as pair-wise
self-attention, incurring heavy computational burden when being applied on the
complex 3-dimensional video signals.
  This paper presents Token Shift Module (i.e., TokShift), a novel,
zero-parameter, zero-FLOPs operator, for modeling temporal relations within
each transformer encoder. Specifically, the TokShift barely temporally shifts
partial [Class] token features back-and-forth across adjacent frames. Then, we
densely plug the module into each encoder of a plain 2D vision transformer for
learning 3D video representation. It is worth noticing that our TokShift
transformer is a pure convolutional-free video transformer pilot with
computational efficiency for video understanding. Experiments on standard
benchmarks verify its robustness, effectiveness, and efficiency. Particularly,
with input clips of 8/12 frames, the TokShift transformer achieves SOTA
precision: 79.83%/80.40% on the Kinetics-400, 66.56% on EGTEA-Gaze+, and 96.80%
on UCF-101 datasets, comparable or better than existing SOTA convolutional
counterparts. Our code is open-sourced in:
https://github.com/VideoNetworks/TokShift-Transformer.","cs.CV,cs.MM"
"FairCanary: Rapid Continuous Explainable Fairness. Machine Learning (ML) models are being used in all facets of today's society
to make high stake decisions like bail granting or credit lending, with very
minimal regulations. Such systems are extremely vulnerable to both propagating
and amplifying social biases, and have therefore been subject to growing
research interest. One of the main issues with conventional fairness metrics is
their narrow definitions which hide the complete extent of the bias by focusing
primarily on positive and/or negative outcomes, whilst not paying attention to
the overall distributional shape. Moreover, these metrics are often
contradictory to each other, are severely restrained by the contextual and
legal landscape of the problem, have technical constraints like poor support
for continuous outputs, the requirement of class labels, and are not
explainable.
  In this paper, we present Quantile Demographic Drift, which addresses the
shortcomings mentioned above. This metric can also be used to measure
intra-group privilege. It is easily interpretable via existing attribution
techniques, and also extends naturally to individual fairness via the principle
of like-for-like comparison. We make this new fairness score the basis of a new
system that is designed to detect bias in production ML models without the need
for labels. We call the system FairCanary because of its capability to detect
bias in a live deployed model and narrow down the alert to the responsible set
of features, like the proverbial canary in a coal mine.","cs.CY,cs.LG"
"A Recurrent Neural Network Survival Model: Predicting Web User Return Time. The size of a website's active user base directly affects its value. Thus, it
is important to monitor and influence a user's likelihood to return to a site.
Essential to this is predicting when a user will return. Current state of the
art approaches to solve this problem come in two flavors: (1) Recurrent Neural
Network (RNN) based solutions and (2) survival analysis methods. We observe
that both techniques are severely limited when applied to this problem.
Survival models can only incorporate aggregate representations of users instead
of automatically learning a representation directly from a raw time series of
user actions. RNNs can automatically learn features, but can not be directly
trained with examples of non-returning users who have no target value for their
return time. We develop a novel RNN survival model that removes the limitations
of the state of the art methods. We demonstrate that this model can
successfully be applied to return time prediction on a large e-commerce dataset
with a superior ability to discriminate between returning and non-returning
users than either method applied in isolation.","cs.CY,cs.IR,cs.LG,cs.NE,stat.ML"
"Scaling up graph homomorphism for classification via sampling. Feature generation is an open topic of investigation in graph machine
learning. In this paper, we study the use of graph homomorphism density
features as a scalable alternative to homomorphism numbers which retain similar
theoretical properties and ability to take into account inductive bias. For
this, we propose a high-performance implementation of a simple sampling
algorithm which computes additive approximations of homomorphism densities. In
the context of graph machine learning, we demonstrate in experiments that
simple linear models trained on sample homomorphism densities can achieve
performance comparable to graph neural networks on standard graph
classification datasets. Finally, we show in experiments on synthetic data that
this algorithm scales to very large graphs when implemented with Bloom filters.","I.5.1; I.5.2,cs.DS,cs.LG"
"Robust Learning of Fixed-Structure Bayesian Networks in Nearly-Linear Time. We study the problem of learning Bayesian networks where an
$\epsilon$-fraction of the samples are adversarially corrupted. We focus on the
fully-observable case where the underlying graph structure is known. In this
work, we present the first nearly-linear time algorithm for this problem with a
dimension-independent error guarantee. Previous robust algorithms with
comparable error guarantees are slower by at least a factor of $(d/\epsilon)$,
where $d$ is the number of variables in the Bayesian network and $\epsilon$ is
the fraction of corrupted samples.
  Our algorithm and analysis are considerably simpler than those in previous
work. We achieve this by establishing a direct connection between robust
learning of Bayesian networks and robust mean estimation. As a subroutine in
our algorithm, we develop a robust mean estimation algorithm whose runtime is
nearly-linear in the number of nonzeros in the input samples, which may be of
independent interest.","cs.DS,cs.LG,math.ST,stat.ML,stat.TH"
"Identifying Misinformation from Website Screenshots. Can the look and the feel of a website give information about the
trustworthiness of an article? In this paper, we propose to use a promising,
yet neglected aspect in detecting the misinformativeness: the overall look of
the domain webpage. To capture this overall look, we take screenshots of news
articles served by either misinformative or trustworthy web domains and
leverage a tensor decomposition based semi-supervised classification technique.
The proposed approach i.e., VizFake is insensitive to a number of image
transformations such as converting the image to grayscale, vectorizing the
image and losing some parts of the screenshots. VizFake leverages a very small
amount of known labels, mirroring realistic and practical scenarios, where
labels (especially for known misinformative articles), are scarce and quickly
become dated. The F1 score of VizFake on a dataset of 50k screenshots of news
articles spanning more than 500 domains is roughly 85% using only 5% of ground
truth labels. Furthermore, tensor representations of VizFake, obtained in an
unsupervised manner, allow for exploratory analysis of the data that provides
valuable insights into the problem. Finally, we compare VizFake with deep
transfer learning, since it is a very popular black-box approach for image
classification and also well-known text text-based methods. VizFake achieves
competitive accuracy with deep transfer learning models while being two orders
of magnitude faster and not requiring laborious hyper-parameter tuning.","cs.AI,cs.CY,cs.LG,cs.SI"
"Interactive Learning with Corrective Feedback for Policies based on Deep Neural Networks. Deep Reinforcement Learning (DRL) has become a powerful strategy to solve
complex decision making problems based on Deep Neural Networks (DNNs). However,
it is highly data demanding, so unfeasible in physical systems for most
applications. In this work, we approach an alternative Interactive Machine
Learning (IML) strategy for training DNN policies based on human corrective
feedback, with a method called Deep COACH (D-COACH). This approach not only
takes advantage of the knowledge and insights of human teachers as well as the
power of DNNs, but also has no need of a reward function (which sometimes
implies the need of external perception for computing rewards). We combine Deep
Learning with the COrrective Advice Communicated by Humans (COACH) framework,
in which non-expert humans shape policies by correcting the agent's actions
during execution. The D-COACH framework has the potential to solve complex
problems without much data or time required. Experimental results validated the
efficiency of the framework in three different problems (two simulated, one
with a real robot), with state spaces of low and high dimensions, showing the
capacity to successfully learn policies for continuous action spaces like in
the Car Racing and Cart-Pole problems faster than with DRL.","68T05,68T40,93C85,cs.LG,stat.ML"
"An Unsupervised Sampling Approach for Image-Sentence Matching Using Document-Level Structural Information. In this paper, we focus on the problem of unsupervised image-sentence
matching. Existing research explores to utilize document-level structural
information to sample positive and negative instances for model training.
Although the approach achieves positive results, it introduces a sampling bias
and fails to distinguish instances with high semantic similarity. To alleviate
the bias, we propose a new sampling strategy to select additional
intra-document image-sentence pairs as positive or negative samples.
Furthermore, to recognize the complex pattern in intra-document samples, we
propose a Transformer based model to capture fine-grained features and
implicitly construct a graph for each document, where concepts in a document
are introduced to bridge the representation learning of images and sentences in
the context of a document. Experimental results show the effectiveness of our
approach to alleviate the bias and learn well-aligned multimodal
representations.","cs.AI,cs.CV,cs.MM"
"Learning Multiple Stock Trading Patterns with Temporal Routing Adaptor and Optimal Transport. Successful quantitative investment usually relies on precise predictions of
the future movement of the stock price. Recently, machine learning based
solutions have shown their capacity to give more accurate stock prediction and
become indispensable components in modern quantitative investment systems.
However, the i.i.d. assumption behind existing methods is inconsistent with the
existence of diverse trading patterns in the stock market, which inevitably
limits their ability to achieve better stock prediction performance. In this
paper, we propose a novel architecture, Temporal Routing Adaptor (TRA), to
empower existing stock prediction models with the ability to model multiple
stock trading patterns. Essentially, TRA is a lightweight module that consists
of a set of independent predictors for learning multiple patterns as well as a
router to dispatch samples to different predictors. Nevertheless, the lack of
explicit pattern identifiers makes it quite challenging to train an effective
TRA-based model. To tackle this challenge, we further design a learning
algorithm based on Optimal Transport (OT) to obtain the optimal sample to
predictor assignment and effectively optimize the router with such assignment
through an auxiliary loss term. Experiments on the real-world stock ranking
task show that compared to the state-of-the-art baselines, e.g., Attention LSTM
and Transformer, the proposed method can improve information coefficient (IC)
from 0.053 to 0.059 and 0.051 to 0.056 respectively. Our dataset and code used
in this work are publicly available:
https://github.com/microsoft/qlib/tree/main/examples/benchmarks/TRA.","cs.CE,cs.LG,q-fin.ST"
"Improving Generative Visual Dialog by Answering Diverse Questions. Prior work on training generative Visual Dialog models with reinforcement
learning(Das et al.) has explored a Qbot-Abot image-guessing game and shown
that this 'self-talk' approach can lead to improved performance at the
downstream dialog-conditioned image-guessing task. However, this improvement
saturates and starts degrading after a few rounds of interaction, and does not
lead to a better Visual Dialog model. We find that this is due in part to
repeated interactions between Qbot and Abot during self-talk, which are not
informative with respect to the image. To improve this, we devise a simple
auxiliary objective that incentivizes Qbot to ask diverse questions, thus
reducing repetitions and in turn enabling Abot to explore a larger state space
during RL ie. be exposed to more visual concepts to talk about, and varied
questions to answer. We evaluate our approach via a host of automatic metrics
and human studies, and demonstrate that it leads to better dialog, ie. dialog
that is more diverse (ie. less repetitive), consistent (ie. has fewer
conflicting exchanges), fluent (ie. more human-like),and detailed, while still
being comparably image-relevant as prior work and ablations.","cs.AI,cs.CL,cs.CV,cs.LG,stat.ML"
"Structure estimation for discrete graphical models: Generalized covariance matrices and their inverses. We investigate the relationship between the structure of a discrete graphical
model and the support of the inverse of a generalized covariance matrix. We
show that for certain graph structures, the support of the inverse covariance
matrix of indicator variables on the vertices of a graph reflects the
conditional independence structure of the graph. Our work extends results that
have previously been established only in the context of multivariate Gaussian
graphical models, thereby addressing an open question about the significance of
the inverse covariance matrix of a non-Gaussian distribution. The proof
exploits a combination of ideas from the geometry of exponential families,
junction tree theory and convex analysis. These population-level results have
various consequences for graph selection methods, both known and novel,
including a novel method for structure estimation for missing or corrupted
observations. We provide nonasymptotic guarantees for such methods and
illustrate the sharpness of these predictions via simulations.","math.ST,stat.ML,stat.TH"
"PP-LinkNet: Improving Semantic Segmentation of High Resolution Satellite Imagery with Multi-stage Training. Road network and building footprint extraction is essential for many
applications such as updating maps, traffic regulations, city planning,
ride-hailing, disaster response \textit{etc}. Mapping road networks is
currently both expensive and labor-intensive. Recently, improvements in image
segmentation through the application of deep neural networks has shown
promising results in extracting road segments from large scale, high resolution
satellite imagery. However, significant challenges remain due to lack of enough
labeled training data needed to build models for industry grade applications.
In this paper, we propose a two-stage transfer learning technique to improve
robustness of semantic segmentation for satellite images that leverages noisy
pseudo ground truth masks obtained automatically (without human labor) from
crowd-sourced OpenStreetMap (OSM) data. We further propose Pyramid
Pooling-LinkNet (PP-LinkNet), an improved deep neural network for segmentation
that uses focal loss, poly learning rate, and context module. We demonstrate
the strengths of our approach through evaluations done on three popular
datasets over two tasks, namely, road extraction and building foot-print
detection. Specifically, we obtain 78.19\% meanIoU on SpaceNet building
footprint dataset, 67.03\% and 77.11\% on the road topology metric on SpaceNet
and DeepGlobe road extraction dataset, respectively.","cs.CV,cs.LG,eess.IV"
"Exacerbating Algorithmic Bias through Fairness Attacks. Algorithmic fairness has attracted significant attention in recent years,
with many quantitative measures suggested for characterizing the fairness of
different machine learning algorithms. Despite this interest, the robustness of
those fairness measures with respect to an intentional adversarial attack has
not been properly addressed. Indeed, most adversarial machine learning has
focused on the impact of malicious attacks on the accuracy of the system,
without any regard to the system's fairness. We propose new types of data
poisoning attacks where an adversary intentionally targets the fairness of a
system. Specifically, we propose two families of attacks that target fairness
measures. In the anchoring attack, we skew the decision boundary by placing
poisoned points near specific target points to bias the outcome. In the
influence attack on fairness, we aim to maximize the covariance between the
sensitive attributes and the decision outcome and affect the fairness of the
model. We conduct extensive experiments that indicate the effectiveness of our
proposed attacks.","cs.AI,cs.CR,cs.LG"
"Sample-Efficient Learning of Stackelberg Equilibria in General-Sum Games. Real world applications such as economics and policy making often involve
solving multi-agent games with two unique features: (1) The agents are
inherently asymmetric and partitioned into leaders and followers; (2) The
agents have different reward functions, thus the game is general-sum. The
majority of existing results in this field focuses on either symmetric solution
concepts (e.g. Nash equilibrium) or zero-sum games. It remains vastly open how
to learn the Stackelberg equilibrium -- an asymmetric analog of the Nash
equilibrium -- in general-sum games efficiently from samples.
  This paper initiates the theoretical study of sample-efficient learning of
the Stackelberg equilibrium, in the bandit feedback setting where we only
observe noisy samples of the reward. We consider three representative
two-player general-sum games: bandit games, bandit-reinforcement learning
(bandit-RL) games, and linear bandit games. In all these games, we identify a
fundamental gap between the exact value of the Stackelberg equilibrium and its
estimated version using finitely many noisy samples, which can not be closed
information-theoretically regardless of the algorithm. We then establish sharp
positive results on sample-efficient learning of Stackelberg equilibrium with
value optimal up to the gap identified above, with matching lower bounds in the
dependency on the gap, error tolerance, and the size of the action spaces.
Overall, our results unveil unique challenges in learning Stackelberg
equilibria under noisy bandit feedback, which we hope could shed light on
future research on this topic.","cs.AI,cs.GT,cs.LG,stat.ML"
"Smoothed Bernstein Online Aggregation for Day-Ahead Electricity Demand Forecasting. We present a winning method of the IEEE DataPort Competition on Day-Ahead
Electricity Demand Forecasting: Post-COVID Paradigm. The day-ahead load
forecasting approach is based on online forecast combination of multiple point
prediction models. It contains four steps: i) data cleaning and preprocessing,
ii) a holiday adjustment procedure, iii) training of individual forecasting
models, iv) forecast combination by smoothed Bernstein Online Aggregation
(BOA). The approach is flexible and can quickly adopt to new energy system
situations as they occurred during and after COVID-19 shutdowns. The pool of
individual prediction models ranges from rather simple time series models to
sophisticated models like generalized additive models (GAMs) and
high-dimensional linear models estimated by lasso. They incorporate
autoregressive, calendar and weather effects efficiently. All steps contain
novel concepts that contribute to the excellent forecasting performance of the
proposed method. This holds particularly for the holiday adjustment procedure
and the fully adaptive smoothed BOA approach.","37M10,62J07,62M10,62P12,62P30,G.3; I.5,cs.CE,cs.LG,cs.SY,eess.SY,stat.AP,stat.ML"
"Deep least-squares methods: an unsupervised learning-based numerical method for solving elliptic PDEs. This paper studies an unsupervised deep learning-based numerical approach for
solving partial differential equations (PDEs). The approach makes use of the
deep neural network to approximate solutions of PDEs through the compositional
construction and employs least-squares functionals as loss functions to
determine parameters of the deep neural network. There are various
least-squares functionals for a partial differential equation. This paper
focuses on the so-called first-order system least-squares (FOSLS) functional
studied in [3], which is based on a first-order system of scalar second-order
elliptic PDEs. Numerical results for second-order elliptic PDEs in one
dimension are presented.","35Q68,cs.LG,cs.NA,math.NA,physics.comp-ph,stat.ML"
"Information-Bottleneck-Based Behavior Representation Learning for Multi-agent Reinforcement learning. In multi-agent deep reinforcement learning, extracting sufficient and compact
information of other agents is critical to attain efficient convergence and
scalability of an algorithm. In canonical frameworks, distilling of such
information is often done in an implicit and uninterpretable manner, or
explicitly with cost functions not able to reflect the relationship between
information compression and utility in representation. In this paper, we
present Information-Bottleneck-based Other agents' behavior Representation
learning for Multi-agent reinforcement learning (IBORM) to explicitly seek
low-dimensional mapping encoder through which a compact and informative
representation relevant to other agents' behaviors is established. IBORM
leverages the information bottleneck principle to compress observation
information, while retaining sufficient information relevant to other agents'
behaviors used for cooperation decision. Empirical results have demonstrated
that IBORM delivers the fastest convergence rate and the best performance of
the learned policies, as compared with implicit behavior representation
learning and explicit behavior representation learning without explicitly
considering information compression and utility.","cs.IT,cs.LG,cs.MA,math.IT"
"FeatGraph: A Flexible and Efficient Backend for Graph Neural Network Systems. Graph neural networks (GNNs) are gaining increasing popularity as a promising
approach to machine learning on graphs. Unlike traditional graph workloads
where each vertex/edge is associated with a scalar, GNNs attach a feature
tensor to each vertex/edge. This additional feature dimension, along with
consequently more complex vertex- and edge-wise computations, has enormous
implications on locality and parallelism, which existing graph processing
systems fail to exploit.
  This paper proposes FeatGraph to accelerate GNN workloads by co-optimizing
graph traversal and feature dimension computation. FeatGraph provides a
flexible programming interface to express diverse GNN models by composing
coarse-grained sparse templates with fine-grained user-defined functions (UDFs)
on each vertex/edge. FeatGraph incorporates optimizations for graph traversal
into the sparse templates and allows users to specify optimizations for UDFs
with a feature dimension schedule (FDS). FeatGraph speeds up end-to-end GNN
training and inference by up to 32x on CPU and 7x on GPU.","cs.DC,cs.LG"
"Efficient channel charting via phase-insensitive distance computation. Channel charting is an unsupervised learning task whose objective is to
encode channels so that the obtained representation reflects the relative
spatial locations of the corresponding users. It has many potential
applications, ranging from user scheduling to proactive handover. In this
paper, a channel charting method is proposed, based on a distance measure
specifically designed to reduce the effect of small scale fading, which is an
irrelevant phenomenon with respect to the channel charting task. A nonlinear
dimensionality reduction technique aimed at preserving local distances (Isomap)
is then applied to actually get the channel representation. The approach is
empirically validated on realistic synthetic \new{multipath} MIMO channels,
achieving better results than previously proposed approaches, at a lower cost.","cs.LG,cs.NI,eess.SP"
"PISA: Pixelwise Image Saliency by Aggregating Complementary Appearance Contrast Measures with Edge-Preserving Coherence. Driven by recent vision and graphics applications such as image segmentation
and object recognition, computing pixel-accurate saliency values to uniformly
highlight foreground objects becomes increasingly important. In this paper, we
propose a unified framework called PISA, which stands for Pixelwise Image
Saliency Aggregating various bottom-up cues and priors. It generates spatially
coherent yet detail-preserving, pixel-accurate and fine-grained saliency, and
overcomes the limitations of previous methods which use homogeneous
superpixel-based and color only treatment. PISA aggregates multiple saliency
cues in a global context such as complementary color and structure contrast
measures with their spatial priors in the image domain. The saliency confidence
is further jointly modeled with a neighborhood consistence constraint into an
energy minimization formulation, in which each pixel will be evaluated with
multiple hypothetical saliency levels. Instead of using global discrete
optimization methods, we employ the cost-volume filtering technique to solve
our formulation, assigning the saliency levels smoothly while preserving the
edge-aware structure details. In addition, a faster version of PISA is
developed using a gradient-driven image sub-sampling strategy to greatly
improve the runtime efficiency while keeping comparable detection accuracy.
Extensive experiments on a number of public datasets suggest that PISA
convincingly outperforms other state-of-the-art approaches. In addition, with
this work we also create a new dataset containing $800$ commodity images for
evaluating saliency detection. The dataset and source code of PISA can be
downloaded at http://vision.sysu.edu.cn/project/PISA/","68U10,cs.CV"
"Physics-informed learning of governing equations from scarce data. Harnessing data to discover the underlying governing laws or equations that
describe the behavior of complex physical systems can significantly advance our
modeling, simulation and understanding of such systems in various science and
engineering disciplines. This work introduces a novel physics-informed deep
learning framework to discover governing partial differential equations (PDEs)
from scarce and noisy data for nonlinear spatiotemporal systems. In particular,
this approach seamlessly integrates the strengths of deep neural networks for
rich representation learning, physics embedding, automatic differentiation and
sparse regression to (1) approximate the solution of system variables, (2)
compute essential derivatives, as well as (3) identify the key derivative terms
and parameters that form the structure and explicit expression of the PDEs. The
efficacy and robustness of this method are demonstrated, both numerically and
experimentally, on discovering a variety of PDE systems with different levels
of data scarcity and noise accounting for different initial/boundary
conditions. The resulting computational framework shows the potential for
closed-form model discovery in practical applications where large and accurate
datasets are intractable to capture.","cs.LG,physics.comp-ph,physics.data-an,stat.ML"
"Model-Agnostic Counterfactual Explanations for Consequential Decisions. Predictive models are being increasingly used to support consequential
decision making at the individual level in contexts such as pretrial bail and
loan approval. As a result, there is increasing social and legal pressure to
provide explanations that help the affected individuals not only to understand
why a prediction was output, but also how to act to obtain a desired outcome.
To this end, several works have proposed optimization-based methods to generate
nearest counterfactual explanations. However, these methods are often
restricted to a particular subset of models (e.g., decision trees or linear
models) and differentiable distance functions. In contrast, we build on
standard theory and tools from formal verification and propose a novel
algorithm that solves a sequence of satisfiability problems, where both the
distance function (objective) and predictive model (constraints) are
represented as logic formulae. As shown by our experiments on real-world data,
our algorithm is: i) model-agnostic ({non-}linear, {non-}differentiable,
{non-}convex); ii) data-type-agnostic (heterogeneous features); iii)
distance-agnostic ($\ell_0, \ell_1, \ell_\infty$, and combinations thereof);
iv) able to generate plausible and diverse counterfactuals for any sample
(i.e., 100% coverage); and v) at provably optimal distances.","cs.AI,cs.LG,cs.LO,stat.ML"
"The Distance Transform and its Computation. Distance transformation is an image processing technique used for many
different applications. Related to a binary image, the general idea is to
determine the distance of all background points to the nearest object point (or
vice versa). In this tutorial, different approaches are explained in detail and
compared using examples. Corresponding source code is provided to facilitate
own investigations. A particular objective of this tutorial is to clarify the
difference between arbitrary distance transforms and exact Euclidean distance
transformations.","cs.CG,cs.CV"
"Multiresolution Tree Networks for 3D Point Cloud Processing. We present multiresolution tree-structured networks to process point clouds
for 3D shape understanding and generation tasks. Our network represents a 3D
shape as a set of locality-preserving 1D ordered list of points at multiple
resolutions. This allows efficient feed-forward processing through 1D
convolutions, coarse-to-fine analysis through a multi-grid architecture, and it
leads to faster convergence and small memory footprint during training. The
proposed tree-structured encoders can be used to classify shapes and outperform
existing point-based architectures on shape classification benchmarks, while
tree-structured decoders can be used for generating point clouds directly and
they outperform existing approaches for image-to-shape inference tasks learned
using the ShapeNet dataset. Our model also allows unsupervised learning of
point-cloud based shapes by using a variational autoencoder, leading to
higher-quality generated shapes.","cs.CV,cs.GR,cs.LG"
"Deep composition of tensor-trains using squared inverse Rosenblatt transports. Characterising intractable high-dimensional random variables is one of the
fundamental challenges in stochastic computation. The recent surge of transport
maps offers a mathematical foundation and new insights for tackling this
challenge by coupling intractable random variables with tractable reference
random variables. This paper generalises the functional tensor-train
approximation of the inverse Rosenblatt transport recently developed by Dolgov
et al. (Stat Comput 30:603--625, 2020) to a wide class of high-dimensional
non-negative functions, such as unnormalised probability density functions.
First, we extend the inverse Rosenblatt transform to enable the transport to
general reference measures other than the uniform measure. We develop an
efficient procedure to compute this transport from a squared tensor-train
decomposition which preserves the monotonicity. More crucially, we integrate
the proposed order-preserving functional tensor-train transport into a nested
variable transformation framework inspired by the layered structure of deep
neural networks. The resulting deep inverse Rosenblatt transport significantly
expands the capability of tensor approximations and transport maps to random
variables with complicated nonlinear interactions and concentrated density
functions. We demonstrate the efficiency of the proposed approach on a range of
applications in statistical learning and uncertainty quantification, including
parameter estimation for dynamical systems and inverse problems constrained by
partial differential equations.","cs.LG,cs.NA,math.NA,stat.CO,stat.ML"
"Agent Environment Cycle Games. Partially Observable Stochastic Games (POSGs) are the most general and common
model of games used in Multi-Agent Reinforcement Learning (MARL). We argue that
the POSG model is conceptually ill suited to software MARL environments, and
offer case studies from the literature where this mismatch has led to severely
unexpected behavior. In response to this, we introduce the Agent Environment
Cycle Games (AEC Games) model, which is more representative of software
implementation. We then prove it's as an equivalent model to POSGs. The AEC
games model is also uniquely useful in that it can elegantly represent both all
forms of MARL environments, whereas for example POSGs cannot elegantly
represent strictly turn based games like chess.","cs.AI,cs.GT,cs.LG,cs.MA,stat.ML"
"DDP-GCN: Multi-Graph Convolutional Network for Spatiotemporal Traffic Forecasting. Traffic speed forecasting is one of the core problems in Intelligent
Transportation Systems. For a more accurate prediction, recent studies started
using not only the temporal speed patterns but also the spatial information on
the road network through the graph convolutional networks. Even though the road
network is highly complex due to its non-Euclidean and directional
characteristics, previous approaches mainly focus on modeling the spatial
dependencies only with the distance. In this paper, we identify two essential
spatial dependencies in traffic forecasting in addition to distance, direction
and positional relationship, for designing basic graph elements as the smallest
building blocks. Using the building blocks, we suggest DDP-GCN (Distance,
Direction, and Positional relationship Graph Convolutional Network) to
incorporate the three spatial relationships into prediction network for traffic
forecasting. We evaluate the proposed model with two large-scale real-world
datasets, and find 7.40% average improvement for 1-hour forecasting in highly
complex urban networks.","cs.LG,eess.SP"
"Bootstrapped Representation Learning on Graphs. Current state-of-the-art self-supervised learning methods for graph neural
networks (GNNs) are based on contrastive learning. As such, they heavily depend
on the construction of augmentations and negative examples. For example, on the
standard PPI benchmark, increasing the number of negative pairs improves
performance, thereby requiring computation and memory cost quadratic in the
number of nodes to achieve peak performance. Inspired by BYOL, a recently
introduced method for self-supervised learning that does not require negative
pairs, we present Bootstrapped Graph Latents, BGRL, a self-supervised graph
representation method that gets rid of this potentially quadratic bottleneck.
BGRL outperforms or matches the previous unsupervised state-of-the-art results
on several established benchmark datasets. Moreover, it enables the effective
usage of graph attentional (GAT) encoders, allowing us to further improve the
state of the art. In particular on the PPI dataset, using GAT as an encoder we
achieve state-of-the-art 70.49% Micro-F1, using the linear evaluation protocol.
On all other datasets under consideration, our model is competitive with the
equivalent supervised GNN results, often exceeding them.","cs.LG,cs.SI,stat.ML"
"Free Energy and the Generalized Optimality Equations for Sequential Decision Making. The free energy functional has recently been proposed as a variational
principle for bounded rational decision-making, since it instantiates a natural
trade-off between utility gains and information processing costs that can be
axiomatically derived. Here we apply the free energy principle to general
decision trees that include both adversarial and stochastic environments. We
derive generalized sequential optimality equations that not only include the
Bellman optimality equations as a limit case, but also lead to well-known
decision-rules such as Expectimax, Minimax and Expectiminimax. We show how
these decision-rules can be derived from a single free energy principle that
assigns a resource parameter to each node in the decision tree. These resource
parameters express a concrete computational cost that can be measured as the
amount of samples that are needed from the distribution that belongs to each
node. The free energy principle therefore provides the normative basis for
generalized optimality equations that account for both adversarial and
stochastic environments.","cs.AI,cs.GT,cs.SY,stat.ML"
"Crowdsourcing in Computer Vision. Computer vision systems require large amounts of manually annotated data to
properly learn challenging visual concepts. Crowdsourcing platforms offer an
inexpensive method to capture human knowledge and understanding, for a vast
number of visual perception tasks. In this survey, we describe the types of
annotations computer vision researchers have collected using crowdsourcing, and
how they have ensured that this data is of high quality while annotation effort
is minimized. We begin by discussing data collection on both classic (e.g.,
object recognition) and recent (e.g., visual story-telling) vision tasks. We
then summarize key design decisions for creating effective data collection
interfaces and workflows, and present strategies for intelligently selecting
the most important data instances to annotate. Finally, we conclude with some
thoughts on the future of crowdsourcing in computer vision.","cs.CV,cs.HC"
"Transferable and Distributed User Association Policies for 5G and Beyond Networks. We study the problem of user association, namely finding the optimal
assignment of user equipment to base stations to achieve a targeted network
performance. In this paper, we focus on the knowledge transferability of
association policies. Indeed, traditional non-trivial user association schemes
are often scenario-specific or deployment-specific and require a policy
re-design or re-learning when the number or the position of the users change.
In contrast, transferability allows to apply a single user association policy,
devised for a specific scenario, to other distinct user deployments, without
needing a substantial re-learning or re-design phase and considerably reducing
its computational and management complexity. To achieve transferability, we
first cast user association as a multi-agent reinforcement learning problem.
Then, based on a neural attention mechanism that we specifically conceived for
this context, we propose a novel distributed policy network architecture, which
is transferable among users with zero-shot generalization capability i.e.,
without requiring additional training.Numerical results show the effectiveness
of our solution in terms of overall network communication rate, outperforming
centralized benchmarks even when the number of users doubles with respect to
the initial training point.","cs.LG,cs.MA,cs.NI"
"Rotation Invariant Graph Neural Networks using Spin Convolutions. Progress towards the energy breakthroughs needed to combat climate change can
be significantly accelerated through the efficient simulation of atomic
systems. Simulation techniques based on first principles, such as Density
Functional Theory (DFT), are limited in their practical use due to their high
computational expense. Machine learning approaches have the potential to
approximate DFT in a computationally efficient manner, which could dramatically
increase the impact of computational simulations on real-world problems.
Approximating DFT poses several challenges. These include accurately modeling
the subtle changes in the relative positions and angles between atoms, and
enforcing constraints such as rotation invariance or energy conservation. We
introduce a novel approach to modeling angular information between sets of
neighboring atoms in a graph neural network. Rotation invariance is achieved
for the network's edge messages through the use of a per-edge local coordinate
frame and a novel spin convolution over the remaining degree of freedom. Two
model variants are proposed for the applications of structure relaxation and
molecular dynamics. State-of-the-art results are demonstrated on the
large-scale Open Catalyst 2020 dataset. Comparisons are also performed on the
MD17 and QM9 datasets.","I.2.6; J.2,cs.CE,cs.LG"
"Multimodal Polynomial Fusion for Detecting Driver Distraction. Distracted driving is deadly, claiming 3,477 lives in the U.S. in 2015 alone.
Although there has been a considerable amount of research on modeling the
distracted behavior of drivers under various conditions, accurate automatic
detection using multiple modalities and especially the contribution of using
the speech modality to improve accuracy has received little attention. This
paper introduces a new multimodal dataset for distracted driving behavior and
discusses automatic distraction detection using features from three modalities:
facial expression, speech and car signals. Detailed multimodal feature analysis
shows that adding more modalities monotonically increases the predictive
accuracy of the model. Finally, a simple and effective multimodal fusion
technique using a polynomial fusion layer shows superior distraction detection
results compared to the baseline SVM and neural network models.","cs.AI,cs.CV,cs.HC"
"PointCutMix: Regularization Strategy for Point Cloud Classification. As 3D point cloud analysis has received increasing attention, the
insufficient scale of point cloud datasets and the weak generalization ability
of networks become prominent. In this paper, we propose a simple and effective
augmentation method for the point cloud data, named PointCutMix, to alleviate
those problems. It finds the optimal assignment between two point clouds and
generates new training data by replacing the points in one sample with their
optimal assigned pairs. Two replacement strategies are proposed to adapt to the
accuracy or robustness requirement for different tasks, one of which is to
randomly select all replacing points while the other one is to select k nearest
neighbors of a single random point. Both strategies consistently and
significantly improve the performance of various models on point cloud
classification problems. By introducing the saliency maps to guide the
selection of replacing points, the performance further improves. Moreover,
PointCutMix is validated to enhance the model robustness against the point
attack. It is worth noting that when using as a defense method, our method
outperforms the state-of-the-art defense algorithms. The code is available
at:https://github.com/cuge1995/PointCutMix",cs.CV
"Cross-Shape Graph Convolutional Networks. We present a method that processes 3D point clouds by performing graph
convolution operations across shapes. In this manner, point descriptors are
learned by allowing interaction and propagation of feature representations
within a shape collection. To enable this form of non-local, cross-shape graph
convolution, our method learns a pairwise point attention mechanism indicating
the degree of interaction between points on different shapes. Our method also
learns to create a graph over shapes of an input collection whose edges connect
shapes deemed as useful for performing cross-shape convolution. The edges are
also equipped with learned weights indicating the compatibility of each shape
pair for cross-shape convolution. Our experiments demonstrate that this
interaction and propagation of point representations across shapes make them
more discriminative. In particular, our results show significantly improved
performance for 3D point cloud semantic segmentation compared to conventional
approaches, especially in cases with the limited number of training examples.","cs.CV,cs.GR,cs.LG,eess.IV"
"Conjugate Variables as a Resource in Signal and Image Processing. In this paper we develop a new technique to model joint distributions of
signals. Our technique is based on quantum mechanical conjugate variables. We
show that the transition probability of quantum states leads to a distance
function on the signals. This distance function obeys the triangle inequality
on all quantum states and becomes a metric on pure quantum states. Treating
signals as conjugate variables allows us to create a new approach to segment
them.
  Keywords: Quantum information, transition probability, Euclidean distance,
Fubini-study metric, Bhattacharyya coefficients, conjugate variable,
signal/sensor fusion, signal and image segmentation.","cs.CV,physics.data-an,quant-ph"
"Augmented Semantic Signatures of Airborne LiDAR Point Clouds for Comparison. LiDAR point clouds provide rich geometric information, which is particularly
useful for the analysis of complex scenes of urban regions. Finding structural
and semantic differences between two different three-dimensional point clouds,
say, of the same region but acquired at different time instances is an
important problem. A comparison of point clouds involves computationally
expensive registration and segmentation. We are interested in capturing the
relative differences in the geometric uncertainty and semantic content of the
point cloud without the registration process. Hence, we propose an
orientation-invariant geometric signature of the point cloud, which integrates
its probabilistic geometric and semantic classifications. We study different
properties of the geometric signature, which are an image-based encoding of
geometric uncertainty and semantic content. We explore different metrics to
determine differences between these signatures, which in turn compare point
clouds without performing point-to-point registration. Our results show that
the differences in the signatures corroborate with the geometric and semantic
differences of the point clouds.","65C50,68U05,68U10,68U20,G.1.3; I.4.10; I.4.8; J.2,cs.CV,cs.GR,eess.IV"
"Explainable Deep One-Class Classification. Deep one-class classification variants for anomaly detection learn a mapping
that concentrates nominal samples in feature space causing anomalies to be
mapped away. Because this transformation is highly non-linear, finding
interpretations poses a significant challenge. In this paper we present an
explainable deep one-class classification method, Fully Convolutional Data
Description (FCDD), where the mapped samples are themselves also an explanation
heatmap. FCDD yields competitive detection performance and provides reasonable
explanations on common anomaly detection benchmarks with CIFAR-10 and ImageNet.
On MVTec-AD, a recent manufacturing dataset offering ground-truth anomaly maps,
FCDD sets a new state of the art in the unsupervised setting. Our method can
incorporate ground-truth anomaly maps during training and using even a few of
these (~5) improves performance significantly. Finally, using FCDD's
explanations we demonstrate the vulnerability of deep one-class classification
models to spurious image features such as image watermarks.","cs.CV,cs.LG,stat.ML"
"Mutual Information Scaling and Expressive Power of Sequence Models. Sequence models assign probabilities to variable-length sequences such as
natural language texts. The ability of sequence models to capture temporal
dependence can be characterized by the temporal scaling of correlation and
mutual information. In this paper, we study the mutual information of recurrent
neural networks (RNNs) including long short-term memories and self-attention
networks such as Transformers. Through a combination of theoretical study of
linear RNNs and empirical study of nonlinear RNNs, we find their mutual
information decays exponentially in temporal distance. On the other hand,
Transformers can capture long-range mutual information more efficiently, making
them preferable in modeling sequences with slow power-law mutual information,
such as natural languages and stock prices. We discuss the connection of these
results with statistical mechanics. We also point out the non-uniformity
problem in many natural language datasets. We hope this work provides a new
perspective in understanding the expressive power of sequence models and shed
new light on improving the architecture of them.","cond-mat.dis-nn,cs.IT,cs.LG,math.IT,stat.ML"
"A game-theoretic analysis of networked system control for common-pool resource management using multi-agent reinforcement learning. Multi-agent reinforcement learning has recently shown great promise as an
approach to networked system control. Arguably, one of the most difficult and
important tasks for which large scale networked system control is applicable is
common-pool resource management. Crucial common-pool resources include arable
land, fresh water, wetlands, wildlife, fish stock, forests and the atmosphere,
of which proper management is related to some of society's greatest challenges
such as food security, inequality and climate change. Here we take inspiration
from a recent research program investigating the game-theoretic incentives of
humans in social dilemma situations such as the well-known tragedy of the
commons. However, instead of focusing on biologically evolved human-like
agents, our concern is rather to better understand the learning and operating
behaviour of engineered networked systems comprising general-purpose
reinforcement learning agents, subject only to nonbiological constraints such
as memory, computation and communication bandwidth. Harnessing tools from
empirical game-theoretic analysis, we analyse the differences in resulting
solution concepts that stem from employing different information structures in
the design of networked multi-agent systems. These information structures
pertain to the type of information shared between agents as well as the
employed communication protocol and network topology. Our analysis contributes
new insights into the consequences associated with certain design choices and
provides an additional dimension of comparison between systems beyond
efficiency, robustness, scalability and mean control performance.","cs.GT,cs.LG,cs.MA"
"Using Transfer Learning for Image-Based Cassava Disease Detection. Cassava is the third largest source of carbohydrates for human food in the
world but is vulnerable to virus diseases, which threaten to destabilize food
security in sub-Saharan Africa. Novel methods of cassava disease detection are
needed to support improved control which will prevent this crisis. Image
recognition offers both a cost effective and scalable technology for disease
detection. New transfer learning methods offer an avenue for this technology to
be easily deployed on mobile devices. Using a dataset of cassava disease images
taken in the field in Tanzania, we applied transfer learning to train a deep
convolutional neural network to identify three diseases and two types of pest
damage (or lack thereof). The best trained model accuracies were 98% for brown
leaf spot (BLS), 96% for red mite damage (RMD), 95% for green mite damage
(GMD), 98% for cassava brown streak disease (CBSD), and 96% for cassava mosaic
disease (CMD). The best model achieved an overall accuracy of 93% for data not
used in the training process. Our results show that the transfer learning
approach for image recognition of field images offers a fast, affordable, and
easily deployable strategy for digital plant disease detection.","cs.CV,cs.CY"
"Variational inference for the multi-armed contextual bandit. In many biomedical, science, and engineering problems, one must sequentially
decide which action to take next so as to maximize rewards. One general class
of algorithms for optimizing interactions with the world, while simultaneously
learning how the world operates, is the multi-armed bandit setting and, in
particular, the contextual bandit case. In this setting, for each executed
action, one observes rewards that are dependent on a given 'context', available
at each interaction with the world. The Thompson sampling algorithm has
recently been shown to enjoy provable optimality properties for this set of
problems, and to perform well in real-world settings. It facilitates generative
and interpretable modeling of the problem at hand. Nevertheless, the design and
complexity of the model limit its application, since one must both sample from
the distributions modeled and calculate their expected rewards. We here show
how these limitations can be overcome using variational inference to
approximate complex models, applying to the reinforcement learning case
advances developed for the inference case in the machine learning community
over the past two decades. We consider contextual multi-armed bandit
applications where the true reward distribution is unknown and complex, which
we approximate with a mixture model whose parameters are inferred via
variational inference. We show how the proposed variational Thompson sampling
approach is accurate in approximating the true distribution, and attains
reduced regrets even with complex reward distributions. The proposed algorithm
is valuable for practical scenarios where restrictive modeling assumptions are
undesirable.","I.2.6,cs.LG,stat.CO,stat.ML"
"Evolutionary Reinforcement Learning for Sample-Efficient Multiagent Coordination. Many cooperative multiagent reinforcement learning environments provide
agents with a sparse team-based reward, as well as a dense agent-specific
reward that incentivizes learning basic skills. Training policies solely on the
team-based reward is often difficult due to its sparsity. Furthermore, relying
solely on the agent-specific reward is sub-optimal because it usually does not
capture the team coordination objective. A common approach is to use reward
shaping to construct a proxy reward by combining the individual rewards.
However, this requires manual tuning for each environment. We introduce
Multiagent Evolutionary Reinforcement Learning (MERL), a split-level training
platform that handles the two objectives separately through two optimization
processes. An evolutionary algorithm maximizes the sparse team-based objective
through neuroevolution on a population of teams. Concurrently, a gradient-based
optimizer trains policies to only maximize the dense agent-specific rewards.
The gradient-based policies are periodically added to the evolutionary
population as a way of information transfer between the two optimization
processes. This enables the evolutionary algorithm to use skills learned via
the agent-specific rewards toward optimizing the global objective. Results
demonstrate that MERL significantly outperforms state-of-the-art methods, such
as MADDPG, on a number of difficult coordination benchmarks.","cs.AI,cs.LG,cs.MA,stat.ML"
"Graph Spectral Feature Learning for Mixed Data of Categorical and Numerical Type. Feature learning in the presence of a mixed type of variables, numerical and
categorical types, is an important issue for related modeling problems. For
simple neighborhood queries under mixed data space, standard practice is to
consider numerical and categorical variables separately and combining them
based on some suitable distance functions. Alternatives, such as Kernel
learning or Principal Component do not explicitly consider the inter-dependence
structure among the mixed type of variables. In this work, we propose a novel
strategy to explicitly model the probabilistic dependence structure among the
mixed type of variables by an undirected graph. Spectral decomposition of the
graph Laplacian provides the desired feature transformation. The Eigen spectrum
of the transformed feature space shows increased separability and more
prominent clusterability among the observations. The main novelty of our paper
lies in capturing interactions of the mixed feature type in an unsupervised
framework using a graphical model. We numerically validate the implications of
the feature learning strategy","cs.LG,stat.AP,stat.ML"
"Learning to Maximize Influence. As the field of machine learning for combinatorial optimization advances,
traditional problems are resurfaced and readdressed through this new
perspective. The overwhelming majority of the literature focuses on small graph
problems, while several real-world problems are devoted to large graphs. Here,
we focus on two such problems that are related: influence estimation, a
\#P-hard counting problem, and influence maximization, an NP-hard problem. We
develop GLIE, a Graph Neural Network (GNN) that inherently parameterizes an
upper bound of influence estimation and train it on small simulated graphs.
Experiments show that GLIE can provide accurate predictions faster than the
alternatives for graphs 10 times larger than the train set. More importantly,
it can be used on arbitrary large graphs for influence maximization, as the
predictions can rank effectively seed sets even when the accuracy deteriorates.
To showcase this, we propose a version of a standard Influence Maximization
(IM) algorithm where we substitute traditional influence estimation with the
predictions of GLIE.We also transfer GLIE into a reinforcement learning model
that learns how to choose seeds to maximize influence sequentially using GLIE's
hidden representations and predictions. The final results show that the
proposed methods surpasses a previous GNN-RL approach and perform on par with a
state-of-the-art IM algorithm.","cs.AI,cs.LG,cs.SI"
"Particle methods enable fast and simple approximation of Sobolev gradients in image segmentation. Bio-image analysis is challenging due to inhomogeneous intensity
distributions and high levels of noise in the images. Bayesian inference
provides a principled way for regularizing the problem using prior knowledge. A
fundamental choice is how one measures ""distances"" between shapes in an image.
It has been shown that the straightforward geometric L2 distance is degenerate
and leads to pathological situations. This is avoided when using Sobolev
gradients, rendering the segmentation problem less ill-posed. The high
computational cost and implementation overhead of Sobolev gradients, however,
have hampered practical applications. We show how particle methods as applied
to image segmentation allow for a simple and computationally efficient
implementation of Sobolev gradients. We show that the evaluation of Sobolev
gradients amounts to particle-particle interactions along the contour in an
image. We extend an existing particle-based segmentation algorithm to using
Sobolev gradients. Using synthetic and real-world images, we benchmark the
results for both 2D and 3D images using piecewise smooth and piecewise constant
region models. The present particle approximation of Sobolev gradients is 2.8
to 10 times faster than the previous reference implementation, but retains the
known favorable properties of Sobolev gradients. This speedup is achieved by
using local particle-particle interactions instead of solving a global Poisson
equation at each iteration. The computational time per iteration is higher for
Sobolev gradients than for L2 gradients. Since Sobolev gradients precondition
the optimization problem, however, a smaller number of overall iterations may
be necessary for the algorithm to converge, which can in some cases amortize
the higher per-iteration cost.","cs.CE,cs.CV,cs.NA,q-bio.QM"
"Conformal k-NN Anomaly Detector for Univariate Data Streams. Anomalies in time-series data give essential and often actionable information
in many applications. In this paper we consider a model-free anomaly detection
method for univariate time-series which adapts to non-stationarity in the data
stream and provides probabilistic abnormality scores based on the conformal
prediction paradigm. Despite its simplicity the method performs on par with
complex prediction-based models on the Numenta Anomaly Detection benchmark and
the Yahoo! S5 dataset.","cs.DS,stat.AP,stat.CO,stat.ME,stat.ML"
"On Relating 'Why?' and 'Why Not?' Explanations. Explanations of Machine Learning (ML) models often address a 'Why?' question.
Such explanations can be related with selecting feature-value pairs which are
sufficient for the prediction. Recent work has investigated explanations that
address a 'Why Not?' question, i.e. finding a change of feature values that
guarantee a change of prediction. Given their goals, these two forms of
explaining predictions of ML models appear to be mostly unrelated. However,
this paper demonstrates otherwise, and establishes a rigorous formal
relationship between 'Why?' and 'Why Not?' explanations. Concretely, the paper
proves that, for any given instance, 'Why?' explanations are minimal hitting
sets of 'Why Not?' explanations and vice-versa. Furthermore, the paper devises
novel algorithms for extracting and enumerating both forms of explanations.","cs.AI,cs.LG,cs.LO"
"Meta Decision Trees for Explainable Recommendation Systems. We tackle the problem of building explainable recommendation systems that are
based on a per-user decision tree, with decision rules that are based on single
attribute values. We build the trees by applying learned regression functions
to obtain the decision rules as well as the values at the leaf nodes. The
regression functions receive as input the embedding of the user's training set,
as well as the embedding of the samples that arrive at the current node. The
embedding and the regressors are learned end-to-end with a loss that encourages
the decision rules to be sparse. By applying our method, we obtain a
collaborative filtering solution that provides a direct explanation to every
rating it provides. With regards to accuracy, it is competitive with other
algorithms. However, as expected, explainability comes at a cost and the
accuracy is typically slightly lower than the state of the art result reported
in the literature.","cs.IR,cs.LG,stat.ML"
"Semi-Supervised Speech Recognition via Graph-based Temporal Classification. Semi-supervised learning has demonstrated promising results in automatic
speech recognition (ASR) by self-training using a seed ASR model with
pseudo-labels generated for unlabeled data. The effectiveness of this approach
largely relies on the pseudo-label accuracy, for which typically only the
1-best ASR hypothesis is used. However, alternative ASR hypotheses of an N-best
list can provide more accurate labels for an unlabeled speech utterance and
also reflect uncertainties of the seed ASR model. In this paper, we propose a
generalized form of the connectionist temporal classification (CTC) objective
that accepts a graph representation of the training labels. The newly proposed
graph-based temporal classification (GTC) objective is applied for
self-training with WFST-based supervision, which is generated from an N-best
list of pseudo-labels. In this setup, GTC is used to learn not only a temporal
alignment, similarly to CTC, but also a label alignment to obtain the optimal
pseudo-label sequence from the weighted graph. Results show that this approach
can effectively exploit an N-best list of pseudo-labels with associated scores,
considerably outperforming standard pseudo-labeling, with ASR results
approaching an oracle experiment in which the best hypotheses of the N-best
lists are selected manually.","cs.CL,cs.LG,cs.SD,eess.AS"
"Attentive Representation Learning with Adversarial Training for Short Text Clustering. Short text clustering has far-reaching effects on semantic analysis, showing
its importance for multiple applications such as corpus summarization and
information retrieval. However, it inevitably encounters the severe sparsity of
short text representations, making the previous clustering approaches still far
from satisfactory. In this paper, we present a novel attentive representation
learning model for shot text clustering, wherein cluster-level attention is
proposed to capture the correlations between text representations and cluster
representations. Relying on this, the representation learning and clustering
for short texts are seamlessly integrated into a unified model. To further
ensure robust model training for short texts, we apply adversarial training to
the unsupervised clustering setting, by injecting perturbations into the
cluster representations. The model parameters and perturbations are optimized
alternately through a minimax game. Extensive experiments on four real-world
short text datasets demonstrate the superiority of the proposed model over
several strong competitors, verifying that robust adversarial training yields
substantial performance gains.","cs.CL,cs.IR,cs.LG"
"US-Cut: Interactive Algorithm for rapid Detection and Segmentation of Liver Tumors in Ultrasound Acquisitions. Ultrasound (US) is the most commonly used liver imaging modality worldwide.
It plays an important role in follow-up of cancer patients with liver
metastases. We present an interactive segmentation approach for liver tumors in
US acquisitions. Due to the low image quality and the low contrast between the
tumors and the surrounding tissue in US images, the segmentation is very
challenging. Thus, the clinical practice still relies on manual measurement and
outlining of the tumors in the US images. We target this problem by applying an
interactive segmentation algorithm to the US data, allowing the user to get
real-time feedback of the segmentation results. The algorithm has been
developed and tested hand-in-hand by physicians and computer scientists to make
sure a future practical usage in a clinical setting is feasible. To cover
typical acquisitions from the clinical routine, the approach has been evaluated
with dozens of datasets where the tumors are hyperechoic (brighter), hypoechoic
(darker) or isoechoic (similar) in comparison to the surrounding liver tissue.
Due to the interactive real-time behavior of the approach, it was possible even
in difficult cases to find satisfying segmentations of the tumors within
seconds and without parameter settings, and the average tumor deviation was
only 1.4mm compared with manual measurements. However, the long term goal is to
ease the volumetric acquisition of liver tumors in order to evaluate for
treatment response. Additional aim is the registration of intraoperative US
images via the interactive segmentations to the patient's pre-interventional CT
acquisitions.","cs.CE,cs.CG,cs.CV,cs.GR"
"Optical Flow Techniques for Facial Expression Analysis -- a Practical Evaluation Study. Optical flow techniques are becoming increasingly performant and robust when
estimating motion in a scene, but their performance has yet to be proven in the
area of facial expression recognition. In this work, a variety of optical flow
approaches are evaluated across multiple facial expression datasets, so as to
provide a consistent performance evaluation. The aim of this work is not to
propose a new expression recognition technique, but to understand better the
adequacy of existing state-of-the art optical flow for encoding facial motion
in the context of facial expression recognition. Our evaluations highlight the
fact that motion approximation methods used to overcome motion discontinuities
have a significant impact when optical flows are used to characterize facial
expressions.",cs.CV
"Fairness Violations and Mitigation under Covariate Shift. We study the problem of learning fair prediction models for unseen test sets
distributed differently from the train set. Stability against changes in data
distribution is an important mandate for responsible deployment of models. The
domain adaptation literature addresses this concern, albeit with the notion of
stability limited to that of prediction accuracy. We identify sufficient
conditions under which stable models, both in terms of prediction accuracy and
fairness, can be learned. Using the causal graph describing the data and the
anticipated shifts, we specify an approach based on feature selection that
exploits conditional independencies in the data to estimate accuracy and
fairness metrics for the test set. We show that for specific fairness
definitions, the resulting model satisfies a form of worst-case optimality. In
context of a healthcare task, we illustrate the advantages of the approach in
making more equitable decisions.","cs.CY,cs.LG,stat.ML"
"Collaborative Graph Learning with Auxiliary Text for Temporal Event Prediction in Healthcare. Accurate and explainable health event predictions are becoming crucial for
healthcare providers to develop care plans for patients. The availability of
electronic health records (EHR) has enabled machine learning advances in
providing these predictions. However, many deep learning based methods are not
satisfactory in solving several key challenges: 1) effectively utilizing
disease domain knowledge; 2) collaboratively learning representations of
patients and diseases; and 3) incorporating unstructured text. To address these
issues, we propose a collaborative graph learning model to explore
patient-disease interactions and medical domain knowledge. Our solution is able
to capture structural features of both patients and diseases. The proposed
model also utilizes unstructured text data by employing an attention regulation
strategy and then integrates attentive text features into a sequential learning
process. We conduct extensive experiments on two important healthcare problems
to show the competitive prediction performance of the proposed method compared
with various state-of-the-art models. We also confirm the effectiveness of
learned representations and model interpretability by a set of ablation and
case studies.","cs.AI,cs.IR,cs.LG"
"Incremental Visual-Inertial 3D Mesh Generation with Structural Regularities. Visual-Inertial Odometry (VIO) algorithms typically rely on a point cloud
representation of the scene that does not model the topology of the
environment. A 3D mesh instead offers a richer, yet lightweight, model.
Nevertheless, building a 3D mesh out of the sparse and noisy 3D landmarks
triangulated by a VIO algorithm often results in a mesh that does not fit the
real scene. In order to regularize the mesh, previous approaches decouple state
estimation from the 3D mesh regularization step, and either limit the 3D mesh
to the current frame or let the mesh grow indefinitely. We propose instead to
tightly couple mesh regularization and state estimation by detecting and
enforcing structural regularities in a novel factor-graph formulation. We also
propose to incrementally build the mesh by restricting its extent to the
time-horizon of the VIO optimization; the resulting 3D mesh covers a larger
portion of the scene than a per-frame approach while its memory usage and
computational complexity remain bounded. We show that our approach successfully
regularizes the mesh, while improving localization accuracy, when structural
regularities are present, and remains operational in scenes without
regularities.","cs.CG,cs.CV,cs.RO"
"DPOD: 6D Pose Object Detector and Refiner. In this paper we present a novel deep learning method for 3D object detection
and 6D pose estimation from RGB images. Our method, named DPOD (Dense Pose
Object Detector), estimates dense multi-class 2D-3D correspondence maps between
an input image and available 3D models. Given the correspondences, a 6DoF pose
is computed via PnP and RANSAC. An additional RGB pose refinement of the
initial pose estimates is performed using a custom deep learning-based
refinement scheme. Our results and comparison to a vast number of related works
demonstrate that a large number of correspondences is beneficial for obtaining
high-quality 6D poses both before and after refinement. Unlike other methods
that mainly use real data for training and do not train on synthetic
renderings, we perform evaluation on both synthetic and real training data
demonstrating superior results before and after refinement when compared to all
recent detectors. While being precise, the presented approach is still
real-time capable.","cs.CV,cs.RO"
"Single image super-resolution by approximated Heaviside functions. Image super-resolution is a process to enhance image resolution. It is widely
used in medical imaging, satellite imaging, target recognition, etc. In this
paper, we conduct continuous modeling and assume that the unknown image
intensity function is defined on a continuous domain and belongs to a space
with a redundant basis. We propose a new iterative model for single image
super-resolution based on an observation: an image is consisted of smooth
components and non-smooth components, and we use two classes of approximated
Heaviside functions (AHFs) to represent them respectively. Due to sparsity of
the non-smooth components, a $L_{1}$ model is employed. In addition, we apply
the proposed iterative model to image patches to reduce computation and
storage. Comparisons with some existing competitive methods show the
effectiveness of the proposed method.","cs.CV,cs.IT,math.IT,math.OC"
"Parameter Inference with Bifurcation Diagrams. Estimation of parameters in differential equation models can be achieved by
applying learning algorithms to quantitative time-series data. However,
sometimes it is only possible to measure qualitative changes of a system in
response to a controlled condition. In dynamical systems theory, such change
points are known as bifurcations and lie on a function of the controlled
condition called the bifurcation diagram. In this work, we propose a
gradient-based semi-supervised approach for inferring the parameters of
differential equations that produce a user-specified bifurcation diagram. The
cost function contains a supervised error term that is minimal when the model
bifurcations match the specified targets and an unsupervised bifurcation
measure which has gradients that push optimisers towards bifurcating parameter
regimes. The gradients can be computed without the need to differentiate
through the operations of the solver that was used to compute the diagram. We
demonstrate parameter inference with minimal models which explore the space of
saddle-node and pitchfork diagrams and the genetic toggle switch from synthetic
biology. Furthermore, the cost landscape allows us to organise models in terms
of topological and geometric equivalence.","cs.LG,math.DS,q-bio.QM"
"Efficient Methods for Unsupervised Learning of Probabilistic Models. In this thesis I develop a variety of techniques to train, evaluate, and
sample from intractable and high dimensional probabilistic models. Abstract
exceeds arXiv space limitations -- see PDF.","cs.AI,cs.IT,cs.LG,cs.NE,math.IT,physics.data-an"
"Learning CHARME models with neural networks. In this paper, we consider a model called CHARME (Conditional Heteroscedastic
Autoregressive Mixture of Experts), a class of generalized mixture of nonlinear
nonparametric AR-ARCH time series. Under certain Lipschitz-type conditions on
the autoregressive and volatility functions, we prove that this model is
stationary, ergodic and $\tau$-weakly dependent. These conditions are much
weaker than those presented in the literature that treats this model. Moreover,
this result forms the theoretical basis for deriving an asymptotic theory of
the underlying (non)parametric estimation, which we present for this model. As
an application, from the universal approximation property of neural networks
(NN), we develop a learning theory for the NN-based autoregressive functions of
the model, where the strong consistency and asymptotic normality of the
considered estimator of the NN weights and biases are guaranteed under weak
conditions.","cs.LG,math.ST,stat.ML,stat.TH"
"Deep Learning and Gaussian Process based Band Assignment in Dual Band Systems. We consider the band assignment (BA) problem in dual-band systems, where the
basestation (BS) chooses one of the two available frequency bands
(centimeter-wave and millimeter-wave bands) to communicate with the user
equipment (UE). While the millimeter-wave band might offer higher data rate,
there is a significant probability of outage during which the communication
should be carried on the (more reliable) centimeter-wave band. We consider two
variations of the BA problem, one-shot and sequential BA. For the former the BS
uses only the currently observed information to decide whether to switch to the
other frequency band, for the sequential BA, the BS uses a window of previously
observed information to predict the best band for a future time step. We
provide two approaches to solve the BA problem, (i) a deep learning approach
that is based on Long Short Term Memory and/or multi-layer Neural Networks, and
(ii) a Gaussian Process based approach, which relies on the assumption that the
channel states are jointly Gaussian. We compare the achieved performances to
several benchmarks in two environments: (i) a stochastic environment, and (ii)
microcellular outdoor channels obtained by ray-tracing. In general, the deep
learning solution shows superior performance in both environments.","cs.IT,cs.LG,math.IT,stat.ML"
"Meta Reinforcement Learning with Autonomous Inference of Subtask Dependencies. We propose and address a novel few-shot RL problem, where a task is
characterized by a subtask graph which describes a set of subtasks and their
dependencies that are unknown to the agent. The agent needs to quickly adapt to
the task over few episodes during adaptation phase to maximize the return in
the test phase. Instead of directly learning a meta-policy, we develop a
Meta-learner with Subtask Graph Inference(MSGI), which infers the latent
parameter of the task by interacting with the environment and maximizes the
return given the latent parameter. To facilitate learning, we adopt an
intrinsic reward inspired by upper confidence bound (UCB) that encourages
efficient exploration. Our experiment results on two grid-world domains and
StarCraft II environments show that the proposed method is able to accurately
infer the latent task parameter, and to adapt more efficiently than existing
meta RL and hierarchical RL methods.","cs.AI,cs.LG,stat.ML"
"MM-Deacon: Multimodal molecular domain embedding analysis via contrastive learning. Molecular representation learning plays an essential role in cheminformatics.
Recently, language model-based approaches have been popular as an alternative
to traditional expert-designed features to encode molecules. However, these
approaches only utilize a single modality for representing molecules. Driven by
the fact that a given molecule can be described through different modalities
such as Simplified Molecular Line Entry System (SMILES), The International
Union of Pure and Applied Chemistry (IUPAC), and The IUPAC International
Chemical Identifier (InChI), we propose a multimodal molecular embedding
generation approach called MM-Deacon (multimodal molecular domain embedding
analysis via contrastive learning). MM-Deacon is trained using SMILES and IUPAC
molecule representations as two different modalities. First, SMILES and IUPAC
strings are encoded by using two different transformer-based language models
independently, then the contrastive loss is utilized to bring these encoded
representations from different modalities closer to each other if they belong
to the same molecule, and to push embeddings farther from each other if they
belong to different molecules. We evaluate the robustness of our molecule
embeddings on molecule clustering, cross-modal molecule search, drug similarity
assessment and drug-drug interaction tasks.","cs.CL,cs.LG,physics.chem-ph"
"HipaccVX: Wedding of OpenVX and DSL-based Code Generation. Writing programs for heterogeneous platforms optimized for high performance
is hard since this requires the code to be tuned at a low level with
architecture-specific optimizations that are most times based on fundamentally
differing programming paradigms and languages. OpenVX promises to solve this
issue for computer vision applications with a royalty-free industry standard
that is based on a graph-execution model. Yet, the OpenVX' algorithm space is
constrained to a small set of vision functions. This hinders accelerating
computations that are not included in the standard.
  In this paper, we analyze OpenVX vision functions to find an orthogonal set
of computational abstractions. Based on these abstractions, we couple an
existing Domain-Specific Language (DSL) back end to the OpenVX environment and
provide language constructs to the programmer for the definition of
user-defined nodes. In this way, we enable optimizations that are not possible
to detect with OpenVX graph implementations using the standard computer vision
functions. These optimizations can double the throughput on an Nvidia GTX GPU
and decrease the resource usage of a Xilinx Zynq FPGA by 50% for our
benchmarks. Finally, we show that our proposed compiler framework, called
HipaccVX, can achieve better results than the state-of-the-art approaches
Nvidia VisionWorks and Halide-HLS.","cs.CV,cs.DC,cs.PL"
"Hyperdimensional Computing for Efficient Distributed Classification with Randomized Neural Networks. In the supervised learning domain, considering the recent prevalence of
algorithms with high computational cost, the attention is steering towards
simpler, lighter, and less computationally extensive training and inference
approaches. In particular, randomized algorithms are currently having a
resurgence, given their generalized elementary approach. By using randomized
neural networks, we study distributed classification, which can be employed in
situations were data cannot be stored at a central location nor shared. We
propose a more efficient solution for distributed classification by making use
of a lossy compression approach applied when sharing the local classifiers with
other agents. This approach originates from the framework of hyperdimensional
computing, and is adapted herein. The results of experiments on a collection of
datasets demonstrate that the proposed approach has usually higher accuracy
than local classifiers and getting close to the benchmark - the centralized
classifier. This work can be considered as the first step towards analyzing the
variegated horizon of distributed randomized neural networks.","cs.DC,cs.LG"
"Deterministic Completion of Rectangular Matrices Using Asymmetric Ramanujan Graphs: Exact and Stable Recovery. In this paper we study the matrix completion problem: Suppose $X \in {\mathbb
R}^{n_r \times n_c}$ is unknown except for a known upper bound $r$ on its rank.
By measuring a small number $m \ll n_r n_c$ of elements of $X$, is it possible
to recover $X$ exactly with noise-free measurements, or to construct a good
approximation of $X$ with noisy measurements? Existing solutions to these
problems involve sampling the elements uniformly and at random, and can
guarantee exact recovery of the unknown matrix only with high probability. In
this paper, we present a \textit{deterministic} sampling method for matrix
completion. We achieve this by choosing the sampling set as the edge set of an
asymmetric Ramanujan bigraph, and constrained nuclear norm minimization is the
recovery method. Specifically, we derive sufficient conditions under which the
unknown matrix is completed exactly with noise-free measurements, and is
approximately completed with noisy measurements, which we call ""stable""
completion.
  The conditions derived here are only sufficient and more restrictive than
random sampling. To study how close they are to being necessary, we conducted
numerical simulations on randomly generated low rank matrices, using the LPS
families of Ramanujan graphs. These simulations demonstrate two facts: (i) In
order to achieve exact completion, it appears sufficient to choose the degree
$d$ of the Ramanujan graph to be $\geq 3r$. (ii) There is a ""phase transition,""
whereby the likelihood of success suddenly drops from 100\% to 0\% if the rank
is increased by just one or two beyond a critical value. The phase transition
phenomenon is well-known and well-studied in vector recovery using
$\ell_1$-norm minimization. However, it is less studied in matrix completion
and nuclear norm minimization, and not much understood.","68T05,cs.LG,stat.ML"
"Cognitive Radio Network Throughput Maximization with Deep Reinforcement Learning. Radio Frequency powered Cognitive Radio Networks (RF-CRN) are likely to be
the eyes and ears of upcoming modern networks such as Internet of Things (IoT),
requiring increased decentralization and autonomous operation. To be considered
autonomous, the RF-powered network entities need to make decisions locally to
maximize the network throughput under the uncertainty of any network
environment. However, in complex and large-scale networks, the state and action
spaces are usually large, and existing Tabular Reinforcement Learning technique
is unable to find the optimal state-action policy quickly. In this paper, deep
reinforcement learning is proposed to overcome the mentioned shortcomings and
allow a wireless gateway to derive an optimal policy to maximize network
throughput. When benchmarked against advanced DQN techniques, our proposed DQN
configuration offers performance speedup of up to 1.8x with good overall
performance.","cs.LG,cs.NI,eess.SP,stat.ML"
"Lookahead optimizer improves the performance of Convolutional Autoencoders for reconstruction of natural images. Autoencoders are a class of artificial neural networks which have gained a
lot of attention in the recent past. Using the encoder block of an autoencoder
the input image can be compressed into a meaningful representation. Then a
decoder is employed to reconstruct the compressed representation back to a
version which looks like the input image. It has plenty of applications in the
field of data compression and denoising. Another version of Autoencoders (AE)
exist, called Variational AE (VAE) which acts as a generative model like GAN.
Recently, an optimizer was introduced which is known as lookahead optimizer
which significantly enhances the performances of Adam as well as SGD. In this
paper, we implement Convolutional Autoencoders (CAE) and Convolutional
Variational Autoencoders (CVAE) with lookahead optimizer (with Adam) and
compare them with the Adam (only) optimizer counterparts. For this purpose, we
have used a movie dataset comprising of natural images for the former case and
CIFAR100 for the latter case. We show that lookahead optimizer (with Adam)
improves the performance of CAEs for reconstruction of natural images.","cs.AI,cs.CV,cs.GR,cs.LG,physics.data-an"
"Deep Graph Generators: A Survey. Deep generative models have achieved great success in areas such as image,
speech, and natural language processing in the past few years. Thanks to the
advances in graph-based deep learning, and in particular graph representation
learning, deep graph generation methods have recently emerged with new
applications ranging from discovering novel molecular structures to modeling
social networks. This paper conducts a comprehensive survey on deep
learning-based graph generation approaches and classifies them into five broad
categories, namely, autoregressive, autoencoder-based, RL-based, adversarial,
and flow-based graph generators, providing the readers a detailed description
of the methods in each class. We also present publicly available source codes,
commonly used datasets, and the most widely utilized evaluation metrics.
Finally, we highlight the existing challenges and discuss future research
directions.","cs.AI,cs.LG,cs.SI"
"XGBoost energy consumption prediction based on multi-system data HVAC. The energy consumption of the HVAC system accounts for a significant portion
of the energy consumption of the public building system, and using an efficient
energy consumption prediction model can assist it in carrying out effective
energy-saving transformation. Unlike the traditional energy consumption
prediction model, this paper extracts features from large data sets using
XGBoost, trains them separately to obtain multiple models, then fuses them with
LightGBM's independent prediction results using MAE, infers energy consumption
related variables, and successfully applies this model to the self-developed
Internet of Things platform.","cs.LG,cs.SY,eess.SY"
"On Hyper-parameter Tuning for Stochastic Optimization Algorithms. This paper proposes the first-ever algorithmic framework for tuning
hyper-parameters of stochastic optimization algorithm based on reinforcement
learning. Hyper-parameters impose significant influences on the performance of
stochastic optimization algorithms, such as evolutionary algorithms (EAs) and
meta-heuristics. Yet, it is very time-consuming to determine optimal
hyper-parameters due to the stochastic nature of these algorithms. We propose
to model the tuning procedure as a Markov decision process, and resort the
policy gradient algorithm to tune the hyper-parameters. Experiments on tuning
stochastic algorithms with different kinds of hyper-parameters (continuous and
discrete) for different optimization problems (continuous and discrete) show
that the proposed hyper-parameter tuning algorithms do not require much less
running times of the stochastic algorithms than bayesian optimization method.
The proposed framework can be used as a standard tool for hyper-parameter
tuning in stochastic algorithms.","cs.LG,cs.NE,stat.ML"
"ExKMC: Expanding Explainable $k$-Means Clustering. Despite the popularity of explainable AI, there is limited work on effective
methods for unsupervised learning. We study algorithms for $k$-means
clustering, focusing on a trade-off between explainability and accuracy.
Following prior work, we use a small decision tree to partition a dataset into
$k$ clusters. This enables us to explain each cluster assignment by a short
sequence of single-feature thresholds. While larger trees produce more accurate
clusterings, they also require more complex explanations. To allow flexibility,
we develop a new explainable $k$-means clustering algorithm, ExKMC, that takes
an additional parameter $k' \geq k$ and outputs a decision tree with $k'$
leaves. We use a new surrogate cost to efficiently expand the tree and to label
the leaves with one of $k$ clusters. We prove that as $k'$ increases, the
surrogate cost is non-increasing, and hence, we trade explainability for
accuracy. Empirically, we validate that ExKMC produces a low cost clustering,
outperforming both standard decision tree methods and other algorithms for
explainable clustering. Implementation of ExKMC available at
https://github.com/navefr/ExKMC.","cs.CG,cs.DS,cs.LG,stat.ML"
"Near-Optimal Explainable $k$-Means for All Dimensions. Many clustering algorithms are guided by certain cost functions such as the
widely-used $k$-means cost. These algorithms divide data points into clusters
with often complicated boundaries, creating difficulties in explaining the
clustering decision. In a recent work, Dasgupta, Frost, Moshkovitz, and
Rashtchian (ICML'20) introduced explainable clustering, where the cluster
boundaries are axis-parallel hyperplanes and the clustering is obtained by
applying a decision tree to the data. The central question here is: how much
does the explainability constraint increase the value of the cost function?
  Given $d$-dimensional data points, we show an efficient algorithm that finds
an explainable clustering whose $k$-means cost is at most $k^{1 -
2/d}\mathrm{poly}(d\log k)$ times the minimum cost achievable by a clustering
without the explainability constraint, assuming $k,d\ge 2$. Combining this with
an independent work by Makarychev and Shan (ICML'21), we get an improved bound
of $k^{1 - 2/d}\mathrm{polylog}(k)$, which we show is optimal for every choice
of $k,d\ge 2$ up to a poly-logarithmic factor in $k$. For $d = 2$ in
particular, we show an $O(\log k\log\log k)$ bound, improving exponentially
over the previous best bound of $\widetilde O(k)$.","cs.CG,cs.DS,cs.LG,stat.ML"
"The Tree Ensemble Layer: Differentiability meets Conditional Computation. Neural networks and tree ensembles are state-of-the-art learners, each with
its unique statistical and computational advantages. We aim to combine these
advantages by introducing a new layer for neural networks, composed of an
ensemble of differentiable decision trees (a.k.a. soft trees). While
differentiable trees demonstrate promising results in the literature, they are
typically slow in training and inference as they do not support conditional
computation. We mitigate this issue by introducing a new sparse activation
function for sample routing, and implement true conditional computation by
developing specialized forward and backward propagation algorithms that exploit
sparsity. Our efficient algorithms pave the way for jointly training over deep
and wide tree ensembles using first-order methods (e.g., SGD). Experiments on
23 classification datasets indicate over 10x speed-ups compared to the
differentiable trees used in the literature and over 20x reduction in the
number of parameters compared to gradient boosted trees, while maintaining
competitive performance. Moreover, experiments on CIFAR, MNIST, and Fashion
MNIST indicate that replacing dense layers in CNNs with our tree layer reduces
the test loss by 7-53% and the number of parameters by 8x. We provide an
open-source TensorFlow implementation with a Keras API.","cs.CV,cs.LG,stat.ML"
"Can We Learn Heuristics For Graphical Model Inference Using Reinforcement Learning?. Combinatorial optimization is frequently used in computer vision. For
instance, in applications like semantic segmentation, human pose estimation and
action recognition, programs are formulated for solving inference in
Conditional Random Fields (CRFs) to produce a structured output that is
consistent with visual features of the image. However, solving inference in
CRFs is in general intractable, and approximation methods are computationally
demanding and limited to unary, pairwise and hand-crafted forms of higher order
potentials. In this paper, we show that we can learn program heuristics, i.e.,
policies, for solving inference in higher order CRFs for the task of semantic
segmentation, using reinforcement learning. Our method solves inference tasks
efficiently without imposing any constraints on the form of the potentials. We
show compelling results on the Pascal VOC and MOTS datasets.","I.2.6,I.4.6,cs.CV"
"RtFPS: An Interactive Map that Visualizes and Predicts Wildfires in the US. Climate change has largely impacted our daily lives. As one of its
consequences, we are experiencing more wildfires. In the year 2020, wildfires
burned a record number of 8,888,297 acres in the US. To awaken people's
attention to climate change, and to visualize the current risk of wildfires, We
developed RtFPS, ""Real-Time Fire Prediction System"". It provides a real-time
prediction visualization of wildfire risk at specific locations base on a
Machine Learning model. It also provides interactive map features that show the
historical wildfire events with environmental info.","68T30,68U05,J.2.5; H.4.0; I.5.1,cs.HC,cs.IR,cs.LG"
"Neural Logic Reasoning. Recent years have witnessed the success of deep neural networks in many
research areas. The fundamental idea behind the design of most neural networks
is to learn similarity patterns from data for prediction and inference, which
lacks the ability of cognitive reasoning. However, the concrete ability of
reasoning is critical to many theoretical and practical problems. On the other
hand, traditional symbolic reasoning methods do well in making logical
inference, but they are mostly hard rule-based reasoning, which limits their
generalization ability to different tasks since difference tasks may require
different rules. Both reasoning and generalization ability are important for
prediction tasks such as recommender systems, where reasoning provides strong
connection between user history and target items for accurate prediction, and
generalization helps the model to draw a robust user portrait over noisy
inputs.
  In this paper, we propose Logic-Integrated Neural Network (LINN) to integrate
the power of deep learning and logic reasoning. LINN is a dynamic neural
architecture that builds the computational graph according to input logical
expressions. It learns basic logical operations such as AND, OR, NOT as neural
modules, and conducts propositional logical reasoning through the network for
inference. Experiments on theoretical task show that LINN achieves significant
performance on solving logical equations and variables. Furthermore, we test
our approach on the practical task of recommendation by formulating the task
into a logical inference problem. Experiments show that LINN significantly
outperforms state-of-the-art recommendation models in Top-K recommendation,
which verifies the potential of LINN in practice.","cs.AI,cs.IR,cs.LG,cs.LO,stat.ML"
"Multiple Causes: A Causal Graphical View. Unobserved confounding is a major hurdle for causal inference from
observational data. Confounders---the variables that affect both the causes and
the outcome---induce spurious non-causal correlations between the two. Wang &
Blei (2018) lower this hurdle with ""the blessings of multiple causes,"" where
the correlation structure of multiple causes provides indirect evidence for
unobserved confounding. They leverage these blessings with an algorithm, called
the deconfounder, that uses probabilistic factor models to correct for the
confounders. In this paper, we take a causal graphical view of the
deconfounder. In a graph that encodes shared confounding, we show how the
multiplicity of causes can help identify intervention distributions. We then
justify the deconfounder, showing that it makes valid inferences of the
intervention. Finally, we expand the class of graphs, and its theory, to those
that include other confounders and selection variables. Our results expand the
theory in Wang & Blei (2018), justify the deconfounder for causal graphs, and
extend the settings where it can be used.","cs.LG,stat.ME,stat.ML"
"Efficient Exploration via State Marginal Matching. Exploration is critical to a reinforcement learning agent's performance in
its given environment. Prior exploration methods are often based on using
heuristic auxiliary predictions to guide policy behavior, lacking a
mathematically-grounded objective with clear properties. In contrast, we recast
exploration as a problem of State Marginal Matching (SMM), where we aim to
learn a policy for which the state marginal distribution matches a given target
state distribution. The target distribution is a uniform distribution in most
cases, but can incorporate prior knowledge if available. In effect, SMM
amortizes the cost of learning to explore in a given environment. The SMM
objective can be viewed as a two-player, zero-sum game between a state density
model and a parametric policy, an idea that we use to build an algorithm for
optimizing the SMM objective. Using this formalism, we further demonstrate that
prior work approximately maximizes the SMM objective, offering an explanation
for the success of these methods. On both simulated and real-world tasks, we
demonstrate that agents that directly optimize the SMM objective explore faster
and adapt more quickly to new tasks as compared to prior exploration methods.","cs.AI,cs.LG,cs.RO,stat.ML"
"Robust Maximum Likelihood Estimation of Sparse Vector Error Correction Model. In econometrics and finance, the vector error correction model (VECM) is an
important time series model for cointegration analysis, which is used to
estimate the long-run equilibrium variable relationships. The traditional
analysis and estimation methodologies assume the underlying Gaussian
distribution but, in practice, heavy-tailed data and outliers can lead to the
inapplicability of these methods. In this paper, we propose a robust model
estimation method based on the Cauchy distribution to tackle this issue. In
addition, sparse cointegration relations are considered to realize feature
selection and dimension reduction. An efficient algorithm based on the
majorization-minimization (MM) method is applied to solve the proposed
nonconvex problem. The performance of this algorithm is shown through numerical
simulations.","cs.NA,q-fin.ST,stat.AP,stat.CO,stat.ML"
"Fast and Robust Online Inference with Stochastic Gradient Descent via Random Scaling. We develop a new method of online inference for a vector of parameters
estimated by the Polyak-Ruppert averaging procedure of stochastic gradient
descent (SGD) algorithms. We leverage insights from time series regression in
econometrics and construct asymptotically pivotal statistics via random
scaling. Our approach is fully operational with online data and is rigorously
underpinned by a functional central limit theorem. Our proposed inference
method has a couple of key advantages over the existing methods. First, the
test statistic is computed in an online fashion with only SGD iterates and the
critical values can be obtained without any resampling methods, thereby
allowing for efficient implementation suitable for massive online data. Second,
there is no need to estimate the asymptotic variance and our inference method
is shown to be robust to changes in the tuning parameters for SGD algorithms in
simulation experiments with synthetic data.","62M02,G.3,Primary 62J10,cs.LG,econ.EM,math.ST,secondary 60K35,stat.ML,stat.TH"
"Inferring Granger Causality from Irregularly Sampled Time Series. Continuous, automated surveillance systems that incorporate machine learning
models are becoming increasingly more common in healthcare environments. These
models can capture temporally dependent changes across multiple patient
variables and can enhance a clinician's situational awareness by providing an
early warning alarm of an impending adverse event such as sepsis. However, most
commonly used methods, e.g., XGBoost, fail to provide an interpretable
mechanism for understanding why a model produced a sepsis alarm at a given
time. The black-box nature of many models is a severe limitation as it prevents
clinicians from independently corroborating those physiologic features that
have contributed to the sepsis alarm. To overcome this limitation, we propose a
generalized linear model (GLM) approach to fit a Granger causal graph based on
the physiology of several major sepsis-associated derangements (SADs). We adopt
a recently developed stochastic monotone variational inequality-based estimator
coupled with forwarding feature selection to learn the graph structure from
both continuous and discrete-valued as well as regularly and irregularly
sampled time series. Most importantly, we develop a non-asymptotic upper bound
on the estimation error for any monotone link function in the GLM. We conduct
real-data experiments and demonstrate that our proposed method can achieve
comparable performance to popular and powerful prediction methods such as
XGBoost while simultaneously maintaining a high level of interpretability.","cs.LG,math.ST,stat.AP,stat.ME,stat.TH"
"Decentralized Reinforcement Learning: Global Decision-Making via Local Economic Transactions. This paper seeks to establish a framework for directing a society of simple,
specialized, self-interested agents to solve what traditionally are posed as
monolithic single-agent sequential decision problems. What makes it challenging
to use a decentralized approach to collectively optimize a central objective is
the difficulty in characterizing the equilibrium strategy profile of
non-cooperative games. To overcome this challenge, we design a mechanism for
defining the learning environment of each agent for which we know that the
optimal solution for the global objective coincides with a Nash equilibrium
strategy profile of the agents optimizing their own local objectives. The
society functions as an economy of agents that learn the credit assignment
process itself by buying and selling to each other the right to operate on the
environment state. We derive a class of decentralized reinforcement learning
algorithms that are broadly applicable not only to standard reinforcement
learning but also for selecting options in semi-MDPs and dynamically composing
computation graphs. Lastly, we demonstrate the potential advantages of a
society's inherent modular structure for more efficient transfer learning.","cs.GT,cs.LG,cs.MA,cs.NE,stat.ML"
"Learning Dynamic Graph Representation of Brain Connectome with Spatio-Temporal Attention. Functional connectivity (FC) between regions of the brain can be assessed by
the degree of temporal correlation measured with functional neuroimaging
modalities. Based on the fact that these connectivities build a network,
graph-based approaches for analyzing the brain connectome have provided
insights into the functions of the human brain. The development of graph neural
networks (GNNs) capable of learning representation from graph structured data
has led to increased interest in learning the graph representation of the brain
connectome. Although recent attempts to apply GNN to the FC network have shown
promising results, there is still a common limitation that they usually do not
incorporate the dynamic characteristics of the FC network which fluctuates over
time. In addition, a few studies that have attempted to use dynamic FC as an
input for the GNN reported a reduction in performance compared to static FC
methods, and did not provide temporal explainability. Here, we propose STAGIN,
a method for learning dynamic graph representation of the brain connectome with
spatio-temporal attention. Specifically, a temporal sequence of brain graphs is
input to the STAGIN to obtain the dynamic graph representation, while novel
READOUT functions and the Transformer encoder provide spatial and temporal
explainability with attention, respectively. Experiments on the HCP-Rest and
the HCP-Task datasets demonstrate exceptional performance of our proposed
method. Analysis of the spatio-temporal attention also provide concurrent
interpretation with the neuroscientific knowledge, which further validates our
method. Code is available at https://github.com/egyptdj/stagin","cs.CV,cs.LG,q-bio.NC"
"SEGAN: Speech Enhancement Generative Adversarial Network. Current speech enhancement techniques operate on the spectral domain and/or
exploit some higher-level feature. The majority of them tackle a limited number
of noise conditions and rely on first-order statistics. To circumvent these
issues, deep networks are being increasingly used, thanks to their ability to
learn complex functions from large example sets. In this work, we propose the
use of generative adversarial networks for speech enhancement. In contrast to
current techniques, we operate at the waveform level, training the model
end-to-end, and incorporate 28 speakers and 40 different noise conditions into
the same model, such that model parameters are shared across them. We evaluate
the proposed model using an independent, unseen test set with two speakers and
20 alternative noise conditions. The enhanced samples confirm the viability of
the proposed model, and both objective and subjective evaluations confirm the
effectiveness of it. With that, we open the exploration of generative
architectures for speech enhancement, which may progressively incorporate
further speech-centric design choices to improve their performance.","cs.LG,cs.NE,cs.SD"
"Effect of Superpixel Aggregation on Explanations in LIME -- A Case Study with Biological Data. End-to-end learning with deep neural networks, such as convolutional neural
networks (CNNs), has been demonstrated to be very successful for different
tasks of image classification. To make decisions of black-box approaches
transparent, different solutions have been proposed. LIME is an approach to
explainable AI relying on segmenting images into superpixels based on the
Quick-Shift algorithm. In this paper, we present an explorative study of how
different superpixel methods, namely Felzenszwalb, SLIC and Compact-Watershed,
impact the generated visual explanations. We compare the resulting relevance
areas with the image parts marked by a human reference. Results show that image
parts selected as relevant strongly vary depending on the applied method.
Quick-Shift resulted in the least and Compact-Watershed in the highest
correspondence with the reference relevance areas.","cs.LG,q-bio.QM,stat.ML"
"Online Optimization in Games via Control Theory: Connecting Regret, Passivity and Poincar Recurrence. We present a novel control-theoretic understanding of online optimization and
learning in games, via the notion of passivity. Passivity is a fundamental
concept in control theory, which abstracts energy conservation and dissipation
in physical systems. It has become a standard tool in analysis of general
feedback systems, to which game dynamics belong. Our starting point is to show
that all continuous-time Follow-the-Regularized-Leader (FTRL) dynamics, which
include the well-known Replicator Dynamic, are lossless, i.e. it is passive
with no energy dissipation. Interestingly, we prove that passivity implies
bounded regret, connecting two fundamental primitives of control theory and
online optimization.
  The observation of energy conservation in FTRL inspires us to present a
family of lossless learning dynamics, each of which has an underlying energy
function with a simple gradient structure. This family is closed under convex
combination; as an immediate corollary, any convex combination of FTRL dynamics
is lossless and thus has bounded regret. This allows us to extend the framework
of Fox and Shamma [Games, 2013] to prove not just global asymptotic stability
results for game dynamics, but Poincar\'e recurrence results as well.
Intuitively, when a lossless game (e.g. graphical constant-sum game) is coupled
with lossless learning dynamics, their feedback interconnection is also
lossless, which results in a pendulum-like energy-preserving recurrent
behavior, generalizing the results of Piliouras and Shamma [SODA, 2014] and
Mertikopoulos, Papadimitriou and Piliouras [SODA, 2018].","cs.GT,cs.LG,cs.SY,eess.SY,math.DS"
"Multiresolution Graph Variational Autoencoder. In this paper, we propose Multiresolution Graph Networks (MGN) and
Multiresolution Graph Variational Autoencoders (MGVAE) to learn and generate
graphs in a multiresolution and equivariant manner. At each resolution level,
MGN employs higher order message passing to encode the graph while learning to
partition it into mutually exclusive clusters and coarsening into a lower
resolution. MGVAE constructs a hierarchical generative model based on MGN to
variationally autoencode the hierarchy of coarsened graphs. Our proposed
framework is end-to-end permutation equivariant with respect to node ordering.
Our methods have been successful with several generative tasks including link
prediction on citation graphs, unsupervised molecular representation learning
to predict molecular properties, molecular generation, general graph generation
and graph-based image generation.","cs.LG,cs.SI,physics.chem-ph"
"Debiased-CAM for bias-agnostic faithful visual explanations of deep convolutional networks. Class activation maps (CAMs) explain convolutional neural network predictions
by identifying salient pixels, but they become misaligned and misleading when
explaining predictions on images under bias, such as images blurred
accidentally or deliberately for privacy protection, or images with improper
white balance. Despite model fine-tuning to improve prediction performance on
these biased images, we demonstrate that CAM explanations become more deviated
and unfaithful with increased image bias. We present Debiased-CAM to recover
explanation faithfulness across various bias types and levels by training a
multi-input, multi-task model with auxiliary tasks for CAM and bias level
predictions. With CAM as a prediction task, explanations are made tunable by
retraining the main model layers and made faithful by self-supervised learning
from CAMs of unbiased images. The model provides representative, bias-agnostic
CAM explanations about the predictions on biased images as if generated from
their unbiased form. In four simulation studies with different biases and
prediction tasks, Debiased-CAM improved both CAM faithfulness and task
performance. We further conducted two controlled user studies to validate its
truthfulness and helpfulness, respectively. Quantitative and qualitative
analyses of participant responses confirmed Debiased-CAM as more truthful and
helpful. Debiased-CAM thus provides a basis to generate more faithful and
relevant explanations for a wide range of real-world applications with various
sources of bias.","cs.CV,cs.CY,cs.HC,cs.LG"
"Decision Tree-Based Predictive Models for Academic Achievement Using College Students' Support Networks. In this study, we examine a set of primary data collected from 484 students
enrolled in a large public university in the Mid-Atlantic United States region
during the early stages of the COVID-19 pandemic. The data, called Ties data,
included students' demographic and support network information. The support
network data comprised of information that highlighted the type of support,
(i.e. emotional or educational; routine or intense). Using this data set,
models for predicting students' academic achievement, quantified by their
self-reported GPA, were created using Chi-Square Automatic Interaction
Detection (CHAID), a decision tree algorithm, and cforest, a random forest
algorithm that uses conditional inference trees. We compare the methods'
accuracy and variation in the set of important variables suggested by each
algorithm. Each algorithm found different variables important for different
student demographics with some overlap. For White students, different types of
educational support were important in predicting academic achievement, while
for non-White students, different types of emotional support were important in
predicting academic achievement. The presence of differing types of routine
support were important in predicting academic achievement for cisgender women,
while differing types of intense support were important in predicting academic
achievement for cisgender men.","cs.CY,cs.LG,stat.ML"
"ChemBERTa: Large-Scale Self-Supervised Pretraining for Molecular Property Prediction. GNNs and chemical fingerprints are the predominant approaches to representing
molecules for property prediction. However, in NLP, transformers have become
the de-facto standard for representation learning thanks to their strong
downstream task transfer. In parallel, the software ecosystem around
transformers is maturing rapidly, with libraries like HuggingFace and BertViz
enabling streamlined training and introspection. In this work, we make one of
the first attempts to systematically evaluate transformers on molecular
property prediction tasks via our ChemBERTa model. ChemBERTa scales well with
pretraining dataset size, offering competitive downstream performance on
MoleculeNet and useful attention-based visualization modalities. Our results
suggest that transformers offer a promising avenue of future work for molecular
representation learning and property prediction. To facilitate these efforts,
we release a curated dataset of 77M SMILES from PubChem suitable for
large-scale self-supervised pretraining.","I.2.7; I.2.1; J.2; J.3,cs.CL,cs.LG,physics.chem-ph,q-bio.BM"
"Model Selection With Graphical Neighbour Information. Accurate model selection is a fundamental requirement for statistical
analysis. In many real-world applications of graphical modelling, correct model
structure identification is the ultimate objective. Standard model validation
procedures such as information theoretic scores and cross validation have
demonstrated poor performance in the high dimensional setting. Specialised
methods such as EBIC, StARS and RIC have been developed for the explicit
purpose of high-dimensional Gaussian graphical model selection. We present a
novel model score criterion, Graphical Neighbour Information. This method
demonstrates oracle performance in high-dimensional model selection,
outperforming the current state-of-the-art in our simulations. The Graphical
Neighbour Information criterion has the additional advantage of efficient,
closed-form computability, sparing the costly inference of multiple models on
data subsamples. We provide a theoretical analysis of the method and benchmark
simulations versus the current state of the art.","cs.LG,cs.SI,stat.ML"
"Efficient Competitive Self-Play Policy Optimization. Reinforcement learning from self-play has recently reported many successes.
Self-play, where the agents compete with themselves, is often used to generate
training data for iterative policy improvement. In previous work, heuristic
rules are designed to choose an opponent for the current learner. Typical rules
include choosing the latest agent, the best agent, or a random historical
agent. However, these rules may be inefficient in practice and sometimes do not
guarantee convergence even in the simplest matrix games. In this paper, we
propose a new algorithmic framework for competitive self-play reinforcement
learning in two-player zero-sum games. We recognize the fact that the Nash
equilibrium coincides with the saddle point of the stochastic payoff function,
which motivates us to borrow ideas from classical saddle point optimization
literature. Our method trains several agents simultaneously, and intelligently
takes each other as opponent based on simple adversarial rules derived from a
principled perturbation-based saddle optimization method. We prove
theoretically that our algorithm converges to an approximate equilibrium with
high probability in convex-concave games under standard assumptions. Beyond the
theory, we further show the empirical superiority of our method over baseline
methods relying on the aforementioned opponent-selection heuristics in matrix
games, grid-world soccer, Gomoku, and simulated robot sumo, with neural net
policy function approximators.","cs.AI,cs.GT,cs.LG,stat.ML"
"LocalGLMnet: interpretable deep learning for tabular data. Deep learning models have gained great popularity in statistical modeling
because they lead to very competitive regression models, often outperforming
classical statistical models such as generalized linear models. The
disadvantage of deep learning models is that their solutions are difficult to
interpret and explain, and variable selection is not easily possible because
deep learning models solve feature engineering and variable selection
internally in a nontransparent way. Inspired by the appealing structure of
generalized linear models, we propose a new network architecture that shares
similar features as generalized linear models, but provides superior predictive
power benefiting from the art of representation learning. This new architecture
allows for variable selection of tabular data and for interpretation of the
calibrated deep learning model, in fact, our approach provides an additive
decomposition in the spirit of Shapley values and integrated gradients.","62,68,cs.AI,cs.LG,q-fin.ST,stat.AP,stat.ML"
"The Information Complexity of Learning Tasks, their Structure and their Distance. We introduce an asymmetric distance in the space of learning tasks, and a
framework to compute their complexity. These concepts are foundational for the
practice of transfer learning, whereby a parametric model is pre-trained for a
task, and then fine-tuned for another. The framework we develop is
non-asymptotic, captures the finite nature of the training dataset, and allows
distinguishing learning from memorization. It encompasses, as special cases,
classical notions from Kolmogorov complexity, Shannon, and Fisher Information.
However, unlike some of those frameworks, it can be applied to large-scale
models and real-world datasets. Our framework is the first to measure
complexity in a way that accounts for the effect of the optimization scheme,
which is critical in Deep Learning.","cs.IT,cs.LG,math.IT,stat.ML"
"Accelerating Mini-batch SARAH by Step Size Rules. StochAstic Recursive grAdient algoritHm (SARAH), originally proposed for
convex optimization and also proven to be effective for general nonconvex
optimization, has received great attention due to its simple recursive
framework for updating stochastic gradient estimates. The performance of SARAH
significantly depends on the choice of step size sequence. However, SARAH and
its variants often employ a best-tuned step size by mentor, which is time
consuming in practice. Motivated by this gap, we proposed a variant of the
Barzilai-Borwein (BB) method, referred to as the Random Barzilai-Borwein (RBB)
method, to calculate step size for SARAH in the mini-batch setting, thereby
leading to a new SARAH method: MB-SARAH-RBB. We prove that MB-SARAH-RBB
converges linearly in expectation for strongly convex objective functions. We
analyze the complexity of MB-SARAH-RBB and show that it is better than the
original method. Numerical experiments on standard data sets indicate that
MB-SARAH-RBB outperforms or matches state-of-the-art algorithms.","cs.LG,math.OC,stat.ML"
"Reward learning from human preferences and demonstrations in Atari. To solve complex real-world problems with reinforcement learning, we cannot
rely on manually specified reward functions. Instead, we can have humans
communicate an objective to the agent directly. In this work, we combine two
approaches to learning from human feedback: expert demonstrations and
trajectory preferences. We train a deep neural network to model the reward
function and use its predicted reward to train an DQN-based deep reinforcement
learning agent on 9 Atari games. Our approach beats the imitation learning
baseline in 7 games and achieves strictly superhuman performance on 2 games
without using game rewards. Additionally, we investigate the goodness of fit of
the reward model, present some reward hacking problems, and study the effects
of noise in the human labels.","cs.AI,cs.LG,cs.NE,stat.ML"
"Unifying Heterogenous Electronic Health Records Systems via Text-Based Code Embedding. Substantial increase in the use of Electronic Health Records (EHRs) has
opened new frontiers for predictive healthcare. However, while EHR systems are
nearly ubiquitous, they lack a unified code system for representing medical
concepts. Heterogeneous formats of EHR present a substantial barrier for the
training and deployment of state-of-the-art deep learning models at scale. To
overcome this problem, we introduce Description-based Embedding, DescEmb, a
code-agnostic description-based representation learning framework for
predictive modeling on EHR. DescEmb takes advantage of the flexibility of
neural language understanding models while maintaining a neutral approach that
can be combined with prior frameworks for task-specific representation learning
or predictive modeling. We tested our model's capacity on various experiments
including prediction tasks, transfer learning and pooled learning. DescEmb
shows higher performance in overall experiments compared to code-based
approach, opening the door to a text-based approach in predictive healthcare
research that is not constrained by EHR structure nor special domain knowledge.","cs.LG,cs.NE"
"Explanation from Specification. Explainable components in XAI algorithms often come from a familiar set of
models, such as linear models or decision trees. We formulate an approach where
the type of explanation produced is guided by a specification. Specifications
are elicited from the user, possibly using interaction with the user and
contributions from other areas. Areas where a specification could be obtained
include forensic, medical, and scientific applications. Providing a menu of
possible types of specifications in an area is an exploratory knowledge
representation and reasoning task for the algorithm designer, aiming at
understanding the possibilities and limitations of efficiently computable modes
of explanations. Two examples are discussed: explanations for Bayesian networks
using the theory of argumentation, and explanations for graph neural networks.
The latter case illustrates the possibility of having a representation
formalism available to the user for specifying the type of explanation
requested, for example, a chemical query language for classifying molecules.
The approach is motivated by a theory of explanation in the philosophy of
science, and it is related to current questions in the philosophy of science on
the role of machine learning.","cs.AI,cs.HC,cs.LG,stat.ML"
"SumGNN: Multi-typed Drug Interaction Prediction via Efficient Knowledge Graph Summarization. Thanks to the increasing availability of drug-drug interactions (DDI)
datasets and large biomedical knowledge graphs (KGs), accurate detection of
adverse DDI using machine learning models becomes possible. However, it remains
largely an open problem how to effectively utilize large and noisy biomedical
KG for DDI detection. Due to its sheer size and amount of noise in KGs, it is
often less beneficial to directly integrate KGs with other smaller but higher
quality data (e.g., experimental data). Most of the existing approaches ignore
KGs altogether. Some try to directly integrate KGs with other data via graph
neural networks with limited success. Furthermore, most previous works focus on
binary DDI prediction whereas the multi-typed DDI pharmacological effect
prediction is a more meaningful but harder task. To fill the gaps, we propose a
new method SumGNN: knowledge summarization graph neural network, which is
enabled by a subgraph extraction module that can efficiently anchor on relevant
subgraphs from a KG, a self-attention based subgraph summarization scheme to
generate a reasoning path within the subgraph, and a multi-channel knowledge
and data integration module that utilizes massive external biomedical knowledge
for significantly improved multi-typed DDI predictions. SumGNN outperforms the
best baseline by up to 5.54\%, and the performance gain is particularly
significant in low data relation types. In addition, SumGNN provides
interpretable prediction via the generated reasoning paths for each prediction.","cs.CL,cs.IR,cs.LG,q-bio.QM"
"Hybrid modeling of the human cardiovascular system using NeuralFMUs. Hybrid modeling, the combination of first principle and machine learning
models, is an emerging research field that gathers more and more attention.
Even if hybrid models produce formidable results for academic examples, there
are still different technical challenges that hinder the use of hybrid modeling
in real-world applications. By presenting NeuralFMUs, the fusion of a FMU, a
numerical ODE solver and an ANN, we are paving the way for the use of a variety
of first principle models from different modeling tools as parts of hybrid
models. This contribution handles the hybrid modeling of a complex, real-world
example: Starting with a simplified 1D-fluid model of the human cardiovascular
system (arterial side), the aim is to learn neglected physical effects like
arterial elasticity from data. We will show that the hybrid modeling process is
more comfortable, needs less system knowledge and is therefore less error-prone
compared to modeling solely based on first principle. Further, the resulting
hybrid model has improved in computation performance, compared to a pure first
principle white-box model, while still fulfilling the requirements regarding
accuracy of the considered hemodynamic quantities. The use of the presented
techniques is explained in a general manner and the considered use-case can
serve as example for other modeling and simulation applications in and beyond
the medical domain.","cs.CE,cs.LG"
"Learning Multiple Stock Trading Patterns with Temporal Routing Adaptor and Optimal Transport. Successful quantitative investment usually relies on precise predictions of
the future movement of the stock price. Recently, machine learning based
solutions have shown their capacity to give more accurate stock prediction and
become indispensable components in modern quantitative investment systems.
However, the i.i.d. assumption behind existing methods is inconsistent with the
existence of diverse trading patterns in the stock market, which inevitably
limits their ability to achieve better stock prediction performance. In this
paper, we propose a novel architecture, Temporal Routing Adaptor (TRA), to
empower existing stock prediction models with the ability to model multiple
stock trading patterns. Essentially, TRA is a lightweight module that consists
of a set of independent predictors for learning multiple patterns as well as a
router to dispatch samples to different predictors. Nevertheless, the lack of
explicit pattern identifiers makes it quite challenging to train an effective
TRA-based model. To tackle this challenge, we further design a learning
algorithm based on Optimal Transport (OT) to obtain the optimal sample to
predictor assignment and effectively optimize the router with such assignment
through an auxiliary loss term. Experiments on the real-world stock ranking
task show that compared to the state-of-the-art baselines, e.g., Attention LSTM
and Transformer, the proposed method can improve information coefficient (IC)
from 0.053 to 0.059 and 0.051 to 0.056 respectively. Our dataset and code used
in this work are publicly available:
https://github.com/microsoft/qlib/tree/main/examples/benchmarks/TRA.","cs.CE,cs.LG,q-fin.ST"
"Graphical modelling in continuous-time: consistency guarantees and algorithms using Neural ODEs. The discovery of structure from time series data is a key problem in fields
of study working with complex systems. Most identifiability results and
learning algorithms assume the underlying dynamics to be discrete in time.
Comparatively few, in contrast, explicitly define dependencies in infinitesimal
intervals of time, independently of the scale of observation and of the
regularity of sampling. In this paper, we consider score-based graph learning
for the study of dynamical systems. We prove that for vector fields
parameterized in a large class of neural networks, least squares optimization
with adaptive regularization schemes consistently recovers directed graphs of
local independencies in systems of stochastic differential equations. Using
this insight, we propose a score-based learning algorithm based on penalized
Neural Ordinary Differential Equations (modelling the mean process) that we
show to be applicable to the general setting of irregularly-sampled
multivariate time series and to outperform the state of the art across a range
of dynamical systems.","cs.LG,math.DS,stat.ML"
"Online Embedding Compression for Text Classification using Low Rank Matrix Factorization. Deep learning models have become state of the art for natural language
processing (NLP) tasks, however deploying these models in production system
poses significant memory constraints. Existing compression methods are either
lossy or introduce significant latency. We propose a compression method that
leverages low rank matrix factorization during training,to compress the word
embedding layer which represents the size bottleneck for most NLP models. Our
models are trained, compressed and then further re-trained on the downstream
task to recover accuracy while maintaining the reduced size. Empirically, we
show that the proposed method can achieve 90% compression with minimal impact
in accuracy for sentence classification tasks, and outperforms alternative
methods like fixed-point quantization or offline word embedding compression. We
also analyze the inference time and storage space for our method through FLOP
calculations, showing that we can compress DNN models by a configurable ratio
and regain accuracy loss without introducing additional latency compared to
fixed point quantization. Finally, we introduce a novel learning rate schedule,
the Cyclically Annealed Learning Rate (CALR), which we empirically demonstrate
to outperform other popular adaptive learning rate algorithms on a sentence
classification benchmark.","cs.CL,cs.LG,cs.NA,stat.ML"
"A causal view on compositional data. Many scientific datasets are compositional in nature. Important examples
include species abundances in ecology, rock compositions in geology, topic
compositions in large-scale text corpora, and sequencing count data in
molecular biology. Here, we provide a causal view on compositional data in an
instrumental variable setting where the composition acts as the cause.
Throughout, we pay particular attention to the interpretation of compositional
causes from the viewpoint of interventions and crisply articulate potential
pitfalls for practitioners. Focusing on modern high-dimensional microbiome
sequencing data as a timely illustrative use case, our analysis first reveals
that popular one-dimensional information-theoretic summary statistics, such as
diversity and richness, may be insufficient for drawing causal conclusions from
ecological data. Instead, we advocate for multivariate alternatives using
statistical data transformations and regression techniques that take the
special structure of the compositional sample space into account. In a
comparative analysis on synthetic and semi-synthetic data we show the
advantages and limitations of our proposal. We posit that our framework may
provide a useful starting point for cause-effect estimation in the context of
compositional data.","cs.LG,q-bio.QM,stat.AP,stat.ML"
"PointGMM: a Neural GMM Network for Point Clouds. Point clouds are a popular representation for 3D shapes. However, they encode
a particular sampling without accounting for shape priors or non-local
information. We advocate for the use of a hierarchical Gaussian mixture model
(hGMM), which is a compact, adaptive and lightweight representation that
probabilistically defines the underlying 3D surface. We present PointGMM, a
neural network that learns to generate hGMMs which are characteristic of the
shape class, and also coincide with the input point cloud. PointGMM is trained
over a collection of shapes to learn a class-specific prior. The hierarchical
representation has two main advantages: (i) coarse-to-fine learning, which
avoids converging to poor local-minima; and (ii) (an unsupervised) consistent
partitioning of the input shape. We show that as a generative model, PointGMM
learns a meaningful latent space which enables generating consistent
interpolations between existing shapes, as well as synthesizing novel shapes.
We also present a novel framework for rigid registration using PointGMM, that
learns to disentangle orientation from structure of an input shape.","cs.CV,cs.GR,cs.LG,stat.ML"
"Context-based Image Segment Labeling (CBISL). Working with images, one often faces problems with incomplete or unclear
information. Image inpainting can be used to restore missing image regions but
focuses, however, on low-level image features such as pixel intensity, pixel
gradient orientation, and color. This paper aims to recover semantic image
features (objects and positions) in images. Based on published gated PixelCNNs,
we demonstrate a new approach referred to as quadro-directional PixelCNN to
recover missing objects and return probable positions for objects based on the
context. We call this approach context-based image segment labeling (CBISL).
The results suggest that our four-directional model outperforms one-directional
models (gated PixelCNN) and returns a human-comparable performance.","68T45,J.6; J.0,cs.CV"
"Unsupervised Super-Resolution of Satellite Imagery for High Fidelity Material Label Transfer. Urban material recognition in remote sensing imagery is a highly relevant,
yet extremely challenging problem due to the difficulty of obtaining human
annotations, especially on low resolution satellite images. To this end, we
propose an unsupervised domain adaptation based approach using adversarial
learning. We aim to harvest information from smaller quantities of high
resolution data (source domain) and utilize the same to super-resolve low
resolution imagery (target domain). This can potentially aid in semantic as
well as material label transfer from a richly annotated source to a target
domain.","cs.CV,cs.LG,eess.IV"
"World-GAN: a Generative Model for Minecraft Worlds. This work introduces World-GAN, the first method to perform data-driven
Procedural Content Generation via Machine Learning in Minecraft from a single
example. Based on a 3D Generative Adversarial Network (GAN) architecture, we
are able to create arbitrarily sized world snippets from a given sample. We
evaluate our approach on creations from the community as well as structures
generated with the Minecraft World Generator. Our method is motivated by the
dense representations used in Natural Language Processing (NLP) introduced with
word2vec [1]. The proposed block2vec representations make World-GAN independent
from the number of different blocks, which can vary a lot in Minecraft, and
enable the generation of larger levels. Finally, we demonstrate that changing
this new representation space allows us to change the generated style of an
already trained generator. World-GAN enables its users to generate Minecraft
worlds based on parts of their creations.","cs.CV,cs.LG,cs.NE"
"Self-Supervised Learning of Audio-Visual Objects from Video. Our objective is to transform a video into a set of discrete audio-visual
objects using self-supervised learning. To this end, we introduce a model that
uses attention to localize and group sound sources, and optical flow to
aggregate information over time. We demonstrate the effectiveness of the
audio-visual object embeddings that our model learns by using them for four
downstream speech-oriented tasks: (a) multi-speaker sound source separation,
(b) localizing and tracking speakers, (c) correcting misaligned audio-visual
data, and (d) active speaker detection. Using our representation, these tasks
can be solved entirely by training on unlabeled video, without the aid of
object detectors. We also demonstrate the generality of our method by applying
it to non-human speakers, including cartoons and puppets.Our model
significantly outperforms other self-supervised approaches, and obtains
performance competitive with methods that use supervised face detection.","cs.CV,cs.SD,eess.AS"
"SLGAN: Style- and Latent-guided Generative Adversarial Network for Desirable Makeup Transfer and Removal. There are five features to consider when using generative adversarial
networks to apply makeup to photos of the human face. These features include
(1) facial components, (2) interactive color adjustments, (3) makeup
variations, (4) robustness to poses and expressions, and the (5) use of
multiple reference images. Several related works have been proposed, mainly
using generative adversarial networks (GAN). Unfortunately, none of them have
addressed all five features simultaneously. This paper closes the gap with an
innovative style- and latent-guided GAN (SLGAN). We provide a novel, perceptual
makeup loss and a style-invariant decoder that can transfer makeup styles based
on histogram matching to avoid the identity-shift problem. In our experiments,
we show that our SLGAN is better than or comparable to state-of-the-art
methods. Furthermore, we show that our proposal can interpolate facial makeup
images to determine the unique features, compare existing methods, and help
users find desirable makeup configurations.","cs.CV,cs.MM"
"A Pipeline for Vision-Based On-Orbit Proximity Operations Using Deep Learning and Synthetic Imagery. Deep learning has become the gold standard for image processing over the past
decade. Simultaneously, we have seen growing interest in orbital activities
such as satellite servicing and debris removal that depend on proximity
operations between spacecraft. However, two key challenges currently pose a
major barrier to the use of deep learning for vision-based on-orbit proximity
operations. Firstly, efficient implementation of these techniques relies on an
effective system for model development that streamlines data curation,
training, and evaluation. Secondly, a scarcity of labeled training data (images
of a target spacecraft) hinders creation of robust deep learning models. This
paper presents an open-source deep learning pipeline, developed specifically
for on-orbit visual navigation applications, that addresses these challenges.
The core of our work consists of two custom software tools built on top of a
cloud architecture that interconnects all stages of the model development
process. The first tool leverages Blender, an open-source 3D graphics toolset,
to generate labeled synthetic training data with configurable model poses
(positions and orientations), lighting conditions, backgrounds, and commonly
observed in-space image aberrations. The second tool is a plugin-based
framework for effective dataset curation and model training; it provides common
functionality like metadata generation and remote storage access to all
projects while giving complete independence to project-specific code.
Time-consuming, graphics-intensive processes such as synthetic image generation
and model training run on cloud-based computational resources which scale to
any scope and budget and allow development of even the largest datasets and
models from any machine. The presented system has been used in the Texas
Spacecraft Laboratory with marked benefits in development speed and quality.","cs.CV,cs.LG,cs.SE"
"HetEmotionNet: Two-Stream Heterogeneous Graph Recurrent Neural Network for Multi-modal Emotion Recognition. The research on human emotion under multimedia stimulation based on
physiological signals is an emerging field, and important progress has been
achieved for emotion recognition based on multi-modal signals. However, it is
challenging to make full use of the complementarity among
spatial-spectral-temporal domain features for emotion recognition, as well as
model the heterogeneity and correlation among multi-modal signals. In this
paper, we propose a novel two-stream heterogeneous graph recurrent neural
network, named HetEmotionNet, fusing multi-modal physiological signals for
emotion recognition. Specifically, HetEmotionNet consists of the
spatial-temporal stream and the spatial-spectral stream, which can fuse
spatial-spectral-temporal domain features in a unified framework. Each stream
is composed of the graph transformer network for modeling the heterogeneity,
the graph convolutional network for modeling the correlation, and the gated
recurrent unit for capturing the temporal domain or spectral domain dependency.
Extensive experiments on two real-world datasets demonstrate that our proposed
model achieves better performance than state-of-the-art baselines.","cs.AI,cs.HC,cs.LG,cs.MM"
"Fine-Grained Dynamic Head for Object Detection. The Feature Pyramid Network (FPN) presents a remarkable approach to alleviate
the scale variance in object representation by performing instance-level
assignments. Nevertheless, this strategy ignores the distinct characteristics
of different sub-regions in an instance. To this end, we propose a fine-grained
dynamic head to conditionally select a pixel-level combination of FPN features
from different scales for each instance, which further releases the ability of
multi-scale feature representation. Moreover, we design a spatial gate with the
new activation function to reduce computational complexity dramatically through
spatially sparse convolutions. Extensive experiments demonstrate the
effectiveness and efficiency of the proposed method on several state-of-the-art
detection benchmarks. Code is available at
https://github.com/StevenGrove/DynamicHead.","68T45,cs.AI,cs.CV"
"Alternating the Population and Control Neural Networks to Solve High-Dimensional Stochastic Mean-Field Games. We present APAC-Net, an alternating population and agent control neural
network for solving stochastic mean field games (MFGs). Our algorithm is geared
toward high-dimensional instances of MFGs that are beyond reach with existing
solution methods. We achieve this in two steps. First, we take advantage of the
underlying variational primal-dual structure that MFGs exhibit and phrase it as
a convex-concave saddle point problem. Second, we parameterize the value and
density functions by two neural networks, respectively. By phrasing the problem
in this manner, solving the MFG can be interpreted as a special case of
training a generative adversarial network (GAN). We show the potential of our
method on up to 100-dimensional MFG problems.","cs.LG,cs.MA,math.OC,stat.ML"
"Certified Adversarial Robustness with Additive Noise. The existence of adversarial data examples has drawn significant attention in
the deep-learning community; such data are seemingly minimally perturbed
relative to the original data, but lead to very different outputs from a
deep-learning algorithm. Although a significant body of work on developing
defensive models has been considered, most such models are heuristic and are
often vulnerable to adaptive attacks. Defensive methods that provide
theoretical robustness guarantees have been studied intensively, yet most fail
to obtain non-trivial robustness when a large-scale model and data are present.
To address these limitations, we introduce a framework that is scalable and
provides certified bounds on the norm of the input manipulation for
constructing adversarial examples. We establish a connection between robustness
against adversarial perturbation and additive random noise, and propose a
training strategy that can significantly improve the certified bounds. Our
evaluation on MNIST, CIFAR-10 and ImageNet suggests that the proposed method is
scalable to complicated models and large data sets, while providing competitive
robustness to state-of-the-art provable defense methods.","cs.CR,cs.LG,stat.ML"
"K-Core based Temporal Graph Convolutional Network for Dynamic Graphs. Graph representation learning is a fundamental task in various applications
that strives to learn low-dimensional embeddings for nodes that can preserve
graph topology information. However, many existing methods focus on static
graphs while ignoring evolving graph patterns. Inspired by the success of graph
convolutional networks(GCNs) in static graph embedding, we propose a novel
k-core based temporal graph convolutional network, the CTGCN, to learn node
representations for dynamic graphs. In contrast to previous dynamic graph
embedding methods, CTGCN can preserve both local connective proximity and
global structural similarity while simultaneously capturing graph dynamics. In
the proposed framework, the traditional graph convolution is generalized into
two phases, feature transformation and feature aggregation, which gives the
CTGCN more flexibility and enables the CTGCN to learn connective and structural
information under the same framework. Experimental results on 7 real-world
graphs demonstrate that the CTGCN outperforms existing state-of-the-art graph
embedding methods in several tasks, including link prediction and structural
role classification. The source code of this work can be obtained from
\url{https://github.com/jhljx/CTGCN}.","cs.LG,cs.SI,stat.ML"
"Road-network-based Rapid Geolocalization. It has always been a research hotspot to use geographic information to assist
the navigation of unmanned aerial vehicles. In this paper, a road-network-based
localization method is proposed. We match roads in the measurement images to
the reference road vector map, and realize successful localization on areas as
large as a whole city. The road network matching problem is treated as a point
cloud registration problem under two-dimensional projective transformation, and
solved under a hypothesise-and-test framework. To deal with the projective
point cloud registration problem, a global projective invariant feature is
proposed, which consists of two road intersections augmented with the
information of their tangents. We call it two road intersections tuple. We
deduce the closed-form solution for determining the alignment transformation
from a pair of matching two road intersections tuples. In addition, we propose
the necessary conditions for the tuples to match. This can reduce the candidate
matching tuples, thus accelerating the search to a great extent. We test all
the candidate matching tuples under a hypothesise-and-test framework to search
for the best match. The experiments show that our method can localize the
target area over an area of 400 within 1 second on a single cpu.","cs.AI,cs.CV,eess.IV"
"Make One-Shot Video Object Segmentation Efficient Again. Video object segmentation (VOS) describes the task of segmenting a set of
objects in each frame of a video. In the semi-supervised setting, the first
mask of each object is provided at test time. Following the one-shot principle,
fine-tuning VOS methods train a segmentation model separately on each given
object mask. However, recently the VOS community has deemed such a test time
optimization and its impact on the test runtime as unfeasible. To mitigate the
inefficiencies of previous fine-tuning approaches, we present efficient
One-Shot Video Object Segmentation (e-OSVOS). In contrast to most VOS
approaches, e-OSVOS decouples the object detection task and predicts only local
segmentation masks by applying a modified version of Mask R-CNN. The one-shot
test runtime and performance are optimized without a laborious and handcrafted
hyperparameter search. To this end, we meta learn the model initialization and
learning rates for the test time optimization. To achieve optimal learning
behavior, we predict individual learning rates at a neuron level. Furthermore,
we apply an online adaptation to address the common performance degradation
throughout a sequence by continuously fine-tuning the model on previous mask
predictions supported by a frame-to-frame bounding box propagation. e-OSVOS
provides state-of-the-art results on DAVIS 2016, DAVIS 2017, and YouTube-VOS
for one-shot fine-tuning methods while reducing the test runtime substantially.
  Code is available at https://github.com/dvl-tum/e-osvos.","cs.CV,cs.LG,cs.RO"
"Representation Learning for Networks in Biology and Medicine: Advancements, Challenges, and Opportunities. With the remarkable success of representation learning in providing powerful
predictions and data insights, we have witnessed a rapid expansion of
representation learning techniques into modeling, analysis, and learning with
networks. Biomedical networks are universal descriptors of systems of
interacting elements, from protein interactions to disease networks, all the
way to healthcare systems and scientific knowledge. In this review, we put
forward an observation that long-standing principles of network biology and
medicine -- while often unspoken in machine learning research -- can provide
the conceptual grounding for representation learning, explain its current
successes and limitations, and inform future advances. We synthesize a spectrum
of algorithmic approaches that, at their core, leverage topological features to
embed networks into compact vector spaces. We also provide a taxonomy of
biomedical areas that are likely to benefit most from algorithmic innovation.
Representation learning techniques are becoming essential for identifying
causal variants underlying complex traits, disentangling behaviors of single
cells and their impact on health, and diagnosing and treating diseases with
safe and effective medicines.","cs.LG,cs.SI,q-bio.BM,q-bio.GN,q-bio.MN"
"Strudel: Learning Structured-Decomposable Probabilistic Circuits. Probabilistic circuits (PCs) represent a probability distribution as a
computational graph. Enforcing structural properties on these graphs guarantees
that several inference scenarios become tractable. Among these properties,
structured decomposability is a particularly appealing one: it enables the
efficient and exact computations of the probability of complex logical
formulas, and can be used to reason about the expected output of certain
predictive models under missing data. This paper proposes Strudel, a simple,
fast and accurate learning algorithm for structured-decomposable PCs. Compared
to prior work for learning structured-decomposable PCs, Strudel delivers more
accurate single PC models in fewer iterations, and dramatically scales learning
when building ensembles of PCs. It achieves this scalability by exploiting
another structural property of PCs, called determinism, and by sharing the same
computational graph across mixture components. We show these advantages on
standard density estimation benchmarks and challenging inference scenarios.","I.2.6,cs.AI,cs.LG"
"Constructing Phrase-level Semantic Labels to Form Multi-Grained Supervision for Image-Text Retrieval. Existing research for image text retrieval mainly relies on sentence-level
supervision to distinguish matched and mismatched sentences for a query image.
However, semantic mismatch between an image and sentences usually happens in
finer grain, i.e., phrase level. In this paper, we explore to introduce
additional phrase-level supervision for the better identification of mismatched
units in the text. In practice, multi-grained semantic labels are automatically
constructed for a query image in both sentence-level and phrase-level. We
construct text scene graphs for the matched sentences and extract entities and
triples as the phrase-level labels. In order to integrate both supervision of
sentence-level and phrase-level, we propose Semantic Structure Aware Multimodal
Transformer (SSAMT) for multi-modal representation learning. Inside the SSAMT,
we utilize different kinds of attention mechanisms to enforce interactions of
multi-grain semantic units in both sides of vision and language. For the
training, we propose multi-scale matching losses from both global and local
perspectives, and penalize mismatched phrases. Experimental results on MS-COCO
and Flickr30K show the effectiveness of our approach compared to some
state-of-the-art models.","cs.CL,cs.CV"
"Distributional Generalization: A New Kind of Generalization. We introduce a new notion of generalization -- Distributional Generalization
-- which roughly states that outputs of a classifier at train and test time are
close *as distributions*, as opposed to close in just their average error. For
example, if we mislabel 30% of dogs as cats in the train set of CIFAR-10, then
a ResNet trained to interpolation will in fact mislabel roughly 30% of dogs as
cats on the *test set* as well, while leaving other classes unaffected. This
behavior is not captured by classical generalization, which would only consider
the average error and not the distribution of errors over the input domain. Our
formal conjectures, which are much more general than this example, characterize
the form of distributional generalization that can be expected in terms of
problem parameters: model architecture, training procedure, number of samples,
and data distribution. We give empirical evidence for these conjectures across
a variety of domains in machine learning, including neural networks, kernel
machines, and decision trees. Our results thus advance our empirical
understanding of interpolating classifiers.","cs.LG,cs.NE,math.ST,stat.ML,stat.TH"
"Using Persistent Homology Topological Features to Characterize Medical Images: Case Studies on Lung and Brain Cancers. Tumor shape is a key factor that affects tumor growth and metastasis. This
paper proposes a topological feature computed by persistent homology to
characterize tumor progression from digital pathology and radiology images and
examines its effect on the time-to-event data. The proposed topological
features are invariant to scale-preserving transformation and can summarize
various tumor shape patterns. The topological features are represented in
functional space and used as functional predictors in a functional Cox
proportional hazards model. The proposed model enables interpretable inference
about the association between topological shape features and survival risks.
Two case studies are conducted using consecutive 143 lung cancer and 77 brain
tumor patients. The results of both studies show that the topological features
predict survival prognosis after adjusting clinical variables, and the
predicted high-risk groups have significantly (at the level of 0.001) worse
survival outcomes than the low-risk groups. Also, the topological shape
features found to be positively associated with survival hazards are irregular
and heterogeneous shape patterns, which are known to be related to tumor
progression.","cs.CV,stat.AP,stat.ME"
"Light Direction and Color Estimation from Single Image with Deep Regression. We present a method to estimate the direction and color of the scene light
source from a single image. Our method is based on two main ideas: (a) we use a
new synthetic dataset with strong shadow effects with similar constraints to
the SID dataset; (b) we define a deep architecture trained on the mentioned
dataset to estimate the direction and color of the scene light source. Apart
from showing good performance on synthetic images, we additionally propose a
preliminary procedure to obtain light positions of the Multi-Illumination
dataset, and, in this way, we also prove that our trained model achieves good
performance when it is applied to real scenes.","cs.CV,cs.GR"
"Using GANs for Sharing Networked Time Series Data: Challenges, Initial Promise, and Open Questions. Limited data access is a longstanding barrier to data-driven research and
development in the networked systems community. In this work, we explore if and
how generative adversarial networks (GANs) can be used to incentivize data
sharing by enabling a generic framework for sharing synthetic datasets with
minimal expert knowledge. As a specific target, our focus in this paper is on
time series datasets with metadata (e.g., packet loss rate measurements with
corresponding ISPs). We identify key challenges of existing GAN approaches for
such workloads with respect to fidelity (e.g., long-term dependencies, complex
multidimensional relationships, mode collapse) and privacy (i.e., existing
guarantees are poorly understood and can sacrifice fidelity). To improve
fidelity, we design a custom workflow called DoppelGANger (DG) and demonstrate
that across diverse real-world datasets (e.g., bandwidth measurements, cluster
requests, web sessions) and use cases (e.g., structural characterization,
predictive modeling, algorithm comparison), DG achieves up to 43% better
fidelity than baseline models. Although we do not resolve the privacy problem
in this work, we identify fundamental challenges with both classical notions of
privacy and recent advances to improve the privacy properties of GANs, and
suggest a potential roadmap for addressing these challenges. By shedding light
on the promise and challenges, we hope our work can rekindle the conversation
on workflows for data sharing.","cs.DC,cs.LG,cs.NI,stat.ML"
"RST-MODNet: Real-time Spatio-temporal Moving Object Detection for Autonomous Driving. Moving Object Detection (MOD) is a critical task for autonomous vehicles as
moving objects represent higher collision risk than static ones. The trajectory
of the ego-vehicle is planned based on the future states of detected moving
objects. It is quite challenging as the ego-motion has to be modelled and
compensated to be able to understand the motion of the surrounding objects. In
this work, we propose a real-time end-to-end CNN architecture for MOD utilizing
spatio-temporal context to improve robustness. We construct a novel time-aware
architecture exploiting temporal motion information embedded within sequential
images in addition to explicit motion maps using optical flow images.We
demonstrate the impact of our algorithm on KITTI dataset where we obtain an
improvement of 8% relative to the baselines. We compare our algorithm with
state-of-the-art methods and achieve competitive results on KITTI-Motion
dataset in terms of accuracy at three times better run-time. The proposed
algorithm runs at 23 fps on a standard desktop GPU targeting deployment on
embedded platforms.","cs.CV,cs.LG,cs.RO,stat.ML"
"Reverb: A Framework For Experience Replay. A central component of training in Reinforcement Learning (RL) is Experience:
the data used for training. The mechanisms used to generate and consume this
data have an important effect on the performance of RL algorithms.
  In this paper, we introduce Reverb: an efficient, extensible, and easy to use
system designed specifically for experience replay in RL. Reverb is designed to
work efficiently in distributed configurations with up to thousands of
concurrent clients.
  The flexible API provides users with the tools to easily and accurately
configure the replay buffer. It includes strategies for selecting and removing
elements from the buffer, as well as options for controlling the ratio between
sampled and inserted elements. This paper presents the core design of Reverb,
gives examples of how it can be applied, and provides empirical results of
Reverb's performance characteristics.","cs.AI,cs.DC,cs.LG"
"Finite-Sample Analysis of Decentralized Temporal-Difference Learning with Linear Function Approximation. Motivated by the emerging use of multi-agent reinforcement learning (MARL) in
engineering applications such as networked robotics, swarming drones, and
sensor networks, we investigate the policy evaluation problem in a fully
decentralized setting, using temporal-difference (TD) learning with linear
function approximation to handle large state spaces in practice. The goal of a
group of agents is to collaboratively learn the value function of a given
policy from locally private rewards observed in a shared environment, through
exchanging local estimates with neighbors. Despite their simplicity and
widespread use, our theoretical understanding of such decentralized TD learning
algorithms remains limited. Existing results were obtained based on i.i.d. data
samples, or by imposing an `additional' projection step to control the
`gradient' bias incurred by the Markovian observations. In this paper, we
provide a finite-sample analysis of the fully decentralized TD(0) learning
under both i.i.d. as well as Markovian samples, and prove that all local
estimates converge linearly to a small neighborhood of the optimum. The
resultant error bounds are the first of its type---in the sense that they hold
under the most practical assumptions ---which is made possible by means of a
novel multi-step Lyapunov analysis.","cs.IT,cs.LG,cs.SY,eess.SY,math.IT,math.OC,stat.ML"
"Graphical modelling in continuous-time: consistency guarantees and algorithms using Neural ODEs. The discovery of structure from time series data is a key problem in fields
of study working with complex systems. Most identifiability results and
learning algorithms assume the underlying dynamics to be discrete in time.
Comparatively few, in contrast, explicitly define dependencies in infinitesimal
intervals of time, independently of the scale of observation and of the
regularity of sampling. In this paper, we consider score-based graph learning
for the study of dynamical systems. We prove that for vector fields
parameterized in a large class of neural networks, least squares optimization
with adaptive regularization schemes consistently recovers directed graphs of
local independencies in systems of stochastic differential equations. Using
this insight, we propose a score-based learning algorithm based on penalized
Neural Ordinary Differential Equations (modelling the mean process) that we
show to be applicable to the general setting of irregularly-sampled
multivariate time series and to outperform the state of the art across a range
of dynamical systems.","cs.LG,math.DS,stat.ML"
"Learning Temporal Causal Sequence Relationships from Real-Time Time-Series. We aim to mine temporal causal sequences that explain observed events
(consequents) in time-series traces. Causal explanations of key events in a
time-series has applications in design debugging, anomaly detection, planning,
root-cause analysis and many more. We make use of decision trees and interval
arithmetic to mine sequences that explain defining events in the time-series.
We propose modified decision tree construction metrics to handle the
non-determinism introduced by the temporal dimension. The mined sequences are
expressed in a readable temporal logic language that is easy to interpret. The
application of the proposed methodology is illustrated through various
examples.","cs.AI,cs.LG,cs.LO"
"Learning Contextualized Music Semantics from Tags via a Siamese Network. Music information retrieval faces a challenge in modeling contextualized
musical concepts formulated by a set of co-occurring tags. In this paper, we
investigate the suitability of our recently proposed approach based on a
Siamese neural network in fighting off this challenge. By means of tag features
and probabilistic topic models, the network captures contextualized semantics
from tags via unsupervised learning. This leads to a distributed semantics
space and a potential solution to the out of vocabulary problem which has yet
to be sufficiently addressed. We explore the nature of the resultant
music-based semantics and address computational needs. We conduct experiments
on three public music tag collections -namely, CAL500, MagTag5K and Million
Song Dataset- and compare our approach to a number of state-of-the-art
semantics learning approaches. Comparative results suggest that this approach
outperforms previous approaches in terms of semantic priming and music tag
completion.","I.2.6,cs.LG"
"A Numerical Transform of Random Forest Regressors corrects Systematically-Biased Predictions. Over the past decade, random forest models have become widely used as a
robust method for high-dimensional data regression tasks. In part, the
popularity of these models arises from the fact that they require little
hyperparameter tuning and are not very susceptible to overfitting. Random
forest regression models are comprised of an ensemble of decision trees that
independently predict the value of a (continuous) dependent variable;
predictions from each of the trees are ultimately averaged to yield an overall
predicted value from the forest. Using a suite of representative real-world
datasets, we find a systematic bias in predictions from random forest models.
We find that this bias is recapitulated in simple synthetic datasets,
regardless of whether or not they include irreducible error (noise) in the
data, but that models employing boosting do not exhibit this bias. Here we
demonstrate the basis for this problem, and we use the training data to define
a numerical transformation that fully corrects it. Application of this
transformation yields improved predictions in every one of the real-world and
synthetic datasets evaluated in our study.","cs.LG,q-bio.QM,stat.ML"
"Gradient Play in Multi-Agent Markov Stochastic Games: Stationary Points and Convergence. We study the performance of the gradient play algorithm for multi-agent
tabular Markov decision processes (MDPs), which are also known as stochastic
games (SGs), where each agent tries to maximize its own total discounted reward
by making decisions independently based on current state information which is
shared between agents. Policies are directly parameterized by the probability
of choosing a certain action at a given state. We show that Nash equilibria
(NEs) and first order stationary policies are equivalent in this setting, and
give a non-asymptotic global convergence rate analysis to an $\epsilon$-NE for
a subclass of multi-agent MDPs called Markov potential games, which includes
the cooperative setting with identical rewards among agents as an important
special case. Our result shows that the number of iterations to reach an
$\epsilon$-NE scales linearly, instead of exponentially, with the number of
agents. Local geometry and local stability are also considered. For Markov
potential games, we prove that strict NEs are local maxima of the total
potential function and fully-mixed NEs are saddle points. We also give a local
convergence rate around strict NEs for more general settings.","cs.GT,cs.LG,cs.MA,math.OC"
"A Method for Robust Online Classification using Dictionary Learning: Development and Assessment for Monitoring Manual Material Handling Activities Using Wearable Sensors. Classification methods based on sparse estimation have drawn much attention
recently, due to their effectiveness in processing high-dimensional data such
as images. In this paper, a method to improve the performance of a sparse
representation classification (SRC) approach is proposed; it is then applied to
the problem of online process monitoring of human workers, specifically manual
material handling (MMH) operations monitored using wearable sensors (involving
111 sensor channels). Our proposed method optimizes the design matrix (aka
dictionary) in the linear model used for SRC, minimizing its ill-posedness to
achieve a sparse solution. This procedure is based on the idea of dictionary
learning (DL): we optimize the design matrix formed by training datasets to
minimize both redundancy and coherency as well as reducing the size of these
datasets. Use of such optimized training data can subsequently improve
classification accuracy and help decrease the computational time needed for the
SRC; it is thus more applicable for online process monitoring. Performance of
the proposed methodology is demonstrated using wearable sensor data obtained
from manual material handling experiments, and is found to be superior to those
of benchmark methods in terms of accuracy, while also requiring computational
time appropriate for MMH online monitoring.","cs.LG,stat.ME,stat.ML"
"Support Feature Machines. Support Vector Machines (SVMs) with various kernels have played dominant role
in machine learning for many years, finding numerous applications. Although
they have many attractive features interpretation of their solutions is quite
difficult, the use of a single kernel type may not be appropriate in all areas
of the input space, convergence problems for some kernels are not uncommon, the
standard quadratic programming solution has $O(m^3)$ time and $O(m^2)$ space
complexity for $m$ training patterns. Kernel methods work because they
implicitly provide new, useful features. Such features, derived from various
kernels and other vector transformations, may be used directly in any machine
learning algorithm, facilitating multiresolution, heterogeneous models of data.
Therefore Support Feature Machines (SFM) based on linear models in the extended
feature spaces, enabling control over selection of support features, give at
least as good results as any kernel-based SVMs, removing all problems related
to interpretation, scaling and convergence. This is demonstrated for a number
of benchmark datasets analyzed with linear discrimination, SVM, decision trees
and nearest neighbor methods.","68T05,91E40,I.2.6,cs.LG,stat.ML"
"Deep Networks and the Multiple Manifold Problem. We study the multiple manifold problem, a binary classification task modeled
on applications in machine vision, in which a deep fully-connected neural
network is trained to separate two low-dimensional submanifolds of the unit
sphere. We provide an analysis of the one-dimensional case, proving for a
simple manifold configuration that when the network depth $L$ is large relative
to certain geometric and statistical properties of the data, the network width
$n$ grows as a sufficiently large polynomial in $L$, and the number of i.i.d.
samples from the manifolds is polynomial in $L$, randomly-initialized gradient
descent rapidly learns to classify the two manifolds perfectly with high
probability. Our analysis demonstrates concrete benefits of depth and width in
the context of a practically-motivated model problem: the depth acts as a
fitting resource, with larger depths corresponding to smoother networks that
can more readily separate the class manifolds, and the width acts as a
statistical resource, enabling concentration of the randomly-initialized
network and its gradients. The argument centers around the neural tangent
kernel and its role in the nonasymptotic analysis of training overparameterized
neural networks; to this literature, we contribute essentially optimal rates of
concentration for the neural tangent kernel of deep fully-connected networks,
requiring width $n \gtrsim L\,\mathrm{poly}(d_0)$ to achieve uniform
concentration of the initial kernel over a $d_0$-dimensional submanifold of the
unit sphere $\mathbb{S}^{n_0-1}$, and a nonasymptotic framework for
establishing generalization of networks trained in the NTK regime with
structured data. The proof makes heavy use of martingale concentration to
optimally treat statistical dependencies across layers of the initial random
network. This approach should be of use in establishing similar results for
other network architectures.","cs.LG,math.OC,stat.ML"
"Signal Transformer: Complex-valued Attention and Meta-Learning for Signal Recognition. Deep neural networks have been shown as a class of useful tools for
addressing signal recognition issues in recent years, especially for
identifying the nonlinear feature structures of signals. However, this power of
most deep learning techniques heavily relies on an abundant amount of training
data, so the performance of classic neural nets decreases sharply when the
number of training data samples is small or unseen data are presented in the
testing phase. This calls for an advanced strategy, i.e., model-agnostic
meta-learning (MAML), which is able to capture the invariant representation of
the data samples or signals. In this paper, inspired by the special structure
of the signal, i.e., real and imaginary parts consisted in practical
time-series signals, we propose a Complex-valued Attentional MEta Learner
(CAMEL) for the problem of few-shot signal recognition by leveraging attention
and meta-learning in the complex domain. To the best of our knowledge, this is
also the first complex-valued MAML that can find the first-order stationary
points of general nonconvex problems with theoretical convergence guarantees.
Extensive experiments results showcase the superiority of the proposed CAMEL
compared with the state-of-the-art methods.","cs.AI,cs.LG,eess.SP"
"Scalable Surface Reconstruction with Delaunay-Graph Neural Networks. We introduce a novel learning-based, visibility-aware, surface reconstruction
method for large-scale, defect-laden point clouds. Our approach can cope with
the scale and variety of point cloud defects encountered in real-life
Multi-View Stereo (MVS) acquisitions. Our method relies on a 3D Delaunay
tetrahedralization whose cells are classified as inside or outside the surface
by a graph neural network and an energy model solvable with a graph cut. Our
model, making use of both local geometric attributes and line-of-sight
visibility information, is able to learn a visibility model from a small amount
of synthetic training data and generalizes to real-life acquisitions. Combining
the efficiency of deep learning methods and the scalability of energy based
models, our approach outperforms both learning and non learning-based
reconstruction algorithms on two publicly available reconstruction benchmarks.","cs.CG,cs.CV"
"Direct White Matter Bundle Segmentation using Stacked U-Nets. The state-of-the-art method for automatically segmenting white matter bundles
in diffusion-weighted MRI is tractography in conjunction with streamline
cluster selection. This process involves long chains of processing steps which
are not only computationally expensive but also complex to setup and tedious
with respect to quality control. Direct bundle segmentation methods treat the
task as a traditional image segmentation problem. While they so far did not
deliver competitive results, they can potentially mitigate many of the
mentioned issues. We present a novel supervised approach for direct tract
segmentation that shows major performance gains. It builds upon a stacked U-Net
architecture which is trained on manual bundle segmentations from Human
Connectome Project subjects. We evaluate our approach \textit{in vivo} as well
as \textit{in silico} using the ISMRM 2015 Tractography Challenge phantom
dataset. We achieve human segmentation performance and a major performance gain
over previous pipelines. We show how the learned spatial priors efficiently
guide the segmentation even at lower image qualities with little quality loss.","cs.CV,q-bio.NC,q-bio.QM"
"MPC-based Reinforcement Learning for Economic Problems with Application to Battery Storage. In this paper, we are interested in optimal control problems with purely
economic costs, which often yield optimal policies having a (nearly) bang-bang
structure. We focus on policy approximations based on Model Predictive Control
(MPC) and the use of the deterministic policy gradient method to optimize the
MPC closed-loop performance in the presence of unmodelled stochasticity or
model error. When the policy has a (nearly) bang-bang structure, we observe
that the policy gradient method can struggle to produce meaningful steps in the
policy parameters. To tackle this issue, we propose a homotopy strategy based
on the interior-point method, providing a relaxation of the policy during the
learning. We investigate a specific well-known battery storage problem, and
show that the proposed method delivers a homogeneous and faster learning than a
classical policy gradient approach.","cs.LG,cs.SY,eess.SY"
"Graphical modelling in continuous-time: consistency guarantees and algorithms using Neural ODEs. The discovery of structure from time series data is a key problem in fields
of study working with complex systems. Most identifiability results and
learning algorithms assume the underlying dynamics to be discrete in time.
Comparatively few, in contrast, explicitly define dependencies in infinitesimal
intervals of time, independently of the scale of observation and of the
regularity of sampling. In this paper, we consider score-based graph learning
for the study of dynamical systems. We prove that for vector fields
parameterized in a large class of neural networks, least squares optimization
with adaptive regularization schemes consistently recovers directed graphs of
local independencies in systems of stochastic differential equations. Using
this insight, we propose a score-based learning algorithm based on penalized
Neural Ordinary Differential Equations (modelling the mean process) that we
show to be applicable to the general setting of irregularly-sampled
multivariate time series and to outperform the state of the art across a range
of dynamical systems.","cs.LG,math.DS,stat.ML"
"ScoreGrad: Multivariate Probabilistic Time Series Forecasting with Continuous Energy-based Generative Models. Multivariate time series prediction has attracted a lot of attention because
of its wide applications such as intelligence transportation, AIOps. Generative
models have achieved impressive results in time series modeling because they
can model data distribution and take noise into consideration. However, many
existing works can not be widely used because of the constraints of functional
form of generative models or the sensitivity to hyperparameters. In this paper,
we propose ScoreGrad, a multivariate probabilistic time series forecasting
framework based on continuous energy-based generative models. ScoreGrad is
composed of time series feature extraction module and conditional stochastic
differential equation based score matching module. The prediction can be
achieved by iteratively solving reverse-time SDE. To the best of our knowledge,
ScoreGrad is the first continuous energy based generative model used for time
series forecasting. Furthermore, ScoreGrad achieves state-of-the-art results on
six real-world datasets. The impact of hyperparameters and sampler types on the
performance are also explored. Code is available at
https://github.com/yantijin/ScoreGradPred.","cs.LG,stat.ML"
"Finding Anomalous Periodic Time Series: An Application to Catalogs of Periodic Variable Stars. Catalogs of periodic variable stars contain large numbers of periodic
light-curves (photometric time series data from the astrophysics domain).
Separating anomalous objects from well-known classes is an important step
towards the discovery of new classes of astronomical objects. Most anomaly
detection methods for time series data assume either a single continuous time
series or a set of time series whose periods are aligned. Light-curve data
precludes the use of these methods as the periods of any given pair of
light-curves may be out of sync. One may use an existing anomaly detection
method if, prior to similarity calculation, one performs the costly act of
aligning two light-curves, an operation that scales poorly to massive data
sets. This paper presents PCAD, an unsupervised anomaly detection method for
large sets of unsynchronized periodic time-series data, that outputs a ranked
list of both global and local anomalies. It calculates its anomaly score for
each light-curve in relation to a set of centroids produced by a modified
k-means clustering algorithm. Our method is able to scale to large data sets
through the use of sampling. We validate our method on both light-curve data
and other time series data sets. We demonstrate its effectiveness at finding
known anomalies, and discuss the effect of sample size and number of centroids
on our results. We compare our method to naive solutions and existing time
series anomaly detection methods for unphased data, and show that PCAD's
reported anomalies are comparable to or better than all other methods. Finally,
astrophysicists on our team have verified that PCAD finds true anomalies that
might be indicative of novel astrophysical phenomena.","astro-ph.IM,cs.LG,physics.data-an"
"A Smartphone-Based Skin Disease Classification Using MobileNet CNN. The MobileNet model was used by applying transfer learning on the 7 skin
diseases to create a skin disease classification system on Android application.
The proponents gathered a total of 3,406 images and it is considered as
imbalanced dataset because of the unequal number of images on its classes.
Using different sampling method and preprocessing of input data was explored to
further improved the accuracy of the MobileNet. Using under-sampling method and
the default preprocessing of input data achieved an 84.28% accuracy. While,
using imbalanced dataset and default preprocessing of input data achieved a
93.6% accuracy. Then, researchers explored oversampling the dataset and the
model attained a 91.8% accuracy. Lastly, by using oversampling technique and
data augmentation on preprocessing the input data provide a 94.4% accuracy and
this model was deployed on the developed Android application.","cs.CV,cs.CY,cs.LG,eess.IV,stat.ML"
"Consistency of Anchor-based Spectral Clustering. Anchor-based techniques reduce the computational complexity of spectral
clustering algorithms. Although empirical tests have shown promising results,
there is currently a lack of theoretical support for the anchoring approach. We
define a specific anchor-based algorithm and show that it is amenable to
rigorous analysis, as well as being effective in practice. We establish the
theoretical consistency of the method in an asymptotic setting where data is
sampled from an underlying continuous probability distribution. In particular,
we provide sharp asymptotic conditions for the algorithm parameters which
ensure that the anchor-based method can recover with high probability disjoint
clusters that are mutually separated by a positive distance. We illustrate the
performance of the algorithm on synthetic data and explain how the theoretical
convergence analysis can be used to inform the practical choice of parameter
scalings. We also test the accuracy and efficiency of the algorithm on two
large scale real data sets. We find that the algorithm offers clear advantages
over standard spectral clustering. We also find that it is competitive with the
state-of-the-art LSC method of Chen and Cai (Twenty-Fifth AAAI Conference on
Artificial Intelligence, 2011), while having the added benefit of a consistency
guarantee.","cs.LG,math.ST,stat.ML,stat.TH"
"DADI: Dynamic Discovery of Fair Information with Adversarial Reinforcement Learning. We introduce a framework for dynamic adversarial discovery of information
(DADI), motivated by a scenario where information (a feature set) is used by
third parties with unknown objectives. We train a reinforcement learning agent
to sequentially acquire a subset of the information while balancing accuracy
and fairness of predictors downstream. Based on the set of already acquired
features, the agent decides dynamically to either collect more information from
the set of available features or to stop and predict using the information that
is currently available. Building on previous work exploring adversarial
representation learning, we attain group fairness (demographic parity) by
rewarding the agent with the adversary's loss, computed over the final feature
set. Importantly, however, the framework provides a more general starting point
for fair or private dynamic information discovery. Finally, we demonstrate
empirically, using two real-world datasets, that we can trade-off fairness and
predictive performance","cs.CY,cs.LG,stat.ML"
"Hierarchical Protein Function Prediction with Tail-GNNs. Protein function prediction may be framed as predicting subgraphs (with
certain closure properties) of a directed acyclic graph describing the
hierarchy of protein functions. Graph neural networks (GNNs), with their
built-in inductive bias for relational data, are hence naturally suited for
this task. However, in contrast with most GNN applications, the graph is not
related to the input, but to the label space. Accordingly, we propose
Tail-GNNs, neural networks which naturally compose with the output space of any
neural network for multi-task prediction, to provide relationally-reinforced
labels. For protein function prediction, we combine a Tail-GNN with a dilated
convolutional network which learns representations of the protein sequence,
making significant improvement in F_1 score and demonstrating the ability of
Tail-GNNs to learn useful representations of labels and exploit them in
real-world problem solving.","cs.LG,q-bio.BM,stat.ML"
"RobustPeriod: Time-Frequency Mining for Robust Multiple Periodicity Detection. Periodicity detection is a crucial step in time series tasks, including
monitoring and forecasting of metrics in many areas, such as IoT applications
and self-driving database management system. In many of these applications,
multiple periodic components exist and are often interlaced with each other.
Such dynamic and complicated periodic patterns make the accurate periodicity
detection difficult. In addition, other components in the time series, such as
trend, outliers and noises, also pose additional challenges for accurate
periodicity detection. In this paper, we propose a robust and general framework
for multiple periodicity detection. Our algorithm applies maximal overlap
discrete wavelet transform to transform the time series into multiple
temporal-frequency scales such that different periodic components can be
isolated. We rank them by wavelet variance, and then at each scale detect
single periodicity by our proposed Huber-periodogram and Huber-ACF robustly. We
rigorously prove the theoretical properties of Huber-periodogram and justify
the use of Fisher's test on Huber-periodogram for periodicity detection. To
further refine the detected periods, we compute unbiased autocorrelation
function based on Wiener-Khinchin theorem from Huber-periodogram for improved
robustness and efficiency. Experiments on synthetic and real-world datasets
show that our algorithm outperforms other popular ones for both single and
multiple periodicity detection.","cs.LG,eess.SP,stat.AP,stat.ML"
"Efficient Model-Based Reinforcement Learning through Optimistic Policy Search and Planning. Model-based reinforcement learning algorithms with probabilistic dynamical
models are amongst the most data-efficient learning methods. This is often
attributed to their ability to distinguish between epistemic and aleatoric
uncertainty. However, while most algorithms distinguish these two uncertainties
for learning the model, they ignore it when optimizing the policy, which leads
to greedy and insufficient exploration. At the same time, there are no
practical solvers for optimistic exploration algorithms. In this paper, we
propose a practical optimistic exploration algorithm (H-UCRL). H-UCRL
reparameterizes the set of plausible models and hallucinates control directly
on the epistemic uncertainty. By augmenting the input space with the
hallucinated inputs, H-UCRL can be solved using standard greedy planners.
Furthermore, we analyze H-UCRL and construct a general regret bound for
well-calibrated models, which is provably sublinear in the case of Gaussian
Process models. Based on this theoretical foundation, we show how optimistic
exploration can be easily combined with state-of-the-art reinforcement learning
algorithms and different probabilistic models. Our experiments demonstrate that
optimistic exploration significantly speeds-up learning when there are
penalties on actions, a setting that is notoriously difficult for existing
model-based reinforcement learning algorithms.","cs.LG,cs.RO,cs.SY,eess.SY,stat.ML"
"Malicious Network Traffic Detection via Deep Learning: An Information Theoretic View. The attention that deep learning has garnered from the academic community and
industry continues to grow year over year, and it has been said that we are in
a new golden age of artificial intelligence research. However, neural networks
are still often seen as a ""black box"" where learning occurs but cannot be
understood in a human-interpretable way. Since these machine learning systems
are increasingly being adopted in security contexts, it is important to explore
these interpretations. We consider an Android malware traffic dataset for
approaching this problem. Then, using the information plane, we explore how
homeomorphism affects learned representation of the data and the invariance of
the mutual information captured by the parameters on that data. We empirically
validate these results, using accuracy as a second measure of similarity of
learned representations.
  Our results suggest that although the details of learned representations and
the specific coordinate system defined over the manifold of all parameters
differ slightly, the functional approximations are the same. Furthermore, our
results show that since mutual information remains invariant under
homeomorphism, only feature engineering methods that alter the entropy of the
dataset will change the outcome of the neural network. This means that for some
datasets and tasks, neural networks require meaningful, human-driven feature
engineering or changes in architecture to provide enough information for the
neural network to generate a sufficient statistic. Applying our results can
serve to guide analysis methods for machine learning engineers and suggests
that neural networks that can exploit the convolution theorem are equally
accurate as standard convolutional neural networks, and can be more
computationally efficient.","cs.CR,cs.IT,cs.LG,math.IT,stat.ML"
"gradSim: Differentiable simulation for system identification and visuomotor control. We consider the problem of estimating an object's physical properties such as
mass, friction, and elasticity directly from video sequences. Such a system
identification problem is fundamentally ill-posed due to the loss of
information during image formation. Current solutions require precise 3D labels
which are labor-intensive to gather, and infeasible to create for many systems
such as deformable solids or cloth. We present gradSim, a framework that
overcomes the dependence on 3D supervision by leveraging differentiable
multiphysics simulation and differentiable rendering to jointly model the
evolution of scene dynamics and image formation. This novel combination enables
backpropagation from pixels in a video sequence through to the underlying
physical attributes that generated them. Moreover, our unified computation
graph -- spanning from the dynamics and through the rendering process --
enables learning in challenging visuomotor control tasks, without relying on
state-based (3D) supervision, while obtaining performance competitive to or
better than techniques that rely on precise 3D labels.","cs.AI,cs.CV,cs.LG,cs.RO"
"Discrete schemes for Gaussian curvature and their convergence. In this paper, several discrete schemes for Gaussian curvature are surveyed.
The convergence property of a modified discrete scheme for the Gaussian
curvature is considered. Furthermore, a new discrete scheme for Gaussian
curvature is resented. We prove that the new scheme converges at the regular
vertex with valence not less than 5. By constructing a counterexample, we also
show that it is impossible for building a discrete scheme for Gaussian
curvature which converges over the regular vertex with valence 4. Finally,
asymptotic errors of several discrete scheme for Gaussian curvature are
compared.","cs.CG,cs.CV,cs.GR,cs.NA"
"Learning to Optimize in Swarms. Learning to optimize has emerged as a powerful framework for various
optimization and machine learning tasks. Current such ""meta-optimizers"" often
learn in the space of continuous optimization algorithms that are point-based
and uncertainty-unaware. To overcome the limitations, we propose a
meta-optimizer that learns in the algorithmic space of both point-based and
population-based optimization algorithms. The meta-optimizer targets at a
meta-loss function consisting of both cumulative regret and entropy.
Specifically, we learn and interpret the update formula through a population of
LSTMs embedded with sample- and feature-level attentions. Meanwhile, we
estimate the posterior directly over the global optimum and use an uncertainty
measure to help guide the learning process. Empirical results over non-convex
test functions and the protein-docking application demonstrate that this new
meta-optimizer outperforms existing competitors.","cs.LG,q-bio.BM,stat.ML"
"Graph Attention Network For Microwave Imaging of Brain Anomaly. So far, numerous learned models have been pressed to use in microwave imaging
problems. These models however, are oblivious to the imaging geometry. It has
always been hard to bake the physical setup of the imaging array into the
structure of the network, resulting in a data-intensive models that are not
practical. This work put forward a graph formulation of the microwave imaging
array. The architectures proposed is made cognizant of the physical setup,
allowing it to incorporate the symmetries, resulting in a less data
requirements. Graph convolution and attention mechanism is deployed to handle
the cases of fully-connected graphs corresponding to multi-static arrays. The
graph-treatment of the problem is evaluated on experimental setup in context of
brain anomaly localization with microwave imaging.","cs.LG,eess.IV,q-bio.NC"
"TacticZero: Learning to Prove Theorems from Scratch with Deep Reinforcement Learning. We propose a novel approach to interactive theorem-proving (ITP) using deep
reinforcement learning. The proposed framework is able to learn proof search
strategies as well as tactic and arguments prediction in an end-to-end manner.
We formulate the process of ITP as a Markov decision process (MDP) in which
each state represents a set of potential derivation paths. This structure
allows us to introduce a novel backtracking mechanism which enables the agent
to efficiently discard (predicted) dead-end derivations and restart from
promising alternatives. We implement the framework in the HOL4 theorem prover.
Experimental results show that the framework outperforms existing automated
theorem provers (i.e., hammers) available in HOL4 when evaluated on unseen
problems. We further elaborate the role of key components of the framework
using ablation studies.","cs.AI,cs.LG,cs.LO"
"Learning to attend in a brain-inspired deep neural network. Recent machine learning models have shown that including attention as a
component results in improved model accuracy and interpretability, despite the
concept of attention in these approaches only loosely approximating the brain's
attention mechanism. Here we extend this work by building a more brain-inspired
deep network model of the primate ATTention Network (ATTNet) that learns to
shift its attention so as to maximize the reward. Using deep reinforcement
learning, ATTNet learned to shift its attention to the visual features of a
target category in the context of a search task. ATTNet's dorsal layers also
learned to prioritize these shifts of attention so as to maximize success of
the ventral pathway classification and receive greater reward. Model behavior
was tested against the fixations made by subjects searching images for the same
cued category. Both subjects and ATTNet showed evidence for attention being
preferentially directed to target goals, behaviorally measured as oculomotor
guidance to targets. More fundamentally, ATTNet learned to shift its attention
to target like objects and spatially route its visual inputs to accomplish the
task. This work makes a step toward a better understanding of the role of
attention in the brain and other computational systems.","cs.CV,q-bio.NC"
"Variable Name Recovery in Decompiled Binary Code using Constrained Masked Language Modeling. Decompilation is the procedure of transforming binary programs into a
high-level representation, such as source code, for human analysts to examine.
While modern decompilers can reconstruct and recover much information that is
discarded during compilation, inferring variable names is still extremely
difficult. Inspired by recent advances in natural language processing, we
propose a novel solution to infer variable names in decompiled code based on
Masked Language Modeling, Byte-Pair Encoding, and neural architectures such as
Transformers and BERT. Our solution takes \textit{raw} decompiler output, the
less semantically meaningful code, as input, and enriches it using our proposed
\textit{finetuning} technique, Constrained Masked Language Modeling. Using
Constrained Masked Language Modeling introduces the challenge of predicting the
number of masked tokens for the original variable name. We address this
\textit{count of token prediction} challenge with our post-processing
algorithm. Compared to the state-of-the-art approaches, our trained VarBERT
model is simpler and of much better performance. We evaluated our model on an
existing large-scale data set with 164,632 binaries and showed that it can
predict variable names identical to the ones present in the original source
code up to 84.15\% of the time.","cs.CL,cs.CR,cs.LG"
"Subsampling Bias and The Best-Discrepancy Systematic Cross Validation. Statistical machine learning models should be evaluated and validated before
putting to work. Conventional k-fold Monte Carlo Cross-Validation (MCCV)
procedure uses a pseudo-random sequence to partition instances into k subsets,
which usually causes subsampling bias, inflates generalization errors and
jeopardizes the reliability and effectiveness of cross-validation. Based on
ordered systematic sampling theory in statistics and low-discrepancy sequence
theory in number theory, we propose a new k-fold cross-validation procedure by
replacing a pseudo-random sequence with a best-discrepancy sequence, which
ensures low subsampling bias and leads to more precise
Expected-Prediction-Error estimates. Experiments with 156 benchmark datasets
and three classifiers (logistic regression, decision tree and naive bayes) show
that in general, our cross-validation procedure can extrude subsampling bias in
the MCCV by lowering the EPE around 7.18% and the variances around 26.73%. In
comparison, the stratified MCCV can reduce the EPE and variances of the MCCV
around 1.58% and 11.85% respectively. The Leave-One-Out (LOO) can lower the EPE
around 2.50% but its variances are much higher than the any other CV procedure.
The computational time of our cross-validation procedure is just 8.64% of the
MCCV, 8.67% of the stratified MCCV and 16.72% of the LOO. Experiments also show
that our approach is more beneficial for datasets characterized by relatively
small size and large aspect ratio. This makes our approach particularly
pertinent when solving bioscience classification problems. Our proposed
systematic subsampling technique could be generalized to other machine learning
algorithms that involve random subsampling mechanism.","11J71,62-07,62G09,68T05,cs.LG,stat.CO,stat.ME,stat.ML"
"Comparing Prophet and Deep Learning to ARIMA in Forecasting Wholesale Food Prices. Setting sale prices correctly is of great importance for firms, and the study
and forecast of prices time series is therefore a relevant topic not only from
a data science perspective but also from an economic and applicative one. In
this paper we examine different techniques to forecast sale prices applied by
an Italian food wholesaler, as a step towards the automation of pricing tasks
usually taken care by human workforce. We consider ARIMA models and compare
them to Prophet, a scalable forecasting tool by Facebook based on a generalized
additive model, and to deep learning models exploiting Long Short--Term Memory
(LSTM) and Convolutional Neural Networks (CNNs). ARIMA models are frequently
used in econometric analyses, providing a good benchmark for the problem under
study. Our results indicate that ARIMA models and LSTM neural networks perform
similarly for the forecasting task under consideration, while the combination
of CNNs and LSTMs attains the best overall accuracy, but requires more time to
be tuned. On the contrary, Prophet is quick and easy to use, but considerably
less accurate.t overall accuracy, but requires more time to be tuned. On the
contrary, Prophet is quick and easy to use, but considerably less accurate.","cs.LG,stat.AP"
"Efficient Robust Watermarking Based on Quaternion Singular Value Decomposition and Coefficient Pair Selection. Quaternion singular value decomposition (QSVD) is a robust technique of
digital watermarking which can extract high quality watermarks from watermarked
images with low distortion. In this paper, QSVD technique is further
investigated and an efficient robust watermarking scheme is proposed. The
improved algebraic structure-preserving method is proposed to handle the
problem of ""explosion of complexity"" occurred in the conventional QSVD design.
Secret information is transmitted blindly by incorporating in QSVD two new
strategies, namely, coefficient pair selection and adaptive embedding. Unlike
conventional QSVD which embeds watermarks in a single imaginary unit, we
propose to adaptively embed the watermark into the optimal hiding position
using the Normalized Cross-Correlation (NC) method. This avoids the selection
of coefficient pair with less correlation, and thus, it reduces embedding
impact by decreasing the maximum modification of coefficient values. In this
way, compared with conventional QSVD, the proposed watermarking strategy avoids
more modifications to a single color image layer and a better visual quality of
the watermarked image is observed. Meanwhile, adaptive QSVD resists some common
geometric attacks, and it improves the robustness of conventional QSVD. With
these improvements, our method outperforms conventional QSVD. Its superiority
over other state-of-the-art methods is also demonstrated experimentally.","65F55,I.4.1,cs.CV,cs.NA,math.NA"
"BreakingBED -- Breaking Binary and Efficient Deep Neural Networks by Adversarial Attacks. Deploying convolutional neural networks (CNNs) for embedded applications
presents many challenges in balancing resource-efficiency and task-related
accuracy. These two aspects have been well-researched in the field of CNN
compression. In real-world applications, a third important aspect comes into
play, namely the robustness of the CNN. In this paper, we thoroughly study the
robustness of uncompressed, distilled, pruned and binarized neural networks
against white-box and black-box adversarial attacks (FGSM, PGD, C&W, DeepFool,
LocalSearch and GenAttack). These new insights facilitate defensive training
schemes or reactive filtering methods, where the attack is detected and the
input is discarded and/or cleaned. Experimental results are shown for distilled
CNNs, agent-based state-of-the-art pruned models, and binarized neural networks
(BNNs) such as XNOR-Net and ABC-Net, trained on CIFAR-10 and ImageNet datasets.
We present evaluation methods to simplify the comparison between CNNs under
different attack schemes using loss/accuracy levels, stress-strain graphs,
box-plots and class activation mapping (CAM). Our analysis reveals susceptible
behavior of uncompressed and pruned CNNs against all kinds of attacks. The
distilled models exhibit their strength against all white box attacks with an
exception of C&W. Furthermore, binary neural networks exhibit resilient
behavior compared to their baselines and other compressed variants.","cs.CR,cs.LG"
"On Attribution of Deepfakes. Progress in generative modelling, especially generative adversarial networks,
have made it possible to efficiently synthesize and alter media at scale.
Malicious individuals now rely on these machine-generated media, or deepfakes,
to manipulate social discourse. In order to ensure media authenticity, existing
research is focused on deepfake detection. Yet, the adversarial nature of
frameworks used for generative modeling suggests that progress towards
detecting deepfakes will enable more realistic deepfake generation. Therefore,
it comes at no surprise that developers of generative models are under the
scrutiny of stakeholders dealing with misinformation campaigns. At the same
time, generative models have a lot of positive applications. As such, there is
a clear need to develop tools that ensure the transparent use of generative
modeling, while minimizing the harm caused by malicious applications.
  Our technique optimizes over the source of entropy of each generative model
to probabilistically attribute a deepfake to one of the models. We evaluate our
method on the seminal example of face synthesis, demonstrating that our
approach achieves 97.62% attribution accuracy, and is less sensitive to
perturbations and adversarial examples. We discuss the ethical implications of
our work, identify where our technique can be used, and highlight that a more
meaningful legislative framework is required for a more transparent and ethical
use of generative modeling. Finally, we argue that model developers should be
capable of claiming plausible deniability and propose a second framework to do
so -- this allows a model developer to produce evidence that they did not
produce media that they are being accused of having produced.","cs.CR,cs.CV,cs.CY,cs.LG"
"LIME: Learning Inductive Bias for Primitives of Mathematical Reasoning. While designing inductive bias in neural architectures has been widely
studied, we hypothesize that transformer networks are flexible enough to learn
inductive bias from suitable generic tasks. Here, we replace architecture
engineering by encoding inductive bias in the form of datasets. Inspired by
Peirce's view that deduction, induction, and abduction form an irreducible set
of reasoning primitives, we design three synthetic tasks that are intended to
require the model to have these three abilities. We specifically design these
synthetic tasks in a way that they are devoid of mathematical knowledge to
ensure that only the fundamental reasoning biases can be learned from these
tasks. This defines a new pre-training methodology called ""LIME"" (Learning
Inductive bias for Mathematical rEasoning). Models trained with LIME
significantly outperform vanilla transformers on three very different large
mathematical reasoning benchmarks. Unlike dominating the computation cost as
traditional pre-training approaches, LIME requires only a small fraction of the
computation cost of the typical downstream task.","cs.AI,cs.LG,cs.LO"
"FinGAT: Financial Graph Attention Networks for Recommending Top-K Profitable Stocks. Financial technology (FinTech) has drawn much attention among investors and
companies. While conventional stock analysis in FinTech targets at predicting
stock prices, less effort is made for profitable stock recommendation. Besides,
in existing approaches on modeling time series of stock prices, the
relationships among stocks and sectors (i.e., categories of stocks) are either
neglected or pre-defined. Ignoring stock relationships will miss the
information shared between stocks while using pre-defined relationships cannot
depict the latent interactions or influence of stock prices between stocks. In
this work, we aim at recommending the top-K profitable stocks in terms of
return ratio using time series of stock prices and sector information. We
propose a novel deep learning-based model, Financial Graph Attention Networks
(FinGAT), to tackle the task under the setting that no pre-defined
relationships between stocks are given. The idea of FinGAT is three-fold.
First, we devise a hierarchical learning component to learn short-term and
long-term sequential patterns from stock time series. Second, a fully-connected
graph between stocks and a fully-connected graph between sectors are
constructed, along with graph attention networks, to learn the latent
interactions among stocks and sectors. Third, a multi-task objective is devised
to jointly recommend the profitable stocks and predict the stock movement.
Experiments conducted on Taiwan Stock, S&P 500, and NASDAQ datasets exhibit
remarkable recommendation performance of our FinGAT, comparing to
state-of-the-art methods.","cs.CE,cs.IR,cs.LG,cs.SI"
"Geometry Based Machining Feature Retrieval with Inductive Transfer Learning. Manufacturing industries have widely adopted the reuse of machine parts as a
method to reduce costs and as a sustainable manufacturing practice.
Identification of reusable features from the design of the parts and finding
their similar features from the database is an important part of this process.
In this project, with the help of fully convolutional geometric features, we
are able to extract and learn the high level semantic features from CAD models
with inductive transfer learning. The extracted features are then compared with
that of other CAD models from the database using Frobenius norm and identical
features are retrieved. Later we passed the extracted features to a deep
convolutional neural network with a spatial pyramid pooling layer and the
performance of the feature retrieval increased significantly. It was evident
from the results that the model could effectively capture the geometrical
elements from machining features.","68T07,I.4; I.2; I.5,cs.AI,cs.CV"
"Explainable $k$-Means and $k$-Medians Clustering. Clustering is a popular form of unsupervised learning for geometric data.
Unfortunately, many clustering algorithms lead to cluster assignments that are
hard to explain, partially because they depend on all the features of the data
in a complicated way. To improve interpretability, we consider using a small
decision tree to partition a data set into clusters, so that clusters can be
characterized in a straightforward manner. We study this problem from a
theoretical viewpoint, measuring cluster quality by the $k$-means and
$k$-medians objectives: Must there exist a tree-induced clustering whose cost
is comparable to that of the best unconstrained clustering, and if so, how can
it be found? In terms of negative results, we show, first, that popular
top-down decision tree algorithms may lead to clusterings with arbitrarily
large cost, and second, that any tree-induced clustering must in general incur
an $\Omega(\log k)$ approximation factor compared to the optimal clustering. On
the positive side, we design an efficient algorithm that produces explainable
clusters using a tree with $k$ leaves. For two means/medians, we show that a
single threshold cut suffices to achieve a constant factor approximation, and
we give nearly-matching lower bounds. For general $k \geq 2$, our algorithm is
an $O(k)$ approximation to the optimal $k$-medians and an $O(k^2)$
approximation to the optimal $k$-means. Prior to our work, no algorithms were
known with provable guarantees independent of dimension and input size.","cs.CG,cs.DS,cs.LG,stat.ML"
"Improving GAN Training with Probability Ratio Clipping and Sample Reweighting. Despite success on a wide range of problems related to vision, generative
adversarial networks (GANs) often suffer from inferior performance due to
unstable training, especially for text generation. To solve this issue, we
propose a new variational GAN training framework which enjoys superior training
stability. Our approach is inspired by a connection of GANs and reinforcement
learning under a variational perspective. The connection leads to (1)
probability ratio clipping that regularizes generator training to prevent
excessively large updates, and (2) a sample re-weighting mechanism that
improves discriminator training by downplaying bad-quality fake samples.
Moreover, our variational GAN framework can provably overcome the training
issue in many GANs that an optimal discriminator cannot provide any informative
gradient to training generator. By plugging the training approach in diverse
state-of-the-art GAN architectures, we obtain significantly improved
performance over a range of tasks, including text generation, text style
transfer, and image generation.","cs.CL,cs.LG,stat.ML"
"DISIR: Deep Image Segmentation with Interactive Refinement. This paper presents an interactive approach for multi-class segmentation of
aerial images. Precisely, it is based on a deep neural network which exploits
both RGB images and annotations. Starting from an initial output based on the
image only, our network then interactively refines this segmentation map using
a concatenation of the image and user annotations. Importantly, user
annotations modify the inputs of the network - not its weights - enabling a
fast and smooth process. Through experiments on two public aerial datasets, we
show that user annotations are extremely rewarding: each click corrects roughly
5000 pixels. We analyze the impact of different aspects of our framework such
as the representation of the annotations, the volume of training data or the
network architecture. Code is available at https://github.com/delair-ai/DISIR.","cs.CV,cs.HC"
"NeMo: a toolkit for building AI applications using Neural Modules. NeMo (Neural Modules) is a Python framework-agnostic toolkit for creating AI
applications through re-usability, abstraction, and composition. NeMo is built
around neural modules, conceptual blocks of neural networks that take typed
inputs and produce typed outputs. Such modules typically represent data layers,
encoders, decoders, language models, loss functions, or methods of combining
activations. NeMo makes it easy to combine and re-use these building blocks
while providing a level of semantic correctness checking via its neural type
system. The toolkit comes with extendable collections of pre-built modules for
automatic speech recognition and natural language processing. Furthermore, NeMo
provides built-in support for distributed training and mixed precision on
latest NVIDIA GPUs. NeMo is open-source https://github.com/NVIDIA/NeMo","cs.CL,cs.LG,cs.SD,eess.AS"
"WaveFuse: A Unified Deep Framework for Image Fusion with Discrete Wavelet Transform. We propose an unsupervised image fusion architecture for multiple application
scenarios based on the combination of multi-scale discrete wavelet transform
through regional energy and deep learning. To our best knowledge, this is the
first time the conventional image fusion method has been combined with deep
learning. The useful information of feature maps can be utilized adequately
through multi-scale discrete wavelet transform in our proposed method.Compared
with other state-of-the-art fusion method, the proposed algorithm exhibits
better fusion performance in both subjective and objective evaluation.
Moreover, it's worth mentioning that comparable fusion performance trained in
COCO dataset can be obtained by training with a much smaller dataset with only
hundreds of images chosen randomly from COCO. Hence, the training time is
shortened substantially, leading to the improvement of the model's performance
both in practicality and training efficiency.","cs.CV,cs.IT,cs.LG,math.IT"
"BoundaryNet: An Attentive Deep Network with Fast Marching Distance Maps for Semi-automatic Layout Annotation. Precise boundary annotations of image regions can be crucial for downstream
applications which rely on region-class semantics. Some document collections
contain densely laid out, highly irregular and overlapping multi-class region
instances with large range in aspect ratio. Fully automatic boundary estimation
approaches tend to be data intensive, cannot handle variable-sized images and
produce sub-optimal results for aforementioned images. To address these issues,
we propose BoundaryNet, a novel resizing-free approach for high-precision
semi-automatic layout annotation. The variable-sized user selected region of
interest is first processed by an attention-guided skip network. The network
optimization is guided via Fast Marching distance maps to obtain a good quality
initial boundary estimate and an associated feature representation. These
outputs are processed by a Residual Graph Convolution Network optimized using
Hausdorff loss to obtain the final region boundary. Results on a challenging
image manuscript dataset demonstrate that BoundaryNet outperforms strong
baselines and produces high-quality semantic region boundaries. Qualitatively,
our approach generalizes across multiple document image datasets containing
different script systems and layouts, all without additional fine-tuning. We
integrate BoundaryNet into a document annotation system and show that it
provides high annotation throughput compared to manual and fully automatic
alternatives.","cs.CL,cs.CV,cs.MM"
"Compositional Abstraction Error and a Category of Causal Models. Interventional causal models describe several joint distributions over some
variables used to describe a system, one for each intervention setting. They
provide a formal recipe for how to move between the different joint
distributions and make predictions about the variables upon intervening on the
system. Yet, it is difficult to formalise how we may change the underlying
variables used to describe the system, say moving from fine-grained to
coarse-grained variables. Here, we argue that compositionality is a desideratum
for such model transformations and the associated errors: When abstracting a
reference model M iteratively, first obtaining M' and then further simplifying
that to obtain M'', we expect the composite transformation from M to M'' to
exist and its error to be bounded by the errors incurred by each individual
transformation step. Category theory, the study of mathematical objects via
compositional transformations between them, offers a natural language to
develop our framework for model transformations and abstractions. We introduce
a category of finite interventional causal models and, leveraging theory of
enriched categories, prove the desired compositionality properties for our
framework.","cs.AI,cs.LG,cs.LO,math.CT,stat.ML"
"Knowledge Distillation in Wide Neural Networks: Risk Bound, Data Efficiency and Imperfect Teacher. Knowledge distillation is a strategy of training a student network with guide
of the soft output from a teacher network. It has been a successful method of
model compression and knowledge transfer. However, currently knowledge
distillation lacks a convincing theoretical understanding. On the other hand,
recent finding on neural tangent kernel enables us to approximate a wide neural
network with a linear model of the network's random features. In this paper, we
theoretically analyze the knowledge distillation of a wide neural network.
First we provide a transfer risk bound for the linearized model of the network.
Then we propose a metric of the task's training difficulty, called data
inefficiency. Based on this metric, we show that for a perfect teacher, a high
ratio of teacher's soft labels can be beneficial. Finally, for the case of
imperfect teacher, we find that hard labels can correct teacher's wrong
prediction, which explains the practice of mixing hard and soft labels.","cs.AI,cs.LG,stat.ML"
"3D Geometric salient patterns analysis on 3D meshes. Pattern analysis is a wide domain that has wide applicability in many fields.
In fact, texture analysis is one of those fields, since the texture is defined
as a set of repetitive or quasi-repetitive patterns. Despite its importance in
analyzing 3D meshes, geometric texture analysis is less studied by geometry
processing community. This paper presents a new efficient approach for
geometric texture analysis on 3D triangular meshes. The proposed method is a
scale-aware approach that takes as input a 3D mesh and a user-scale. It
provides, as a result, a similarity-based clustering of texels in meaningful
classes. Experimental results of the proposed algorithm are presented for both
real-world and synthetic meshes within various textures. Furthermore, the
efficiency of the proposed approach was experimentally demonstrated under mesh
simplification and noise addition on the mesh surface. In this paper, we
present a practical application for semantic annotation of 3D geometric salient
texels.","cs.CV,cs.GR"
"The Neural Tangent Link Between CNN Denoisers and Non-Local Filters. Convolutional Neural Networks (CNNs) are now a well-established tool for
solving computational imaging problems. Modern CNN-based algorithms obtain
state-of-the-art performance in diverse image restoration problems.
Furthermore, it has been recently shown that, despite being highly
overparameterized, networks trained with a single corrupted image can still
perform as well as fully trained networks. We introduce a formal link between
such networks through their neural tangent kernel (NTK), and well-known
non-local filtering techniques, such as non-local means or BM3D. The filtering
function associated with a given network architecture can be obtained in closed
form without need to train the network, being fully characterized by the random
initialization of the network weights. While the NTK theory accurately predicts
the filter associated with networks trained using standard gradient descent,
our analysis shows that it falls short to explain the behaviour of networks
trained using the popular Adam optimizer. The latter achieves a larger change
of weights in hidden layers, adapting the non-local filtering function during
training. We evaluate our findings via extensive image denoising experiments.","68T07,cs.CV,eess.IV,eess.SP"
"Meta-learning based Alternating Minimization Algorithm for Non-convex Optimization. In this paper, we propose a novel solution for non-convex problems of
multiple variables, especially for those typically solved by an alternating
minimization (AM) strategy that splits the original optimization problem into a
set of sub-problems corresponding to each variable, and then iteratively
optimize each sub-problem using a fixed updating rule. However, due to the
intrinsic non-convexity of the original optimization problem, the optimization
can usually be trapped into spurious local minimum even when each sub-problem
can be optimally solved at each iteration. Meanwhile, learning-based
approaches, such as deep unfolding algorithms, are highly limited by the lack
of labelled data and restricted explainability. To tackle these issues, we
propose a meta-learning based alternating minimization (MLAM) method, which
aims to minimize a partial of the global losses over iterations instead of
carrying minimization on each sub-problem, and it tends to learn an adaptive
strategy to replace the handcrafted counterpart resulting in advance on
superior performance. Meanwhile, the proposed MLAM still maintains the original
algorithmic principle, which contributes to a better interpretability. We
evaluate the proposed method on two representative problems, namely, bi-linear
inverse problem: matrix completion, and non-linear problem: Gaussian mixture
models. The experimental results validate that our proposed approach
outperforms AM-based methods in standard settings, and is able to achieve
effective optimization in challenging cases while other comparing methods would
typically fail.","cs.LG,math.OC,stat.ML"
"A Review of Graph Neural Networks and Their Applications in Power Systems. Deep neural networks have revolutionized many machine learning tasks in power
systems, ranging from pattern recognition to signal processing. The data in
these tasks is typically represented in Euclidean domains. Nevertheless, there
is an increasing number of applications in power systems, where data are
collected from non-Euclidean domains and represented as graph-structured data
with high dimensional features and interdependency among nodes. The complexity
of graph-structured data has brought significant challenges to the existing
deep neural networks defined in Euclidean domains. Recently, many publications
generalizing deep neural networks for graph-structured data in power systems
have emerged. In this paper, a comprehensive overview of graph neural networks
(GNNs) in power systems is proposed. Specifically, several classical paradigms
of GNNs structures (e.g., graph convolutional networks) are summarized, and key
applications in power systems, such as fault scenario application, time series
prediction, power flow calculation, and data generation are reviewed in detail.
Furthermore, main issues and some research trends about the applications of
GNNs in power systems are discussed.","cs.AI,cs.LG,cs.SY,eess.SY"
"A game-theoretic approach for Generative Adversarial Networks. Generative adversarial networks (GANs) are a class of generative models,
known for producing accurate samples. The key feature of GANs is that there are
two antagonistic neural networks: the generator and the discriminator. The main
bottleneck for their implementation is that the neural networks are very hard
to train. One way to improve their performance is to design reliable algorithms
for the adversarial process. Since the training can be cast as a stochastic
Nash equilibrium problem, we rewrite it as a variational inequality and
introduce an algorithm to compute an approximate solution. Specifically, we
propose a stochastic relaxed forward-backward algorithm for GANs. We prove that
when the pseudogradient mapping of the game is monotone, we have convergence to
an exact solution or in a neighbourhood of it.","cs.GT,cs.LG,math.OC,stat.ML"
"Deep learning-based super-resolution in coherent imaging systems. We present a deep learning framework based on a generative adversarial
network (GAN) to perform super-resolution in coherent imaging systems. We
demonstrate that this framework can enhance the resolution of both pixel
size-limited and diffraction-limited coherent imaging systems. We
experimentally validated the capabilities of this deep learning-based coherent
imaging approach by super-resolving complex images acquired using a lensfree
on-chip holographic microscope, the resolution of which was pixel size-limited.
Using the same GAN-based approach, we also improved the resolution of a
lens-based holographic imaging system that was limited in resolution by the
numerical aperture of its objective lens. This deep learning-based
super-resolution framework can be broadly applied to enhance the
space-bandwidth product of coherent imaging systems using image data and
convolutional neural networks, and provides a rapid, non-iterative method for
solving inverse image reconstruction or enhancement problems in optics.","62M45,68T01,68T05,68U10,78M32,92C55,94A08,I.2; I.2.1; I.2.6; I.2.10; I.4.5; I.4.9,cs.CV,cs.LG,physics.app-ph,physics.optics"
"RATQ: A Universal Fixed-Length Quantizer for Stochastic Optimization. We present Rotated Adaptive Tetra-iterated Quantizer (RATQ), a fixed-length
quantizer for gradients in first order stochastic optimization. RATQ is easy to
implement and involves only a Hadamard transform computation and adaptive
uniform quantization with appropriately chosen dynamic ranges. For noisy
gradients with almost surely bounded Euclidean norms, we establish an
information theoretic lower bound for optimization accuracy using finite
precision gradients and show that RATQ almost attains this lower bound.
  For mean square bounded noisy gradients, we use a gain-shape quantizer which
separately quantizes the Euclidean norm and uses RATQ to quantize the
normalized unit norm vector. We establish lower bounds for performance of any
optimization procedure and shape quantizer, when used with a uniform gain
quantizer. Finally, we propose an adaptive quantizer for gain which when used
with RATQ for shape quantizer outperforms uniform gain quantization and is, in
fact, close to optimal.
  As a by-product, we show that our fixed-length quantizer RATQ has almost the
same performance as the optimal variable-length quantizers for distributed mean
estimation. Also, we obtain an efficient quantizer for Gaussian vectors which
attains a rate very close to the Gaussian rate-distortion function and is, in
fact, universal for subgaussian input vectors.","cs.IT,cs.LG,math.IT,math.OC,stat.ML"
"Learning Personal Style from Few Examples. A key task in design work is grasping the client's implicit tastes. Designers
often do this based on a set of examples from the client. However, recognizing
a common pattern among many intertwining variables such as color, texture, and
layout and synthesizing them into a composite preference can be challenging. In
this paper, we leverage the pattern recognition capability of computational
models to aid in this task. We offer a set of principles for computationally
learning personal style. The principles are manifested in PseudoClient, a deep
learning framework that learns a computational model for personal graphic
design style from only a handful of examples. In several experiments, we found
that PseudoClient achieves a 79.40% accuracy with only five positive and
negative examples, outperforming several alternative methods. Finally, we
discuss how PseudoClient can be utilized as a building block to support the
development of future design applications.","H.5.0; I.5.4; J.5,cs.CV,cs.HC"
"Accuracy and Privacy Evaluations of Collaborative Data Analysis. Distributed data analysis without revealing the individual data has recently
attracted significant attention in several applications. A collaborative data
analysis through sharing dimensionality reduced representations of data has
been proposed as a non-model sharing-type federated learning. This paper
analyzes the accuracy and privacy evaluations of this novel framework. In the
accuracy analysis, we provided sufficient conditions for the equivalence of the
collaborative data analysis and the centralized analysis with dimensionality
reduction. In the privacy analysis, we proved that collaborative users' private
datasets are protected with a double privacy layer against insider and external
attacking scenarios.","cs.CR,cs.LG"
"Penalized Likelihood Methods for Estimation of Sparse High Dimensional Directed Acyclic Graphs. Directed acyclic graphs (DAGs) are commonly used to represent causal
relationships among random variables in graphical models. Applications of these
models arise in the study of physical, as well as biological systems, where
directed edges between nodes represent the influence of components of the
system on each other. The general problem of estimating DAGs from observed data
is computationally NP-hard, Moreover two directed graphs may be observationally
equivalent. When the nodes exhibit a natural ordering, the problem of
estimating directed graphs reduces to the problem of estimating the structure
of the network. In this paper, we propose a penalized likelihood approach that
directly estimates the adjacency matrix of DAGs. Both lasso and adaptive lasso
penalties are considered and an efficient algorithm is proposed for estimation
of high dimensional DAGs. We study variable selection consistency of the two
penalties when the number of variables grows to infinity with the sample size.
We show that although lasso can only consistently estimate the true network
under stringent assumptions, adaptive lasso achieves this task under mild
regularity conditions. The performance of the proposed methods is compared to
alternative methods in simulated, as well as real, data examples.","math.ST,stat.ME,stat.ML,stat.TH"
"Stochastic Approximation with Markov Noise: Analysis and applications in reinforcement learning. We present for the first time an asymptotic convergence analysis of two
time-scale stochastic approximation driven by ""controlled"" Markov noise. In
particular, the faster and slower recursions have non-additive controlled
Markov noise components in addition to martingale difference noise. We analyze
the asymptotic behavior of our framework by relating it to limiting
differential inclusions in both time scales that are defined in terms of the
ergodic occupation measures associated with the controlled Markov processes.
Using a special case of our results, we present a solution to the off-policy
convergence problem for temporal-difference learning with linear function
approximation. We compile several aspects of the dynamics of stochastic
approximation algorithms with Markov iterate-dependent noise when the iterates
are not known to be stable beforehand. We achieve the same by extending the
lock-in probability (i.e. the probability of convergence to a specific
attractor of the limiting o.d.e. given that the iterates are in its domain of
attraction after a sufficiently large number of iterations (say) n_0) framework
to such recursions. We use these results to prove almost sure convergence of
the iterates to the specified attractor when the iterates satisfy an
""asymptotic tightness"" condition. This, in turn, is shown to be useful in
analyzing the tracking ability of general ""adaptive"" algorithms. Finally, we
obtain the first informative error bounds on function approximation for the
policy evaluation algorithm proposed by Basu et al. when the aim is to find the
risk-sensitive cost represented using exponential utility. We show that this
happens due to the absence of difference term in the earlier bound which is
always present in all our bounds when the state space is large.","cs.LG,math.DS,math.PR,stat.ML"
"Diffusion-based nonlinear filtering for multimodal data fusion with application to sleep stage assessment. The problem of information fusion from multiple data-sets acquired by
multimodal sensors has drawn significant research attention over the years. In
this paper, we focus on a particular problem setting consisting of a physical
phenomenon or a system of interest observed by multiple sensors. We assume that
all sensors measure some aspects of the system of interest with additional
sensor-specific and irrelevant components. Our goal is to recover the variables
relevant to the observed system and to filter out the nuisance effects of the
sensor-specific variables. We propose an approach based on manifold learning,
which is particularly suitable for problems with multiple modalities, since it
aims to capture the intrinsic structure of the data and relies on minimal prior
model knowledge. Specifically, we propose a nonlinear filtering scheme, which
extracts the hidden sources of variability captured by two or more sensors,
that are independent of the sensor-specific components. In addition to
presenting a theoretical analysis, we demonstrate our technique on real
measured data for the purpose of sleep stage assessment based on multiple,
multimodal sensor measurements. We show that without prior knowledge on the
different modalities and on the measured system, our method gives rise to a
data-driven representation that is well correlated with the underlying sleep
process and is robust to noise and sensor-specific effects.","62-07,cs.LG,physics.data-an,stat.ML"
"Reducing the Communication Cost of Federated Learning through Multistage Optimization. A central question in federated learning (FL) is how to design optimization
algorithms that minimize the communication cost of training a model over
heterogeneous data distributed across many clients. A popular technique for
reducing communication is the use of local steps, where clients take multiple
optimization steps over local data before communicating with the server (e.g.,
FedAvg, SCAFFOLD). This contrasts with centralized methods, where clients take
one optimization step per communication round (e.g., Minibatch SGD). A recent
lower bound on the communication complexity of first-order methods shows that
centralized methods are optimal over highly-heterogeneous data, whereas local
methods are optimal over purely homogeneous data [Woodworth et al., 2020]. For
intermediate heterogeneity levels, no algorithm is known to match the lower
bound. In this paper, we propose a multistage optimization scheme that nearly
matches the lower bound across all heterogeneity levels. The idea is to first
run a local method up to a heterogeneity-induced error floor; next, we switch
to a centralized method for the remaining steps. Our analysis may help explain
empirically-successful stepsize decay methods in FL [Charles et al., 2020;
Reddi et al., 2020]. We demonstrate the scheme's practical utility in image
classification tasks.","cs.DC,cs.LG,math.OC"
"Penalty Constraints and Kernelization of M-Estimation Based Fuzzy C-Means. A framework of M-estimation based fuzzy C-means clustering (MFCM) algorithm
is proposed with iterative reweighted least squares (IRLS) algorithm, and
penalty constraint and kernelization extensions of MFCM algorithms are also
developed. Introducing penalty information to the object functions of MFCM
algorithms, the spatially constrained fuzzy C-means (SFCM) is extended to
penalty constraints MFCM algorithms(abbr. pMFCM).Substituting the Euclidean
distance with kernel method, the MFCM and pMFCM algorithms are extended to
kernelized MFCM (abbr. KMFCM) and kernelized pMFCM (abbr.pKMFCM) algorithms.
The performances of MFCM, pMFCM, KMFCM and pKMFCM algorithms are evaluated in
three tasks: pattern recognition on 10 standard data sets from UCI Machine
Learning databases, noise image segmentation performances on a synthetic image,
a magnetic resonance brain image (MRI), and image segmentation of a standard
images from Berkeley Segmentation Dataset and Benchmark. The experimental
results demonstrate the effectiveness of our proposed algorithms in pattern
recognition and image segmentation.","cs.CV,stat.CO"
"Direct White Matter Bundle Segmentation using Stacked U-Nets. The state-of-the-art method for automatically segmenting white matter bundles
in diffusion-weighted MRI is tractography in conjunction with streamline
cluster selection. This process involves long chains of processing steps which
are not only computationally expensive but also complex to setup and tedious
with respect to quality control. Direct bundle segmentation methods treat the
task as a traditional image segmentation problem. While they so far did not
deliver competitive results, they can potentially mitigate many of the
mentioned issues. We present a novel supervised approach for direct tract
segmentation that shows major performance gains. It builds upon a stacked U-Net
architecture which is trained on manual bundle segmentations from Human
Connectome Project subjects. We evaluate our approach \textit{in vivo} as well
as \textit{in silico} using the ISMRM 2015 Tractography Challenge phantom
dataset. We achieve human segmentation performance and a major performance gain
over previous pipelines. We show how the learned spatial priors efficiently
guide the segmentation even at lower image qualities with little quality loss.","cs.CV,q-bio.NC,q-bio.QM"
"Local Class-Specific and Global Image-Level Generative Adversarial Networks for Semantic-Guided Scene Generation. In this paper, we address the task of semantic-guided scene generation. One
open challenge in scene generation is the difficulty of the generation of small
objects and detailed local texture, which has been widely observed in global
image-level generation methods. To tackle this issue, in this work we consider
learning the scene generation in a local context, and correspondingly design a
local class-specific generative network with semantic maps as a guidance, which
separately constructs and learns sub-generators concentrating on the generation
of different classes, and is able to provide more scene details. To learn more
discriminative class-specific feature representations for the local generation,
a novel classification module is also proposed. To combine the advantage of
both the global image-level and the local class-specific generation, a joint
generation network is designed with an attention fusion module and a
dual-discriminator structure embedded. Extensive experiments on two scene image
generation tasks show superior generation performance of the proposed model.
The state-of-the-art results are established by large margins on both tasks and
on challenging public benchmarks. The source code and trained models are
available at https://github.com/Ha0Tang/LGGAN.","cs.CV,cs.LG,eess.IV"
"Evaluating image matching methods for book cover identification. Humans are capable of identifying a book only by looking at its cover, but
how can computers do the same? In this paper, we explore different feature
detectors and matching methods for book cover identification, and compare their
performances in terms of both speed and accuracy. This will allow, for example,
libraries to develop interactive services based on cover book picture. Only one
single image of a cover book needs to be available through a database. Tests
have been performed by taking into account different transformations of each
book cover image. Encouraging results have been achieved.","cs.CV,cs.MM,eess.IV"
"COLOGNE: Coordinated Local Graph Neighborhood Sampling. Representation learning for graphs enables the application of standard
machine learning algorithms and data analysis tools to graph data. Replacing
discrete unordered objects such as graph nodes by real-valued vectors is at the
heart of many approaches to learning from graph data. Such vector
representations, or embeddings, capture the discrete relationships in the
original data by representing nodes as vectors in a high-dimensional space.
  In most applications graphs model the relationship between real-life objects
and often nodes contain valuable meta-information about the original objects.
While being a powerful machine learning tool, embeddings are not able to
preserve such node attributes. We address this shortcoming and consider the
problem of learning discrete node embeddings such that the coordinates of the
node vector representations are graph nodes. This opens the door to designing
interpretable machine learning algorithms for graphs as all attributes
originally present in the nodes are preserved.
  We present a framework for coordinated local graph neighborhood sampling
(COLOGNE) such that each node is represented by a fixed number of graph nodes,
together with their attributes. Individual samples are coordinated and they
preserve the similarity between node neighborhoods. We consider different
notions of similarity for which we design scalable algorithms. We show
theoretical results for all proposed algorithms. Experiments on benchmark
graphs evaluate the quality of the designed embeddings and demonstrate how the
proposed embeddings can be used in training interpretable machine learning
algorithms for graph data.","cs.AI,cs.DS,cs.LG"
"Variational Osmosis for Non-linear Image Fusion. We propose a new variational model for non-linear image fusion. Our approach
is based on the use of an osmosis energy term related to the one studied in
Vogel et al. (2013) and Weickert et al. (2013) The minimization of the proposed
non-convex energy realizes visually plausible image data fusion, invariant to
multiplicative brightness changes. On the practical side, it requires minimal
supervision and parameter tuning and can encode prior information on the
structure of the images to be fused. For the numerical solution of the proposed
model, we develop a primal-dual algorithm and we apply the resulting
minimization scheme to solve multi-modal face fusion, color transfer and
cultural heritage conservation problems. Visual and quantitative comparisons to
state-of-the-art approaches prove the out-performance and the flexibility of
our method.","35A15,62H35,65K10,94A08,cs.CV,cs.NA,math.NA"
"Direct Estimation of Appearance Models for Segmentation. Image segmentation algorithms often depend on appearance models that
characterize the distribution of pixel values in different image regions. We
describe a new approach for estimating appearance models directly from an
image, without explicit consideration of the pixels that make up each region.
Our approach is based on novel algebraic expressions that relate local image
statistics to the appearance of spatially coherent regions. We describe two
algorithms that can use the aforementioned algebraic expressions to estimate
appearance models directly from an image. The first algorithm solves a system
of linear and quadratic equations using a least squares formulation. The second
algorithm is a spectral method based on an eigenvector computation. We present
experimental results that demonstrate the proposed methods work well in
practice and lead to effective image segmentation algorithms.","62H30,62M05,65C20,68U10,cs.CV"
"Improving Compositional Generalization in Classification Tasks via Structure Annotations. Compositional generalization is the ability to generalize systematically to a
new data distribution by combining known components. Although humans seem to
have a great ability to generalize compositionally, state-of-the-art neural
models struggle to do so. In this work, we study compositional generalization
in classification tasks and present two main contributions. First, we study
ways to convert a natural language sequence-to-sequence dataset to a
classification dataset that also requires compositional generalization. Second,
we show that providing structural hints (specifically, providing parse trees
and entity links as attention masks for a Transformer model) helps
compositional generalization.","cs.CL,cs.LG"
"The Origins and Prevalence of Texture Bias in Convolutional Neural Networks. Recent work has indicated that, unlike humans, ImageNet-trained CNNs tend to
classify images by texture rather than by shape. How pervasive is this bias,
and where does it come from? We find that, when trained on datasets of images
with conflicting shape and texture, CNNs learn to classify by shape at least as
easily as by texture. What factors, then, produce the texture bias in CNNs
trained on ImageNet? Different unsupervised training objectives and different
architectures have small but significant and largely independent effects on the
level of texture bias. However, all objectives and architectures still lead to
models that make texture-based classification decisions a majority of the time,
even if shape information is decodable from their hidden representations. The
effect of data augmentation is much larger. By taking less aggressive random
crops at training time and applying simple, naturalistic augmentation (color
distortion, noise, and blur), we train models that classify ambiguous images by
shape a majority of the time, and outperform baselines on out-of-distribution
test sets. Our results indicate that apparent differences in the way humans and
ImageNet-trained CNNs process images may arise not primarily from differences
in their internal workings, but from differences in the data that they see.","cs.CV,cs.LG,q-bio.NC"
"Learning by Analogy: Reliable Supervision from Transformations for Unsupervised Optical Flow Estimation. Unsupervised learning of optical flow, which leverages the supervision from
view synthesis, has emerged as a promising alternative to supervised methods.
However, the objective of unsupervised learning is likely to be unreliable in
challenging scenes. In this work, we present a framework to use more reliable
supervision from transformations. It simply twists the general unsupervised
learning pipeline by running another forward pass with transformed data from
augmentation, along with using transformed predictions of original data as the
self-supervision signal. Besides, we further introduce a lightweight network
with multiple frames by a highly-shared flow decoder. Our method consistently
gets a leap of performance on several benchmarks with the best accuracy among
deep unsupervised methods. Also, our method achieves competitive results to
recent fully supervised methods while with much fewer parameters.",cs.CV
"Distributed Multi-agent Meta Learning for Trajectory Design in Wireless Drone Networks. In this paper, the problem of the trajectory design for a group of
energy-constrained drones operating in dynamic wireless network environments is
studied. In the considered model, a team of drone base stations (DBSs) is
dispatched to cooperatively serve clusters of ground users that have dynamic
and unpredictable uplink access demands. In this scenario, the DBSs must
cooperatively navigate in the considered area to maximize coverage of the
dynamic requests of the ground users. This trajectory design problem is posed
as an optimization framework whose goal is to find optimal trajectories that
maximize the fraction of users served by all DBSs. To find an optimal solution
for this non-convex optimization problem under unpredictable environments, a
value decomposition based reinforcement learning (VDRL) solution coupled with a
meta-training mechanism is proposed. This algorithm allows the DBSs to
dynamically learn their trajectories while generalizing their learning to
unseen environments. Analytical results show that, the proposed VD-RL algorithm
is guaranteed to converge to a local optimal solution of the non-convex
optimization problem. Simulation results show that, even without meta-training,
the proposed VD-RL algorithm can achieve a 53.2% improvement of the service
coverage and a 30.6% improvement in terms of the convergence speed, compared to
baseline multi-agent algorithms. Meanwhile, the use of meta-learning improves
the convergence speed of the VD-RL algorithm by up to 53.8% when the DBSs must
deal with a previously unseen task.","cs.LG,cs.MA,cs.RO"
"Generative chemical transformer: attention makes neural machine learn molecular geometric structures via text. Chemical formula is an artificial language that expresses molecules as text.
Neural machines that have learned chemical language can be used as a tool for
inverse molecular design. Here, we propose a neural machine that creates
molecules that meet some desired conditions based on a deep understanding of
chemical language (generative chemical Transformer, GCT). Attention-mechanism
in GCT allows a deeper understanding of molecular structures, beyond the
limitations of chemical language itself that cause semantic discontinuity, by
paying attention to characters sparsely. We investigate the significance of
language models to inverse molecular design problems by quantitatively
evaluating the quality of generated molecules. GCT generates highly realistic
chemical strings that satisfy both a chemical rule and grammars of a language.
Molecules parsed from generated strings simultaneously satisfy the multiple
target properties and are various for a single condition set. GCT generates de
novo molecules, and this is done in a short time that human experts cannot.
These advances will contribute to improving the quality of human life by
accelerating the process of desired material discovery.","cs.LG,physics.chem-ph"
"Particle methods enable fast and simple approximation of Sobolev gradients in image segmentation. Bio-image analysis is challenging due to inhomogeneous intensity
distributions and high levels of noise in the images. Bayesian inference
provides a principled way for regularizing the problem using prior knowledge. A
fundamental choice is how one measures ""distances"" between shapes in an image.
It has been shown that the straightforward geometric L2 distance is degenerate
and leads to pathological situations. This is avoided when using Sobolev
gradients, rendering the segmentation problem less ill-posed. The high
computational cost and implementation overhead of Sobolev gradients, however,
have hampered practical applications. We show how particle methods as applied
to image segmentation allow for a simple and computationally efficient
implementation of Sobolev gradients. We show that the evaluation of Sobolev
gradients amounts to particle-particle interactions along the contour in an
image. We extend an existing particle-based segmentation algorithm to using
Sobolev gradients. Using synthetic and real-world images, we benchmark the
results for both 2D and 3D images using piecewise smooth and piecewise constant
region models. The present particle approximation of Sobolev gradients is 2.8
to 10 times faster than the previous reference implementation, but retains the
known favorable properties of Sobolev gradients. This speedup is achieved by
using local particle-particle interactions instead of solving a global Poisson
equation at each iteration. The computational time per iteration is higher for
Sobolev gradients than for L2 gradients. Since Sobolev gradients precondition
the optimization problem, however, a smaller number of overall iterations may
be necessary for the algorithm to converge, which can in some cases amortize
the higher per-iteration cost.","cs.CE,cs.CV,cs.NA,q-bio.QM"
"Neural Dynamic Mode Decomposition for End-to-End Modeling of Nonlinear Dynamics. Koopman spectral analysis has attracted attention for understanding nonlinear
dynamical systems by which we can analyze nonlinear dynamics with a linear
regime by lifting observations using a nonlinear function. For analysis, we
need to find an appropriate lift function. Although several methods have been
proposed for estimating a lift function based on neural networks, the existing
methods train neural networks without spectral analysis. In this paper, we
propose neural dynamic mode decomposition, in which neural networks are trained
such that the forecast error is minimized when the dynamics is modeled based on
spectral decomposition in the lifted space. With our proposed method, the
forecast error is backpropagated through the neural networks and the spectral
decomposition, enabling end-to-end learning of Koopman spectral analysis. When
information is available on the frequencies or the growth rates of the
dynamics, the proposed method can exploit it as regularizers for training. We
also propose an extension of our approach when observations are influenced by
exogenous control time-series. Our experiments demonstrate the effectiveness of
our proposed method in terms of eigenvalue estimation and forecast performance.","cs.LG,math.DS,stat.ML"
"A COLD Approach to Generating Optimal Samples. Optimising discrete data for a desired characteristic using gradient-based
methods involves projecting the data into a continuous latent space and
carrying out optimisation in this space. Carrying out global optimisation is
difficult as optimisers are likely to follow gradients into regions of the
latent space that the model has not been exposed to during training; samples
generated from these regions are likely to be too dissimilar to the training
data to be useful. We propose Constrained Optimisation with Latent
Distributions (COLD), a constrained global optimisation procedure to find
samples with high values of a desired property that are similar to yet distinct
from the training data. We find that on MNIST, our procedure yields optima for
each of three different objectives, and that enforcing tighter constraints
improves the quality and increases the diversity of the generated images. On
the ChEMBL molecular dataset, our method generates a diverse set of new
molecules with drug-likeness scores similar to those of the highest-scoring
molecules in the training data. We also demonstrate a computationally efficient
way to approximate the constraint when evaluating it exactly is computationally
expensive.","cs.LG,q-bio.BM,q-bio.QM,stat.ML"
"AvatarMe: Realistically Renderable 3D Facial Reconstruction ""in-the-wild"". Over the last years, with the advent of Generative Adversarial Networks
(GANs), many face analysis tasks have accomplished astounding performance, with
applications including, but not limited to, face generation and 3D face
reconstruction from a single ""in-the-wild"" image. Nevertheless, to the best of
our knowledge, there is no method which can produce high-resolution
photorealistic 3D faces from ""in-the-wild"" images and this can be attributed to
the: (a) scarcity of available data for training, and (b) lack of robust
methodologies that can successfully be applied on very high-resolution data. In
this paper, we introduce AvatarMe, the first method that is able to reconstruct
photorealistic 3D faces from a single ""in-the-wild"" image with an increasing
level of detail. To achieve this, we capture a large dataset of facial shape
and reflectance and build on a state-of-the-art 3D texture and shape
reconstruction method and successively refine its results, while generating the
per-pixel diffuse and specular components that are required for realistic
rendering. As we demonstrate in a series of qualitative and quantitative
experiments, AvatarMe outperforms the existing arts by a significant margin and
reconstructs authentic, 4K by 6K-resolution 3D faces from a single
low-resolution image that, for the first time, bridges the uncanny valley.","I.2.10; I.3.7; I.4.1,cs.CV,cs.GR"
"Align, then memorise: the dynamics of learning with feedback alignment. Direct Feedback Alignment (DFA) is emerging as an efficient and biologically
plausible alternative to the ubiquitous backpropagation algorithm for training
deep neural networks. Despite relying on random feedback weights for the
backward pass, DFA successfully trains state-of-the-art models such as
Transformers. On the other hand, it notoriously fails to train convolutional
networks. An understanding of the inner workings of DFA to explain these
diverging results remains elusive. Here, we propose a theory for the success of
DFA. We first show that learning in shallow networks proceeds in two steps: an
alignment phase, where the model adapts its weights to align the approximate
gradient with the true gradient of the loss function, is followed by a
memorisation phase, where the model focuses on fitting the data. This two-step
process has a degeneracy breaking effect: out of all the low-loss solutions in
the landscape, a network trained with DFA naturally converges to the solution
which maximises gradient alignment. We also identify a key quantity underlying
alignment in deep linear networks: the conditioning of the alignment matrices.
The latter enables a detailed understanding of the impact of data structure on
alignment, and suggests a simple explanation for the well-known failure of DFA
to train convolutional neural networks. Numerical experiments on MNIST and
CIFAR10 clearly demonstrate degeneracy breaking in deep non-linear networks and
show that the align-then-memorise process occurs sequentially from the bottom
layers of the network to the top.","cond-mat.dis-nn,cs.LG,cs.NE,stat.ML"
"NFFT meets Krylov methods: Fast matrix-vector products for the graph Laplacian of fully connected networks. The graph Laplacian is a standard tool in data science, machine learning, and
image processing. The corresponding matrix inherits the complex structure of
the underlying network and is in certain applications densely populated. This
makes computations, in particular matrix-vector products, with the graph
Laplacian a hard task. A typical application is the computation of a number of
its eigenvalues and eigenvectors. Standard methods become infeasible as the
number of nodes in the graph is too large. We propose the use of the fast
summation based on the nonequispaced fast Fourier transform (NFFT) to perform
the dense matrix-vector product with the graph Laplacian fast without ever
forming the whole matrix. The enormous flexibility of the NFFT algorithm allows
us to embed the accelerated multiplication into Lanczos-based eigenvalues
routines or iterative linear system solvers and even consider other than the
standard Gaussian kernels. We illustrate the feasibility of our approach on a
number of test problems from image segmentation to semi-supervised learning
based on graph-based PDEs. In particular, we compare our approach with the
Nystr\""om method. Moreover, we present and test an enhanced, hybrid version of
the Nystr\""om method, which internally uses the NFFT.","05C50,62H30,65F15,65T50,68R10,68T05,cs.LG,math.NA,stat.ML"
"GNisi: A graph network for reconstructing Ising models from multivariate binarized data. Ising models are a simple generative approach to describing interacting
binary variables. They have proven useful in a number of biological settings
because they enable one to represent observed many-body correlations as the
separable consequence of many direct, pairwise statistical interactions. The
inference of Ising models from data can be computationally very challenging and
often one must be satisfied with numerical approximations or limited precision.
In this paper we present a novel method for the determination of Ising
parameters from data, called GNisi, which uses a Graph Neural network trained
on known Ising models in order to construct the parameters for unseen data. We
show that GNisi is more accurate than the existing state of the art software,
and we illustrate our method by applying GNisi to gene expression data.","cs.LG,physics.comp-ph"
"Statistical and Computational Guarantees for the Baum-Welch Algorithm. The Hidden Markov Model (HMM) is one of the mainstays of statistical modeling
of discrete time series, with applications including speech recognition,
computational biology, computer vision and econometrics. Estimating an HMM from
its observation process is often addressed via the Baum-Welch algorithm, which
is known to be susceptible to local optima. In this paper, we first give a
general characterization of the basin of attraction associated with any global
optimum of the population likelihood. By exploiting this characterization, we
provide non-asymptotic finite sample guarantees on the Baum-Welch updates,
guaranteeing geometric convergence to a small ball of radius on the order of
the minimax rate around a global optimum. As a concrete example, we prove a
linear rate of convergence for a hidden Markov mixture of two isotropic
Gaussians given a suitable mean separation and an initialization within a ball
of large radius around (one of) the true parameters. To our knowledge, these
are the first rigorous local convergence guarantees to global optima for the
Baum-Welch algorithm in a setting where the likelihood function is nonconvex.
We complement our theoretical results with thorough numerical simulations
studying the convergence of the Baum-Welch algorithm and illustrating the
accuracy of our predictions.","cs.IT,math.IT,math.ST,stat.ML,stat.TH"
"TensorNetwork for Machine Learning. We demonstrate the use of tensor networks for image classification with the
TensorNetwork open source library. We explain in detail the encoding of image
data into a matrix product state form, and describe how to contract the network
in a way that is parallelizable and well-suited to automatic gradients for
optimization. Applying the technique to the MNIST and Fashion-MNIST datasets we
find out-of-the-box performance of 98% and 88% accuracy, respectively, using
the same tensor network architecture. The TensorNetwork library allows us to
seamlessly move from CPU to GPU hardware, and we see a factor of more than 10
improvement in computational speed using a GPU.","cond-mat.str-el,cs.CV,cs.LG,physics.comp-ph,stat.ML"
"A survey of Monte Carlo methods for noisy and costly densities with application to reinforcement learning. This survey gives an overview of Monte Carlo methodologies using surrogate
models, for dealing with densities which are intractable, costly, and/or noisy.
This type of problem can be found in numerous real-world scenarios, including
stochastic optimization and reinforcement learning, where each evaluation of a
density function may incur some computationally-expensive or even physical
(real-world activity) cost, likely to give different results each time. The
surrogate model does not incur this cost, but there are important trade-offs
and considerations involved in the choice and design of such methodologies. We
classify the different methodologies into three main classes and describe
specific instances of algorithms under a unified notation. A modular scheme
which encompasses the considered methods is also presented. A range of
application scenarios is discussed, with special attention to the
likelihood-free setting and reinforcement learning. Several numerical
comparisons are also provided.","cs.LG,stat.CO,stat.ML"
"Does Dataset Complexity Matters for Model Explainers?. Strategies based on Explainable Artificial Intelligence - XAI have emerged in
computing to promote a better understanding of predictions made by black box
models. Most XAI-based tools used today explain these types of models,
generating attribute rankings aimed at explaining the same, that is, the
analysis of Attribute Importance. There is no consensus on which XAI tool
generates a general rank of explainability, for this reason, several proposals
for tools have emerged (Ciu, Dalex, Eli5, Lofo, Shap and Skater). Here, we
present an experimental benchmark of explainable AI techniques capable of
producing model-agnostic global explainability ranks based on tabular data
related to different problems. Seeking to answer questions such as ""Are the
explanations generated by the different tools the same, similar or different?""
and ""How does data complexity play along model explainability?"". The results
from the construction of 82 computational models and 592 ranks give us some
light on the other side of the problem of explainability: dataset complexity!","I.2.6,cs.AI,cs.LG"
"Simultaneous Corn and Soybean Yield Prediction from Remote Sensing Data Using Deep Transfer Learning. Large-scale crop yield estimation is, in part, made possible due to the
availability of remote sensing data allowing for the continuous monitoring of
crops throughout their growth cycle. Having this information allows
stakeholders the ability to make real-time decisions to maximize yield
potential. Although various models exist that predict yield from remote sensing
data, there currently does not exist an approach that can estimate yield for
multiple crops simultaneously, and thus leads to more accurate predictions. A
model that predicts the yield of multiple crops and concurrently considers the
interaction between multiple crop yields. We propose a new convolutional neural
network model called YieldNet which utilizes a novel deep learning framework
that uses transfer learning between corn and soybean yield predictions by
sharing the weights of the backbone feature extractor. Additionally, to
consider the multi-target response variable, we propose a new loss function. We
conduct our experiment using data from 1,132 counties for corn and 1,076
counties for soybean across the United States. Numerical results demonstrate
that our proposed method accurately predicts corn and soybean yield from one to
four months before the harvest with a MAE being 8.74% and 8.70% of the average
yield, respectively, and is competitive to other state-of-the-art approaches.","cs.CV,cs.LG,eess.IV,q-bio.QM,stat.AP"
"Transfer Learning as an Enhancement for Reconfiguration Management of Cyber-Physical Production Systems. Reconfiguration demand is increasing due to frequent requirement changes for
manufacturing systems. Recent approaches aim at investigating feasible
configuration alternatives from which they select the optimal one. This relies
on processes whose behavior is not reliant on e.g. the production sequence.
However, when machine learning is used, components' behavior depends on the
process' specifics, requiring additional concepts to successfully conduct
reconfiguration management. Therefore, we propose the enhancement of the
comprehensive reconfiguration management with transfer learning. This provides
the ability to assess the machine learning dependent behavior of the different
CPPS configurations with reduced effort and further assists the recommissioning
of the chosen one. A real cyber-physical production system from the discrete
manufacturing domain is utilized to demonstrate the aforementioned proposal.","cs.AI,cs.LG,cs.SE,cs.SY,eess.SY"
"Interaction Matters: A Note on Non-asymptotic Local Convergence of Generative Adversarial Networks. Motivated by the pursuit of a systematic computational and algorithmic
understanding of Generative Adversarial Networks (GANs), we present a simple
yet unified non-asymptotic local convergence theory for smooth two-player
games, which subsumes several discrete-time gradient-based saddle point
dynamics. The analysis reveals the surprising nature of the off-diagonal
interaction term as both a blessing and a curse. On the one hand, this
interaction term explains the origin of the slow-down effect in the convergence
of Simultaneous Gradient Ascent (SGA) to stable Nash equilibria. On the other
hand, for the unstable equilibria, exponential convergence can be proved thanks
to the interaction term, for four modified dynamics proposed to stabilize GAN
training: Optimistic Mirror Descent (OMD), Consensus Optimization (CO),
Implicit Updates (IU) and Predictive Method (PM). The analysis uncovers the
intimate connections among these stabilizing techniques, and provides detailed
characterization on the choice of learning rate. As a by-product, we present a
new analysis for OMD proposed in Daskalakis, Ilyas, Syrgkanis, and Zeng [2017]
with improved rates.","cs.GT,cs.LG,stat.ML"
"Learning credit assignment. Deep learning has achieved impressive prediction accuracies in a variety of
scientific and industrial domains. However, the nested non-linear feature of
deep learning makes the learning highly non-transparent, i.e., it is still
unknown how the learning coordinates a huge number of parameters to achieve a
decision making. To explain this hierarchical credit assignment, we propose a
mean-field learning model by assuming that an ensemble of sub-networks, rather
than a single network, are trained for a classification task. Surprisingly, our
model reveals that apart from some deterministic synaptic weights connecting
two neurons at neighboring layers, there exist a large number of connections
that can be absent, and other connections can allow for a broad distribution of
their weight values. Therefore, synaptic connections can be classified into
three categories: very important ones, unimportant ones, and those of
variability that may partially encode nuisance factors. Therefore, our model
learns the credit assignment leading to the decision, and predicts an ensemble
of sub-networks that can accomplish the same task, thereby providing insights
toward understanding the macroscopic behavior of deep learning through the lens
of distinct roles of synaptic weights.","cond-mat.dis-nn,cond-mat.stat-mech,cs.LG,q-bio.NC,stat.ML"
"Learning Deep Visual Object Models From Noisy Web Data: How to Make it Work. Deep networks thrive when trained on large scale data collections. This has
given ImageNet a central role in the development of deep architectures for
visual object classification. However, ImageNet was created during a specific
period in time, and as such it is prone to aging, as well as dataset bias
issues. Moving beyond fixed training datasets will lead to more robust visual
systems, especially when deployed on robots in new environments which must
train on the objects they encounter there. To make this possible, it is
important to break free from the need for manual annotators. Recent work has
begun to investigate how to use the massive amount of images available on the
Web in place of manual image annotations. We contribute to this research thread
with two findings: (1) a study correlating a given level of noisily labels to
the expected drop in accuracy, for two deep architectures, on two different
types of noise, that clearly identifies GoogLeNet as a suitable architecture
for learning from Web data; (2) a recipe for the creation of Web datasets with
minimal noise and maximum visual variability, based on a visual and natural
language processing concept expansion strategy. By combining these two results,
we obtain a method for learning powerful deep object models automatically from
the Web. We confirm the effectiveness of our approach through object
categorization experiments using our Web-derived version of ImageNet on a
popular robot vision benchmark database, and on a lifelong object discovery
task on a mobile robot.","cs.CV,cs.DB,cs.LG,cs.RO"
"Algorithm to Compilation Co-design: An Integrated View of Neural Network Sparsity. Reducing computation cost, inference latency, and memory footprint of neural
networks are frequently cited as research motivations for pruning and sparsity.
However, operationalizing those benefits and understanding the end-to-end
effect of algorithm design and regularization on the runtime execution is not
often examined in depth.
  Here we apply structured and unstructured pruning to attention weights of
transformer blocks of the BERT language model, while also expanding block
sparse representation (BSR) operations in the TVM compiler. Integration of BSR
operations enables the TVM runtime execution to leverage structured pattern
sparsity induced by model regularization.
  This integrated view of pruning algorithms enables us to study relationships
between modeling decisions and their direct impact on sparsity-enhanced
execution. Our main findings are: 1) we validate that performance benefits of
structured sparsity block regularization must be enabled by the BSR
augmentations to TVM, with 4x speedup relative to vanilla PyTorch and 2.2x
speedup relative to standard TVM compilation (without expanded BSR support). 2)
for BERT attention weights, the end-to-end optimal block sparsity shape in this
CPU inference context is not a square block (as in \cite{gray2017gpu}) but
rather a linear 32x1 block 3) the relationship between performance and block
size / shape is is suggestive of how model regularization parameters interact
with task scheduler optimizations resulting in the observed end-to-end
performance.","cs.AI,cs.CL,cs.LG,cs.SY,eess.SY"
"Bio-Inspired Audio-Visual Cues Integration for Visual Attention Prediction. Visual Attention Prediction (VAP) methods simulates the human selective
attention mechanism to perceive the scene, which is significant and imperative
in many vision tasks. Most existing methods only consider visual cues, while
neglect the accompanied audio information, which can provide complementary
information for the scene understanding. In fact, there exists a strong
relation between auditory and visual cues, and humans generally perceive the
surrounding scene by simultaneously sensing these cues. Motivated by this, a
bio-inspired audio-visual cues integration method is proposed for the VAP task,
which explores the audio modality to better predict the visual attention map by
assisting vision modality. The proposed method consists of three parts: 1)
audio-visual encoding, 2) audio-visual location, and 3) multi-cues aggregation
parts. Firstly, a refined SoundNet architecture is adopted to encode audio
modality for obtaining corresponding features, and a modified 3D ResNet-50
architecture is employed to learn visual features, containing both spatial
location and temporal motion information. Secondly, an audio-visual location
part is devised to locate the sound source in the visual scene by learning the
correspondence between audio-visual information. Thirdly, a multi-cues
aggregation part is devised to adaptively aggregate audio-visual information
and center-bias prior to generate the final visual attention map. Extensive
experiments are conducted on six challenging audiovisual eye-tracking datasets,
including DIEM, AVAD, Coutrot1, Coutrot2, SumMe, and ETMD, which shows
significant superiority over state-of-the-art visual attention models.","cs.CV,cs.MM"
"Gaussian process modelling of multiple short time series. We present techniques for effective Gaussian process (GP) modelling of
multiple short time series. These problems are common when applying GP models
independently to each gene in a gene expression time series data set. Such sets
typically contain very few time points. Naive application of common GP
modelling techniques can lead to severe over-fitting or under-fitting in a
significant fraction of the fitted models, depending on the details of the data
set. We propose avoiding over-fitting by constraining the GP length-scale to
values that focus most of the energy spectrum to frequencies below the Nyquist
frequency corresponding to the sampling frequency in the data set.
Under-fitting can be avoided by more informative priors on observation noise.
Combining these methods allows applying GP methods reliably automatically to
large numbers of independent instances of short time series. This is
illustrated with experiments with both synthetic data and real gene expression
data.","q-bio.QM,stat.ME,stat.ML"
"Semi-supervised Learning for Aggregated Multilayer Graphs Using Diffuse Interface Methods and Fast Matrix Vector Products. We generalize a graph-based multiclass semi-supervised classification
technique based on diffuse interface methods to multilayer graphs. Besides the
treatment of various applications with an inherent multilayer structure, we
present a very flexible approach that interprets high-dimensional data in a
low-dimensional multilayer graph representation. Highly efficient numerical
methods involving the spectral decomposition of the corresponding differential
graph operators as well as fast matrix-vector products based on the
nonequispaced fast Fourier transform (NFFT) enable the rapid treatment of large
and high-dimensional data sets. We perform various numerical tests putting a
special focus on image segmentation. In particular, we test the performance of
our method on data sets with up to 10 million nodes per layer as well as up to
104 dimensions resulting in graphs with up to 52 layers. While all presented
numerical experiments can be run on an average laptop computer, the linear
dependence per iteration step of the runtime on the network size in all stages
of our algorithm makes it scalable to even larger and higher-dimensional
problems.","05C50,62H30,65F15,65T50,68R10,68T05,cs.LG,cs.NA,math.NA,stat.ML"
"Using GPI-2 for Distributed Memory Paralleliziation of the Caffe Toolbox to Speed up Deep Neural Network Training. Deep Neural Network (DNN) are currently of great inter- est in research and
application. The training of these net- works is a compute intensive and time
consuming task. To reduce training times to a bearable amount at reasonable
cost we extend the popular Caffe toolbox for DNN with an efficient distributed
memory communication pattern. To achieve good scalability we emphasize the
overlap of computation and communication and prefer fine granu- lar
synchronization patterns over global barriers. To im- plement these
communication patterns we rely on the the Global address space Programming
Interface version 2 (GPI-2) communication library. This interface provides a
light-weight set of asynchronous one-sided communica- tion primitives
supplemented by non-blocking fine gran- ular data synchronization mechanisms.
Therefore, Caf- feGPI is the name of our parallel version of Caffe. First
benchmarks demonstrate better scaling behavior com- pared with other
extensions, e.g., the Intel TM Caffe. Even within a single symmetric
multiprocessing machine with four graphics processing units, the CaffeGPI
scales bet- ter than the standard Caffe toolbox. These first results
demonstrate that the use of standard High Performance Computing (HPC) hardware
is a valid cost saving ap- proach to train large DDNs. I/O is an other
bottleneck to work with DDNs in a standard parallel HPC setting, which we will
consider in more detail in a forthcoming paper.","cs.DC,cs.LG"
"Memory-based Deep Reinforcement Learning for POMDPs. A promising characteristic of Deep Reinforcement Learning (DRL) is its
capability to learn optimal policy in an end-to-end manner without relying on
feature engineering. However, most approaches assume a fully observable state
space, i.e. fully observable Markov Decision Processes (MDPs). In real-world
robotics, this assumption is unpractical, because of issues such as sensor
sensitivity limitations and sensor noise, and the lack of knowledge about
whether the observation design is complete or not. These scenarios lead to
Partially Observable MDPs (POMDPs). In this paper, we propose
Long-Short-Term-Memory-based Twin Delayed Deep Deterministic Policy Gradient
(LSTM-TD3) by introducing a memory component to TD3, and compare its
performance with other DRL algorithms in both MDPs and POMDPs. Our results
demonstrate the significant advantages of the memory component in addressing
POMDPs, including the ability to handle missing and noisy observation data.","cs.AI,cs.LG,cs.RO"
"The Geometry of Deep Generative Image Models and its Applications. Generative adversarial networks (GANs) have emerged as a powerful
unsupervised method to model the statistical patterns of real-world data sets,
such as natural images. These networks are trained to map random inputs in
their latent space to new samples representative of the learned data. However,
the structure of the latent space is hard to intuit due to its high
dimensionality and the non-linearity of the generator, which limits the
usefulness of the models. Understanding the latent space requires a way to
identify input codes for existing real-world images (inversion), and a way to
identify directions with known image transformations (interpretability). Here,
we use a geometric framework to address both issues simultaneously. We develop
an architecture-agnostic method to compute the Riemannian metric of the image
manifold created by GANs. The eigen-decomposition of the metric isolates axes
that account for different levels of image variability. An empirical analysis
of several pretrained GANs shows that image variation around each position is
concentrated along surprisingly few major axes (the space is highly
anisotropic) and the directions that create this large variation are similar at
different positions in the space (the space is homogeneous). We show that many
of the top eigenvectors correspond to interpretable transforms in the image
space, with a substantial part of eigenspace corresponding to minor transforms
which could be compressed out. This geometric understanding unifies key
previous results related to GAN interpretability. We show that the use of this
metric allows for more efficient optimization in the latent space (e.g. GAN
inversion) and facilitates unsupervised discovery of interpretable axes. Our
results illustrate that defining the geometry of the GAN image manifold can
serve as a general framework for understanding GANs.","I.2.10; I.3.3; I.3.5; G.1.4,cs.LG,cs.NA,cs.NE,math.NA"
"Training on the test set? An analysis of Spampinato et al. [31]. A recent paper [31] claims to classify brain processing evoked in subjects
watching ImageNet stimuli as measured with EEG and to use a representation
derived from this processing to create a novel object classifier. That paper,
together with a series of subsequent papers [8, 15, 17, 20, 21, 30, 35], claims
to revolutionize the field by achieving extremely successful results on several
computer-vision tasks, including object classification, transfer learning, and
generation of images depicting human perception and thought using brain-derived
representations measured through EEG. Our novel experiments and analyses
demonstrate that their results crucially depend on the block design that they
use, where all stimuli of a given class are presented together, and fail with a
rapid-event design, where stimuli of different classes are randomly intermixed.
The block design leads to classification of arbitrary brain states based on
block-level temporal correlations that tend to exist in all EEG data, rather
than stimulus-related activity. Because every trial in their test sets comes
from the same block as many trials in the corresponding training sets, their
block design thus leads to surreptitiously training on the test set. This
invalidates all subsequent analyses performed on this data in multiple
published papers and calls into question all of the purported results. We
further show that a novel object classifier constructed with a random codebook
performs as well as or better than a novel object classifier constructed with
the representation extracted from EEG data, suggesting that the performance of
their classifier constructed with a representation extracted from EEG data does
not benefit at all from the brain-derived representation. Our results calibrate
the underlying difficulty of the tasks involved and caution against sensational
and overly optimistic, but false, claims to the contrary.","cs.CV,cs.LG,q-bio.NC"
"Learning to Discretize: Solving 1D Scalar Conservation Laws via Deep Reinforcement Learning. Conservation laws are considered to be fundamental laws of nature. It has
broad applications in many fields, including physics, chemistry, biology,
geology, and engineering. Solving the differential equations associated with
conservation laws is a major branch in computational mathematics. The recent
success of machine learning, especially deep learning in areas such as computer
vision and natural language processing, has attracted a lot of attention from
the community of computational mathematics and inspired many intriguing works
in combining machine learning with traditional methods. In this paper, we are
the first to view numerical PDE solvers as an MDP and to use (deep) RL to learn
new solvers. As proof of concept, we focus on 1-dimensional scalar conservation
laws. We deploy the machinery of deep reinforcement learning to train a policy
network that can decide on how the numerical solutions should be approximated
in a sequential and spatial-temporal adaptive manner. We will show that the
problem of solving conservation laws can be naturally viewed as a sequential
decision-making process, and the numerical schemes learned in such a way can
easily enforce long-term accuracy. Furthermore, the learned policy network is
carefully designed to determine a good local discrete approximation based on
the current state of the solution, which essentially makes the proposed method
a meta-learning approach. In other words, the proposed method is capable of
learning how to discretize for a given situation mimicking human experts.
Finally, we will provide details on how the policy network is trained, how well
it performs compared with some state-of-the-art numerical solvers such as WENO
schemes, and supervised learning based approach L3D and PINN, and how well it
generalizes.","cs.LG,cs.NA,math.NA,physics.comp-ph"
"DISPATCH: Design Space Exploration of Cyber-Physical Systems. Design of cyber-physical systems (CPSs) is a challenging task that involves
searching over a large search space of various CPS configurations and possible
values of components composing the system. Hence, there is a need for
sample-efficient CPS design space exploration to select the system architecture
and component values that meet the target system requirements. We address this
challenge by formulating CPS design as a multi-objective optimization problem
and propose DISPATCH, a two-step methodology for sample-efficient search over
the design space. First, we use a genetic algorithm to search over discrete
choices of system component values for architecture search and component
selection or only component selection and terminate the algorithm even before
meeting the system requirements, thus yielding a coarse design. In the second
step, we use an inverse design to search over a continuous space to fine-tune
the component values and meet the diverse set of system requirements. We use a
neural network as a surrogate function for the inverse design of the system.
The neural network, converted into a mixed-integer linear program, is used for
active learning to sample component values efficiently in a continuous search
space. We illustrate the efficacy of DISPATCH on electrical circuit benchmarks:
two-stage and three-stage transimpedence amplifiers. Simulation results show
that the proposed methodology improves sample efficiency by 5-14x compared to a
prior synthesis method that relies on reinforcement learning. It also
synthesizes circuits with the best performance (highest bandwidth/lowest area)
compared to designs synthesized using reinforcement learning, Bayesian
optimization, or humans.","cs.LG,cs.NE,cs.SY,eess.SY"
"ClickBAIT: Click-based Accelerated Incremental Training of Convolutional Neural Networks. Today's general-purpose deep convolutional neural networks (CNN) for image
classification and object detection are trained offline on large static
datasets. Some applications, however, will require training in real-time on
live video streams with a human-in-the-loop. We refer to this class of problem
as Time-ordered Online Training (ToOT) - these problems will require a
consideration of not only the quantity of incoming training data, but the human
effort required to tag and use it. In this paper, we define training benefit as
a metric to measure the effectiveness of a sequence in using each user
interaction. We demonstrate and evaluate a system tailored to performing ToOT
in the field, capable of training an image classifier on a live video stream
through minimal input from a human operator. We show that by exploiting the
time-ordered nature of the video stream through optical flow-based object
tracking, we can increase the effectiveness of human actions by about 8 times.","C.1.3,cs.AI,cs.CV,cs.HC"
"Towards Geometry Guided Neural Relighting with Flash Photography. Previous image based relighting methods require capturing multiple images to
acquire high frequency lighting effect under different lighting conditions,
which needs nontrivial effort and may be unrealistic in certain practical use
scenarios. While such approaches rely entirely on cleverly sampling the color
images under different lighting conditions, little has been done to utilize
geometric information that crucially influences the high-frequency features in
the images, such as glossy highlight and cast shadow. We therefore propose a
framework for image relighting from a single flash photograph with its
corresponding depth map using deep learning. By incorporating the depth map,
our approach is able to extrapolate realistic high-frequency effects under
novel lighting via geometry guided image decomposition from the flashlight
image, and predict the cast shadow map from the shadow-encoding transformed
depth map. Moreover, the single-image based setup greatly simplifies the data
capture process. We experimentally validate the advantage of our geometry
guided approach over state-of-the-art image-based approaches in intrinsic image
decomposition and image relighting, and also demonstrate our performance on
real mobile phone photo examples.","cs.CV,cs.GR,cs.LG"
"Deep Neural Networks and PIDE discretizations. In this paper, we propose neural networks that tackle the problems of
stability and field-of-view of a Convolutional Neural Network (CNN). As an
alternative to increasing the network's depth or width to improve performance,
we propose integral-based spatially nonlocal operators which are related to
global weighted Laplacian, fractional Laplacian and inverse fractional
Laplacian operators that arise in several problems in the physical sciences.
The forward propagation of such networks is inspired by partial
integro-differential equations (PIDEs). We test the effectiveness of the
proposed neural architectures on benchmark image classification datasets and
semantic segmentation tasks in autonomous driving. Moreover, we investigate the
extra computational costs of these dense operators and the stability of forward
propagation of the proposed neural networks.","47G20,47G30,65D15,65L07,68T05,68W25,I.2.6,cs.LG"
"Transfer Learning under High-dimensional Generalized Linear Models. In this work, we study the transfer learning problem under high-dimensional
generalized linear models (GLMs), which aim to improve the fit on target data
by borrowing information from useful source data. Given which sources to
transfer, we propose an oracle algorithm and derive its $\ell_2$-estimation
error bounds. The theoretical analysis shows that under certain conditions,
when the target and source are sufficiently close to each other, the estimation
error bound could be improved over that of the classical penalized estimator
using only target data. When we don't know which sources to transfer, an
algorithm-free transferable source detection approach is introduced to detect
informative sources. The detection consistency is proved under the
high-dimensional GLM transfer learning setting. Extensive simulations and a
real-data experiment verify the effectiveness of our algorithms.","cs.LG,stat.ME,stat.ML"
"Depth-Aware Arbitrary Style Transfer Using Instance Normalization. Style transfer is the process of rendering one image with some content in the
style of another image, representing the style. Recent studies of Liu et al.
(2017) show that traditional style transfer methods of Gatys et al. (2016) and
Johnson et al. (2016) fail to reproduce the depth of the content image, which
is critical for human perception. They suggest to preserve the depth map by
additional regularizer in the optimized loss function, forcing preservation of
the depth map. However these traditional methods are either computationally
inefficient or require training a separate neural network for each style. AdaIN
method of Huang et al. (2017) allows efficient transferring of arbitrary style
without training a separate model but is not able to reproduce the depth map of
the content image. We propose an extension to this method, allowing depth map
preservation by applying variable stylization strength. Qualitative analysis
and results of user evaluation study indicate that the proposed method provides
better stylizations, compared to the original AdaIN style transfer method.","68T45,I.4.9; I.4.10,cs.CV,eess.IV"
"Depth Evaluation for Metal Surface Defects by Eddy Current Testing using Deep Residual Convolutional Neural Networks. Eddy current testing (ECT) is an effective technique in the evaluation of the
depth of metal surface defects. However, in practice, the evaluation primarily
relies on the experience of an operator and is often carried out by manual
inspection. In this paper, we address the challenges of automatic depth
evaluation of metal surface defects by virtual of state-of-the-art deep
learning (DL) techniques. The main contributions are three-fold. Firstly, a
highly-integrated portable ECT device is developed, which takes advantage of an
advanced field programmable gate array (Zynq-7020 system on chip) and provides
fast data acquisition and in-phase/quadrature demodulation. Secondly, a
dataset, termed as MDDECT, is constructed using the ECT device by human
operators and made openly available. It contains 48,000 scans from 18 defects
of different depths and lift-offs. Thirdly, the depth evaluation problem is
formulated as a time series classification problem, and various
state-of-the-art 1-d residual convolutional neural networks are trained and
evaluated on the MDDECT dataset. A 38-layer 1-d ResNeXt achieves an accuracy of
93.58% in discriminating the surface defects in a stainless steel sheet. The
depths of the defects vary from 0.3 mm to 2.0 mm in a resolution of 0.1 mm. In
addition, results show that the trained ResNeXt1D-38 model is immune to
lift-off signals.","cs.LG,eess.IV,eess.SP"
"Dynamical Pose Estimation. We study the problem of aligning two sets of 3D geometric primitives given
known correspondences. Our first contribution is to show that this primitive
alignment framework unifies five perception problems including point cloud
registration, primitive (mesh) registration, category-level 3D registration,
absolution pose estimation (APE), and category-level APE. Our second
contribution is to propose DynAMical Pose estimation (DAMP), the first general
and practical algorithm to solve primitive alignment problem by simulating
rigid body dynamics arising from virtual springs and damping, where the springs
span the shortest distances between corresponding primitives. We evaluate DAMP
in simulated and real datasets across all five problems, and demonstrate (i)
DAMP always converges to the globally optimal solution in the first three
problems with 3D-3D correspondences; (ii) although DAMP sometimes converges to
suboptimal solutions in the last two problems with 2D-3D correspondences, using
a scheme for escaping local minima, DAMP always succeeds. Our third
contribution is to demystify the surprising empirical performance of DAMP and
formally prove a global convergence result in the case of point cloud
registration by charactering local stability of the equilibrium points of the
underlying dynamical system.","cs.CV,cs.RO,math.DS"
"Social Determinants of Recidivism: A Machine Learning Solution. In criminal justice analytics, the widely-studied problem of recidivism
prediction (forecasting re-offenses after release or parole) is fraught with
ethical missteps. In particular, Machine Learning (ML) models rely on
historical patterns of behavior to predict future outcomes, engendering a
vicious feedback loop of recidivism and incarceration. This paper repurposes ML
to instead identify social factors that can serve as levers to prevent
recidivism. Our contributions are along three dimensions. (1) Recidivism models
typically agglomerate individuals into one dataset, but we invoke unsupervised
learning to extract homogeneous subgroups with similar features. (2) We then
apply subgroup-level supervised learning to determine factors correlated to
recidivism. (3) We therefore shift the focus from predicting which individuals
will re-offend to identifying broader underlying factors that explain
recidivism, with the goal of informing preventative policy intervention. We
demonstrate that this approach can guide the ethical application of ML using
real-world data.","cs.CY,cs.LG,stat.AP"
"Interpretabilit des modles : tat des lieux des mthodes et application  l'assurance. Since May 2018, the General Data Protection Regulation (GDPR) has introduced
new obligations to industries. By setting a legal framework, it notably imposes
strong transparency on the use of personal data. Thus, people must be informed
of the use of their data and must consent the usage of it. Data is the raw
material of many models which today make it possible to increase the quality
and performance of digital services. Transparency on the use of data also
requires a good understanding of its use through different models. The use of
models, even if efficient, must be accompanied by an understanding at all
levels of the process that transform data (upstream and downstream of a model),
thus making it possible to define the relationships between the individual's
data and the choice that an algorithm could make based on the analysis of the
latter. (For example, the recommendation of one product or one promotional
offer or an insurance rate representative of the risk.) Models users must
ensure that models do not discriminate against and that it is also possible to
explain its result. The widening of the panel of predictive algorithms - made
possible by the evolution of computing capacities -- leads scientists to be
vigilant about the use of models and to consider new tools to better understand
the decisions deduced from them . Recently, the community has been particularly
active on model transparency with a marked intensification of publications over
the past three years. The increasingly frequent use of more complex algorithms
(\textit{deep learning}, Xgboost, etc.) presenting attractive performances is
undoubtedly one of the causes of this interest. This article thus presents an
inventory of methods of interpreting models and their uses in an insurance
context.","cs.LG,stat.AP,stat.ML"
"Item2Vec: Neural Item Embedding for Collaborative Filtering. Many Collaborative Filtering (CF) algorithms are item-based in the sense that
they analyze item-item relations in order to produce item similarities.
Recently, several works in the field of Natural Language Processing (NLP)
suggested to learn a latent representation of words using neural embedding
algorithms. Among them, the Skip-gram with Negative Sampling (SGNS), also known
as word2vec, was shown to provide state-of-the-art results on various
linguistics tasks. In this paper, we show that item-based CF can be cast in the
same framework of neural word embedding. Inspired by SGNS, we describe a method
we name item2vec for item-based CF that produces embedding for items in a
latent space. The method is capable of inferring item-item relations even when
user information is not available. We present experimental results that
demonstrate the effectiveness of the item2vec method and show it is competitive
with SVD.","cs.AI,cs.IR,cs.LG"
"Explainable Activity Recognition for Smart Home Systems. Smart home environments are designed to provide services that help improve
the quality of life for the occupant via a variety of sensors and actuators
installed throughout the space. Many automated actions taken by a smart home
are governed by the output of an underlying activity recognition system.
However, activity recognition systems may not be perfectly accurate and
therefore inconsistencies in smart home operations can lead a user to wonder
""why did the smart home do that?"" In this work, we build on insights from
Explainable Artificial Intelligence (XAI) techniques to contribute
computational methods for explainable activity recognition. Specifically, we
generate explanations for smart home activity recognition systems that explain
what about an activity led to the given classification. To do so, we introduce
four computational techniques for generating natural language explanations of
smart home data and compare their effectiveness at generating meaningful
explanations. Through a study with everyday users, we evaluate user preferences
towards the four explanation types. Our results show that the leading approach,
SHAP, has a 92% success rate in generating accurate explanations. Moreover, 84%
of sampled scenarios users preferred natural language explanations over a
simple activity label, underscoring the need for explainable activity
recognition systems. Finally, we show that explanations generated by some XAI
methods can lead users to lose confidence in the accuracy of the underlying
activity recognition model, while others lead users to gain confidence. Taking
all studied factors into consideration, we make a recommendation regarding
which existing XAI method leads to the best performance in the domain of smart
home automation, and discuss a range of topics for future work in this area.","cs.HC,cs.LG"
"Multiphase flow prediction with deep neural networks. This paper proposes a deep neural network approach for predicting multiphase
flow in heterogeneous domains with high computational efficiency. The deep
neural network model is able to handle permeability heterogeneity in high
dimensional systems, and can learn the interplay of viscous, gravity, and
capillary forces from small data sets. Using the example of carbon dioxide
(CO2) storage, we demonstrate that the model can generate highly accurate
predictions of a CO2 saturation distribution given a permeability field,
injection duration, injection rate, and injection location. The trained neural
network model has an excellent ability to interpolate and to a limited extent,
the ability to extrapolate beyond the training data ranges. To improve the
prediction accuracy when the neural network model needs to extrapolate, we
propose a transfer learning (fine-tuning) procedure that can quickly teach the
neural network model new information without going through massive data
collection and retraining. Based on this trained neural network model, a
web-based tool is provided that allows users to perform CO2-water multiphase
flow calculations online. With the tools provided in this paper, the deep
neural network approach can provide a computationally efficient substitute for
repetitive forward multiphase flow simulations, which can be adopted to the
context of history matching and uncertainty quantification.","cs.LG,physics.comp-ph,stat.ML"
"A Bayesian Deep Learning Framework for End-To-End Prediction of Emotion from Heartbeat. Automatic prediction of emotion promises to revolutionise human-computer
interaction. Recent trends involve fusion of multiple data modalities - audio,
visual, and physiological - to classify emotional state. However, in practice,
collection of physiological data `in the wild' is currently limited to
heartbeat time series of the kind generated by affordable wearable heart
monitors. Furthermore, real-world applications of emotion prediction often
require some measure of uncertainty over model output, in order to inform
downstream decision-making. We present here an end-to-end deep learning model
for classifying emotional valence from unimodal heartbeat time series. We
further propose a Bayesian framework for modelling uncertainty over these
valence predictions, and describe a probabilistic procedure for choosing to
accept or reject model output according to the intended application. We
benchmarked our framework against two established datasets and achieved peak
classification accuracy of 90%. These results lay the foundation for
applications of affective computing in real-world domains such as healthcare,
where a high premium is placed on non-invasive collection of data, and
predictive certainty.","cs.HC,cs.LG,stat.ML"
"Uncertainty Quantification for Inferring Hawkes Networks. Multivariate Hawkes processes are commonly used to model streaming networked
event data in a wide variety of applications. However, it remains a challenge
to extract reliable inference from complex datasets with uncertainty
quantification. Aiming towards this, we develop a statistical inference
framework to learn causal relationships between nodes from networked data,
where the underlying directed graph implies Granger causality. We provide
uncertainty quantification for the maximum likelihood estimate of the network
multivariate Hawkes process by providing a non-asymptotic confidence set. The
main technique is based on the concentration inequalities of continuous-time
martingales. We compare our method to the previously-derived asymptotic Hawkes
process confidence interval, and demonstrate the strengths of our method in an
application to neuronal connectivity reconstruction.","cs.LG,math.ST,stat.ML,stat.TH"
"Towards mental time travel: a hierarchical memory for reinforcement learning agents. Reinforcement learning agents often forget details of the past, especially
after delays or distractor tasks. Agents with common memory architectures
struggle to recall and integrate across multiple timesteps of a past event, or
even to recall the details of a single timestep that is followed by distractor
tasks. To address these limitations, we propose a Hierarchical Transformer
Memory (HTM), which helps agents to remember the past in detail. HTM stores
memories by dividing the past into chunks, and recalls by first performing
high-level attention over coarse summaries of the chunks, and then performing
detailed attention within only the most relevant chunks. An agent with HTM can
therefore ""mentally time-travel"" -- remember past events in detail without
attending to all intervening events. We show that agents with HTM substantially
outperform agents with other memory architectures at tasks requiring long-term
recall, retention, or reasoning over memory. These include recalling where an
object is hidden in a 3D environment, rapidly learning to navigate efficiently
in a new neighborhood, and rapidly learning and retaining new object names.
Agents with HTM can extrapolate to task sequences an order of magnitude longer
than they were trained on, and can even generalize zero-shot from a
meta-learning setting to maintaining knowledge across episodes. HTM improves
agent sample efficiency, generalization, and generality (by solving tasks that
previously required specialized architectures). Our work is a step towards
agents that can learn, interact, and adapt in complex and temporally-extended
environments.","I.2.6,cs.AI,cs.LG,cs.NE"
"Single-partition adaptive Q-learning. This paper introduces single-partition adaptive Q-learning (SPAQL), an
algorithm for model-free episodic reinforcement learning (RL), which adaptively
partitions the state-action space of a Markov decision process (MDP), while
simultaneously learning a time-invariant policy (i. e., the mapping from states
to actions does not depend explicitly on the episode time step) for maximizing
the cumulative reward. The trade-off between exploration and exploitation is
handled by using a mixture of upper confidence bounds (UCB) and Boltzmann
exploration during training, with a temperature parameter that is automatically
tuned as training progresses. The algorithm is an improvement over adaptive
Q-learning (AQL). It converges faster to the optimal solution, while also using
fewer arms. Tests on episodes with a large number of time steps show that SPAQL
has no problems scaling, unlike AQL. Based on this empirical evidence, we claim
that SPAQL may have a higher sample efficiency than AQL, thus being a relevant
contribution to the field of efficient model-free RL methods.","68T05,I.2.6,cs.LG,stat.ML"
"Inertial Sensor Data To Image Encoding For Human Action Recognition. Convolutional Neural Networks (CNNs) are successful deep learning models in
the field of computer vision. To get the maximum advantage of CNN model for
Human Action Recognition (HAR) using inertial sensor data, in this paper, we
use 4 types of spatial domain methods for transforming inertial sensor data to
activity images, which are then utilized in a novel fusion framework. These
four types of activity images are Signal Images (SI), Gramian Angular Field
(GAF) Images, Markov Transition Field (MTF) Images and Recurrence Plot (RP)
Images. Furthermore, for creating a multimodal fusion framework and to exploit
activity image, we made each type of activity images multimodal by convolving
with two spatial domain filters : Prewitt filter and High-boost filter.
Resnet-18, a CNN model, is used to learn deep features from multi-modalities.
Learned features are extracted from the last pooling layer of each ReNet and
then fused by canonical correlation based fusion (CCF) for improving the
accuracy of human action recognition. These highly informative features are
served as input to a multiclass Support Vector Machine (SVM). Experimental
results on three publicly available inertial datasets show the superiority of
the proposed method over the current state-of-the-art.","cs.CV,cs.HC,cs.LG,eess.SP"
"Choose a Transformer: Fourier or Galerkin. In this paper, we apply the self-attention from the state-of-the-art
Transformer in Attention Is All You Need the first time to a data-driven
operator learning problem related to partial differential equations. We put
together an effort to explain the heuristics of, and improve the efficacy of
the self-attention by demonstrating that the softmax normalization in the
scaled dot-product attention is sufficient but not necessary, and have proved
the approximation capacity of a linear variant as a Petrov-Galerkin projection.
A new layer normalization scheme is proposed to allow a scaling to propagate
through attention layers, which helps the model achieve remarkable accuracy in
operator learning tasks with unnormalized data. Finally, we present three
operator learning experiments, including the viscid Burgers' equation, an
interface Darcy flow, and an inverse interface coefficient identification
problem. All experiments validate the improvements of the newly proposed simple
attention-based operator learner over their softmax-normalized counterparts.","65D15,65M99,65N99,68T99,cs.LG,cs.NA,math.NA"
"Semi-Supervised Learning for In-Game Expert-Level Music-to-Dance Translation. Music-to-dance translation is a brand-new and powerful feature in recent
role-playing games. Players can now let their characters dance along with
specified music clips and even generate fan-made dance videos. Previous works
of this topic consider music-to-dance as a supervised motion generation problem
based on time-series data. However, these methods suffer from limited training
data pairs and the degradation of movements. This paper provides a new
perspective for this task where we re-formulate the translation problem as a
piece-wise dance phrase retrieval problem based on the choreography theory.
With such a design, players are allowed to further edit the dance movements on
top of our generation while other regression based methods ignore such user
interactivity. Considering that the dance motion capture is an expensive and
time-consuming procedure which requires the assistance of professional dancers,
we train our method under a semi-supervised learning framework with a large
unlabeled dataset (20x than labeled data) collected. A co-ascent mechanism is
introduced to improve the robustness of our network. Using this unlabeled
dataset, we also introduce self-supervised pre-training so that the translator
can understand the melody, rhythm, and other components of music phrases. We
show that the pre-training significantly improves the translation accuracy than
that of training from scratch. Experimental results suggest that our method not
only generalizes well over various styles of music but also succeeds in
expert-level choreography for game players.","cs.CV,cs.MM"
"On The Radon-Nikodym Spectral Approach With Optimal Clustering. Problems of interpolation, classification, and clustering are considered. In
the tenets of Radon--Nikodym approach $\langle f(\mathbf{x})\psi^2 \rangle /
\langle\psi^2\rangle$, where the $\psi(\mathbf{x})$ is a linear function on
input attributes, all the answers are obtained from a generalized eigenproblem
$|f|\psi^{[i]}\rangle = \lambda^{[i]} |\psi^{[i]}\rangle$. The solution to the
interpolation problem is a regular Radon-Nikodym derivative. The solution to
the classification problem requires prior and posterior probabilities that are
obtained using the Lebesgue quadrature[1] technique. Whereas in a Bayesian
approach new observations change only outcome probabilities, in the
Radon-Nikodym approach not only outcome probabilities but also the probability
space $|\psi^{[i]}\rangle$ change with new observations. This is a remarkable
feature of the approach: both the probabilities and the probability space are
constructed from the data. The Lebesgue quadrature technique can be also
applied to the optimal clustering problem. The problem is solved by
constructing a Gaussian quadrature on the Lebesgue measure. A distinguishing
feature of the Radon-Nikodym approach is the knowledge of the invariant group:
all the answers are invariant relatively any non-degenerated linear transform
of input vector $\mathbf{x}$ components. A software product implementing the
algorithms of interpolation, classification, and optimal clustering is
available from the authors.","cs.CV,cs.LG,cs.NA,math.NA,stat.ML"
"Neuron Segmentation Using Deep Complete Bipartite Networks. In this paper, we consider the problem of automatically segmenting neuronal
cells in dual-color confocal microscopy images. This problem is a key task in
various quantitative analysis applications in neuroscience, such as tracing
cell genesis in Danio rerio (zebrafish) brains. Deep learning, especially using
fully convolutional networks (FCN), has profoundly changed segmentation
research in biomedical imaging. We face two major challenges in this problem.
First, neuronal cells may form dense clusters, making it difficult to correctly
identify all individual cells (even to human experts). Consequently,
segmentation results of the known FCN-type models are not accurate enough.
Second, pixel-wise ground truth is difficult to obtain. Only a limited amount
of approximate instance-wise annotation can be collected, which makes the
training of FCN models quite cumbersome. We propose a new FCN-type deep
learning model, called deep complete bipartite networks (CB-Net), and a new
scheme for leveraging approximate instance-wise annotation to train our
pixel-wise prediction model. Evaluated using seven real datasets, our proposed
new CB-Net model outperforms the state-of-the-art FCN models and produces
neuron segmentation results of remarkable quality",cs.CV
"Forecast Network-Wide Traffic States for Multiple Steps Ahead: A Deep Learning Approach Considering Dynamic Non-Local Spatial Correlation and Non-Stationary Temporal Dependency. Obtaining accurate information about future traffic flows of all links in a
traffic network is of great importance for traffic management and control
applications. This research studies two particular problems in traffic
forecasting: (1) capture the dynamic and non-local spatial correlation between
traffic links and (2) model the dynamics of temporal dependency for accurate
multiple steps ahead predictions. To address these issues, we propose a deep
learning framework named Spatial-Temporal Sequence to Sequence model
(STSeq2Seq). This model builds on sequence to sequence (seq2seq) architecture
to capture temporal feature and relies on graph convolution for aggregating
spatial information. Moreover, STSeq2Seq defines and constructs pattern-aware
adjacency matrices (PAMs) based on pair-wise similarity of the recent traffic
patterns on traffic links and integrate it into graph convolution operation. It
also deploys a novel seq2sesq architecture which couples a convolutional
encoder and a recurrent decoder with attention mechanism for dynamic modeling
of long-range dependence between different time steps. We conduct extensive
experiments using two publicly-available large-scale traffic datasets and
compare STSeq2Seq with other baseline models. The numerical results demonstrate
that the proposed model achieves state-of-the-art forecasting performance in
terms of various error measures. The ablation study verifies the effectiveness
of PAMs in capturing dynamic non-local spatial correlation and the superiority
of proposed seq2seq architecture in modeling non-stationary temporal dependency
for multiple steps ahead prediction. Furthermore, qualitative analysis is
conducted on PAMs as well as the attention weights for model interpretation.","cs.LG,eess.SP,stat.ML"
"Time-weighted Attentional Session-Aware Recommender System. Session-based Recurrent Neural Networks (RNNs) are gaining increasing
popularity for recommendation task, due to the high autocorrelation of user's
behavior on the latest session and the effectiveness of RNN to capture the
sequence order information. However, most existing session-based RNN
recommender systems still solely focus on the short-term interactions within a
single session and completely discard all the other long-term data across
different sessions. While traditional Collaborative Filtering (CF) methods have
many advanced research works on exploring long-term dependency, which show
great value to be explored and exploited in deep learning models. Therefore, in
this paper, we propose ASARS, a novel framework that effectively imports the
temporal dynamics methodology in CF into session-based RNN system in DL, such
that the temporal info can act as scalable weights by a parallel attentional
network. Specifically, we first conduct an extensive data analysis to show the
distribution and importance of such temporal interactions data both within
sessions and across sessions. And then, our ASARS framework promotes two novel
models: (1) an inter-session temporal dynamic model that captures the long-term
user interaction for RNN recommender system. We integrate the time changes in
session RNN and add user preferences as model drifting; and (2) a novel
triangle parallel attention network that enhances the original RNN model by
incorporating time information. Such triangle parallel network is also
specially designed for realizing data argumentation in sequence-to-scalar RNN
architecture, and thus it can be trained very efficiently. Our extensive
experiments on four real datasets from different domains demonstrate the
effectiveness and large improvement of ASARS for personalized recommendation.","cs.IR,cs.LG,stat.ML"
"CNN-based Approaches For Cross-Subject Classification in Motor Imagery: From The State-of-The-Art to DynamicNet. Motor imagery (MI)-based brain-computer interface (BCI) systems are being
increasingly employed to provide alternative means of communication and control
for people suffering from neuro-motor impairments, with a special effort to
bring these systems out of the controlled lab environments. Hence, accurately
classifying MI from brain signals, e.g., from electroencephalography (EEG), is
essential to obtain reliable BCI systems. However, MI classification is still a
challenging task, because the signals are characterized by poor SNR, high
intra-subject and cross-subject variability. Deep learning approaches have
started to emerge as valid alternatives to standard machine learning
techniques, e.g., filter bank common spatial pattern (FBCSP), to extract
subject-independent features and to increase the cross-subject classification
performance of MI BCI systems. In this paper, we first present a review of the
most recent studies using deep learning for MI classification, with particular
attention to their cross-subject performance. Second, we propose DynamicNet, a
Python-based tool for quick and flexible implementations of deep learning
models based on convolutional neural networks. We show-case the potentiality of
DynamicNet by implementing EEGNet, a well-established architecture for
effective EEG classification. Finally, we compare its performance with FBCSP in
a 4-class MI classification over public datasets. To explore its cross-subject
classification ability, we applied three different cross-validation schemes.
From our results, we demonstrate that DynamicNet-implemented EEGNet outperforms
FBCSP by about 25%, with a statistically significant difference when
cross-subject validation schemes are applied.","cs.CV,cs.HC,cs.LG,cs.NE,eess.SP"
"Scalable Inference of Sparsely-changing Markov Random Fields with Strong Statistical Guarantees. In this paper, we study the problem of inferring time-varying Markov random
fields (MRF), where the underlying graphical model is both sparse and changes
sparsely over time. Most of the existing methods for the inference of
time-varying MRFs rely on the regularized maximum likelihood estimation (MLE),
that typically suffer from weak statistical guarantees and high computational
time. Instead, we introduce a new class of constrained optimization problems
for the inference of sparsely-changing MRFs. The proposed optimization problem
is formulated based on the exact $\ell_0$ regularization, and can be solved in
near-linear time and memory. Moreover, we show that the proposed estimator
enjoys a provably small estimation error. As a special case, we derive sharp
statistical guarantees for the inference of sparsely-changing Gaussian MRFs
(GMRF) in the high-dimensional regime, showing that such problems can be
learned with as few as one sample per time. Our proposed method is extremely
efficient in practice: it can accurately estimate sparsely-changing graphical
models with more than 500 million variables in less than one hour.","cs.LG,stat.CO,stat.ML"
"Multi-channel Deep 3D Face Recognition. Face recognition has been of great importance in many applications as a
biometric for its throughput, convenience, and non-invasiveness. Recent
advancements in deep Convolutional Neural Network (CNN) architectures have
boosted significantly the performance of face recognition based on
two-dimensional (2D) facial texture images and outperformed the previous state
of the art using conventional methods. However, the accuracy of 2D face
recognition is still challenged by the change of pose, illumination, make-up,
and expression. On the other hand, the geometric information contained in
three-dimensional (3D) face data has the potential to overcome the fundamental
limitations of 2D face data.
  We propose a multi-Channel deep 3D face network for face recognition based on
3D face data. We compute the geometric information of a 3D face based on its
piecewise-linear triangular mesh structure and then conformally flatten
geometric information along with the color from 3D to 2D plane to leverage the
state-of-the-art deep CNN architectures. We modify the input layer of the
network to take images with nine channels instead of three only such that more
geometric information can be explicitly fed to it. We pre-train the network
using images from the VGG-Face \cite{Parkhi2015} and then fine-tune it with the
generated multi-channel face images. The face recognition accuracy of the
multi-Channel deep 3D face network has achieved 98.6. The experimental results
also clearly show that the network performs much better when a 9-channel image
is flattened to plane based on the conformal map compared with the orthographic
projection.","cs.CG,cs.CV"
"Learning Graphical Models With Hubs. We consider the problem of learning a high-dimensional graphical model in
which certain hub nodes are highly-connected to many other nodes. Many authors
have studied the use of an l1 penalty in order to learn a sparse graph in
high-dimensional setting. However, the l1 penalty implicitly assumes that each
edge is equally likely and independent of all other edges. We propose a general
framework to accommodate more realistic networks with hub nodes, using a convex
formulation that involves a row-column overlap norm penalty. We apply this
general framework to three widely-used probabilistic graphical models: the
Gaussian graphical model, the covariance graph model, and the binary Ising
model. An alternating direction method of multipliers algorithm is used to
solve the corresponding convex optimization problems. On synthetic data, we
demonstrate that our proposed framework outperforms competitors that do not
explicitly model hub nodes. We illustrate our proposal on a webpage data set
and a gene expression data set.","stat.CO,stat.ME,stat.ML"
"Convolutional Neural Networks and a Transfer Learning Strategy to Classify Parkinson's Disease from Speech in Three Different Languages. Parkinson's disease patients develop different speech impairments that affect
their communication capabilities. The automatic assessment of the speech of the
patients allows the development of computer aided tools to support the
diagnosis and the evaluation of the disease severity. This paper introduces a
methodology to classify Parkinson's disease from speech in three different
languages: Spanish, German, and Czech. The proposed approach considers
convolutional neural networks trained with time frequency representations and a
transfer learning strategy among the three languages. The transfer learning
scheme aims to improve the accuracy of the models when the weights of the
neural network are initialized with utterances from a different language than
the used for the test set. The results suggest that the proposed strategy
improves the accuracy of the models in up to 8\% when the base model used to
initialize the weights of the classifier is robust enough. In addition, the
results obtained after the transfer learning are in most cases more balanced in
terms of specificity-sensitivity than those trained without the transfer
learning strategy.","cs.CL,cs.LG,eess.AS,stat.ML"
"Learning to Generate Code Sketches. Traditional generative models are limited to predicting sequences of terminal
tokens. However, ambiguities in the generation task may lead to incorrect
outputs. Towards addressing this, we introduce Grammformers, transformer-based
grammar-guided models that learn (without explicit supervision) to generate
sketches -- sequences of tokens with holes. Through reinforcement learning,
Grammformers learn to introduce holes avoiding the generation of incorrect
tokens where there is ambiguity in the target task.
  We train Grammformers for statement-level source code completion, i.e., the
generation of code snippets given an ambiguous user intent, such as a partial
code context. We evaluate Grammformers on code completion for C# and Python and
show that it generates 10-50% more accurate sketches compared to traditional
generative models and 37-50% longer sketches compared to sketch-generating
baselines trained with similar techniques.","cs.LG,cs.SE"
"Scaffold-based molecular design using graph generative model. Searching new molecules in areas like drug discovery often starts from the
core structures of candidate molecules to optimize the properties of interest.
The way as such has called for a strategy of designing molecules retaining a
particular scaffold as a substructure. On this account, our present work
proposes a scaffold-based molecular generative model. The model generates
molecular graphs by extending the graph of a scaffold through sequential
additions of vertices and edges. In contrast to previous related models, our
model guarantees the generated molecules to retain the given scaffold with
certainty. Our evaluation of the model using unseen scaffolds showed the
validity, uniqueness, and novelty of generated molecules as high as the case
using seen scaffolds. This confirms that the model can generalize the learned
chemical rules of adding atoms and bonds rather than simply memorizing the
mapping from scaffolds to molecules during learning. Furthermore, despite the
restraint of fixing core structures, our model could simultaneously control
multiple molecular properties when generating new molecules.","cs.LG,q-bio.BM,stat.ML"
"Order Matters at Fanatics Recommending Sequentially Ordered Products by LSTM Embedded with Word2Vec. A unique challenge for e-commerce recommendation is that customers are often
interested in products that are more advanced than their already purchased
products, but not reversed. The few existing recommender systems modeling
unidirectional sequence output a limited number of categories or continuous
variables. To model the ordered sequence, we design the first recommendation
system that both embed purchased items with Word2Vec, and model the sequence
with stateless LSTM RNN. The click-through rate of this recommender system in
production outperforms its solely Word2Vec based predecessor. Developed in
2017, it was perhaps the first published real-world application that makes
distributed predictions of a single machine trained Keras model on Spark slave
nodes at a scale of more than 0.4 million columns per row.","cs.CL,cs.IR,cs.LG,stat.ML"
"Automated Architecture Design for Deep Neural Networks. Machine learning has made tremendous progress in recent years and received
large amounts of public attention. Though we are still far from designing a
full artificially intelligent agent, machine learning has brought us many
applications in which computers solve human learning tasks remarkably well.
Much of this progress comes from a recent trend within machine learning, called
deep learning. Deep learning models are responsible for many state-of-the-art
applications of machine learning. Despite their success, deep learning models
are hard to train, very difficult to understand, and often times so complex
that training is only possible on very large GPU clusters. Lots of work has
been done on enabling neural networks to learn efficiently. However, the design
and architecture of such neural networks is often done manually through trial
and error and expert knowledge. This thesis inspects different approaches,
existing and novel, to automate the design of deep feedforward neural networks
in an attempt to create less complex models with good performance that take
away the burden of deciding on an architecture and make it more efficient to
design and train such deep networks.","cs.LG,cs.NE,stat.ML"
"Identifying Illicit Drug Dealers on Instagram with Large-scale Multimodal Data Fusion. Illicit drug trafficking via social media sites such as Instagram has become
a severe problem, thus drawing a great deal of attention from law enforcement
and public health agencies. How to identify illicit drug dealers from social
media data has remained a technical challenge due to the following reasons. On
the one hand, the available data are limited because of privacy concerns with
crawling social media sites; on the other hand, the diversity of drug dealing
patterns makes it difficult to reliably distinguish drug dealers from common
drug users. Unlike existing methods that focus on posting-based detection, we
propose to tackle the problem of illicit drug dealer identification by
constructing a large-scale multimodal dataset named Identifying Drug Dealers on
Instagram (IDDIG). Totally nearly 4,000 user accounts, of which over 1,400 are
drug dealers, have been collected from Instagram with multiple data sources
including post comments, post images, homepage bio, and homepage images. We
then design a quadruple-based multimodal fusion method to combine the multiple
data sources associated with each user account for drug dealer identification.
Experimental results on the constructed IDDIG dataset demonstrate the
effectiveness of the proposed method in identifying drug dealers (almost 95%
accuracy). Moreover, we have developed a hashtag-based community detection
technique for discovering evolving patterns, especially those related to
geography and drug types.","cs.LG,eess.IV"
"Statistically Guided Divide-and-Conquer for Sparse Factorization of Large Matrix. The sparse factorization of a large matrix is fundamental in modern
statistical learning. In particular, the sparse singular value decomposition
and its variants have been utilized in multivariate regression, factor
analysis, biclustering, vector time series modeling, among others. The appeal
of this factorization is owing to its power in discovering a
highly-interpretable latent association network, either between samples and
variables or between responses and predictors. However, many existing methods
are either ad hoc without a general performance guarantee, or are
computationally intensive, rendering them unsuitable for large-scale studies.
We formulate the statistical problem as a sparse factor regression and tackle
it with a divide-and-conquer approach. In the first stage of division, we
consider both sequential and parallel approaches for simplifying the task into
a set of co-sparse unit-rank estimation (CURE) problems, and establish the
statistical underpinnings of these commonly-adopted and yet poorly understood
deflation methods. In the second stage of division, we innovate a contended
stagewise learning technique, consisting of a sequence of simple incremental
updates, to efficiently trace out the whole solution paths of CURE. Our
algorithm has a much lower computational complexity than alternating convex
search, and the choice of the step size enables a flexible and principled
tradeoff between statistical accuracy and computational efficiency. Our work is
among the first to enable stagewise learning for non-convex problems, and the
idea can be applicable in many multi-convex problems. Extensive simulation
studies and an application in genetics demonstrate the effectiveness and
scalability of our approach.","cs.LG,math.ST,stat.CO,stat.ML,stat.TH"
"A Random Matrix Perspective on Random Tensors. Tensor models play an increasingly prominent role in many fields, notably in
machine learning. In several applications of such models, such as community
detection, topic modeling and Gaussian mixture learning, one must estimate a
low-rank signal from a noisy tensor. Hence, understanding the fundamental
limits and the attainable performance of estimators of that signal inevitably
calls for the study of random tensors. Substantial progress has been achieved
on this subject thanks to recent efforts, under the assumption that the tensor
dimensions grow large. Yet, some of the most significant among these
results--in particular, a precise characterization of the abrupt phase
transition (in terms of signal-to-noise ratio) that governs the performance of
the maximum likelihood (ML) estimator of a symmetric rank-one model with
Gaussian noise--were derived on the basis of statistical physics ideas, which
are not easily accessible to non-experts.
  In this work, we develop a sharply distinct approach, relying instead on
standard but powerful tools brought by years of advances in random matrix
theory. The key idea is to study the spectra of random matrices arising from
contractions of a given random tensor. We show how this gives access to
spectral properties of the random tensor itself. In the specific case of a
symmetric rank-one model with Gaussian noise, our technique yields a hitherto
unknown characterization of the local maximum of the ML problem that is global
above the phase transition threshold. This characterization is in terms of a
fixed-point equation satisfied by a formula that had only been previously
obtained via statistical physics methods. Moreover, our analysis sheds light on
certain properties of the landscape of the ML problem in the large-dimensional
setting. Our approach is versatile and can be extended to other models, such as
asymmetric, non-Gaussian and higher-order ones.","15A69,60B20,cs.LG,math.PR,stat.ML"
"Style Transfer With Adaptation to the Central Objects of the Scene. Style transfer is a problem of rendering image with some content in the style
of another image, for example a family photo in the style of a painting of some
famous artist. The drawback of classical style transfer algorithm is that it
imposes style uniformly on all parts of the content image, which perturbs
central objects on the content image, such as faces or text, and makes them
unrecognizable. This work proposes a novel style transfer algorithm which
automatically detects central objects on the content image, generates spatial
importance mask and imposes style non-uniformly: central objects are stylized
less to preserve their recognizability and other parts of the image are
stylized as usual to preserve the style. Three methods of automatic central
object detection are proposed and evaluated qualitatively and via a user
evaluation study. Both comparisons demonstrate higher quality of stylization
compared to the classical style transfer method.","68T45,I.4.9; I.4.10,cs.CV"
"Asymptotics of Network Embeddings Learned via Subsampling. Network data are ubiquitous in modern machine learning, with tasks of
interest including node classification, node clustering and link prediction. A
frequent approach begins by learning an Euclidean embedding of the network, to
which algorithms developed for vector-valued data are applied. For large
networks, embeddings are learned using stochastic gradient methods where the
sub-sampling scheme can be freely chosen. Despite the strong empirical
performance of such methods, they are not well understood theoretically. Our
work encapsulates representation methods using a subsampling approach, such as
node2vec, into a single unifying framework. We prove, under the assumption that
the graph is exchangeable, that the distribution of the learned embedding
vectors asymptotically decouples. Moreover, we characterize the asymptotic
distribution and provided rates of convergence, in terms of the latent
parameters, which includes the choice of loss function and the embedding
dimension. This provides a theoretical foundation to understand what the
embedding vectors represent and how well these methods perform on downstream
tasks. Notably, we observe that typically used loss functions may lead to
shortcomings, such as a lack of Fisher consistency.","cs.LG,math.ST,stat.ML,stat.TH"
"DeepSynth: Automata Synthesis for Automatic Task Segmentation in Deep Reinforcement Learning. This paper proposes DeepSynth, a method for effective training of deep
Reinforcement Learning (RL) agents when the reward is sparse and non-Markovian,
but at the same time progress towards the reward requires achieving an unknown
sequence of high-level objectives. Our method employs a novel algorithm for
synthesis of compact automata to uncover this sequential structure
automatically. We synthesise a human-interpretable automaton from trace data
collected by exploring the environment. The state space of the environment is
then enriched with the synthesised automaton so that the generation of a
control policy by deep RL is guided by the discovered structure encoded in the
automaton. The proposed approach is able to cope with both high-dimensional,
low-level features and unknown sparse non-Markovian rewards. We have evaluated
DeepSynth's performance in a set of experiments that includes the Atari game
Montezuma's Revenge. Compared to existing approaches, we obtain a reduction of
two orders of magnitude in the number of iterations required for policy
synthesis, and also a significant improvement in scalability.","cs.AI,cs.LG,cs.LO,stat.ML"
"Mungojerrie: Reinforcement Learning of Linear-Time Objectives. Reinforcement learning synthesizes controllers without prior knowledge of the
system. At each timestep, a reward is given. The controllers optimize the
discounted sum of these rewards. Applying this class of algorithms requires
designing a reward scheme, which is typically done manually. The designer must
ensure that their intent is accurately captured. This may not be trivial, and
is prone to error. An alternative to this manual programming, akin to
programming directly in assembly, is to specify the objective in a formal
language and have it ""compiled"" to a reward scheme. Mungojerrie
(https://plv.colorado.edu/mungojerrie/) is a tool for testing reward schemes
for $\omega$-regular objectives on finite models. The tool contains
reinforcement learning algorithms and a probabilistic model checker.
Mungojerrie supports models specified in PRISM and $\omega$-automata specified
in HOA.","cs.LG,cs.LO,cs.SY,eess.SY"
"Belief Propagation, Bethe Approximation and Polynomials. Factor graphs are important models for succinctly representing probability
distributions in machine learning, coding theory, and statistical physics.
Several computational problems, such as computing marginals and partition
functions, arise naturally when working with factor graphs. Belief propagation
is a widely deployed iterative method for solving these problems. However,
despite its significant empirical success, not much is known about the
correctness and efficiency of belief propagation.
  Bethe approximation is an optimization-based framework for approximating
partition functions. While it is known that the stationary points of the Bethe
approximation coincide with the fixed points of belief propagation, in general,
the relation between the Bethe approximation and the partition function is not
well understood. It has been observed that for a few classes of factor graphs,
the Bethe approximation always gives a lower bound to the partition function,
which distinguishes them from the general case, where neither a lower bound,
nor an upper bound holds universally. This has been rigorously proved for
permanents and for attractive graphical models.
  Here we consider bipartite normal factor graphs and show that if the local
constraints satisfy a certain analytic property, the Bethe approximation is a
lower bound to the partition function. We arrive at this result by viewing
factor graphs through the lens of polynomials. In this process, we reformulate
the Bethe approximation as a polynomial optimization problem. Our sufficient
condition for the lower bound property to hold is inspired by recent
developments in the theory of real stable polynomials. We believe that this way
of viewing factor graphs and its connection to real stability might lead to a
better understanding of belief propagation and factor graphs in general.","cs.DS,cs.IT,cs.LG,math.IT,stat.ML"
"Equivariant Graph Neural Networks for 3D Macromolecular Structure. Representing and reasoning about 3D structures of macromolecules is emerging
as a distinct challenge in machine learning. Here, we extend recent work on
geometric vector perceptrons and apply equivariant graph neural networks to a
wide range of tasks from structural biology. Our method outperforms all
reference architectures on three out of eight tasks in the ATOM3D benchmark, is
tied for first on two others, and is competitive with equivariant networks
using higher-order representations and spherical harmonic convolutions. In
addition, we demonstrate that transfer learning can further improve performance
on certain downstream tasks. Code is available at
https://github.com/drorlab/gvp-pytorch.","cs.LG,q-bio.BM"
"Graphical continuous Lyapunov models. The linear Lyapunov equation of a covariance matrix parametrizes the
equilibrium covariance matrix of a stochastic process. This parametrization can
be interpreted as a new graphical model class, and we show how the model class
behaves under marginalization and introduce a method for structure learning via
$\ell_1$-penalized loss minimization. Our proposed method is demonstrated to
outperform alternative structure learning algorithms in a simulation study, and
we illustrate its application for protein phosphorylation network
reconstruction.","cs.LG,stat.CO,stat.ME,stat.ML"
"Learning in Quantum Control: High-Dimensional Global Optimization for Noisy Quantum Dynamics. Quantum control is valuable for various quantum technologies such as
high-fidelity gates for universal quantum computing, adaptive quantum-enhanced
metrology, and ultra-cold atom manipulation. Although supervised machine
learning and reinforcement learning are widely used for optimizing control
parameters in classical systems, quantum control for parameter optimization is
mainly pursued via gradient-based greedy algorithms. Although the quantum
fitness landscape is often compatible with greedy algorithms, sometimes greedy
algorithms yield poor results, especially for large-dimensional quantum
systems. We employ differential evolution algorithms to circumvent the
stagnation problem of non-convex optimization. We improve quantum control
fidelity for noisy system by averaging over the objective function. To reduce
computational cost, we introduce heuristics for early termination of runs and
for adaptive selection of search subspaces. Our implementation is massively
parallel and vectorized to reduce run time even further. We demonstrate our
methods with two examples, namely quantum phase estimation and quantum gate
design, for which we achieve superior fidelity and scalability than obtained
using greedy algorithms.","cs.LG,cs.SY,quant-ph,stat.ML"
"The State of the Art in Integrating Machine Learning into Visual Analytics. Visual analytics systems combine machine learning or other analytic
techniques with interactive data visualization to promote sensemaking and
analytical reasoning. It is through such techniques that people can make sense
of large, complex data. While progress has been made, the tactful combination
of machine learning and data visualization is still under-explored. This
state-of-the-art report presents a summary of the progress that has been made
by highlighting and synthesizing select research advances. Further, it presents
opportunities and challenges to enhance the synergy between machine learning
and visual analytics for impactful future research directions.","cs.HC,cs.LG,stat.ML"
"Dealing with Non-Stationarity in Multi-Agent Reinforcement Learning via Trust Region Decomposition. Non-stationarity is one thorny issue in multi-agent reinforcement learning,
which is caused by the policy changes of agents during the learning procedure.
Current works to solve this problem have their own limitations in effectiveness
and scalability, such as centralized critic and decentralized actor (CCDA),
population-based self-play, modeling of others and etc. In this paper, we
novelly introduce a $\delta$-stationarity measurement to explicitly model the
stationarity of a policy sequence, which is theoretically proved to be
proportional to the joint policy divergence. However, simple policy
factorization like mean-field approximation will mislead to larger policy
divergence, which can be considered as trust region decomposition dilemma. We
model the joint policy as a general Markov random field and propose a trust
region decomposition network based on message passing to estimate the joint
policy divergence more accurately. The Multi-Agent Mirror descent policy
algorithm with Trust region decomposition, called MAMT, is established with the
purpose to satisfy $\delta$-stationarity. MAMT can adjust the trust region of
the local policies adaptively in an end-to-end manner, thereby approximately
constraining the divergence of joint policy to alleviate the non-stationary
problem. Our method can bring noticeable and stable performance improvement
compared with baselines in coordination tasks of different complexity.","cs.GT,cs.LG,cs.MA"
"Exploiting Categorical Structure Using Tree-Based Methods. Standard methods of using categorical variables as predictors either endow
them with an ordinal structure or assume they have no structure at all.
However, categorical variables often possess structure that is more complicated
than a linear ordering can capture. We develop a mathematical framework for
representing the structure of categorical variables and show how to generalize
decision trees to make use of this structure. This approach is applicable to
methods such as Gradient Boosted Trees which use a decision tree as the
underlying learner. We show results on weather data to demonstrate the
improvement yielded by this approach.","cs.AI,cs.LG,stat.AP,stat.ML"
"How Framelets Enhance Graph Neural Networks. This paper presents a new approach for assembling graph neural networks based
on framelet transforms. The latter provides a multi-scale representation for
graph-structured data. We decompose an input graph into low-pass and high-pass
frequencies coefficients for network training, which then defines a
framelet-based graph convolution. The framelet decomposition naturally induces
a graph pooling strategy by aggregating the graph feature into low-pass and
high-pass spectra, which considers both the feature values and geometry of the
graph data and conserves the total information. The graph neural networks with
the proposed framelet convolution and pooling achieve state-of-the-art
performance in many node and graph prediction tasks. Moreover, we propose
shrinkage as a new activation for the framelet convolution, which thresholds
high-frequency information at different scales. Compared to ReLU, shrinkage
activation improves model performance on denoising and signal compression:
noises in both node and structure can be significantly reduced by accurately
cutting off the high-pass coefficients from framelet decomposition, and the
signal can be compressed to less than half its original size with
well-preserved prediction performance.","05C85,42C40,68T07,I.2.4; I.2.6,cs.AI,cs.LG,cs.NA,math.NA"
"Markerless tracking of user-defined features with deep learning. Quantifying behavior is crucial for many applications in neuroscience.
Videography provides easy methods for the observation and recording of animal
behavior in diverse settings, yet extracting particular aspects of a behavior
for further analysis can be highly time consuming. In motor control studies,
humans or other animals are often marked with reflective markers to assist with
computer-based tracking, yet markers are intrusive (especially for smaller
animals), and the number and location of the markers must be determined a
priori. Here, we present a highly efficient method for markerless tracking
based on transfer learning with deep neural networks that achieves excellent
results with minimal training data. We demonstrate the versatility of this
framework by tracking various body parts in a broad collection of experimental
settings: mice odor trail-tracking, egg-laying behavior in drosophila, and
mouse hand articulation in a skilled forelimb task. For example, during the
skilled reaching behavior, individual joints can be automatically tracked (and
a confidence score is reported). Remarkably, even when a small number of frames
are labeled ($\approx 200$), the algorithm achieves excellent tracking
performance on test frames that is comparable to human accuracy.","cs.CV,q-bio.NC,q-bio.QM,stat.ML"
"The Fast Kernel Transform. Kernel methods are a highly effective and widely used collection of modern
machine learning algorithms. A fundamental limitation of virtually all such
methods are computations involving the kernel matrix that naively scale
quadratically (e.g., constructing the kernel matrix and matrix-vector
multiplication) or cubically (solving linear systems) with the size of the data
set $N.$ We propose the Fast Kernel Transform (FKT), a general algorithm to
compute matrix-vector multiplications (MVMs) for datasets in moderate
dimensions with quasilinear complexity. Typically, analytically grounded fast
multiplication methods require specialized development for specific kernels. In
contrast, our scheme is based on auto-differentiation and automated symbolic
computations that leverage the analytical structure of the underlying kernel.
This allows the FKT to be easily applied to a broad class of kernels, including
Gaussian, Matern, and Rational Quadratic covariance functions and physically
motivated Green's functions, including those of the Laplace and Helmholtz
equations. Furthermore, the FKT maintains a high, quantifiable, and
controllable level of accuracy -- properties that many acceleration methods
lack. We illustrate the efficacy and versatility of the FKT by providing timing
and accuracy benchmarks and by applying it to scale the stochastic neighborhood
embedding (t-SNE) and Gaussian processes to large real-world data sets.","cs.LG,cs.NA,math.NA"
"Reinforcement Learning via Recurrent Convolutional Neural Networks. Deep Reinforcement Learning has enabled the learning of policies for complex
tasks in partially observable environments, without explicitly learning the
underlying model of the tasks. While such model-free methods achieve
considerable performance, they often ignore the structure of task. We present a
natural representation of to Reinforcement Learning (RL) problems using
Recurrent Convolutional Neural Networks (RCNNs), to better exploit this
inherent structure. We define 3 such RCNNs, whose forward passes execute an
efficient Value Iteration, propagate beliefs of state in partially observable
environments, and choose optimal actions respectively. Backpropagating
gradients through these RCNNs allows the system to explicitly learn the
Transition Model and Reward Function associated with the underlying MDP,
serving as an elegant alternative to classical model-based RL. We evaluate the
proposed algorithms in simulation, considering a robot planning problem. We
demonstrate the capability of our framework to reduce the cost of replanning,
learn accurate MDP models, and finally re-plan with learnt models to achieve
near-optimal policies.","68T05,cs.AI,cs.LG"
"GEFCOM 2014 - Probabilistic Electricity Price Forecasting. Energy price forecasting is a relevant yet hard task in the field of
multi-step time series forecasting. In this paper we compare a well-known and
established method, ARMA with exogenous variables with a relatively new
technique Gradient Boosting Regression. The method was tested on data from
Global Energy Forecasting Competition 2014 with a year long rolling window
forecast. The results from the experiment reveal that a multi-model approach is
significantly better performing in terms of error metrics. Gradient Boosting
can deal with seasonality and auto-correlation out-of-the box and achieve lower
rate of normalized mean absolute error on real-world data.","cs.CE,cs.LG,stat.AP,stat.ML"
"Inverse Reinforcement Learning Based Stochastic Driver Behavior Learning. Drivers have unique and rich driving behaviors when operating vehicles in
traffic. This paper presents a novel driver behavior learning approach that
captures the uniqueness and richness of human driver behavior in realistic
driving scenarios. A stochastic inverse reinforcement learning (SIRL) approach
is proposed to learn a distribution of cost function, which represents the
richness of the human driver behavior with a given set of driver-specific
demonstrations. Evaluations are conducted on the realistic driving data
collected from the 3D driver-in-the-loop driving simulation. The results show
that the learned stochastic driver model is capable of expressing the richness
of the human driving strategies under different realistic driving scenarios.
Compared to the deterministic baseline driver behavior model, the results
reveal that the proposed stochastic driver behavior model can better replicate
the driver's unique and rich driving strategies in a variety of traffic
conditions.","cs.LG,cs.SY,eess.SY"
"Constraining Implicit Space with Minimum Description Length: An Unsupervised Attention Mechanism across Neural Network Layers. Inspired by the adaptation phenomenon of neuronal firing, we propose the
regularity normalization (RN) as an unsupervised attention mechanism (UAM)
which computes the statistical regularity in the implicit space of neural
networks under the Minimum Description Length (MDL) principle. Treating the
neural network optimization process as a partially observable model selection
problem, UAM constrains the implicit space by a normalization factor, the
universal code length. We compute this universal code incrementally across
neural network layers and demonstrated the flexibility to include data priors
such as top-down attention and other oracle information. Empirically, our
approach outperforms existing normalization methods in tackling limited,
imbalanced and non-stationary input distribution in image classification,
classic control, procedurally-generated reinforcement learning, generative
modeling, handwriting generation and question answering tasks with various
neural network architectures. Lastly, UAM tracks dependency and critical
learning stages across layers and recurrent time steps of deep networks.","cs.CV,cs.IT,cs.LG,math.IT,q-bio.NC,stat.ML"
"Probabilistic Integration: A Role in Statistical Computation?. A research frontier has emerged in scientific computation, wherein numerical
error is regarded as a source of epistemic uncertainty that can be modelled.
This raises several statistical challenges, including the design of statistical
methods that enable the coherent propagation of probabilities through a
(possibly deterministic) computational work-flow. This paper examines the case
for probabilistic numerical methods in routine statistical computation. Our
focus is on numerical integration, where a probabilistic integrator is equipped
with a full distribution over its output that reflects the presence of an
unknown numerical error. Our main technical contribution is to establish, for
the first time, rates of posterior contraction for these methods. These show
that probabilistic integrators can in principle enjoy the ""best of both
worlds"", leveraging the sampling efficiency of Monte Carlo methods whilst
providing a principled route to assess the impact of numerical error on
scientific conclusions. Several substantial applications are provided for
illustration and critical evaluation, including examples from statistical
modelling, computer graphics and a computer model for an oil reservoir.","cs.NA,math.NA,math.ST,stat.CO,stat.ML,stat.TH"
"Fully Convolutional Networks for Diabetic Foot Ulcer Segmentation. Diabetic Foot Ulcer (DFU) is a major complication of Diabetes, which if not
managed properly can lead to amputation. DFU can appear anywhere on the foot
and can vary in size, colour, and contrast depending on various pathologies.
Current clinical approaches to DFU treatment rely on patients and clinician
vigilance, which has significant limitations such as the high cost involved in
the diagnosis, treatment and lengthy care of the DFU. We introduce a dataset of
705 foot images. We provide the ground truth of ulcer region and the
surrounding skin that is an important indicator for clinicians to assess the
progress of ulcer. Then, we propose a two-tier transfer learning from bigger
datasets to train the Fully Convolutional Networks (FCNs) to automatically
segment the ulcer and surrounding skin. Using 5-fold cross-validation, the
proposed two-tier transfer learning FCN Models achieve a Dice Similarity
Coefficient of 0.794 ($\pm$0.104) for ulcer region, 0.851 ($\pm$0.148) for
surrounding skin region, and 0.899 ($\pm$0.072) for the combination of both
regions. This demonstrates the potential of FCNs in DFU segmentation, which can
be further improved with a larger dataset.",cs.CV
"Anti-Transfer Learning for Task Invariance in Convolutional Neural Networks for Speech Processing. We introduce the novel concept of anti-transfer learning for speech
processing with convolutional neural networks. While transfer learning assumes
that the learning process for a target task will benefit from re-using
representations learned for another task, anti-transfer avoids the learning of
representations that have been learned for an orthogonal task, i.e., one that
is not relevant and potentially misleading for the target task, such as speaker
identity for speech recognition or speech content for emotion recognition. In
anti-transfer learning, we penalize similarity between activations of a network
being trained and another one previously trained on an orthogonal task, which
yields more suitable representations. This leads to better generalization and
provides a degree of control over correlations that are spurious or
undesirable, e.g. to avoid social bias. We have implemented anti-transfer for
convolutional neural networks in different configurations with several
similarity metrics and aggregation functions, which we evaluate and analyze
with several speech and audio tasks and settings, using six datasets. We show
that anti-transfer actually leads to the intended invariance to the orthogonal
task and to more appropriate features for the target task at hand.
Anti-transfer learning consistently improves classification accuracy in all
test cases. While anti-transfer creates computation and memory cost at training
time, there is relatively little computation cost when using pre-trained models
for orthogonal tasks. Anti-transfer is widely applicable and particularly
useful where a specific invariance is desirable or where trained models are
available and labeled data for orthogonal tasks are difficult to obtain.","cs.LG,cs.NE,cs.SD,eess.AS,stat.ML"
"Deciphering Environmental Air Pollution with Large Scale City Data. Out of the numerous hazards posing a threat to sustainable environmental
conditions in the 21st century, only a few have a graver impact than air
pollution. Its importance in determining the health and living standards in
urban settings is only expected to increase with time. Various factors ranging
from emissions from traffic and power plants, household emissions, natural
causes are known to be primary causal agents or influencers behind rising air
pollution levels. However, the lack of large scale data involving the major
factors has hindered the research on the causes and relations governing the
variability of the different air pollutants. Through this work, we introduce a
large scale city-wise dataset for exploring the relationships among these
agents over a long period of time. We analyze and explore the dataset to bring
out inferences which we can derive by modeling the data. Also, we provide a set
of benchmarks for the problem of estimating or forecasting pollutant levels
with a set of diverse models and methodologies. Through our paper, we seek to
provide a ground base for further research into this domain that will demand
critical attention of ours in the near future.","cs.AI,cs.LG,physics.data-an"
"Exathlon: A Benchmark for Explainable Anomaly Detection over Time Series. Access to high-quality data repositories and benchmarks have been
instrumental in advancing the state of the art in many experimental research
domains. While advanced analytics tasks over time series data have been gaining
lots of attention, lack of such community resources severely limits scientific
progress. In this paper, we present Exathlon, the first comprehensive public
benchmark for explainable anomaly detection over high-dimensional time series
data. Exathlon has been systematically constructed based on real data traces
from repeated executions of large-scale stream processing jobs on an Apache
Spark cluster. Some of these executions were intentionally disturbed by
introducing instances of six different types of anomalous events (e.g.,
misbehaving inputs, resource contention, process failures). For each of the
anomaly instances, ground truth labels for the root cause interval as well as
those for the extended effect interval are provided, supporting the development
and evaluation of a wide range of anomaly detection (AD) and explanation
discovery (ED) tasks. We demonstrate the practical utility of Exathlon's
dataset, evaluation methodology, and end-to-end data science pipeline design
through an experimental study with three state-of-the-art AD and ED techniques.","cs.DB,cs.LG"
"A Grammar-Based Structural CNN Decoder for Code Generation. Code generation maps a program description to executable source code in a
programming language. Existing approaches mainly rely on a recurrent neural
network (RNN) as the decoder. However, we find that a program contains
significantly more tokens than a natural language sentence, and thus it may be
inappropriate for RNN to capture such a long sequence. In this paper, we
propose a grammar-based structural convolutional neural network (CNN) for code
generation. Our model generates a program by predicting the grammar rules of
the programming language; we design several CNN modules, including the
tree-based convolution and pre-order convolution, whose information is further
aggregated by dedicated attentive pooling layers. Experimental results on the
HearthStone benchmark dataset show that our CNN code generator significantly
outperforms the previous state-of-the-art method by 5 percentage points;
additional experiments on several semantic parsing tasks demonstrate the
robustness of our model. We also conduct in-depth ablation test to better
understand each component of our model.","cs.LG,cs.SE,stat.ML"
"Data-driven Neural Architecture Learning For Financial Time-series Forecasting. Forecasting based on financial time-series is a challenging task since most
real-world data exhibits nonstationary property and nonlinear dependencies. In
addition, different data modalities often embed different nonlinear
relationships which are difficult to capture by human-designed models. To
tackle the supervised learning task in financial time-series prediction, we
propose the application of a recently formulated algorithm that adaptively
learns a mapping function, realized by a heterogeneous neural architecture
composing of Generalized Operational Perceptron, given a set of labeled data.
With a modified objective function, the proposed algorithm can accommodate the
frequently observed imbalanced data distribution problem. Experiments on a
large-scale Limit Order Book dataset demonstrate that the proposed algorithm
outperforms related algorithms, including tensor-based methods which have
access to a broader set of input information.","cs.CE,cs.LG,q-fin.ST,stat.ML"
"Inference with Multivariate Heavy-Tails in Linear Models. Heavy-tailed distributions naturally occur in many real life problems.
Unfortunately, it is typically not possible to compute inference in closed-form
in graphical models which involve such heavy-tailed distributions.
  In this work, we propose a novel simple linear graphical model for
independent latent random variables, called linear characteristic model (LCM),
defined in the characteristic function domain. Using stable distributions, a
heavy-tailed family of distributions which is a generalization of Cauchy,
L\'evy and Gaussian distributions, we show for the first time, how to compute
both exact and approximate inference in such a linear multivariate graphical
model. LCMs are not limited to stable distributions, in fact LCMs are always
defined for any random variables (discrete, continuous or a mixture of both).
  We provide a realistic problem from the field of computer networks to
demonstrate the applicability of our construction. Other potential application
is iterative decoding of linear channels with non-Gaussian noise.","cs.IT,cs.LG,math.IT"
"A New Framework for Registration of Semantic Point Clouds from Stereo and RGB-D Cameras. This paper reports on a novel nonparametric rigid point cloud registration
framework that jointly integrates geometric and semantic measurements such as
color or semantic labels into the alignment process and does not require
explicit data association. The point clouds are represented as nonparametric
functions in a reproducible kernel Hilbert space. The alignment problem is
formulated as maximizing the inner product between two functions, essentially a
sum of weighted kernels, each of which exploits the local geometric and
semantic features. As a result of the continuous models, analytical gradients
can be computed, and a local solution can be obtained by optimization over the
rigid body transformation group. Besides, we present a new point cloud
alignment metric that is intrinsic to the proposed framework and takes into
account geometric and semantic information. The evaluations using publicly
available stereo and RGB-D datasets show that the proposed method outperforms
state-of-the-art outdoor and indoor frame-to-frame registration methods. An
open-source GPU implementation is also provided.","cs.CV,cs.RO"
"Model-Based Multi-Agent RL in Zero-Sum Markov Games with Near-Optimal Sample Complexity. Model-based reinforcement learning (RL), which finds an optimal policy using
an empirical model, has long been recognized as one of the corner stones of RL.
It is especially suitable for multi-agent RL (MARL), as it naturally decouples
the learning and the planning phases, and avoids the non-stationarity problem
when all agents are improving their policies simultaneously using samples.
Though intuitive and widely-used, the sample complexity of model-based MARL
algorithms has not been fully investigated. In this paper, our goal is to
address the fundamental question about its sample complexity. We study arguably
the most basic MARL setting: two-player discounted zero-sum Markov games, given
only access to a generative model. We show that model-based MARL achieves a
sample complexity of $\tilde O(|S||A||B|(1-\gamma)^{-3}\epsilon^{-2})$ for
finding the Nash equilibrium (NE) value up to some $\epsilon$ error, and the
$\epsilon$-NE policies with a smooth planning oracle, where $\gamma$ is the
discount factor, and $S,A,B$ denote the state space, and the action spaces for
the two agents. We further show that such a sample bound is minimax-optimal (up
to logarithmic factors) if the algorithm is reward-agnostic, where the
algorithm queries state transition samples without reward knowledge, by
establishing a matching lower bound. This is in contrast to the usual
reward-aware setting, with a
$\tilde\Omega(|S|(|A|+|B|)(1-\gamma)^{-3}\epsilon^{-2})$ lower bound, where
this model-based approach is near-optimal with only a gap on the $|A|,|B|$
dependence. Our results not only demonstrate the sample-efficiency of this
basic model-based approach in MARL, but also elaborate on the fundamental
tradeoff between its power (easily handling the more challenging
reward-agnostic case) and limitation (less adaptive and suboptimal in
$|A|,|B|$), particularly arises in the multi-agent context.","cs.GT,cs.LG,cs.MA,math.OC,stat.ML"
"Am I fit for this physical activity? Neural embedding of physical conditioning from inertial sensors. Inertial Measurement Unit (IMU) sensors are present in everyday devices such
as smartphones and fitness watches. As a result, the array of health-related
research and applications that tap onto this data has been growing, but little
attention has been devoted to the prediction of an individual's heart rate (HR)
from IMU data, when undergoing a physical activity. Would that be even
possible? If so, this could be used to design personalized sets of aerobic
exercises, for instance. In this work, we show that it is viable to obtain
accurate HR predictions from IMU data using Recurrent Neural Networks, provided
only access to HR and IMU data from a short-lived, previously executed
activity. We propose a novel method for initializing an RNN's hidden state
vectors, using a specialized network that attempts to extract an embedding of
the physical conditioning (PCE) of a subject. We show that using a
discriminator in the training phase to help the model learn whether two PCEs
belong to the same individual further reduces the prediction error. We evaluate
the proposed model when predicting the HR of 23 subjects performing a variety
of physical activities from IMU data available in public datasets (PAMAP2,
PPG-DaLiA). For comparison, we use as baselines the only model specifically
proposed for this task and an adapted state-of-the-art model for Human Activity
Recognition (HAR), a closely related task. Our method, PCE-LSTM, yields over
10% lower mean absolute error. We demonstrate empirically that this error
reduction is in part due to the use of the PCE. Last, we use the two datasets
(PPG-DaLiA, WESAD) to show that PCE-LSTM can also be successfully applied when
photoplethysmography (PPG) sensors are available, outperforming the
state-of-the-art deep learning baselines by more than 30%.","68T07,I.2.1; I.5.1; J.3,cs.AI,cs.LG"
"Diffusion Schrdinger Bridge with Applications to Score-Based Generative Modeling. Progressively applying Gaussian noise transforms complex data distributions
to approximately Gaussian. Reversing this dynamic defines a generative model.
When the forward noising process is given by a Stochastic Differential Equation
(SDE), Song et al. (2021) demonstrate how the time inhomogeneous drift of the
associated reverse-time SDE may be estimated using score-matching. A limitation
of this approach is that the forward-time SDE must be run for a sufficiently
long time for the final distribution to be approximately Gaussian. In contrast,
solving the Schr\""odinger Bridge problem (SB), i.e. an entropy-regularized
optimal transport problem on path spaces, yields diffusions which generate
samples from the data distribution in finite time. We present Diffusion SB
(DSB), an original approximation of the Iterative Proportional Fitting (IPF)
procedure to solve the SB problem, and provide theoretical analysis along with
generative modeling experiments. The first DSB iteration recovers the
methodology proposed by Song et al. (2021), with the flexibility of using
shorter time intervals, as subsequent DSB iterations reduce the discrepancy
between the final-time marginal of the forward (resp. backward) SDE with
respect to the prior (resp. data) distribution. Beyond generative modeling, DSB
offers a widely applicable computational optimal transport tool as the
continuous state-space analogue of the popular Sinkhorn algorithm (Cuturi,
2013).","cs.LG,math.PR,stat.ML"
"IntersectGAN: Learning Domain Intersection for Generating Images with Multiple Attributes. Generative adversarial networks (GANs) have demonstrated great success in
generating various visual content. However, images generated by existing GANs
are often of attributes (e.g., smiling expression) learned from one image
domain. As a result, generating images of multiple attributes requires many
real samples possessing multiple attributes which are very resource expensive
to be collected. In this paper, we propose a novel GAN, namely IntersectGAN, to
learn multiple attributes from different image domains through an intersecting
architecture. For example, given two image domains $X_1$ and $X_2$ with certain
attributes, the intersection $X_1 \cap X_2$ denotes a new domain where images
possess the attributes from both $X_1$ and $X_2$ domains. The proposed
IntersectGAN consists of two discriminators $D_1$ and $D_2$ to distinguish
between generated and real samples of different domains, and three generators
where the intersection generator is trained against both discriminators. And an
overall adversarial loss function is defined over three generators. As a
result, our proposed IntersectGAN can be trained on multiple domains of which
each presents one specific attribute, and eventually eliminates the need of
real sample images simultaneously possessing multiple attributes. By using the
CelebFaces Attributes dataset, our proposed IntersectGAN is able to produce
high quality face images possessing multiple attributes (e.g., a face with
black hair and a smiling expression). Both qualitative and quantitative
evaluations are conducted to compare our proposed IntersectGAN with other
baseline methods. Besides, several different applications of IntersectGAN have
been explored with promising results.",cs.CV
"Modeling and Interpreting Real-world Human Risk Decision Making with Inverse Reinforcement Learning. We model human decision-making behaviors in a risk-taking task using inverse
reinforcement learning (IRL) for the purposes of understanding real human
decision making under risk. To the best of our knowledge, this is the first
work applying IRL to reveal the implicit reward function in human risk-taking
decision making and to interpret risk-prone and risk-averse decision-making
policies. We hypothesize that the state history (e.g. rewards and decisions in
previous trials) are related to the human reward function, which leads to
risk-averse and risk-prone decisions. We design features that reflect these
factors in the reward function of IRL and learn the corresponding weight that
is interpretable as the importance of features. The results confirm the
sub-optimal risk-related decisions of human-driven by the personalized reward
function. In particular, the risk-prone person tends to decide based on the
current pump number, while the risk-averse person relies on burst information
from the previous trial and the average end status. Our results demonstrate
that IRL is an effective tool to model human decision-making behavior, as well
as to help interpret the human psychological process in risk decision-making.","cs.LG,q-bio.NC,stat.ML"
"Impossibility of Partial Recovery in the Graph Alignment Problem. Random graph alignment refers to recovering the underlying vertex
correspondence between two random graphs with correlated edges. This can be
viewed as an average-case and noisy version of the well-known graph isomorphism
problem. For the correlated Erd\""os-R\'enyi model, we prove an impossibility
result for partial recovery in the sparse regime, with constant average degree
and correlation, as well as a general bound on the maximal reachable overlap.
Our bound is tight in the noiseless case (the graph isomorphism problem) and we
conjecture that it is still tight with noise. Our proof technique relies on a
careful application of the probabilistic method to build automorphisms between
tree components of a subcritical Erd\""os-R\'enyi graph.","cs.LG,math.PR,math.ST,stat.ML,stat.TH"
"Graph Representation Learning Network via Adaptive Sampling. Graph Attention Network (GAT) and GraphSAGE are neural network architectures
that operate on graph-structured data and have been widely studied for link
prediction and node classification. One challenge raised by GraphSAGE is how to
smartly combine neighbour features based on graph structure. GAT handles this
problem through attention, however the challenge with GAT is its scalability
over large and dense graphs. In this work, we proposed a new architecture to
address these issues that is more efficient and is capable of incorporating
different edge type information. It generates node representations by attending
to neighbours sampled from weighted multi-step transition probabilities. We
conduct experiments on both transductive and inductive settings. Experiments
achieved comparable or better results on several graph benchmarks, including
the Cora, Citeseer, Pubmed, PPI, Twitter, and YouTube datasets.","cs.LG,stat.ML"
"Supervised Negative Binomial Classifier for Probabilistic Record Linkage. Motivated by the need of the linking records across various databases, we
propose a novel graphical model based classifier that uses a mixture of Poisson
distributions with latent variables. The idea is to derive insight into each
pair of hypothesis records that match by inferring its underlying latent rate
of error using Bayesian Modeling techniques. The novel approach of using gamma
priors for learning the latent variables along with supervised labels is unique
and allows for active learning. The naive assumption is made deliberately as to
the independence of the fields to propose a generalized theory for this class
of problems and not to undermine the hierarchical dependencies that could be
present in different scenarios. This classifier is able to work with sparse and
streaming data. The application to record linkage is able to meet several
challenges of sparsity, data streams and varying nature of the data-sets.","cs.DB,cs.LG,stat.ML"
"An Online Algorithm for Maximum-Likelihood Quantum State Tomography. We propose, to the best of our knowledge, the first online algorithm to
compute the maximum-likelihood estimate in quantum state tomography. Suppose
the quantum state to be estimated corresponds to a $D$-by-$D$ density matrix.
The per-iteration computational complexity of the algorithm is $O ( D ^ 3 )$,
independent of the data size. The expected optimization error of the algorithm
is $O(\sqrt{ ( 1 / T ) D \log D })$, where $T$ denotes the number of
iterations. The algorithm can be viewed as a quantum extension of Soft-Bayes, a
recent algorithm for online portfolio selection (Orseau et al. Soft-Bayes: Prod
for mixtures of experts with log-loss. Int. Conf. Algorithmic Learning Theory.
2017).","cs.LG,math.OC,quant-ph"
"Field geology with a wearable computer: 1st results of the Cyborg Astrobiologist System. We present results from the first geological field tests of the `Cyborg
Astrobiologist', which is a wearable computer and video camcorder system that
we are using to test and train a computer-vision system towards having some of
the autonomous decision-making capabilities of a field-geologist. The Cyborg
Astrobiologist platform has thus far been used for testing and development of
these algorithms and systems: robotic acquisition of quasi-mosaics of images,
real-time image segmentation, and real-time determination of interesting points
in the image mosaics. This work is more of a test of the whole system, rather
than of any one part of the system. However, beyond the concept of the system
itself, the uncommon map (despite its simplicity) is the main innovative part
of the system. The uncommon map helps to determine interest-points in a
context-free manner. Overall, the hardware and software systems function
reliably, and the computer-vision algorithms are adequate for the first field
tests. In addition to the proof-of-concept aspect of these field tests, the
main result of these field tests is the enumeration of those issues that we can
improve in the future, including: dealing with structural shadow and
microtexture, and also, controlling the camera's zoom lens in an intelligent
manner. Nonetheless, despite these and other technical inadequacies, this
Cyborg Astrobiologist system, consisting of a camera-equipped wearable-computer
and its computer-vision algorithms, has demonstrated its ability of finding
genuinely interesting points in real-time in the geological scenery, and then
gathering more information about these interest points in an automated manner.
We use these capabilities for autonomous guidance towards geological
points-of-interest.","astro-ph,cs.AI,cs.CE,cs.CV,cs.HC,cs.RO"
"Time Series Analysis and Forecasting of COVID-19 Cases Using LSTM and ARIMA Models. Coronavirus disease 2019 (COVID-19) is a global public health crisis that has
been declared a pandemic by World Health Organization. Forecasting country-wise
COVID-19 cases is necessary to help policymakers and healthcare providers
prepare for the future. This study explores the performance of several Long
Short-Term Memory (LSTM) models and Auto-Regressive Integrated Moving Average
(ARIMA) model in forecasting the number of confirmed COVID-19 cases. Time
series of daily cumulative COVID-19 cases were used for generating 1-day,
3-day, and 5-day forecasts using several LSTM models and ARIMA. Two novel
k-period performance metrics - k-day Mean Absolute Percentage Error (kMAPE) and
k-day Median Symmetric Accuracy (kMdSA) - were developed for evaluating the
performance of the models in forecasting time series values for multiple days.
Errors in prediction using kMAPE and kMdSA for LSTM models were both as low as
0.05%, while those for ARIMA were 0.07% and 0.06% respectively. LSTM models
slightly underestimated while ARIMA slightly overestimated the numbers in the
forecasts. The performance of LSTM models is comparable to ARIMA in forecasting
COVID-19 cases. While ARIMA requires longer sequences, LSTMs can perform
reasonably well with sequence sizes as small as 3. However, LSTMs require a
large number of training samples. Further, the development of k-period
performance metrics proposed is likely to be useful for performance evaluation
of time series models in predicting multiple periods. Based on the k-period
performance metrics proposed, both LSTMs and ARIMA are useful for time series
analysis and forecasting for COVID-19.","cs.LG,stat.AP"
"TCT: A Cross-supervised Learning Method for Multimodal Sequence Representation. Multimodalities provide promising performance than unimodality in most tasks.
However, learning the semantic of the representations from multimodalities
efficiently is extremely challenging. To tackle this, we propose the
Transformer based Cross-modal Translator (TCT) to learn unimodal sequence
representations by translating from other related multimodal sequences on a
supervised learning method. Combined TCT with Multimodal Transformer Network
(MTN), we evaluate MTN-TCT on the video-grounded dialogue which uses
multimodality. The proposed method reports new state-of-the-art performance on
video-grounded dialogue which indicates representations learned by TCT are more
semantics compared to directly use unimodality.","cs.CL,cs.CV,cs.LG,cs.SD,eess.AS,stat.ML"
"Cross-Modal Progressive Comprehension for Referring Segmentation. Given a natural language expression and an image/video, the goal of referring
segmentation is to produce the pixel-level masks of the entities described by
the subject of the expression. Previous approaches tackle this problem by
implicit feature interaction and fusion between visual and linguistic
modalities in a one-stage manner. However, human tends to solve the referring
problem in a progressive manner based on informative words in the expression,
i.e., first roughly locating candidate entities and then distinguishing the
target one. In this paper, we propose a Cross-Modal Progressive Comprehension
(CMPC) scheme to effectively mimic human behaviors and implement it as a CMPC-I
(Image) module and a CMPC-V (Video) module to improve referring image and video
segmentation models. For image data, our CMPC-I module first employs entity and
attribute words to perceive all the related entities that might be considered
by the expression. Then, the relational words are adopted to highlight the
target entity as well as suppress other irrelevant ones by spatial graph
reasoning. For video data, our CMPC-V module further exploits action words
based on CMPC-I to highlight the correct entity matched with the action cues by
temporal graph reasoning. In addition to the CMPC, we also introduce a simple
yet effective Text-Guided Feature Exchange (TGFE) module to integrate the
reasoned multimodal features corresponding to different levels in the visual
backbone under the guidance of textual information. In this way, multi-level
features can communicate with each other and be mutually refined based on the
textual context. Combining CMPC-I or CMPC-V with TGFE can form our image or
video version referring segmentation frameworks and our frameworks achieve new
state-of-the-art performances on four referring image segmentation benchmarks
and three referring video segmentation benchmarks respectively.","cs.CV,cs.MM"
"Single View Depth Estimation from Examples. We describe a non-parametric, ""example-based"" method for estimating the depth
of an object, viewed in a single photo. Our method consults a database of
example 3D geometries, searching for those which look similar to the object in
the photo. The known depths of the selected database objects act as shape
priors which constrain the process of estimating the object's depth. We show
how this process can be performed by optimizing a well defined target
likelihood function, via a hard-EM procedure. We address the problem of
representing the (possibly infinite) variability of viewing conditions with a
finite (and often very small) example set, by proposing an on-the-fly example
update scheme. We further demonstrate the importance of non-stationarity in
avoiding misleading examples when estimating structured shapes. We evaluate our
method and present both qualitative as well as quantitative results for
challenging object classes. Finally, we show how this same technique may be
readily applied to a number of related problems. These include the novel task
of estimating the occluded depth of an object's backside and the task of
tailoring custom fitting image-maps for input depths.","68T45,cs.CV"
"Tensor Regression Using Low-rank and Sparse Tucker Decompositions. This paper studies a tensor-structured linear regression model with a scalar
response variable and tensor-structured predictors, such that the regression
parameters form a tensor of order $d$ (i.e., a $d$-fold multiway array) in
$\mathbb{R}^{n_1 \times n_2 \times \cdots \times n_d}$. It focuses on the task
of estimating the regression tensor from $m$ realizations of the response
variable and the predictors where $m\ll n = \prod \nolimits_{i} n_i$. Despite
the seeming ill-posedness of this problem, it can still be solved if the
parameter tensor belongs to the space of sparse, low Tucker-rank tensors.
Accordingly, the estimation procedure is posed as a non-convex optimization
program over the space of sparse, low Tucker-rank tensors, and a tensor variant
of projected gradient descent is proposed to solve the resulting non-convex
problem. In addition, mathematical guarantees are provided that establish the
proposed method linearly converges to an appropriate solution under a certain
set of conditions. Further, an upper bound on sample complexity of tensor
parameter estimation for the model under consideration is characterized for the
special case when the individual (scalar) predictors independently draw values
from a sub-Gaussian distribution. The sample complexity bound is shown to have
a polylogarithmic dependence on $\bar{n} = \max \big\{n_i: i\in \{1,2,\ldots,d
\} \big\}$ and, orderwise, it matches the bound one can obtain from a heuristic
parameter counting argument. Finally, numerical experiments demonstrate the
efficacy of the proposed tensor model and estimation method on a synthetic
dataset and a collection of neuroimaging datasets pertaining to attention
deficit hyperactivity disorder. Specifically, the proposed method exhibits
better sample complexities on both synthetic and real datasets, demonstrating
the usefulness of the model and the method in settings where $n \gg m$.","41A52,41A63,62F10,62J05,cs.LG,eess.SP,math.ST,stat.ML,stat.TH"
"Multiple Change Point Estimation in Stationary Ergodic Time Series. Given a heterogeneous time-series sample, the objective is to find points in
time (called change points) where the probability distribution generating the
data has changed. The data are assumed to have been generated by arbitrary
unknown stationary ergodic distributions. No modelling, independence or mixing
assumptions are made. A novel, computationally efficient, nonparametric method
is proposed, and is shown to be asymptotically consistent in this general
framework. The theoretical results are complemented with experimental
evaluations.","cs.IT,math.IT,math.ST,stat.ML,stat.TH"
"EvolveGCN: Evolving Graph Convolutional Networks for Dynamic Graphs. Graph representation learning resurges as a trending research subject owing
to the widespread use of deep learning for Euclidean data, which inspire
various creative designs of neural networks in the non-Euclidean domain,
particularly graphs. With the success of these graph neural networks (GNN) in
the static setting, we approach further practical scenarios where the graph
dynamically evolves. Existing approaches typically resort to node embeddings
and use a recurrent neural network (RNN, broadly speaking) to regulate the
embeddings and learn the temporal dynamics. These methods require the knowledge
of a node in the full time span (including both training and testing) and are
less applicable to the frequent change of the node set. In some extreme
scenarios, the node sets at different time steps may completely differ. To
resolve this challenge, we propose EvolveGCN, which adapts the graph
convolutional network (GCN) model along the temporal dimension without
resorting to node embeddings. The proposed approach captures the dynamism of
the graph sequence through using an RNN to evolve the GCN parameters. Two
architectures are considered for the parameter evolution. We evaluate the
proposed approach on tasks including link prediction, edge classification, and
node classification. The experimental results indicate a generally higher
performance of EvolveGCN compared with related approaches. The code is
available at \url{https://github.com/IBM/EvolveGCN}.","cs.LG,cs.SI,stat.ML"
"TD-GEN: Graph Generation With Tree Decomposition. We propose TD-GEN, a graph generation framework based on tree decomposition,
and introduce a reduced upper bound on the maximum number of decisions needed
for graph generation. The framework includes a permutation invariant tree
generation model which forms the backbone of graph generation. Tree nodes are
supernodes, each representing a cluster of nodes in the graph. Graph nodes and
edges are incrementally generated inside the clusters by traversing the tree
supernodes, respecting the structure of the tree decomposition, and following
node sharing decisions between the clusters. Finally, we discuss the
shortcomings of standard evaluation criteria based on statistical properties of
the generated graphs as performance measures. We propose to compare the
performance of models based on likelihood. Empirical results on a variety of
standard graph generation datasets demonstrate the superior performance of our
method.","cs.LG,cs.SI,stat.ML"
"Distributed Training of Graph Convolutional Networks using Subgraph Approximation. Modern machine learning techniques are successfully being adapted to data
modeled as graphs. However, many real-world graphs are typically very large and
do not fit in memory, often making the problem of training machine learning
models on them intractable. Distributed training has been successfully employed
to alleviate memory problems and speed up training in machine learning domains
in which the input data is assumed to be independently identical distributed
(i.i.d). However, distributing the training of non i.i.d data such as graphs
that are used as training inputs in Graph Convolutional Networks (GCNs) causes
accuracy problems since information is lost at the graph partitioning
boundaries.
  In this paper, we propose a training strategy that mitigates the lost
information across multiple partitions of a graph through a subgraph
approximation scheme. Our proposed approach augments each sub-graph with a
small amount of edge and vertex information that is approximated from all other
sub-graphs. The subgraph approximation approach helps the distributed training
system converge at single-machine accuracy, while keeping the memory footprint
low and minimizing synchronization overhead between the machines.","cs.DC,cs.LG"
"Hierarchical modeling of molecular energies using a deep neural network. We introduce the Hierarchically Interacting Particle Neural Network (HIP-NN)
to model molecular properties from datasets of quantum calculations. Inspired
by a many-body expansion, HIP-NN decomposes properties, such as energy, as a
sum over hierarchical terms. These terms are generated from a neural network--a
composition of many nonlinear transformations--acting on a representation of
the molecule. HIP-NN achieves state-of-the-art performance on a dataset of 131k
ground state organic molecules, and predicts energies with 0.26 kcal/mol mean
absolute error. With minimal tuning, our model is also competitive on a dataset
of molecular dynamics trajectories. In addition to enabling accurate energy
predictions, the hierarchical structure of HIP-NN helps to identify regions of
model uncertainty.","physics.chem-ph,stat.ML"
"Roundtrip: A Deep Generative Neural Density Estimator. Density estimation is a fundamental problem in both statistics and machine
learning. In this study, we proposed Roundtrip as a general-purpose neural
density estimator based on deep generative models. Roundtrip retains the
generative power of generative adversarial networks (GANs) but also provides
estimates of density values. Unlike previous neural density estimators that put
stringent conditions on the transformation from the latent space to the data
space, Roundtrip enables the use of much more general mappings. In a series of
experiments, Roundtrip achieves state-of-the-art performance in a diverse range
of density estimation tasks.","cs.LG,stat.ME,stat.ML"
"Short-Term Electricity Price Forecasting based on Graph Convolution Network and Attention Mechanism. In electricity markets, locational marginal price (LMP) forecasting is
particularly important for market participants in making reasonable bidding
strategies, managing potential trading risks, and supporting efficient system
planning and operation. Unlike existing methods that only consider LMPs'
temporal features, this paper tailors a spectral graph convolutional network
(GCN) to greatly improve the accuracy of short-term LMP forecasting. A
three-branch network structure is then designed to match the structure of LMPs'
compositions. Such kind of network can extract the spatial-temporal features of
LMPs, and provide fast and high-quality predictions for all nodes
simultaneously. The attention mechanism is also implemented to assign varying
importance weights between different nodes and time slots. Case studies based
on the IEEE-118 test system and real-world data from the PJM validate that the
proposed model outperforms existing forecasting models in accuracy, and
maintains a robust performance by avoiding extreme errors.","cs.LG,cs.SY,eess.SY"
"Model Uncertainty and Correctability for Directed Graphical Models. Probabilistic graphical models are a fundamental tool in probabilistic
modeling, machine learning and artificial intelligence. They allow us to
integrate in a natural way expert knowledge, physical modeling, heterogeneous
and correlated data and quantities of interest. For exactly this reason,
multiple sources of model uncertainty are inherent within the modular structure
of the graphical model. In this paper we develop information-theoretic, robust
uncertainty quantification methods and non-parametric stress tests for directed
graphical models to assess the effect and the propagation through the graph of
multi-sourced model uncertainties to quantities of interest. These methods
allow us to rank the different sources of uncertainty and correct the graphical
model by targeting its most impactful components with respect to the quantities
of interest. Thus, from a machine learning perspective, we provide a
mathematically rigorous approach to correctability that guarantees a systematic
selection for improvement of components of a graphical model while controlling
potential new errors created in the process in other parts of the model. We
demonstrate our methods in two physico-chemical examples, namely quantum
scale-informed chemical kinetics and materials screening to improve the
efficiency of fuel cells.","62H22,62P30,68T37,80A30,93B35,94A17,cs.IT,cs.LG,math.IT,math.PR,stat.ML"
"Manifold Based Low-rank Regularization for Image Restoration and Semi-supervised Learning. Low-rank structures play important role in recent advances of many problems
in image science and data science. As a natural extension of low-rank
structures for data with nonlinear structures, the concept of the
low-dimensional manifold structure has been considered in many data processing
problems. Inspired by this concept, we consider a manifold based low-rank
regularization as a linear approximation of manifold dimension. This
regularization is less restricted than the global low-rank regularization, and
thus enjoy more flexibility to handle data with nonlinear structures. As
applications, we demonstrate the proposed regularization to classical inverse
problems in image sciences and data sciences including image inpainting, image
super-resolution, X-ray computer tomography (CT) image reconstruction and
semi-supervised learning. We conduct intensive numerical experiments in several
image restoration problems and a semi-supervised learning problem of
classifying handwritten digits using the MINST data. Our numerical tests
demonstrate the effectiveness of the proposed methods and illustrate that the
new regularization methods produce outstanding results by comparing with many
existing methods.","65D18,65J22,68Q32,68U10,cs.CV,math.NA"
"Global Convergence of the ODE Limit for Online Actor-Critic Algorithms in Reinforcement Learning. Actor-critic algorithms are widely used in reinforcement learning, but are
challenging to mathematically analyze due to the online arrival of non-i.i.d.
data samples. The distribution of the data samples dynamically changes as the
model is updated, introducing a complex feedback loop between the data
distribution and the reinforcement learning algorithm. We prove that, under a
time rescaling, the online actor-critic algorithm with tabular parametrization
converges to an ordinary differential equations (ODEs) as the number of updates
becomes large. The proof first establishes the geometric ergodicity of the data
samples under a fixed actor policy. Then, using a Poisson equation, we prove
that the fluctuations of the data samples around a dynamic probability measure,
which is a function of the evolving actor model, vanish as the number of
updates become large. Once the ODE limit has been derived, we study its
convergence properties using a two time-scale analysis which asymptotically
de-couples the critic ODE from the actor ODE. The convergence of the critic to
the solution of the Bellman equation and the actor to the optimal policy are
proven. In addition, a convergence rate to this global minimum is also
established. Our convergence analysis holds under specific choices for the
learning rates and exploration rates in the actor-critic algorithm, which could
provide guidance for the implementation of actor-critic algorithms in practice.","cs.LG,math.OC,stat.ML"
"YOLO Nano: a Highly Compact You Only Look Once Convolutional Neural Network for Object Detection. Object detection remains an active area of research in the field of computer
vision, and considerable advances and successes has been achieved in this area
through the design of deep convolutional neural networks for tackling object
detection. Despite these successes, one of the biggest challenges to widespread
deployment of such object detection networks on edge and mobile scenarios is
the high computational and memory requirements. As such, there has been growing
research interest in the design of efficient deep neural network architectures
catered for edge and mobile usage. In this study, we introduce YOLO Nano, a
highly compact deep convolutional neural network for the task of object
detection. A human-machine collaborative design strategy is leveraged to create
YOLO Nano, where principled network design prototyping, based on design
principles from the YOLO family of single-shot object detection network
architectures, is coupled with machine-driven design exploration to create a
compact network with highly customized module-level macroarchitecture and
microarchitecture designs tailored for the task of embedded object detection.
The proposed YOLO Nano possesses a model size of ~4.0MB (>15.1x and >8.3x
smaller than Tiny YOLOv2 and Tiny YOLOv3, respectively) and requires 4.57B
operations for inference (>34% and ~17% lower than Tiny YOLOv2 and Tiny YOLOv3,
respectively) while still achieving an mAP of ~69.1% on the VOC 2007 dataset
(~12% and ~10.7% higher than Tiny YOLOv2 and Tiny YOLOv3, respectively).
Experiments on inference speed and power efficiency on a Jetson AGX Xavier
embedded module at different power budgets further demonstrate the efficacy of
YOLO Nano for embedded scenarios.","cs.CV,cs.NE"
"Graph Context Encoder: Graph Feature Inpainting for Graph Generation and Self-supervised Pretraining. We propose the Graph Context Encoder (GCE), a simple but efficient approach
for graph representation learning based on graph feature masking and
reconstruction.
  GCE models are trained to efficiently reconstruct input graphs similarly to a
graph autoencoder where node and edge labels are masked. In particular, our
model is also allowed to change graph structures by masking and reconstructing
graphs augmented by random pseudo-edges.
  We show that GCE can be used for novel graph generation, with applications
for molecule generation. Used as a pretraining method, we also show that GCE
improves baseline performances in supervised classification tasks tested on
multiple standard benchmark graph datasets.","68T07,cs.LG"
"Model-free Reinforcement Learning for Branching Markov Decision Processes. We study reinforcement learning for the optimal control of Branching Markov
Decision Processes (BMDPs), a natural extension of (multitype) Branching Markov
Chains (BMCs). The state of a (discrete-time) BMCs is a collection of entities
of various types that, while spawning other entities, generate a payoff. In
comparison with BMCs, where the evolution of a each entity of the same type
follows the same probabilistic pattern, BMDPs allow an external controller to
pick from a range of options. This permits us to study the best/worst behaviour
of the system. We generalise model-free reinforcement learning techniques to
compute an optimal control strategy of an unknown BMDP in the limit. We present
results of an implementation that demonstrate the practicality of the approach.","cs.LG,cs.LO,cs.SY,eess.SY"
"Practical Processing of Mobile Sensor Data for Continual Deep Learning Predictions. We present a practical approach for processing mobile sensor time series data
for continual deep learning predictions. The approach comprises data cleaning,
normalization, capping, time-based compression, and finally classification with
a recurrent neural network. We demonstrate the effectiveness of the approach in
a case study with 279 participants. On the basis of sparse sensor events, the
network continually predicts whether the participants would attend to a
notification within 10 minutes. Compared to a random baseline, the classifier
achieves a 40% performance increase (AUC of 0.702) on a withheld test set. This
approach allows to forgo resource-intensive, domain-specific, error-prone
feature engineering, which may drastically increase the applicability of
machine learning to mobile phone sensor data.","cs.HC,cs.LG"
"IGNNITION: Bridging the Gap Between Graph Neural Networks and Networking Systems. Recent years have seen the vast potential of Graph Neural Networks (GNN) in
many fields where data is structured as graphs (e.g., chemistry, recommender
systems). In particular, GNNs are becoming increasingly popular in the field of
networking, as graphs are intrinsically present at many levels (e.g., topology,
routing). The main novelty of GNNs is their ability to generalize to other
networks unseen during training, which is an essential feature for developing
practical Machine Learning (ML) solutions for networking. However, implementing
a functional GNN prototype is currently a cumbersome task that requires strong
skills in neural network programming. This poses an important barrier to
network engineers that often do not have the necessary ML expertise. In this
article, we present IGNNITION, a novel open-source framework that enables fast
prototyping of GNNs for networking systems. IGNNITION is based on an intuitive
high-level abstraction that hides the complexity behind GNNs, while still
offering great flexibility to build custom GNN architectures. To showcase the
versatility and performance of this framework, we implement two
state-of-the-art GNN models applied to different networking use cases. Our
results show that the GNN models produced by IGNNITION are equivalent in terms
of accuracy and performance to their native implementations in TensorFlow.","cs.AI,cs.LG,cs.NI,eess.SP"
"Delegative Reinforcement Learning: learning to avoid traps with a little help. Most known regret bounds for reinforcement learning are either episodic or
assume an environment without traps. We derive a regret bound without making
either assumption, by allowing the algorithm to occasionally delegate an action
to an external advisor. We thus arrive at a setting of active one-shot
model-based reinforcement learning that we call DRL (delegative reinforcement
learning.) The algorithm we construct in order to demonstrate the regret bound
is a variant of Posterior Sampling Reinforcement Learning supplemented by a
subroutine that decides which actions should be delegated. The algorithm is not
anytime, since the parameters must be adjusted according to the target time
discount. Currently, our analysis is limited to Markov decision processes with
finite numbers of hypotheses, states and actions.","68Q32,I.2.6,cs.LG,stat.ML"
"The Pursuit of Algorithmic Fairness: On ""Correcting"" Algorithmic Unfairness in a Child Welfare Reunification Success Classifier. The algorithmic fairness of predictive analytic tools in the public sector
has increasingly become a topic of rigorous exploration. While instruments
pertaining to criminal recidivism and academic admissions, for example, have
garnered much attention, the predictive instruments of Child Welfare
jurisdictions have received considerably less attention. This is in part
because comparatively few such instruments exist and because even fewer have
been scrutinized through the lens of algorithmic fairness. In this work, we
seek to address both of these gaps. To this end, a novel classification
algorithm for predicting reunification success within Oregon Child Welfare is
presented, including all of the relevant details associated with building such
an instrument. The purpose of this tool is to maximize the number of stable
reunifications and identify potentially unstable reunifications which may
require additional resources and scrutiny. Additionally, because the
algorithmic fairness of the resulting tool, if left unaltered, is
unquestionably lacking, the utilized procedure for mitigating such unfairness
is presented, along with the rationale behind each difficult and unavoidable
choice. This procedure, though similar to other post-processing group-specific
thresholding methods, is novel in its use of a penalized optimizer and
contextually requisite subsampling. These novel methodological components yield
a rich and informative empirical understanding of the trade-off continuum
between fairness and accuracy. As the developed procedure is generalizable
across a variety of group-level definitions of algorithmic fairness, as well as
across an arbitrary number of protected attribute levels and risk thresholds,
the approach is broadly applicable both within and beyond Child Welfare.","cs.LG,stat.CO"
"Constructing fast approximate eigenspaces with application to the fast graph Fourier transforms. We investigate numerically efficient approximations of eigenspaces associated
to symmetric and general matrices. The eigenspaces are factored into a fixed
number of fundamental components that can be efficiently manipulated (we
consider extended orthogonal Givens or scaling and shear transformations). The
number of these components controls the trade-off between approximation
accuracy and the computational complexity of projecting on the eigenspaces. We
write minimization problems for the single fundamental components and provide
closed-form solutions. Then we propose algorithms that iterative update all
these components until convergence. We show results on random matrices and an
application on the approximation of graph Fourier transforms for directed and
undirected graphs.","cs.LG,cs.NA,eess.SP,math.NA,stat.ML"
"Bio-inspired Unsupervised Learning of Visual Features Leads to Robust Invariant Object Recognition. Retinal image of surrounding objects varies tremendously due to the changes
in position, size, pose, illumination condition, background context, occlusion,
noise, and nonrigid deformations. But despite these huge variations, our visual
system is able to invariantly recognize any object in just a fraction of a
second. To date, various computational models have been proposed to mimic the
hierarchical processing of the ventral visual pathway, with limited success.
Here, we show that the association of both biologically inspired network
architecture and learning rule significantly improves the models' performance
when facing challenging invariant object recognition problems. Our model is an
asynchronous feedforward spiking neural network. When the network is presented
with natural images, the neurons in the entry layers detect edges, and the most
activated ones fire first, while neurons in higher layers are equipped with
spike timing-dependent plasticity. These neurons progressively become selective
to intermediate complexity visual features appropriate for object
categorization. The model is evaluated on 3D-Object and ETH-80 datasets which
are two benchmarks for invariant object recognition, and is shown to outperform
state-of-the-art models, including DeepConvNet and HMAX. This demonstrates its
ability to accurately recognize different instances of multiple object classes
even under various appearance conditions (different views, scales, tilts, and
backgrounds). Several statistical analysis techniques are used to show that our
model extracts class specific and highly informative features.","cs.CV,q-bio.NC"
"Learning Particle Physics by Example: Location-Aware Generative Adversarial Networks for Physics Synthesis. We provide a bridge between generative modeling in the Machine Learning
community and simulated physical processes in High Energy Particle Physics by
applying a novel Generative Adversarial Network (GAN) architecture to the
production of jet images -- 2D representations of energy depositions from
particles interacting with a calorimeter. We propose a simple architecture, the
Location-Aware Generative Adversarial Network, that learns to produce realistic
radiation patterns from simulated high energy particle collisions. The pixel
intensities of GAN-generated images faithfully span over many orders of
magnitude and exhibit the desired low-dimensional physical properties (i.e.,
jet mass, n-subjettiness, etc.). We shed light on limitations, and provide a
novel empirical validation of image quality and validity of GAN-produced
simulations of the natural world. This work provides a base for further
explorations of GANs for use in faster simulation in High Energy Particle
Physics.","hep-ex,physics.data-an,stat.ML"
"Leaf Classification Using Shape, Color, and Texture Features. Several methods to identify plants have been proposed by several researchers.
Commonly, the methods did not capture color information, because color was not
recognized as an important aspect to the identification. In this research,
shape and vein, color, and texture features were incorporated to classify a
leaf. In this case, a neural network called Probabilistic Neural network (PNN)
was used as a classifier. The experimental result shows that the method for
classification gives average accuracy of 93.75% when it was tested on Flavia
dataset, that contains 32 kinds of plant leaves. It means that the method gives
better performance compared to the original work.","cs.CV,cs.CY"
"One-pass Stochastic Gradient Descent in Overparametrized Two-layer Neural Networks. There has been a recent surge of interest in understanding the convergence of
gradient descent (GD) and stochastic gradient descent (SGD) in
overparameterized neural networks. Most previous works assume that the training
data is provided a priori in a batch, while less attention has been paid to the
important setting where the training data arrives in a stream. In this paper,
we study the streaming data setup and show that with overparamterization and
random initialization, the prediction error of two-layer neural networks under
one-pass SGD converges in expectation. The convergence rate depends on the
eigen-decomposition of the integral operator associated with the so-called
neural tangent kernel (NTK). A key step of our analysis is to show a random
kernel function converges to the NTK with high probability using the VC
dimension and McDiarmid's inequality.","cs.LG,math.OC,stat.ML"
"3D Human Pose Estimation with Spatial and Temporal Transformers. Transformer architectures have become the model of choice in natural language
processing and are now being introduced into computer vision tasks such as
image classification, object detection, and semantic segmentation. However, in
the field of human pose estimation, convolutional architectures still remain
dominant. In this work, we present PoseFormer, a purely transformer-based
approach for 3D human pose estimation in videos without convolutional
architectures involved. Inspired by recent developments in vision transformers,
we design a spatial-temporal transformer structure to comprehensively model the
human joint relations within each frame as well as the temporal correlations
across frames, then output an accurate 3D human pose of the center frame. We
quantitatively and qualitatively evaluate our method on two popular and
standard benchmark datasets: Human3.6M and MPI-INF-3DHP. Extensive experiments
show that PoseFormer achieves state-of-the-art performance on both datasets.
Code is available at \url{https://github.com/zczcwh/PoseFormer}","cs.AI,cs.CV,cs.HC"
"Robustness via Deep Low-Rank Representations. We investigate the effect of the dimensionality of the representations
learned in Deep Neural Networks (DNNs) on their robustness to input
perturbations, both adversarial and random. To achieve low dimensionality of
learned representations, we propose an easy-to-use, end-to-end trainable,
low-rank regularizer (LR) that can be applied to any intermediate layer
representation of a DNN. This regularizer forces the feature representations to
(mostly) lie in a low-dimensional linear subspace. We perform a wide range of
experiments that demonstrate that the LR indeed induces low rank on the
representations, while providing modest improvements to accuracy as an added
benefit. Furthermore, the learned features make the trained model significantly
more robust to input perturbations such as Gaussian and adversarial noise (even
without adversarial training). Lastly, the low-dimensionality means that the
learned features are highly compressible; thus discriminative features of the
data can be stored using very little memory. Our experiments indicate that
models trained using the LR learn robust classifiers by discovering subspaces
that avoid non-robust features. Algorithmically, the LR is scalable, generic,
and straightforward to implement into existing deep learning frameworks.","cs.AI,cs.LG,stat.ML"
"Inverse design of 3d molecular structures with conditional generative neural networks. The rational design of molecules with desired properties is a long-standing
challenge in chemistry. Generative neural networks have emerged as a powerful
approach to sample novel molecules from a learned distribution. Here, we
propose a conditional generative neural network for 3d molecular structures
with specified structural and chemical properties. This approach is agnostic to
chemical bonding and enables targeted sampling of novel molecules from
conditional distributions, even in domains where reference calculations are
sparse. We demonstrate the utility of our method for inverse design by
generating molecules with specified composition or motifs, discovering
particularly stable molecules, and jointly targeting multiple electronic
properties beyond the training regime.","cs.LG,physics.chem-ph,physics.comp-ph,stat.ML"
"Reinforced Molecular Optimization with Neighborhood-Controlled Grammars. A major challenge in the pharmaceutical industry is to design novel molecules
with specific desired properties, especially when the property evaluation is
costly. Here, we propose MNCE-RL, a graph convolutional policy network for
molecular optimization with molecular neighborhood-controlled embedding
grammars through reinforcement learning. We extend the original
neighborhood-controlled embedding grammars to make them applicable to molecular
graph generation and design an efficient algorithm to infer grammatical
production rules from given molecules. The use of grammars guarantees the
validity of the generated molecular structures. By transforming molecular
graphs to parse trees with the inferred grammars, the molecular structure
generation task is modeled as a Markov decision process where a policy gradient
strategy is utilized. In a series of experiments, we demonstrate that our
approach achieves state-of-the-art performance in a diverse range of molecular
optimization tasks and exhibits significant superiority in optimizing molecular
properties with a limited number of property evaluations.","cs.LG,q-bio.BM"
"Transformer Based Reinforcement Learning For Games. Recent times have witnessed sharp improvements in reinforcement learning
tasks using deep reinforcement learning techniques like Deep Q Networks, Policy
Gradients, Actor Critic methods which are based on deep learning based models
and back-propagation of gradients to train such models. An active area of
research in reinforcement learning is about training agents to play complex
video games, which so far has been something accomplished only by human
intelligence. Some state of the art performances in video game playing using
deep reinforcement learning are obtained by processing the sequence of frames
from video games, passing them through a convolutional network to obtain
features and then using recurrent neural networks to figure out the action
leading to optimal rewards. The recurrent neural network will learn to extract
the meaningful signal out of the sequence of such features. In this work, we
propose a method utilizing a transformer network which have recently replaced
RNNs in Natural Language Processing (NLP), and perform experiments to compare
with existing methods.","cs.LG,cs.NE"
"ACERAC: Efficient reinforcement learning in fine time discretization. We propose a framework for reinforcement learning (RL) in fine time
discretization and a learning algorithm in this framework. One of the main
goals of RL is to provide a way for physical machines to learn optimal behavior
instead of being programmed. However, the machines are usually controlled in
fine time discretization. The most common RL methods apply independent random
elements to each action, which is not suitable in that setting. It is not
feasible because it causes the controlled system to jerk, and does not ensure
sufficient exploration since a single action is not long enough to create a
significant experience that could be translated into policy improvement. In the
RL framework introduced in this paper, policies are considered that produce
actions based on states and random elements autocorrelated in subsequent time
instants. The RL algorithm introduced here approximately optimizes such a
policy. The efficiency of this algorithm is verified against three other RL
methods (PPO, SAC, ACER) in four simulated learning control problems (Ant,
HalfCheetah, Hopper, and Walker2D) in diverse time discretization. The
algorithm introduced here outperforms the competitors in most cases considered.","I.2.6,cs.LG"
"Hypernetwork Dismantling via Deep Reinforcement Learning. Network dismantling aims to degrade the connectivity of a network by removing
an optimal set of nodes and has been widely adopted in many real-world
applications such as epidemic control and rumor containment. However,
conventional methods usually focus on simple network modeling with only
pairwise interactions, while group-wise interactions modeled by hypernetwork
are ubiquitous and critical. In this work, we formulate the hypernetwork
dismantling problem as a node sequence decision problem and propose a deep
reinforcement learning (DRL)-based hypernetwork dismantling framework. Besides,
we design a novel inductive hypernetwork embedding method to ensure the
transferability to various real-world hypernetworks. Generally, our framework
builds an agent. It first generates small-scale synthetic hypernetworks and
embeds the nodes and hypernetworks into a low dimensional vector space to
represent the action and state space in DRL, respectively. Then trial-and-error
dismantling tasks are conducted by the agent on these synthetic hypernetworks,
and the dismantling strategy is continuously optimized. Finally, the
well-optimized strategy is applied to real-world hypernetwork dismantling
tasks. Experimental results on five real-world hypernetworks demonstrate the
effectiveness of our proposed framework.","cs.LG,cs.SY,eess.SY"
"EnsNet: Ensconce Text in the Wild. A new method is proposed for removing text from natural images. The challenge
is to first accurately localize text on the stroke-level and then replace it
with a visually plausible background. Unlike previous methods that require
image patches to erase scene text, our method, namely ensconce network
(EnsNet), can operate end-to-end on a single image without any prior knowledge.
The overall structure is an end-to-end trainable FCN-ResNet-18 network with a
conditional generative adversarial network (cGAN). The feature of the former is
first enhanced by a novel lateral connection structure and then refined by four
carefully designed losses: multiscale regression loss and content loss, which
capture the global discrepancy of different level features; texture loss and
total variation loss, which primarily target filling the text region and
preserving the reality of the background. The latter is a novel local-sensitive
GAN, which attentively assesses the local consistency of the text erased
regions. Both qualitative and quantitative sensitivity experiments on synthetic
images and the ICDAR 2013 dataset demonstrate that each component of the EnsNet
is essential to achieve a good performance. Moreover, our EnsNet can
significantly outperform previous state-of-the-art methods in terms of all
metrics. In addition, a qualitative experiment conducted on the SMBNet dataset
further demonstrates that the proposed method can also preform well on general
object (such as pedestrians) removal tasks. EnsNet is extremely fast, which can
preform at 333 fps on an i5-8600 CPU device.",cs.CV
"A low-rank representation for unsupervised registration of medical images. Registration networks have shown great application potentials in medical
image analysis. However, supervised training methods have a great demand for
large and high-quality labeled datasets, which is time-consuming and sometimes
impractical due to data sharing issues. Unsupervised image registration
algorithms commonly employ intensity-based similarity measures as loss
functions without any manual annotations. These methods estimate the
parameterized transformations between pairs of moving and fixed images through
the optimization of the network parameters during training. However, these
methods become less effective when the image quality varies, e.g., some images
are corrupted by substantial noise or artifacts. In this work, we propose a
novel approach based on a low-rank representation, i.e., Regnet-LRR, to tackle
the problem. We project noisy images into a noise-free low-rank space, and then
compute the similarity between the images. Based on the low-rank similarity
measure, we train the registration network to predict the dense deformation
fields of noisy image pairs. We highlight that the low-rank projection is
reformulated in a way that the registration network can successfully update
gradients. With two tasks, i.e., cardiac and abdominal intra-modality
registration, we demonstrate that the low-rank representation can boost the
generalization ability and robustness of models as well as bring significant
improvements in noisy data registration scenarios.",cs.CV
"SOLO: Search Online, Learn Offline for Combinatorial Optimization Problems. We study combinatorial problems with real world applications such as machine
scheduling, routing, and assignment. We propose a method that combines
Reinforcement Learning (RL) and planning. This method can equally be applied to
both the offline, as well as online, variants of the combinatorial problem, in
which the problem components (e.g., jobs in scheduling problems) are not known
in advance, but rather arrive during the decision-making process. Our solution
is quite generic, scalable, and leverages distributional knowledge of the
problem parameters. We frame the solution process as an MDP, and take a Deep
Q-Learning approach wherein states are represented as graphs, thereby allowing
our trained policies to deal with arbitrary changes in a principled manner.
Though learned policies work well in expectation, small deviations can have
substantial negative effects in combinatorial settings. We mitigate these
drawbacks by employing our graph-convolutional policies as non-optimal
heuristics in a compatible search algorithm, Monte Carlo Tree Search, to
significantly improve overall performance. We demonstrate our method on two
problems: Machine Scheduling and Capacitated Vehicle Routing. We show that our
method outperforms custom-tailored mathematical solvers, state of the art
learning-based algorithms, and common heuristics, both in computation time and
performance.","cs.LG,math.OC"
"On the interpretation of linear Riemannian tangent space model parameters in M/EEG. Riemannian tangent space methods offer state-of-the-art performance in
magnetoencephalography (MEG) and electroencephalography (EEG) based
applications such as brain-computer interfaces and biomarker development. One
limitation, particularly relevant for biomarker development, is limited model
interpretability compared to established component-based methods. Here, we
propose a method to transform the parameters of linear tangent space models
into interpretable patterns. Using typical assumptions, we show that this
approach identifies the true patterns of latent sources, encoding a target
signal. In simulations and two real MEG and EEG datasets, we demonstrate the
validity of the proposed approach and investigate its behavior when the model
assumptions are violated. Our results confirm that Riemannian tangent space
methods are robust to differences in the source patterns across observations.
We found that this robustness property also transfers to the associated
patterns.","cs.LG,eess.SP"
"Meta-Learning for Koopman Spectral Analysis with Short Time-series. Koopman spectral analysis has attracted attention for nonlinear dynamical
systems since we can analyze nonlinear dynamics with a linear regime by
embedding data into a Koopman space by a nonlinear function. For the analysis,
we need to find appropriate embedding functions. Although several neural
network-based methods have been proposed for learning embedding functions,
existing methods require long time-series for training neural networks. This
limitation prohibits performing Koopman spectral analysis in applications where
only short time-series are available. In this paper, we propose a meta-learning
method for estimating embedding functions from unseen short time-series by
exploiting knowledge learned from related but different time-series. With the
proposed method, a representation of a given short time-series is obtained by a
bidirectional LSTM for extracting its properties. The embedding function of the
short time-series is modeled by a neural network that depends on the
time-series representation. By sharing the LSTM and neural networks across
multiple time-series, we can learn common knowledge from different time-series
while modeling time-series-specific embedding functions with the time-series
representation. Our model is trained such that the expected test prediction
error is minimized with the episodic training framework. We experimentally
demonstrate that the proposed method achieves better performance in terms of
eigenvalue estimation and future prediction than existing methods.","cs.LG,math.DS,stat.ML"
"Recovering Facial Reflectance and Geometry from Multi-view Images. While the problem of estimating shapes and diffuse reflectances of human
faces from images has been extensively studied, there is relatively less work
done on recovering the specular albedo. This paper presents a lightweight
solution for inferring photorealistic facial reflectance and geometry. Our
system processes video streams from two views of a subject, and outputs two
reflectance maps for diffuse and specular albedos, as well as a vector map of
surface normals. A model-based optimization approach is used, consisting of the
three stages of multi-view face model fitting, facial reflectance inference and
facial geometry refinement. Our approach is based on a novel formulation built
upon the 3D morphable model (3DMM) for representing 3D textured faces in
conjunction with the Blinn-Phong reflection model. It has the advantage of
requiring only a simple setup with two video streams, and is able to exploit
the interaction between the diffuse and specular reflections across multiple
views as well as time frames. As a result, the method is able to reliably
recover high-fidelity facial reflectance and geometry, which facilitates
various applications such as generating photorealistic facial images under new
viewpoints or illumination conditions.","cs.CV,cs.GR"
"Combinatorial Optimization with Physics-Inspired Graph Neural Networks. We demonstrate how graph neural networks can be used to solve combinatorial
optimization problems. Our approach is broadly applicable to canonical NP-hard
problems in the form of quadratic unconstrained binary optimization problems,
such as maximum cut, minimum vertex cover, maximum independent set, as well as
Ising spin glasses and higher-order generalizations thereof in the form of
polynomial unconstrained binary optimization problems. We apply a relaxation
strategy to the problem Hamiltonian to generate a differentiable loss function
with which we train the graph neural network and apply a simple projection to
integer variables once the unsupervised training process has completed. We
showcase our approach with numerical results for the canonical maximum cut and
maximum independent set problems. We find that the graph neural network
optimizer performs on par or outperforms existing solvers, with the ability to
scale beyond the state of the art to problems with millions of variables.","cond-mat.dis-nn,cs.AI,cs.LG,math.OC,quant-ph"
"Remote sensing of forests using discrete return airborne LiDAR. Airborne discrete return light detection and ranging (LiDAR) point clouds
covering forested areas can be processed to segment individual trees and
retrieve their morphological attributes. Segmenting individual trees in natural
deciduous forests however remained a challenge because of the complex and
multi-layered canopy. In this chapter, we present (i) a robust segmentation
method that avoids a priori assumptions about the canopy structure, (ii) a
vertical canopy stratification procedure that improves segmentation of
understory trees, (iii) an occlusion model for estimating the point density of
each canopy stratum, and (iv) a distributed computing approach for efficient
processing at the forest level. When applied to the University of Kentucky
Robinson Forest, the segmentation method detected about 90% of overstory and
47% of understory trees with over-segmentation rates of 14% and 2%. Stratifying
the canopy improved the detection rate of understory trees to 68% at the cost
of increasing their over-segmentations to 16%. According to our occlusion
model, a point density of ~170 pt/m-sqr is needed to segment understory trees
as accurately as overstory trees. Lastly, using the distributed approach, we
segmented about two million trees in the 7,440-ha forest in 2.5 hours using 192
processors, which is 167 times faster than using a single processor. Keywords:
individual tree segmentation, multi-layered stand, vertical canopy
stratification, segmentation evaluation, point density, canopy occlusion
effect, big data, distributed computing.","cs.CE,cs.CV,cs.DC"
"Beating the World's Best at Super Smash Bros. with Deep Reinforcement Learning. There has been a recent explosion in the capabilities of game-playing
artificial intelligence. Many classes of RL tasks, from Atari games to motor
control to board games, are now solvable by fairly generic algorithms, based on
deep learning, that learn to play from experience with minimal knowledge of the
specific domain of interest. In this work, we will investigate the performance
of these methods on Super Smash Bros. Melee (SSBM), a popular console fighting
game. The SSBM environment has complex dynamics and partial observability,
making it challenging for human and machine alike. The multi-player aspect
poses an additional challenge, as the vast majority of recent advances in RL
have focused on single-agent environments. Nonetheless, we will show that it is
possible to train agents that are competitive against and even surpass human
professionals, a new result for the multi-player video game setting.","I.2.6,cs.AI,cs.LG"
"Segmentation Rectification for Video Cutout via One-Class Structured Learning. Recent works on interactive video object cutout mainly focus on designing
dynamic foreground-background (FB) classifiers for segmentation propagation.
However, the research on optimally removing errors from the FB classification
is sparse, and the errors often accumulate rapidly, causing significant errors
in the propagated frames. In this work, we take the initial steps to addressing
this problem, and we call this new task \emph{segmentation rectification}. Our
key observation is that the possibly asymmetrically distributed false positive
and false negative errors were handled equally in the conventional methods. We,
alternatively, propose to optimally remove these two types of errors. To this
effect, we propose a novel bilayer Markov Random Field (MRF) model for this new
task. We also adopt the well-established structured learning framework to learn
the optimal model from data. Additionally, we propose a novel one-class
structured SVM (OSSVM) which greatly speeds up the structured learning process.
Our method naturally extends to RGB-D videos as well. Comprehensive experiments
on both RGB and RGB-D data demonstrate that our simple and effective method
significantly outperforms the segmentation propagation methods adopted in the
state-of-the-art video cutout systems, and the results also suggest the
potential usefulness of our method in image cutout system.","cs.CV,cs.GR,cs.LG"
"A Queueing-Theoretic Framework for Vehicle Dispatching in Dynamic Car-Hailing [technical report]. With the rapid development of smart mobile devices, the car-hailing platforms
(e.g., Uber or Lyft) have attracted much attention from both the academia and
the industry. In this paper, we consider an important dynamic car-hailing
problem, namely \textit{maximum revenue vehicle dispatching} (MRVD), in which
rider requests dynamically arrive and drivers need to serve as many riders as
possible such that the entire revenue of the platform is maximized. We prove
that the MRVD problem is NP-hard and intractable. In addition, the dynamic
car-hailing platforms have no information of the future riders, which makes the
problem even harder. To handle the MRVD problem, we propose a queueing-based
vehicle dispatching framework, which first uses existing machine learning
algorithms to predict the future vehicle demand of each region, then estimates
the idle time periods of drivers through a queueing model for each region. With
the information of the predicted vehicle demands and estimated idle time
periods of drivers, we propose two batch-based vehicle dispatching algorithms
to efficiently assign suitable drivers to riders such that the expected overall
revenue of the platform is maximized during each batch processing. Through
extensive experiments, we demonstrate the efficiency and effectiveness of our
proposed approaches over both real and synthetic datasets.","cs.DB,cs.LG"
"Federated Learning with Communication Delay in Edge Networks. Federated learning has received significant attention as a potential solution
for distributing machine learning (ML) model training through edge networks.
This work addresses an important consideration of federated learning at the
network edge: communication delays between the edge nodes and the aggregator. A
technique called FedDelAvg (federated delayed averaging) is developed, which
generalizes the standard federated averaging algorithm to incorporate a
weighting between the current local model and the delayed global model received
at each device during the synchronization step. Through theoretical analysis,
an upper bound is derived on the global model loss achieved by FedDelAvg, which
reveals a strong dependency of learning performance on the values of the
weighting and learning rate. Experimental results on a popular ML task indicate
significant improvements in terms of convergence speed when optimizing the
weighting scheme to account for delays.","cs.DC,cs.LG,stat.ML"
"Molecular Mechanics-Driven Graph Neural Network with Multiplex Graph for Molecular Structures. The prediction of physicochemical properties from molecular structures is a
crucial task for artificial intelligence aided molecular design. A growing
number of Graph Neural Networks (GNNs) have been proposed to address this
challenge. These models improve their expressive power by incorporating
auxiliary information in molecules while inevitably increase their
computational complexity. In this work, we aim to design a GNN which is both
powerful and efficient for molecule structures. To achieve such goal, we
propose a molecular mechanics-driven approach by first representing each
molecule as a two-layer multiplex graph, where one layer contains only local
connections that mainly capture the covalent interactions and another layer
contains global connections that can simulate non-covalent interactions. Then
for each layer, a corresponding message passing module is proposed to balance
the trade-off of expression power and computational complexity. Based on these
two modules, we build Multiplex Molecular Graph Neural Network (MXMNet). When
validated by the QM9 dataset for small molecules and PDBBind dataset for large
protein-ligand complexes, MXMNet achieves superior results to the existing
state-of-the-art models under restricted resources.","cs.LG,physics.comp-ph,q-bio.QM"
"RAIN: Reinforced Hybrid Attention Inference Network for Motion Forecasting. Motion forecasting plays a significant role in various domains (e.g.,
autonomous driving, human-robot interaction), which aims to predict future
motion sequences given a set of historical observations. However, the observed
elements may be of different levels of importance. Some information may be
irrelevant or even distracting to the forecasting in certain situations. To
address this issue, we propose a generic motion forecasting framework (named
RAIN) with dynamic key information selection and ranking based on a hybrid
attention mechanism. The general framework is instantiated to handle
multi-agent trajectory prediction and human motion forecasting tasks,
respectively. In the former task, the model learns to recognize the relations
between agents with a graph representation and to determine their relative
significance. In the latter task, the model learns to capture the temporal
proximity and dependency in long-term human motions. We also propose an
effective double-stage training pipeline with an alternating training strategy
to optimize the parameters in different modules of the framework. We validate
the framework on both synthetic simulations and motion forecasting benchmarks
in different domains, demonstrating that our method not only achieves
state-of-the-art forecasting performance, but also provides interpretable and
reasonable hybrid attention weights.","cs.AI,cs.CV,cs.LG,cs.MA,cs.RO"
"Integrating Additional Knowledge Into Estimation of Graphical Models. In applications of graphical models, we typically have more information than
just the samples themselves. A prime example is the estimation of brain
connectivity networks based on fMRI data, where in addition to the samples
themselves, the spatial positions of the measurements are readily available.
With particular regard for this application, we are thus interested in ways to
incorporate additional knowledge most effectively into graph estimation. Our
approach to this is to make neighborhood selection receptive to additional
knowledge by strengthening the role of the tuning parameters. We demonstrate
that this concept (i) can improve reproducibility, (ii) is computationally
convenient and efficient, and (iii) carries a lucid Bayesian interpretation. We
specifically show that the approach provides effective estimations of brain
connectivity graphs from fMRI data. However, providing a general scheme for the
inclusion of additional knowledge, our concept is expected to have applications
in a wide range of domains.","stat.AP,stat.ME,stat.ML"
"Dynamic Game Theoretic Neural Optimizer. The connection between training deep neural networks (DNNs) and optimal
control theory (OCT) has attracted considerable attention as a principled tool
of algorithmic design. Despite few attempts being made, they have been limited
to architectures where the layer propagation resembles a Markovian dynamical
system. This casts doubts on their flexibility to modern networks that heavily
rely on non-Markovian dependencies between layers (e.g. skip connections in
residual networks). In this work, we propose a novel dynamic game perspective
by viewing each layer as a player in a dynamic game characterized by the DNN
itself. Through this lens, different classes of optimizers can be seen as
matching different types of Nash equilibria, depending on the implicit
information structure of each (p)layer. The resulting method, called Dynamic
Game Theoretic Neural Optimizer (DGNOpt), not only generalizes OCT-inspired
optimizers to richer network class; it also motivates a new training principle
by solving a multi-player cooperative game. DGNOpt shows convergence
improvements over existing methods on image classification datasets with
residual and inception networks. Our work marries strengths from both OCT and
game theory, paving ways to new algorithmic opportunities from robust optimal
control and bandit-based optimization.","cs.GT,cs.LG,math.OC"
"Robust Reinforcement Learning using Adversarial Populations. Reinforcement Learning (RL) is an effective tool for controller design but
can struggle with issues of robustness, failing catastrophically when the
underlying system dynamics are perturbed. The Robust RL formulation tackles
this by adding worst-case adversarial noise to the dynamics and constructing
the noise distribution as the solution to a zero-sum minimax game. However,
existing work on learning solutions to the Robust RL formulation has primarily
focused on training a single RL agent against a single adversary. In this work,
we demonstrate that using a single adversary does not consistently yield
robustness to dynamics variations under standard parametrizations of the
adversary; the resulting policy is highly exploitable by new adversaries. We
propose a population-based augmentation to the Robust RL formulation in which
we randomly initialize a population of adversaries and sample from the
population uniformly during training. We empirically validate across robotics
benchmarks that the use of an adversarial population results in a more robust
policy that also improves out-of-distribution generalization. Finally, we
demonstrate that this approach provides comparable robustness and
generalization as domain randomization on these benchmarks while avoiding a
ubiquitous domain randomization failure mode.","cs.LG,cs.MA,cs.RO,stat.ML"
"Model-Invariant State Abstractions for Model-Based Reinforcement Learning. Accuracy and generalization of dynamics models is key to the success of
model-based reinforcement learning (MBRL). As the complexity of tasks
increases, so does the sample inefficiency of learning accurate dynamics
models. However, many complex tasks also exhibit sparsity in the dynamics,
i.e., actions have only a local effect on the system dynamics. In this paper,
we exploit this property with a causal invariance perspective in the
single-task setting, introducing a new type of state abstraction called
\textit{model-invariance}. Unlike previous forms of state abstractions, a
model-invariance state abstraction leverages causal sparsity over state
variables. This allows for compositional generalization to unseen states,
something that non-factored forms of state abstractions cannot do. We prove
that an optimal policy can be learned over this model-invariance state
abstraction and show improved generalization in a simple toy domain. Next, we
propose a practical method to approximately learn a model-invariant
representation for complex domains and validate our approach by showing
improved modelling performance over standard maximum likelihood approaches on
challenging tasks, such as the MuJoCo-based Humanoid. Finally, within the MBRL
setting we show strong performance gains with respect to sample efficiency
across a host of other continuous control tasks.","cs.AI,cs.LG,cs.RO"
"FADACS: A Few-shot Adversarial Domain Adaptation Architecture for Context-Aware Parking Availability Sensing. Existing research on parking availability sensing mainly relies on extensive
contextual and historical information. In practice, the availability of such
information is a challenge as it requires continuous collection of sensory
signals. In this study, we design an end-to-end transfer learning framework for
parking availability sensing to predict parking occupancy in areas in which the
parking data is insufficient to feed into data-hungry models. This framework
overcomes two main challenges: 1) many real-world cases cannot provide enough
data for most existing data-driven models, and 2) it is difficult to merge
sensor data and heterogeneous contextual information due to the differing urban
fabric and spatial characteristics. Our work adopts a widely-used concept,
adversarial domain adaptation, to predict the parking occupancy in an area
without abundant sensor data by leveraging data from other areas with similar
features. In this paper, we utilise more than 35 million parking data records
from sensors placed in two different cities, one a city centre and the other a
coastal tourist town. We also utilise heterogeneous spatio-temporal contextual
information from external resources, including weather and points of interest.
We quantify the strength of our proposed framework in different cases and
compare it to the existing data-driven approaches. The results show that the
proposed framework is comparable to existing state-of-the-art methods and also
provide some valuable insights on parking availability prediction.","cs.CV,cs.LG,cs.SY,eess.SP,eess.SY"
"Text Classification Algorithms: A Survey. In recent years, there has been an exponential growth in the number of
complex documents and texts that require a deeper understanding of machine
learning methods to be able to accurately classify texts in many applications.
Many machine learning approaches have achieved surpassing results in natural
language processing. The success of these learning algorithms relies on their
capacity to understand complex models and non-linear relationships within data.
However, finding suitable structures, architectures, and techniques for text
classification is a challenge for researchers. In this paper, a brief overview
of text classification algorithms is discussed. This overview covers different
text feature extractions, dimensionality reduction methods, existing algorithms
and techniques, and evaluations methods. Finally, the limitations of each
technique and their application in the real-world problem are discussed.","cs.AI,cs.CL,cs.IR,cs.LG,stat.ML"
"UAV-assisted Online Machine Learning over Multi-Tiered Networks: A Hierarchical Nested Personalized Federated Learning Approach. We consider distributed machine learning (ML) through unmanned aerial
vehicles (UAVs) for geo-distributed device clusters. We propose five new
technologies/techniques: (i) stratified UAV swarms with leader, worker, and
coordinator UAVs, (ii) hierarchical nested personalized federated learning
(HN-PFL): a holistic distributed ML framework for personalized model training
across the worker-leader-core network hierarchy, (iii) cooperative UAV resource
pooling for distributed ML using the UAVs' local computational capabilities,
(iv) aerial data caching and relaying for efficient data relaying to conduct
ML, and (v) concept/model drift, capturing online data variations at the
devices. We split the UAV-enabled model training problem as two parts. (a)
Network-aware HN-PFL, where we optimize a tradeoff between energy consumption
and ML model performance by configuring data offloading among devices-UAVs and
UAV-UAVs, UAVs' CPU frequencies, and mini-batch sizes subject to
communication/computation network heterogeneity. We tackle this optimization
problem via the method of posynomial condensation and propose a distributed
algorithm with a performance guarantee. (b) Macro-trajectory and learning
duration design, which we formulate as a sequential decision making problem,
tackled via deep reinforcement learning. Our simulations demonstrate the
superiority of our methodology with regards to the distributed ML performance,
the optimization of network resources, and the swarm trajectory efficiency.","cs.LG,cs.NI,cs.SY,eess.SY"
"Homogeneous Feature Transfer and Heterogeneous Location Fine-tuning for Cross-City Property Appraisal Framework. Most existing real estate appraisal methods focus on building accuracy and
reliable models from a given dataset but pay little attention to the
extensibility of their trained model. As different cities usually contain a
different set of location features (district names, apartment names), most
existing mass appraisal methods have to train a new model from scratch for
different cities or regions. As a result, these approaches require massive data
collection for each city and the total training time for a multi-city property
appraisal system will be extremely long. Besides, some small cities may not
have enough data for training a robust appraisal model. To overcome these
limitations, we develop a novel Homogeneous Feature Transfer and Heterogeneous
Location Fine-tuning (HFT+HLF) cross-city property appraisal framework. By
transferring partial neural network learning from a source city and fine-tuning
on the small amount of location information of a target city, our
semi-supervised model can achieve similar or even superior performance compared
to a fully supervised Artificial neural network (ANN) method.","cs.CY,cs.LG"
"Revisiting the Nystrom Method for Improved Large-Scale Machine Learning. We reconsider randomized algorithms for the low-rank approximation of
symmetric positive semi-definite (SPSD) matrices such as Laplacian and kernel
matrices that arise in data analysis and machine learning applications. Our
main results consist of an empirical evaluation of the performance quality and
running time of sampling and projection methods on a diverse suite of SPSD
matrices. Our results highlight complementary aspects of sampling versus
projection methods; they characterize the effects of common data preprocessing
steps on the performance of these algorithms; and they point to important
differences between uniform sampling and nonuniform sampling methods based on
leverage scores. In addition, our empirical results illustrate that existing
theory is so weak that it does not provide even a qualitative guide to
practice. Thus, we complement our empirical results with a suite of worst-case
theoretical bounds for both random sampling and random projection methods.
These bounds are qualitatively superior to existing bounds---e.g. improved
additive-error bounds for spectral and Frobenius norm error and relative-error
bounds for trace norm error---and they point to future directions to make these
algorithms useful in even larger-scale machine learning applications.","cs.DS,cs.LG,cs.NA"
"Deep Feature Space: A Geometrical Perspective. One of the most prominent attributes of Neural Networks (NNs) constitutes
their capability of learning to extract robust and descriptive features from
high dimensional data, like images. Hence, such an ability renders their
exploitation as feature extractors particularly frequent in an abundant of
modern reasoning systems. Their application scope mainly includes complex
cascade tasks, like multi-modal recognition and deep Reinforcement Learning
(RL). However, NNs induce implicit biases that are difficult to avoid or to
deal with and are not met in traditional image descriptors. Moreover, the lack
of knowledge for describing the intra-layer properties -- and thus their
general behavior -- restricts the further applicability of the extracted
features. With the paper at hand, a novel way of visualizing and understanding
the vector space before the NNs' output layer is presented, aiming to enlighten
the deep feature vectors' properties under classification tasks. Main attention
is paid to the nature of overfitting in the feature space and its adverse
effect on further exploitation. We present the findings that can be derived
from our model's formulation, and we evaluate them on realistic recognition
scenarios, proving its prominence by improving the obtained results.","I.2.6; I.5.1; F.4.1,cs.CG,cs.CV,cs.LG"
"Machine Learning Assisted Orthonormal Basis Selection for Functional Data Analysis. In implementations of the functional data methods, the effect of the initial
choice of an orthonormal basis has not gained much attention in the past.
Typically, several standard bases such as Fourier, wavelets, splines, etc. are
considered to transform observed functional data and a choice is made without
any formal criteria indicating which of the bases is preferable for the initial
transformation of the data into functions. In an attempt to address this issue,
we propose a strictly data-driven method of orthogonal basis selection. The
method uses recently introduced orthogonal spline bases called the splinets
obtained by efficient orthogonalization of the B-splines. The algorithm learns
from the data in the machine learning style to efficiently place knots. The
optimality criterion is based on the average (per functional data point) mean
square error and is utilized both in the learning algorithms and in comparison
studies. The latter indicates efficiency that is particularly evident for the
sparse functional data and to a lesser degree in analyses of responses to
complex physical systems.","cs.LG,stat.CO,stat.ML"
"Distributed Low Precision Training Without Mixed Precision. Low precision training is one of the most popular strategies for deploying
the deep model on limited hardware resources. Fixed point implementation of
DCNs has the potential to alleviate complexities and facilitate potential
deployment on embedded hardware. However, most low precision training solution
is based on a mixed precision strategy. In this paper, we have presented an
ablation study on different low precision training strategy and propose a
solution for IEEE FP-16 format throughout the training process. We tested the
ResNet50 on 128 GPU cluster on ImageNet-full dataset. We have viewed that it is
not essential to use FP32 format to train the deep models. We have viewed that
communication cost reduction, model compression, and large-scale distributed
training are three coupled problems.","cs.CV,cs.DC"
"Learning Some Popular Gaussian Graphical Models without Condition Number Bounds. Gaussian Graphical Models (GGMs) have wide-ranging applications in machine
learning and the natural and social sciences. In most of the settings in which
they are applied, the number of observed samples is much smaller than the
dimension and they are assumed to be sparse. While there are a variety of
algorithms (e.g. Graphical Lasso, CLIME) that provably recover the graph
structure with a logarithmic number of samples, they assume various conditions
that require the precision matrix to be in some sense well-conditioned.
  Here we give the first polynomial-time algorithms for learning attractive
GGMs and walk-summable GGMs with a logarithmic number of samples without any
such assumptions. In particular, our algorithms can tolerate strong
dependencies among the variables. Our result for structure recovery in
walk-summable GGMs is derived from a more general result for efficient sparse
linear regression in walk-summable models without any norm dependencies. We
complement our results with experiments showing that many existing algorithms
fail even in some simple settings where there are long dependency chains,
whereas ours do not.","cs.DS,cs.LG,math.ST,stat.ML,stat.TH"
"Generative Modeling of Hidden Functional Brain Networks. Functional connectivity refers to the temporal statistical relationship
between spatially distinct brain regions and is usually inferred from the time
series coherence/correlation in brain activity between regions of interest. In
human functional brain networks, the network structure is often inferred from
functional magnetic resonance imaging (fMRI) blood oxygen level dependent
(BOLD) signal. Since the BOLD signal is a proxy for neuronal activity, it is of
interest to learn the latent functional network structure. Additionally,
despite a core set of observations about functional networks such as
small-worldness, modularity, exponentially truncated degree distributions, and
presence of various types of hubs, very little is known about the computational
principles which can give rise to these observations. This paper introduces a
Hidden Markov Random Field framework for the purpose of representing,
estimating, and evaluating latent neuronal functional relationships between
different brain regions using fMRI data.","q-bio.NC,stat.ML"
"RLlib Flow: Distributed Reinforcement Learning is a Dataflow Problem. Researchers and practitioners in the field of reinforcement learning (RL)
frequently leverage parallel computation, which has led to a plethora of new
algorithms and systems in the last few years. In this paper, we re-examine the
challenges posed by distributed RL and try to view it through the lens of an
old idea: distributed dataflow. We show that viewing RL as a dataflow problem
leads to highly composable and performant implementations. We propose RLlib
flow, a hybrid actor-dataflow programming model for distributed RL, and
validate its practicality by porting the full suite of algorithms in RLlib, a
widely-adopted distributed RL library.","I.2.11; I.2.6; C.4,cs.DC,cs.LG"
"A Deterministic and Generalized Framework for Unsupervised Learning with Restricted Boltzmann Machines. Restricted Boltzmann machines (RBMs) are energy-based neural-networks which
are commonly used as the building blocks for deep architectures neural
architectures. In this work, we derive a deterministic framework for the
training, evaluation, and use of RBMs based upon the Thouless-Anderson-Palmer
(TAP) mean-field approximation of widely-connected systems with weak
interactions coming from spin-glass theory. While the TAP approach has been
extensively studied for fully-visible binary spin systems, our construction is
generalized to latent-variable models, as well as to arbitrarily distributed
real-valued spin systems with bounded support. In our numerical experiments, we
demonstrate the effective deterministic training of our proposed models and are
able to show interesting features of unsupervised learning which could not be
directly observed with sampling. Additionally, we demonstrate how to utilize
our TAP-based framework for leveraging trained RBMs as joint priors in
denoising problems.","cond-mat.dis-nn,cs.LG,cs.NE,stat.ML"
"Intelligence, physics and information -- the tradeoff between accuracy and simplicity in machine learning. How can we enable machines to make sense of the world, and become better at
learning? To approach this goal, I believe viewing intelligence in terms of
many integral aspects, and also a universal two-term tradeoff between task
performance and complexity, provides two feasible perspectives. In this thesis,
I address several key questions in some aspects of intelligence, and study the
phase transitions in the two-term tradeoff, using strategies and tools from
physics and information. Firstly, how can we make the learning models more
flexible and efficient, so that agents can learn quickly with fewer examples?
Inspired by how physicists model the world, we introduce a paradigm and an AI
Physicist agent for simultaneously learning many small specialized models
(theories) and the domain they are accurate, which can then be simplified,
unified and stored, facilitating few-shot learning in a continual way.
Secondly, for representation learning, when can we learn a good representation,
and how does learning depend on the structure of the dataset? We approach this
question by studying phase transitions when tuning the tradeoff hyperparameter.
In the information bottleneck, we theoretically show that these phase
transitions are predictable and reveal structure in the relationships between
the data, the model, the learned representation and the loss landscape.
Thirdly, how can agents discover causality from observations? We address part
of this question by introducing an algorithm that combines prediction and
minimizing information from the input, for exploratory causal discovery from
observational time series. Fourthly, to make models more robust to label noise,
we introduce Rank Pruning, a robust algorithm for classification with noisy
labels. I believe that building on the work of my thesis we will be one step
closer to enable more intelligent machines that can make sense of the world.","cs.LG,physics.data-an,stat.ML"
"Random Forests for dependent data. Random forest (RF) is one of the most popular methods for estimating
regression functions. The local nature of the RF algorithm, based on intra-node
means and variances, is ideal when errors are i.i.d. For dependent error
processes like time series and spatial settings where data in all the nodes
will be correlated, operating locally ignores this dependence. Also, RF will
involve resampling of correlated data, violating the principles of bootstrap.
Theoretically, consistency of RF has been established for i.i.d. errors, but
little is known about the case of dependent errors.
  We propose RF-GLS, a novel extension of RF for dependent error processes in
the same way Generalized Least Squares (GLS) fundamentally extends Ordinary
Least Squares (OLS) for linear models under dependence. The key to this
extension is the equivalent representation of the local decision-making in a
regression tree as a global OLS optimization which is then replaced with a GLS
loss to create a GLS-style regression tree. This also synergistically addresses
the resampling issue, as the use of GLS loss amounts to resampling uncorrelated
contrasts (pre-whitened data) instead of the correlated data. For spatial
settings, RF-GLS can be used in conjunction with Gaussian Process correlated
errors to generate kriging predictions at new locations. RF becomes a special
case of RF-GLS with an identity working covariance matrix.
  We establish consistency of RF-GLS under beta- (absolutely regular) mixing
error processes and show that this general result subsumes important cases like
autoregressive time series and spatial Matern Gaussian Processes. As a
byproduct, we also establish consistency of RF for beta-mixing processes, which
to our knowledge, is the first such result for RF under dependence.
  We empirically demonstrate the improvement achieved by RF-GLS over RF for
both estimation and prediction under dependence.","cs.LG,math.ST,stat.ME,stat.ML,stat.TH"
"Cross-Subject Statistical Shift Estimation for Generalized Electroencephalography-based Mental Workload Assessment. Assessment of mental workload in real-world conditions is key to ensure the
performance of workers executing tasks that demand sustained attention.
Previous literature has employed electroencephalography (EEG) to this end
despite having observed that EEG correlates of mental workload vary across
subjects and physical strain, thus making it difficult to devise models capable
of simultaneously presenting reliable performance across users. Domain
adaptation consists of a set of strategies that aim at allowing for improving
machine learning systems performance on unseen data at training time. Such
methods, however, might rely on assumptions over the considered data
distributions, which typically do not hold for applications of EEG data.
Motivated by this observation, in this work we propose a strategy to estimate
two types of discrepancies between multiple data distributions, namely marginal
and conditional shifts, observed on data collected from different subjects.
Besides shedding light on the assumptions that hold for a particular dataset,
the estimates of statistical shifts obtained with the proposed approach can be
used for investigating other aspects of a machine learning pipeline, such as
quantitatively assessing the effectiveness of domain adaptation strategies. In
particular, we consider EEG data collected from individuals performing mental
tasks while running on a treadmill and pedaling on a stationary bike and
explore the effects of different normalization strategies commonly used to
mitigate cross-subject variability. We show the effects that different
normalization schemes have on statistical shifts and their relationship with
the accuracy of mental workload prediction as assessed on unseen participants
at training time.","cs.LG,eess.SP,stat.ML"
"SBI -- A toolkit for simulation-based inference. Scientists and engineers employ stochastic numerical simulators to model
empirically observed phenomena. In contrast to purely statistical models,
simulators express scientific principles that provide powerful inductive
biases, improve generalization to new data or scenarios and allow for fewer,
more interpretable and domain-relevant parameters. Despite these advantages,
tuning a simulator's parameters so that its outputs match data is challenging.
Simulation-based inference (SBI) seeks to identify parameter sets that a) are
compatible with prior knowledge and b) match empirical observations.
Importantly, SBI does not seek to recover a single 'best' data-compatible
parameter set, but rather to identify all high probability regions of parameter
space that explain observed data, and thereby to quantify parameter
uncertainty. In Bayesian terminology, SBI aims to retrieve the posterior
distribution over the parameters of interest. In contrast to conventional
Bayesian inference, SBI is also applicable when one can run model simulations,
but no formula or algorithm exists for evaluating the probability of data given
parameters, i.e. the likelihood. We present $\texttt{sbi}$, a PyTorch-based
package that implements SBI algorithms based on neural networks. $\texttt{sbi}$
facilitates inference on black-box simulators for practising scientists and
engineers by providing a unified interface to state-of-the-art algorithms
together with documentation and tutorials.","cs.LG,q-bio.QM,stat.CO,stat.ML"
"Off-policy Evaluation in Infinite-Horizon Reinforcement Learning with Latent Confounders. Off-policy evaluation (OPE) in reinforcement learning is an important problem
in settings where experimentation is limited, such as education and healthcare.
But, in these very same settings, observed actions are often confounded by
unobserved variables making OPE even more difficult. We study an OPE problem in
an infinite-horizon, ergodic Markov decision process with unobserved
confounders, where states and actions can act as proxies for the unobserved
confounders. We show how, given only a latent variable model for states and
actions, policy value can be identified from off-policy data. Our method
involves two stages. In the first, we show how to use proxies to estimate
stationary distribution ratios, extending recent work on breaking the curse of
horizon to the confounded setting. In the second, we show optimal balancing can
be combined with such learned ratios to obtain policy value while avoiding
direct modeling of reward functions. We establish theoretical guarantees of
consistency, and benchmark our method empirically.","cs.AI,cs.LG,stat.ML"
"On the Nystrm and Column-Sampling Methods for the Approximate Principal Components Analysis of Large Data Sets. In this paper we analyze approximate methods for undertaking a principal
components analysis (PCA) on large data sets. PCA is a classical dimension
reduction method that involves the projection of the data onto the subspace
spanned by the leading eigenvectors of the covariance matrix. This projection
can be used either for exploratory purposes or as an input for further
analysis, e.g. regression. If the data have billions of entries or more, the
computational and storage requirements for saving and manipulating the design
matrix in fast memory is prohibitive. Recently, the Nystr\""om and
column-sampling methods have appeared in the numerical linear algebra community
for the randomized approximation of the singular value decomposition of large
matrices. However, their utility for statistical applications remains unclear.
We compare these approximations theoretically by bounding the distance between
the induced subspaces and the desired, but computationally infeasible, PCA
subspace. Additionally we show empirically, through simulations and a real data
example involving a corpus of emails, the trade-off of approximation accuracy
and computational complexity.","stat.CO,stat.ML"
"A Hierarchical Approach for Visual Storytelling Using Image Description. One of the primary challenges of visual storytelling is developing techniques
that can maintain the context of the story over long event sequences to
generate human-like stories. In this paper, we propose a hierarchical deep
learning architecture based on encoder-decoder networks to address this
problem. To better help our network maintain this context while also generating
long and diverse sentences, we incorporate natural language image descriptions
along with the images themselves to generate each story sentence. We evaluate
our system on the Visual Storytelling (VIST) dataset and show that our method
outperforms state-of-the-art techniques on a suite of different automatic
evaluation metrics. The empirical results from this evaluation demonstrate the
necessities of different components of our proposed architecture and shows the
effectiveness of the architecture for visual storytelling.","cs.CL,cs.CV,cs.LG,stat.ML"
"Negational Symmetry of Quantum Neural Networks for Binary Pattern Classification. Entanglement is a physical phenomenon, which has fueled recent successes of
quantum algorithms. Although quantum neural networks (QNNs) have shown
promising results in solving simple machine learning tasks recently, for the
time being, the effect of entanglement in QNNs and the behavior of QNNs in
binary pattern classification are still underexplored. In this work, we provide
some theoretical insight into the properties of QNNs by presenting and
analyzing a new form of invariance embedded in QNNs for both quantum binary
classification and quantum representation learning, which we term negational
symmetry. Given a quantum binary signal and its negational counterpart where a
bitwise NOT operation is applied to each quantum bit of the binary signal, a
QNN outputs the same logits. That is to say, QNNs cannot differentiate a
quantum binary signal and its negational counterpart in a binary classification
task. We further empirically evaluate the negational symmetry of QNNs in binary
pattern classification tasks using Google's quantum computing framework. The
theoretical and experimental results suggest that negational symmetry is a
fundamental property of QNNs, which is not shared by classical models. Our
findings also imply that negational symmetry is a double-edged sword in
practical quantum applications.","cs.LG,quant-ph,stat.ML"
"Flexibly Regularized Mixture Models and Application to Image Segmentation. Probabilistic finite mixture models are widely used for unsupervised
clustering. These models can often be improved by adapting them to the topology
of the data. For instance, in order to classify spatially adjacent data points
similarly, it is common to introduce a Laplacian constraint on the posterior
probability that each data point belongs to a class. Alternatively, the mixing
probabilities can be treated as free parameters, while assuming Gauss-Markov or
more complex priors to regularize those mixing probabilities. However, these
approaches are constrained by the shape of the prior and often lead to
complicated or intractable inference. Here, we propose a new parametrization of
the Dirichlet distribution to flexibly regularize the mixing probabilities of
over-parametrized mixture distributions. Using the Expectation-Maximization
algorithm, we show that our approach allows us to define any linear update rule
for the mixing probabilities, including spatial smoothing regularization as a
special case. We then show that this flexible design can be extended to share
class information between multiple mixture models. We apply our algorithm to
artificial and natural image segmentation tasks, and we provide quantitative
and qualitative comparison of the performance of Gaussian and Student-t
mixtures on the Berkeley Segmentation Dataset. We also demonstrate how to
propagate class information across the layers of deep convolutional neural
networks in a probabilistically optimal way, suggesting a new interpretation
for feedback signals in biological visual systems. Our flexible approach can be
easily generalized to adapt probabilistic mixture models to arbitrary data
topologies.","cs.CV,cs.LG,q-bio.NC"
"Concave Utility Reinforcement Learning: the Mean-field Game viewpoint. Concave Utility Reinforcement Learning (CURL) extends RL from linear to
concave utilities in the occupancy measure induced by the agent's policy. This
encompasses not only RL but also imitation learning and exploration, among
others. Yet, this more general paradigm invalidates the classical Bellman
equations, and calls for new algorithms. Mean-field Games (MFGs) are a
continuous approximation of many-agent RL. They consider the limit case of a
continuous distribution of identical agents, anonymous with symmetric
interests, and reduce the problem to the study of a single representative agent
in interaction with the full population. Our core contribution consists in
showing that CURL is a subclass of MFGs. We think this important to bridge
together both communities. It also allows to shed light on aspects of both
fields: we show the equivalence between concavity in CURL and monotonicity in
the associated MFG, between optimality conditions in CURL and Nash equilibrium
in MFG, or that Fictitious Play (FP) for this class of MFGs is simply
Frank-Wolfe, bringing the first convergence rate for discrete-time FP for MFGs.
We also experimentally demonstrate that, using algorithms recently introduced
for solving MFGs, we can address the CURL problem more efficiently.","cs.LG,cs.MA"
"A deep-structured fully-connected random field model for structured inference. There has been significant interest in the use of fully-connected graphical
models and deep-structured graphical models for the purpose of structured
inference. However, fully-connected and deep-structured graphical models have
been largely explored independently, leaving the unification of these two
concepts ripe for exploration. A fundamental challenge with unifying these two
types of models is in dealing with computational complexity. In this study, we
investigate the feasibility of unifying fully-connected and deep-structured
models in a computationally tractable manner for the purpose of structured
inference. To accomplish this, we introduce a deep-structured fully-connected
random field (DFRF) model that integrates a series of intermediate sparse
auto-encoding layers placed between state layers to significantly reduce
computational complexity. The problem of image segmentation was used to
illustrate the feasibility of using the DFRF for structured inference in a
computationally tractable manner. Results in this study show that it is
feasible to unify fully-connected and deep-structured models in a
computationally tractable manner for solving structured inference problems such
as image segmentation.","cs.IT,cs.LG,math.IT,stat.ME,stat.ML"
"Barrier-Certified Adaptive Reinforcement Learning with Applications to Brushbot Navigation. This paper presents a safe learning framework that employs an adaptive model
learning algorithm together with barrier certificates for systems with possibly
nonstationary agent dynamics. To extract the dynamic structure of the model, we
use a sparse optimization technique. We use the learned model in combination
with control barrier certificates which constrain policies (feedback
controllers) in order to maintain safety, which refers to avoiding particular
undesirable regions of the state space. Under certain conditions, recovery of
safety in the sense of Lyapunov stability after violations of safety due to the
nonstationarity is guaranteed. In addition, we reformulate an action-value
function approximation to make any kernel-based nonlinear function estimation
method applicable to our adaptive learning framework. Lastly, solutions to the
barrier-certified policy optimization are guaranteed to be globally optimal,
ensuring the greedy policy improvement under mild conditions. The resulting
framework is validated via simulations of a quadrotor, which has previously
been used under stationarity assumptions in the safe learnings literature, and
is then tested on a real robot, the brushbot, whose dynamics is unknown, highly
complex and nonstationary.","cs.LG,cs.RO,cs.SY"
"Explaining Anomalies Detected by Autoencoders Using SHAP. Anomaly detection algorithms are often thought to be limited because they
don't facilitate the process of validating results performed by domain experts.
In Contrast, deep learning algorithms for anomaly detection, such as
autoencoders, point out the outliers, saving experts the time-consuming task of
examining normal cases in order to find anomalies. Most outlier detection
algorithms output a score for each instance in the database. The top-k most
intense outliers are returned to the user for further inspection; however the
manual validation of results becomes challenging without additional clues. An
explanation of why an instance is anomalous enables the experts to focus their
investigation on most important anomalies and may increase their trust in the
algorithm.
  Recently, a game theory-based framework known as SHapley Additive
exPlanations (SHAP) has been shown to be effective in explaining various
supervised learning models. In this research, we extend SHAP to explain
anomalies detected by an autoencoder, an unsupervised model. The proposed
method extracts and visually depicts both the features that most contributed to
the anomaly and those that offset it. A preliminary experimental study using
real world data demonstrates the usefulness of the proposed method in assisting
the domain experts to understand the anomaly and filtering out the
uninteresting anomalies, aiming at minimizing the false positive rate of
detected anomalies.","cs.GT,cs.LG,stat.ML"
"Path Integral Based Convolution and Pooling for Graph Neural Networks. Graph neural networks (GNNs) extends the functionality of traditional neural
networks to graph-structured data. Similar to CNNs, an optimized design of
graph convolution and pooling is key to success. Borrowing ideas from physics,
we propose a path integral based graph neural networks (PAN) for classification
and regression tasks on graphs. Specifically, we consider a convolution
operation that involves every path linking the message sender and receiver with
learnable weights depending on the path length, which corresponds to the
maximal entropy random walk. It generalizes the graph Laplacian to a new
transition matrix we call maximal entropy transition (MET) matrix derived from
a path integral formalism. Importantly, the diagonal entries of the MET matrix
are directly related to the subgraph centrality, thus providing a natural and
adaptive pooling mechanism. PAN provides a versatile framework that can be
tailored for different graph data with varying sizes and structures. We can
view most existing GNN architectures as special cases of PAN. Experimental
results show that PAN achieves state-of-the-art performance on various graph
classification/regression tasks, including a new benchmark dataset from
statistical mechanics we propose to boost applications of GNN in physical
sciences.","cond-mat.dis-nn,cs.LG,cs.NI,physics.data-an,stat.ML"
"Character Controllers Using Motion VAEs. A fundamental problem in computer animation is that of realizing purposeful
and realistic human movement given a sufficiently-rich set of motion capture
clips. We learn data-driven generative models of human movement using
autoregressive conditional variational autoencoders, or Motion VAEs. The latent
variables of the learned autoencoder define the action space for the movement
and thereby govern its evolution over time. Planning or control algorithms can
then use this action space to generate desired motions. In particular, we use
deep reinforcement learning to learn controllers that achieve goal-directed
movements. We demonstrate the effectiveness of the approach on multiple tasks.
We further evaluate system-design choices and describe the current limitations
of Motion VAEs.","cs.GR,cs.LG"
"How to Evaluate Dimensionality Reduction? - Improving the Co-ranking Matrix. The growing number of dimensionality reduction methods available for data
visualization has recently inspired the development of quality assessment
measures, in order to evaluate the resulting low-dimensional representation
independently from a methods' inherent criteria. Several (existing) quality
measures can be (re)formulated based on the so-called co-ranking matrix, which
subsumes all rank errors (i.e. differences between the ranking of distances
from every point to all others, comparing the low-dimensional representation to
the original data). The measures are often based on the partioning of the
co-ranking matrix into 4 submatrices, divided at the K-th row and column,
calculating a weighted combination of the sums of each submatrix. Hence, the
evaluation process typically involves plotting a graph over several (or even
all possible) settings of the parameter K. Considering simple artificial
examples, we argue that this parameter controls two notions at once, that need
not necessarily be combined, and that the rectangular shape of submatrices is
disadvantageous for an intuitive interpretation of the parameter. We debate
that quality measures, as general and flexible evaluation tools, should have
parameters with a direct and intuitive interpretation as to which specific
error types are tolerated or penalized. Therefore, we propose to replace K with
two parameters to control these notions separately, and introduce a differently
shaped weighting on the co-ranking matrix. The two new parameters can then
directly be interpreted as a threshold up to which rank errors are tolerated,
and a threshold up to which the rank-distances are significant for the
evaluation. Moreover, we propose a color representation of local quality to
visually support the evaluation process for a given mapping, where every point
in the mapping is colored according to its local contribution to the overall
quality.","cs.IR,cs.LG"
"MBMF: Model-Based Priors for Model-Free Reinforcement Learning. Reinforcement Learning is divided in two main paradigms: model-free and
model-based. Each of these two paradigms has strengths and limitations, and has
been successfully applied to real world domains that are appropriate to its
corresponding strengths. In this paper, we present a new approach aimed at
bridging the gap between these two paradigms. We aim to take the best of the
two paradigms and combine them in an approach that is at the same time
data-efficient and cost-savvy. We do so by learning a probabilistic dynamics
model and leveraging it as a prior for the intertwined model-free optimization.
As a result, our approach can exploit the generality and structure of the
dynamics model, but is also capable of ignoring its inevitable inaccuracies, by
directly incorporating the evidence provided by the direct observation of the
cost. Preliminary results demonstrate that our approach outperforms purely
model-based and model-free approaches, as well as the approach of simply
switching from a model-based to a model-free setting.","cs.AI,cs.LG,cs.RO,cs.SY"
"Path Planning in Support of Smart Mobility Applications using Generative Adversarial Networks. This paper describes and evaluates the use of Generative Adversarial Networks
(GANs) for path planning in support of smart mobility applications such as
indoor and outdoor navigation applications, individualized wayfinding for
people with disabilities (e.g., vision impairments, physical disabilities,
etc.), path planning for evacuations, robotic navigations, and path planning
for autonomous vehicles. We propose an architecture based on GANs to recommend
accurate and reliable paths for navigation applications. The proposed system
can use crowd-sourced data to learn the trajectories and infer new ones. The
system provides users with generated paths that help them navigate from their
local environment to reach a desired location. As a use case, we experimented
with the proposed method in support of a wayfinding application in an indoor
environment. Our experiments assert that the generated paths are correct and
reliable. The accuracy of the classification task for the generated paths is up
to 99% and the quality of the generated paths has a mean opinion score of 89%.","cs.HC,cs.LG,cs.NI,stat.ML"
"Sparse Quadratic Discriminant Analysis and Community Bayes. We develop a class of rules spanning the range between quadratic discriminant
analysis and naive Bayes, through a path of sparse graphical models. A group
lasso penalty is used to introduce shrinkage and encourage a similar pattern of
sparsity across precision matrices. It gives sparse estimates of interactions
and produces interpretable models. Inspired by the connected-components
structure of the estimated precision matrices, we propose the community Bayes
model, which partitions features into several conditional independent
communities and splits the classification problem into separate smaller ones.
The community Bayes idea is quite general and can be applied to non-Gaussian
data and likelihood-based classifiers.","stat.CO,stat.ML"
"Green Deep Reinforcement Learning for Radio Resource Management: Architecture, Algorithm Compression and Challenge. AI heralds a step-change in the performance and capability of wireless
networks and other critical infrastructures. However, it may also cause
irreversible environmental damage due to their high energy consumption. Here,
we address this challenge in the context of 5G and beyond, where there is a
complexity explosion in radio resource management (RRM). On the one hand, deep
reinforcement learning (DRL) provides a powerful tool for scalable optimization
for high dimensional RRM problems in a dynamic environment. On the other hand,
DRL algorithms consume a high amount of energy over time and risk compromising
progress made in green radio research. This paper reviews and analyzes how to
achieve green DRL for RRM via both architecture and algorithm innovations.
Architecturally, a cloud based training and distributed decision-making DRL
scheme is proposed, where RRM entities can make lightweight deep local
decisions whilst assisted by on-cloud training and updating. On the algorithm
level, compression approaches are introduced for both deep neural networks and
the underlying Markov Decision Processes, enabling accurate low-dimensional
representations of challenges. To scale learning across geographic areas, a
spatial transfer learning scheme is proposed to further promote the learning
efficiency of distributed DRL entities by exploiting the traffic demand
correlations. Together, our proposed architecture and algorithms provide a
vision for green and on-demand DRL capability.","cs.AI,cs.LG,cs.NI,eess.SP"
"Deep Learning and Reinforcement Learning for Autonomous Unmanned Aerial Systems: Roadmap for Theory to Deployment. Unmanned Aerial Systems (UAS) are being increasingly deployed for commercial,
civilian, and military applications. The current UAS state-of-the-art still
depends on a remote human controller with robust wireless links to perform
several of these applications. The lack of autonomy restricts the domains of
application and tasks for which a UAS can be deployed. Enabling autonomy and
intelligence to the UAS will help overcome this hurdle and expand its use
improving safety and efficiency. The exponential increase in computing
resources and the availability of large amount of data in this digital era has
led to the resurgence of machine learning from its last winter. Therefore, in
this chapter, we discuss how some of the advances in machine learning,
specifically deep learning and reinforcement learning can be leveraged to
develop next-generation autonomous UAS. We first begin motivating this chapter
by discussing the application, challenges, and opportunities of the current UAS
in the introductory section. We then provide an overview of some of the key
deep learning and reinforcement learning techniques discussed throughout this
chapter. A key area of focus that will be essential to enable autonomy to UAS
is computer vision. Accordingly, we discuss how deep learning approaches have
been used to accomplish some of the basic tasks that contribute to providing
UAS autonomy. Then we discuss how reinforcement learning is explored for using
this information to provide autonomous control and navigation for UAS. Next, we
provide the reader with directions to choose appropriate simulation suites and
hardware platforms that will help to rapidly prototype novel machine learning
based solutions for UAS. We additionally discuss the open problems and
challenges pertaining to each aspect of developing autonomous UAS solutions to
shine light on potential research areas.","cs.LG,cs.SY,eess.SY,stat.ML"
"Representation Learning using Graph Autoencoders with Residual Connections. Graph autoencoders are very efficient at embedding graph-based complex data
sets. However, most of the autoencoders have shallow depths and their
efficiency tends to decrease with the increase of layer depth. In this paper,
we study the effect of adding residual connections to shallow and deep graph
variational and vanilla autoencoders. We show that residual connections improve
the accuracy of the deep graph-based autoencoders. Furthermore, we propose
Res-VGAE, a graph variational autoencoder with different residual connections.
Our experiments show that our model achieves superior results when compared
with other autoencoder-based models for the link prediction task.","cs.LG,cs.SI"
"Novelty Detection via Network Saliency in Visual-based Deep Learning. Machine-learning driven safety-critical autonomous systems, such as
self-driving cars, must be able to detect situations where its trained model is
not able to make a trustworthy prediction. Often viewed as a black-box, it is
non-obvious to determine when a model will make a safe decision and when it
will make an erroneous, perhaps life-threatening one. Prior work on novelty
detection deal with highly structured data and do not translate well to
dynamic, real-world situations. This paper proposes a multi-step framework for
the detection of novel scenarios in vision-based autonomous systems by
leveraging information learned by the trained prediction model and a new image
similarity metric. We demonstrate the efficacy of this method through
experiments on a real-world driving dataset as well as on our in-house indoor
racing environment.","cs.CV,cs.LG,stat.ML"
"The Whole Is Greater Than the Sum of Its Nonrigid Parts. According to Aristotle, a philosopher in Ancient Greece, ""the whole is
greater than the sum of its parts"". This observation was adopted to explain
human perception by the Gestalt psychology school of thought in the twentieth
century. Here, we claim that observing part of an object which was previously
acquired as a whole, one could deal with both partial matching and shape
completion in a holistic manner. More specifically, given the geometry of a
full, articulated object in a given pose, as well as a partial scan of the same
object in a different pose, we address the problem of matching the part to the
whole while simultaneously reconstructing the new pose from its partial
observation. Our approach is data-driven, and takes the form of a Siamese
autoencoder without the requirement of a consistent vertex labeling at
inference time; as such, it can be used on unorganized point clouds as well as
on triangle meshes. We demonstrate the practical effectiveness of our model in
the applications of single-view deformable shape completion and dense shape
correspondence, both on synthetic and real-world geometric data, where we
outperform prior work on these tasks by a large margin.","I.4.5,cs.CG,cs.CV,cs.LG"
"Image Segmentation Using Frequency Locking of Coupled Oscillators. Synchronization of coupled oscillators is observed at multiple levels of
neural systems, and has been shown to play an important function in visual
perception. We propose a computing system based on locally coupled oscillator
networks for image segmentation. The system can serve as the preprocessing
front-end of an image processing pipeline where the common frequencies of
clusters of oscillators reflect the segmentation results. To demonstrate the
feasibility of our design, the system is simulated and tested on a human face
image dataset and its performance is compared with traditional intensity
threshold based algorithms. Our system shows both better performance and higher
noise tolerance than traditional methods.","C.1.3,cs.CV,q-bio.NC"
"Selection of Source Images Heavily Influences the Effectiveness of Adversarial Attacks. Although the adoption rate of deep neural networks (DNNs) has tremendously
increased in recent years, a solution for their vulnerability against
adversarial examples has not yet been found. As a result, substantial research
efforts are dedicated to fix this weakness, with many studies typically using a
subset of source images to generate adversarial examples, treating every image
in this subset as equal. We demonstrate that, in fact, not every source image
is equally suited for this kind of assessment. To do so, we devise a
large-scale model-to-model transferability scenario for which we meticulously
analyze the properties of adversarial examples, generated from every suitable
source image in ImageNet by making use of two of the most frequently deployed
attacks. In this transferability scenario, which involves seven distinct DNN
models, including the recently proposed vision transformers, we reveal that it
is possible to have a difference of up to $12.5\%$ in model-to-model
transferability success, $1.01$ in average $L_2$ perturbation, and $0.03$
($8/225$) in average $L_{\infty}$ perturbation when $1,000$ source images are
sampled randomly among all suitable candidates. We then take one of the first
steps in evaluating the robustness of images used to create adversarial
examples, proposing a number of simple but effective methods to identify
unsuitable source images, thus making it possible to mitigate extreme cases in
experimentation and support high-quality benchmarking.","cs.CR,cs.CV,cs.LG"
"On the Importance of Hyperparameter Optimization for Model-based Reinforcement Learning. Model-based Reinforcement Learning (MBRL) is a promising framework for
learning control in a data-efficient manner. MBRL algorithms can be fairly
complex due to the separate dynamics modeling and the subsequent planning
algorithm, and as a result, they often possess tens of hyperparameters and
architectural choices. For this reason, MBRL typically requires significant
human expertise before it can be applied to new problems and domains. To
alleviate this problem, we propose to use automatic hyperparameter optimization
(HPO). We demonstrate that this problem can be tackled effectively with
automated HPO, which we demonstrate to yield significantly improved performance
compared to human experts. In addition, we show that tuning of several MBRL
hyperparameters dynamically, i.e. during the training itself, further improves
the performance compared to using static hyperparameters which are kept fixed
for the whole training. Finally, our experiments provide valuable insights into
the effects of several hyperparameters, such as plan horizon or learning rate
and their influence on the stability of training and resulting rewards.","cs.AI,cs.LG,cs.NE,cs.SY,eess.SY"
"Off-Policy Evaluation in Partially Observable Environments. This work studies the problem of batch off-policy evaluation for
Reinforcement Learning in partially observable environments. Off-policy
evaluation under partial observability is inherently prone to bias, with risk
of arbitrarily large errors. We define the problem of off-policy evaluation for
Partially Observable Markov Decision Processes (POMDPs) and establish what we
believe is the first off-policy evaluation result for POMDPs. In addition, we
formulate a model in which observed and unobserved variables are decoupled into
two dynamic processes, called a Decoupled POMDP. We show how off-policy
evaluation can be performed under this new model, mitigating estimation errors
inherent to general POMDPs. We demonstrate the pitfalls of off-policy
evaluation in POMDPs using a well-known off-policy method, Importance Sampling,
and compare it with our result on synthetic medical data.","cs.AI,cs.LG,cs.SY,eess.SY,stat.ML"
"Scalable Adversarial Attack on Graph Neural Networks with Alternating Direction Method of Multipliers. Graph neural networks (GNNs) have achieved high performance in analyzing
graph-structured data and have been widely deployed in safety-critical areas,
such as finance and autonomous driving. However, only a few works have explored
GNNs' robustness to adversarial attacks, and their designs are usually limited
by the scale of input datasets (i.e., focusing on small graphs with only
thousands of nodes). In this work, we propose, SAG, the first scalable
adversarial attack method with Alternating Direction Method of Multipliers
(ADMM). We first decouple the large-scale graph into several smaller graph
partitions and cast the original problem into several subproblems. Then, we
propose to solve these subproblems using projected gradient descent on both the
graph topology and the node features that lead to considerably lower memory
consumption compared to the conventional attack methods. Rigorous experiments
further demonstrate that SAG can significantly reduce the computation and
memory overhead compared with the state-of-the-art approach, making SAG
applicable towards graphs with large size of nodes and edges.","cs.AI,cs.LG,stat.ML"
"Deep Cross-modal Hashing via Margin-dynamic-softmax Loss. Due to their high retrieval efficiency and low storage cost for cross-modal
search task, cross-modal hashing methods have attracted considerable attention.
For the supervised cross-modal hashing methods, how to make the learned hash
codes preserve semantic information sufficiently contained in the label of
datapoints is the key to further enhance the retrieval performance. Hence,
almost all supervised cross-modal hashing methods usually depends on defining a
similarity between datapoints with the label information to guide the hashing
model learning fully or partly. However, the defined similarity between
datapoints can only capture the label information of datapoints partially and
misses abundant semantic information, then hinders the further improvement of
retrieval performance. Thus, in this paper, different from previous works, we
propose a novel cross-modal hashing method without defining the similarity
between datapoints, called Deep Cross-modal Hashing via
\textit{Margin-dynamic-softmax Loss} (DCHML). Specifically, DCHML first trains
a proxy hashing network to transform each category information of a dataset
into a semantic discriminative hash code, called proxy hash code. Each proxy
hash code can preserve the semantic information of its corresponding category
well. Next, without defining the similarity between datapoints to supervise the
training process of the modality-specific hashing networks , we propose a novel
\textit{margin-dynamic-softmax loss} to directly utilize the proxy hashing
codes as supervised information. Finally, by minimizing the novel
\textit{margin-dynamic-softmax loss}, the modality-specific hashing networks
can be trained to generate hash codes which can simultaneously preserve the
cross-modal similarity and abundant semantic information well.","cs.CV,cs.IR,cs.MM"
"Learning Neural Generative Dynamics for Molecular Conformation Generation. We study how to generate molecule conformations (i.e., 3D structures) from a
molecular graph. Traditional methods, such as molecular dynamics, sample
conformations via computationally expensive simulations. Recently, machine
learning methods have shown great potential by training on a large collection
of conformation data. Challenges arise from the limited model capacity for
capturing complex distributions of conformations and the difficulty in modeling
long-range dependencies between atoms. Inspired by the recent progress in deep
generative models, in this paper, we propose a novel probabilistic framework to
generate valid and diverse conformations given a molecular graph. We propose a
method combining the advantages of both flow-based and energy-based models,
enjoying: (1) a high model capacity to estimate the multimodal conformation
distribution; (2) explicitly capturing the complex long-range dependencies
between atoms in the observation space. Extensive experiments demonstrate the
superior performance of the proposed method on several benchmarks, including
conformation generation and distance modeling tasks, with a significant
improvement over existing generative models for molecular conformation
sampling.","cs.LG,physics.chem-ph"
"Automatic identification of fossils and abiotic grains during carbonate microfacies analysis using deep convolutional neural networks. Petrographic analysis based on microfacies identification in thin sections is
widely used in sedimentary environment interpretation and paleoecological
reconstruction. Fossil recognition from microfacies is an essential procedure
for petrographers to complete this task. Distinguishing the morphological and
microstructural diversity of skeletal fragments requires extensive prior
knowledge of fossil morphotypes in microfacies and long training sessions under
the microscope. This requirement engenders certain challenges for
sedimentologists and paleontologists, especially novices. However, a machine
classifier can help address this challenge. In this study, we collected a
microfacies image dataset comprising both public data from 1,149 references and
our own materials (including 30,815 images of 22 fossil and abiotic grain
groups). We employed a high-performance workstation to implement four classic
deep convolutional neural networks (DCNNs), which have proven to be highly
efficient in computer vision over the last several years. Our framework uses a
transfer learning technique, which reuses the pre-trained parameters that are
trained on a larger ImageNet dataset as initialization for the network to
achieve high accuracy with low computing costs. We obtained up to 95% of the
top one and 99% of the top three test accuracies in the Inception ResNet v2
architecture. The machine classifier exhibited 0.99 precision on minerals, such
as dolomite and pyrite. Although it had some difficulty on samples having
similar morphologies, such as the bivalve, brachiopod, and ostracod, it
nevertheless obtained 0.88 precision. Our machine learning framework
demonstrated high accuracy with reproducibility and bias avoidance that was
comparable to those of human classifiers. Its application can thus eliminate
much of the tedious, manually intensive efforts by human experts conducting
routine identification.","cs.CV,eess.IV"
"Principal component-based image segmentation: a new approach to outline in vitro cell colonies. The in vitro clonogenic assay is a technique to study the ability of a cell
to form a colony in a culture dish. By optical imaging, dishes with stained
colonies can be scanned and assessed digitally. Identification, segmentation
and counting of stained colonies play a vital part in high-throughput screening
and quantitative assessment of biological assays. Image processing of such
pictured/scanned assays can be affected by image/scan acquisition artifacts
like background noise and spatially varying illumination, and contaminants in
the suspension medium. Although existing approaches tackle these issues, the
segmentation quality requires further improvement, particularly on noisy and
low contrast images. In this work, we present an objective and versatile
machine learning procedure to amend these issues by characterizing, extracting
and segmenting inquired colonies using principal component analysis, k-means
clustering and a modified watershed segmentation algorithm. The intention is to
automatically identify visible colonies through spatial texture assessment and
accordingly discriminate them from background in preparation for successive
segmentation. The proposed segmentation algorithm yielded a similar quality as
manual counting by human observers. High F1 scores (>0.9) and low
root-mean-square errors (around 14%) underlined good agreement with ground
truth data. Moreover, it outperformed a recent state-of-the-art method. The
methodology will be an important tool in future cancer research applications.",cs.CV
"Information-Bottleneck Approach to Salient Region Discovery. We propose a new method for learning image attention masks in a
semi-supervised setting based on the Information Bottleneck principle. Provided
with a set of labeled images, the mask generation model is minimizing mutual
information between the input and the masked image while maximizing the mutual
information between the same masked image and the image label. In contrast with
other approaches, our attention model produces a Boolean rather than a
continuous mask, entirely concealing the information in masked-out pixels.
Using a set of synthetic datasets based on MNIST and CIFAR10 and the SVHN
datasets, we demonstrate that our method can successfully attend to features
known to define the image class.","cs.CV,cs.IT,cs.LG,math.IT"
"Breaking the Limit of Graph Neural Networks by Improving the Assortativity of Graphs with Local Mixing Patterns. Graph neural networks (GNNs) have achieved tremendous success on multiple
graph-based learning tasks by fusing network structure and node features.
Modern GNN models are built upon iterative aggregation of neighbor's/proximity
features by message passing. Its prediction performance has been shown to be
strongly bounded by assortative mixing in the graph, a key property wherein
nodes with similar attributes mix/connect with each other. We observe that real
world networks exhibit heterogeneous or diverse mixing patterns and the
conventional global measurement of assortativity, such as global assortativity
coefficient, may not be a representative statistic in quantifying this mixing.
We adopt a generalized concept, node-level assortativity, one that is based at
the node level to better represent the diverse patterns and accurately quantify
the learnability of GNNs. We find that the prediction performance of a wide
range of GNN models is highly correlated with the node level assortativity. To
break this limit, in this work, we focus on transforming the input graph into a
computation graph which contains both proximity and structural information as
distinct type of edges. The resulted multi-relational graph has an enhanced
level of assortativity and, more importantly, preserves rich information from
the original graph. We then propose to run GNNs on this computation graph and
show that adaptively choosing between structure and proximity leads to improved
performance under diverse mixing. Empirically, we show the benefits of adopting
our transformation framework for semi-supervised node classification task on a
variety of real world graph learning benchmarks.","cs.LG,cs.SI"
"Reversible MCMC on Markov equivalence classes of sparse directed acyclic graphs. Graphical models are popular statistical tools which are used to represent
dependent or causal complex systems. Statistically equivalent causal or
directed graphical models are said to belong to a Markov equivalent class. It
is of great interest to describe and understand the space of such classes.
However, with currently known algorithms, sampling over such classes is only
feasible for graphs with fewer than approximately 20 vertices. In this paper,
we design reversible irreducible Markov chains on the space of Markov
equivalent classes by proposing a perfect set of operators that determine the
transitions of the Markov chain. The stationary distribution of a proposed
Markov chain has a closed form and can be computed easily. Specifically, we
construct a concrete perfect set of operators on sparse Markov equivalence
classes by introducing appropriate conditions on each possible operator.
Algorithms and their accelerated versions are provided to efficiently generate
Markov chains and to explore properties of Markov equivalence classes of sparse
directed acyclic graphs (DAGs) with thousands of vertices. We find
experimentally that in most Markov equivalence classes of sparse DAGs, (1) most
edges are directed, (2) most undirected subgraphs are small and (3) the number
of these undirected subgraphs grows approximately linearly with the number of
vertices. The article contains supplement arXiv:1303.0632,
http://dx.doi.org/10.1214/13-AOS1125SUPP","cs.DM,stat.ME,stat.ML"
"Benchmarking Energy-Conserving Neural Networks for Learning Dynamics from Data. The last few years have witnessed an increased interest in incorporating
physics-informed inductive bias in deep learning frameworks. In particular, a
growing volume of literature has been exploring ways to enforce energy
conservation while using neural networks for learning dynamics from observed
time-series data. In this work, we survey ten recently proposed
energy-conserving neural network models, including HNN, LNN, DeLaN, SymODEN,
CHNN, CLNN and their variants. We provide a compact derivation of the theory
behind these models and explain their similarities and differences. Their
performance are compared in 4 physical systems. We point out the possibility of
leveraging some of these energy-conserving models to design energy-based
controllers.","cs.AI,cs.LG,cs.SY,eess.SY,math.DS"
"Pair-Matching: Links Prediction with Adaptive Queries. The pair-matching problem appears in many applications where one wants to
discover good matches between pairs of entities or individuals. Formally, the
set of individuals is represented by the nodes of a graph where the edges,
unobserved at first, represent the good matches. The algorithm queries pairs of
nodes and observes the presence/absence of edges. Its goal is to discover as
many edges as possible with a fixed budget of queries. Pair-matching is a
particular instance of multi-armed bandit problem in which the arms are pairs
of individuals and the rewards are edges linking these pairs. This bandit
problem is non-standard though, as each arm can only be played once.
  Given this last constraint, sublinear regret can be expected only if the
graph presents some underlying structure. This paper shows that sublinear
regret is achievable in the case where the graph is generated according to a
Stochastic Block Model (SBM) with two communities. Optimal regret bounds are
computed for this pair-matching problem. They exhibit a phase transition
related to the Kesten-Stigum threshold for community detection in SBM. The
pair-matching problem is considered in the case where each node is constrained
to be sampled less than a given amount of times. We show how optimal regret
rates depend on this constraint. The paper is concluded by a conjecture
regarding the optimal regret when the number of communities is larger than 2.
Contrary to the two communities case, we argue that a statistical-computational
gap would appear in this problem.","05C80,62h30,68T05,cs.LG,math.ST,stat.ML,stat.TH"
"Heterogeneous Molecular Graph Neural Networks for Predicting Molecule Properties. As they carry great potential for modeling complex interactions, graph neural
network (GNN)-based methods have been widely used to predict quantum mechanical
properties of molecules. Most of the existing methods treat molecules as
molecular graphs in which atoms are modeled as nodes. They characterize each
atom's chemical environment by modeling its pairwise interactions with other
atoms in the molecule. Although these methods achieve a great success, limited
amount of works explicitly take many-body interactions, i.e., interactions
between three and more atoms, into consideration. In this paper, we introduce a
novel graph representation of molecules, heterogeneous molecular graph (HMG) in
which nodes and edges are of various types, to model many-body interactions.
HMGs have the potential to carry complex geometric information. To leverage the
rich information stored in HMGs for chemical prediction problems, we build
heterogeneous molecular graph neural networks (HMGNN) on the basis of a neural
message passing scheme. HMGNN incorporates global molecule representations and
an attention mechanism into the prediction process. The predictions of HMGNN
are invariant to translation and rotation of atom coordinates, and permutation
of atom indices. Our model achieves state-of-the-art performance in 9 out of 12
tasks on the QM9 dataset.","cs.LG,physics.comp-ph,stat.ML"
"Pedestrian Detection in Thermal Images using Saliency Maps. Thermal images are mainly used to detect the presence of people at night or
in bad lighting conditions, but perform poorly at daytime. To solve this
problem, most state-of-the-art techniques employ a fusion network that uses
features from paired thermal and color images. Instead, we propose to augment
thermal images with their saliency maps, to serve as an attention mechanism for
the pedestrian detector especially during daytime. We investigate how such an
approach results in improved performance for pedestrian detection using only
thermal images, eliminating the need for paired color images. For our
experiments, we train the Faster R-CNN for pedestrian detection and report the
added effect of saliency maps generated using static and deep methods (PiCA-Net
and R3-Net). Our best performing model results in an absolute reduction of miss
rate by 13.4% and 19.4% over the baseline in day and night images respectively.
We also annotate and release pixel level masks of pedestrians on a subset of
the KAIST Multispectral Pedestrian Detection dataset, which is a first publicly
available dataset for salient pedestrian detection.","68T45,I.2.10,cs.CV"
"Tractable Reinforcement Learning of Signal Temporal Logic Objectives. Signal temporal logic (STL) is an expressive language to specify time-bound
real-world robotic tasks and safety specifications. Recently, there has been an
interest in learning optimal policies to satisfy STL specifications via
reinforcement learning (RL). Learning to satisfy STL specifications often needs
a sufficient length of state history to compute reward and the next action. The
need for history results in exponential state-space growth for the learning
problem. Thus the learning problem becomes computationally intractable for most
real-world applications. In this paper, we propose a compact means to capture
state history in a new augmented state-space representation. An approximation
to the objective (maximizing probability of satisfaction) is proposed and
solved for in the new augmented state-space. We show the performance bound of
the approximate solution and compare it with the solution of an existing
technique via simulations.","cs.AI,cs.LG,cs.RO,stat.ML"
"A Graphical Model Formulation of Collaborative Filtering Neighbourhood Methods with Fast Maximum Entropy Training. Item neighbourhood methods for collaborative filtering learn a weighted graph
over the set of items, where each item is connected to those it is most similar
to. The prediction of a user's rating on an item is then given by that rating
of neighbouring items, weighted by their similarity. This paper presents a new
neighbourhood approach which we call item fields, whereby an undirected
graphical model is formed over the item graph. The resulting prediction rule is
a simple generalization of the classical approaches, which takes into account
non-local information in the graph, allowing its best results to be obtained
when using drastically fewer edges than other neighbourhood approaches. A fast
approximate maximum entropy training method based on the Bethe approximation is
presented, which uses a simple gradient ascent procedure. When using
precomputed sufficient statistics on the Movielens datasets, our method is
faster than maximum likelihood approaches by two orders of magnitude.","cs.IR,cs.LG,stat.ML"
"algcomparison: Comparing the Performance of Graphical Structure Learning Algorithms with TETRAD. In this report we describe a tool for comparing the performance of graphical
causal structure learning algorithms implemented in the TETRAD freeware suite
of causal analysis methods. Currently the tool is available as package in the
TETRAD source code (written in Java). Simulations can be done varying the
number of runs, sample sizes, and data modalities. Performance on this
simulated data can then be compared for a number of algorithms, with parameters
varied and with performance statistics as selected, producing a publishable
report. The package presented here may also be used to compare structure
learning methods across platforms and programming languages, i.e., to compare
algorithms implemented in TETRAD with those implemented in MATLAB, Python, or
R.","stat.CO,stat.ML"
"Super Resolution for Root Imaging. High-resolution cameras have become very helpful for plant phenotyping by
providing a mechanism for tasks such as target versus background
discrimination, and the measurement and analysis of fine-above-ground plant
attributes. However, the acquisition of high-resolution (HR) imagery of plant
roots is more challenging than above-ground data collection. Thus, an effective
super-resolution (SR) algorithm is desired for overcoming resolution
limitations of sensors, reducing storage space requirements, and boosting the
performance of later analysis, such as automatic segmentation. We propose a SR
framework for enhancing images of plant roots by using convolutional neural
networks (CNNs). We compare three alternatives for training the SR model: i)
training with non-plant-root images, ii) training with plant-root images, and
iii) pretraining the model with non-plant-root images and fine-tuning with
plant-root images. We demonstrate on a collection of publicly available
datasets that the SR models outperform the basic bicubic interpolation even
when trained with non-root datasets. Also, our segmentation experiments show
that high performance on this task can be achieved independently of the SNR.
Therefore, we conclude that the quality of the image enhancement depends on the
application.","cs.CV,q-bio.QM"
"MOFit: A Framework to reduce Obesity using Machine learning and IoT. From the past few years, due to advancements in technologies, the sedentary
living style in urban areas is at its peak. This results in individuals getting
a victim of obesity at an early age. There are various health impacts of
obesity like Diabetes, Heart disease, Blood pressure problems, and many more.
Machine learning from the past few years is showing its implications in all
expertise like forecasting, healthcare, medical imaging, sentiment analysis,
etc. In this work, we aim to provide a framework that uses machine learning
algorithms namely, Random Forest, Decision Tree, XGBoost, Extra Trees, and KNN
to train models that would help predict obesity levels (Classification),
Bodyweight, and fat percentage levels (Regression) using various parameters. We
also applied and compared various hyperparameter optimization (HPO) algorithms
such as Genetic algorithm, Random Search, Grid Search, Optuna to further
improve the accuracy of the models. The website framework contains various
other features like making customizable Diet plans, workout plans, and a
dashboard to track the progress. The framework is built using the Python Flask.
Furthermore, a weighing scale using the Internet of Things (IoT) is also
integrated into the framework to track calories and macronutrients from food
intake.","cs.AI,cs.CY,cs.LG"
"Learning Quantum Graphical Models using Constrained Gradient Descent on the Stiefel Manifold. Quantum graphical models (QGMs) extend the classical framework for reasoning
about uncertainty by incorporating the quantum mechanical view of probability.
Prior work on QGMs has focused on hidden quantum Markov models (HQMMs), which
can be formulated using quantum analogues of the sum rule and Bayes rule used
in classical graphical models. Despite the focus on developing the QGM
framework, there has been little progress in learning these models from data.
The existing state-of-the-art approach randomly initializes parameters and
iteratively finds unitary transformations that increase the likelihood of the
data. While this algorithm demonstrated theoretical strengths of HQMMs over
HMMs, it is slow and can only handle a small number of hidden states. In this
paper, we tackle the learning problem by solving a constrained optimization
problem on the Stiefel manifold using a well-known retraction-based algorithm.
We demonstrate that this approach is not only faster and yields better
solutions on several datasets, but also scales to larger models that were
prohibitively slow to train via the earlier method.","cs.LG,quant-ph,stat.ML"
"A Measure of Similarity in Textual Data Using Spearman's Rank Correlation Coefficient. In the last decade, many diverse advances have occurred in the field of
information extraction from data. Information extraction in its simplest form
takes place in computing environments, where structured data can be extracted
through a series of queries. The continuous expansion of quantities of data
have therefore provided an opportunity for knowledge extraction (KE) from a
textual document (TD). A typical problem of this kind is the extraction of
common characteristics and knowledge from a group of TDs, with the possibility
to group such similar TDs in a process known as clustering. In this paper we
present a technique for such KE among a group of TDs related to the common
characteristics and meaning of their content. Our technique is based on the
Spearman's Rank Correlation Coefficient (SRCC), for which the conducted
experiments have proven to be comprehensive measure to achieve a high-quality
KE.","cs.CL,cs.IR,cs.LG,stat.ML"
"Segmentation and Restoration of Images on Surfaces by Parametric Active Contours with Topology Changes. In this article, a new method for segmentation and restoration of images on
two-dimensional surfaces is given. Active contour models for image segmentation
are extended to images on surfaces. The evolving curves on the surfaces are
mathematically described using a parametric approach. For image restoration, a
diffusion equation with Neumann boundary conditions is solved in a
postprocessing step in the individual regions. Numerical schemes are presented
which allow to efficiently compute segmentations and denoised versions of
images on surfaces. Also topology changes of the evolving curves are detected
and performed using a fast sub-routine. Finally, several experiments are
presented where the developed methods are applied on different artificial and
real images defined on different surfaces.","cs.CV,math.AP,math.NA"
"Connectivity-Optimized Representation Learning via Persistent Homology. We study the problem of learning representations with controllable
connectivity properties. This is beneficial in situations when the imposed
structure can be leveraged upstream. In particular, we control the connectivity
of an autoencoder's latent space via a novel type of loss, operating on
information from persistent homology. Under mild conditions, this loss is
differentiable and we present a theoretical analysis of the properties induced
by the loss. We choose one-class learning as our upstream task and demonstrate
that the imposed structure enables informed parameter selection for modeling
the in-class distribution via kernel density estimators. Evaluated on computer
vision data, these one-class models exhibit competitive performance and, in a
low sample size regime, outperform other methods by a large margin. Notably,
our results indicate that a single autoencoder, trained on auxiliary
(unlabeled) data, yields a mapping into latent space that can be reused across
datasets for one-class learning.","cs.CG,cs.LG,math.AT,stat.ML"
"Pareto Deterministic Policy Gradients and Its Application in 5G Massive MIMO Networks. In this paper, we consider jointly optimizing cell load balance and network
throughput via a reinforcement learning (RL) approach, where inter-cell
handover (i.e., user association assignment) and massive MIMO antenna tilting
are configured as the RL policy to learn. Our rationale behind using RL is to
circumvent the challenges of analytically modeling user mobility and network
dynamics. To accomplish this joint optimization, we integrate vector rewards
into the RL value network and conduct RL action via a separate policy network.
We name this method as Pareto deterministic policy gradients (PDPG). It is an
actor-critic, model-free and deterministic policy algorithm which can handle
the coupling objectives with the following two merits: 1) It solves the
optimization via leveraging the degree of freedom of vector reward as opposed
to choosing handcrafted scalar-reward; 2) Cross-validation over multiple
policies can be significantly reduced. Accordingly, the RL enabled network
behaves in a self-organized way: It learns out the underlying user mobility
through measurement history to proactively operate handover and antenna tilt
without environment assumptions. Our numerical evaluation demonstrates that the
introduced RL method outperforms scalar-reward based approaches. Meanwhile, to
be self-contained, an ideal static optimization based brute-force search solver
is included as a benchmark. The comparison shows that the RL approach performs
as well as this ideal strategy, though the former one is constrained with
limited environment observations and lower action frequency, whereas the latter
ones have full access to the user mobility. The convergence of our introduced
approach is also tested under different user mobility environment based on our
measurement data from a real scenario.","cs.LG,cs.NI"
"Learning to Incentivize Other Learning Agents. The challenge of developing powerful and general Reinforcement Learning (RL)
agents has received increasing attention in recent years. Much of this effort
has focused on the single-agent setting, in which an agent maximizes a
predefined extrinsic reward function. However, a long-term question inevitably
arises: how will such independent agents cooperate when they are continually
learning and acting in a shared multi-agent environment? Observing that humans
often provide incentives to influence others' behavior, we propose to equip
each RL agent in a multi-agent environment with the ability to give rewards
directly to other agents, using a learned incentive function. Each agent learns
its own incentive function by explicitly accounting for its impact on the
learning of recipients and, through them, the impact on its own extrinsic
objective. We demonstrate in experiments that such agents significantly
outperform standard RL and opponent-shaping agents in challenging general-sum
Markov games, often by finding a near-optimal division of labor. Our work
points toward more opportunities and challenges along the path to ensure the
common good in a multi-agent future.","cs.GT,cs.LG,cs.MA,stat.ML"
"Change your singer: a transfer learning generative adversarial framework for song to song conversion. Have you ever wondered how a song might sound if performed by a different
artist? In this work, we propose SCM-GAN, an end-to-end non-parallel song
conversion system powered by generative adversarial and transfer learning that
allows users to listen to a selected target singer singing any song. SCM-GAN
first separates songs into vocals and instrumental music using a U-Net network,
then converts the vocal segments to the target singer using advanced
CycleGAN-VC, before merging the converted vocals with their corresponding
background music. SCM-GAN is first initialized with feature representations
learned from a state-of-the-art voice-to-voice conversion and then trained on a
dataset of non-parallel songs. Furthermore, SCM-GAN is evaluated against a set
of metrics including global variance GV and modulation spectra MS on the 24
Mel-cepstral coefficients (MCEPs). Transfer learning improves the GV by 35% and
the MS by 13% on average. A subjective comparison is conducted to test the user
satisfaction with the quality and the naturalness of the conversion. Results
show above par similarity between SCM-GAN's output and the target (70\% on
average) as well as great naturalness of the converted songs.","cs.LG,cs.SD,eess.AS,stat.ML"
"Inverse-Dirichlet Weighting Enables Reliable Training of Physics Informed Neural Networks. We characterize and remedy a failure mode that may arise from multi-scale
dynamics with scale imbalances during training of deep neural networks, such as
Physics Informed Neural Networks (PINNs). PINNs are popular machine-learning
templates that allow for seamless integration of physical equation models with
data. Their training amounts to solving an optimization problem over a weighted
sum of data-fidelity and equation-fidelity objectives. Conflicts between
objectives can arise from scale imbalances, heteroscedasticity in the data,
stiffness of the physical equation, or from catastrophic interference during
sequential training. We explain the training pathology arising from this and
propose a simple yet effective inverse-Dirichlet weighting strategy to
alleviate the issue. We compare with Sobolev training of neural networks,
providing the baseline of analytically $\boldsymbol{\epsilon}$-optimal
training. We demonstrate the effectiveness of inverse-Dirichlet weighting in
various applications, including a multi-scale model of active turbulence, where
we show orders of magnitude improvement in accuracy and convergence over
conventional PINN training. For inverse modeling using sequential training, we
find that inverse-Dirichlet weighting protects a PINN against catastrophic
forgetting.","cs.LG,cs.NA,math.NA,physics.comp-ph,q-bio.QM"
"Global registration of multiple point clouds using semidefinite programming. Consider $N$ points in $\mathbb{R}^d$ and $M$ local coordinate systems that
are related through unknown rigid transforms. For each point we are given
(possibly noisy) measurements of its local coordinates in some of the
coordinate systems. Alternatively, for each coordinate system, we observe the
coordinates of a subset of the points. The problem of estimating the global
coordinates of the $N$ points (up to a rigid transform) from such measurements
comes up in distributed approaches to molecular conformation and sensor network
localization, and also in computer vision and graphics.
  The least-squares formulation of this problem, though non-convex, has a well
known closed-form solution when $M=2$ (based on the singular value
decomposition). However, no closed form solution is known for $M\geq 3$.
  In this paper, we demonstrate how the least-squares formulation can be
relaxed into a convex program, namely a semidefinite program (SDP). By setting
up connections between the uniqueness of this SDP and results from rigidity
theory, we prove conditions for exact and stable recovery for the SDP
relaxation. In particular, we prove that the SDP relaxation can guarantee
recovery under more adversarial conditions compared to earlier proposed
spectral relaxations, and derive error bounds for the registration error
incurred by the SDP relaxation.
  We also present results of numerical experiments on simulated data to confirm
the theoretical findings. We empirically demonstrate that (a) unlike the
spectral relaxation, the relaxation gap is mostly zero for the semidefinite
program (i.e., we are able to solve the original non-convex least-squares
problem) up to a certain noise threshold, and (b) the semidefinite program
performs significantly better than spectral and manifold-optimization methods,
particularly at large noise levels.","05C50,52C25,90C22,cs.CV,cs.NA,math.NA,math.OC"
"Fast Convolutional Nets With fbfft: A GPU Performance Evaluation. We examine the performance profile of Convolutional Neural Network training
on the current generation of NVIDIA Graphics Processing Units. We introduce two
new Fast Fourier Transform convolution implementations: one based on NVIDIA's
cuFFT library, and another based on a Facebook authored FFT implementation,
fbfft, that provides significant speedups over cuFFT (over 1.5x) for whole
CNNs. Both of these convolution implementations are available in open source,
and are faster than NVIDIA's cuDNN implementation for many common convolutional
layers (up to 23.5x for some synthetic kernel configurations). We discuss
different performance regimes of convolutions, comparing areas where
straightforward time domain convolutions outperform Fourier frequency domain
convolutions. Details on algorithmic applications of NVIDIA GPU hardware
specifics in the implementation of fbfft are also provided.","cs.DC,cs.LG,cs.NE"
"Representational Rnyi heterogeneity. A discrete system's heterogeneity is measured by the R\'enyi heterogeneity
family of indices (also known as Hill numbers or Hannah--Kay indices), whose
units are {the numbers equivalent}. Unfortunately, numbers equivalent
heterogeneity measures for non-categorical data require {a priori} (A)
categorical partitioning and (B) pairwise distance measurement on the
observable data space, thereby precluding application to problems with
ill-defined categories or where semantically relevant features must be learned
as abstractions from some data. We thus introduce representational R\'enyi
heterogeneity (RRH), which transforms an observable domain onto a latent space
upon which the R\'enyi heterogeneity is both tractable and semantically
relevant. This method requires neither {a priori} binning nor definition of a
distance function on the observable space. We show that RRH can generalize
existing biodiversity and economic equality indices. Compared with existing
indices on a beta-mixture distribution, we show that RRH responds more
appropriately to changes in mixture component separation and weighting.
Finally, we demonstrate the measurement of RRH in a set of natural images, with
respect to abstract representations learned by a deep neural network. The RRH
approach will further enable heterogeneity measurement in disciplines whose
data do not easily conform to the assumptions of existing indices.","cs.LG,q-bio.QM,stat.ML"
"Learning and Sampling of Atomic Interventions from Observations. We study the problem of efficiently estimating the effect of an intervention
on a single variable (atomic interventions) using observational samples in a
causal Bayesian network. Our goal is to give algorithms that are efficient in
both time and sample complexity in a non-parametric setting.
  Tian and Pearl (AAAI `02) have exactly characterized the class of causal
graphs for which causal effects of atomic interventions can be identified from
observational data. We make their result quantitative. Suppose P is a causal
model on a set $\vec{V}$ of n observable variables with respect to a given
causal graph G with observable distribution $P$. Let $P_x$ denote the
interventional distribution over the observables with respect to an
intervention of a designated variable X with x. Assuming that $G$ has bounded
in-degree, bounded c-components ($k$), and that the observational distribution
is identifiable and satisfies certain strong positivity condition, we give an
algorithm that takes $m=\tilde{O}(n\epsilon^{-2})$ samples from $P$ and $O(mn)$
time, and outputs with high probability a description of a distribution
$\hat{P}$ such that $d_{\mathrm{TV}}(P_x, \hat{P}) \leq \epsilon$, and:
  1. [Evaluation] the description can return in $O(n)$ time the probability
$\hat{P}(\vec{v})$ for any assignment $\vec{v}$ to $\vec{V}$
  2. [Generation] the description can return an iid sample from $\hat{P}$ in
$O(n)$ time.
  We also show lower bounds for the sample complexity showing that our sample
complexity has an optimal dependence on the parameters $n$ and $\epsilon$, as
well as if $k=1$ on the strong positivity parameter.","I.2.6,cs.AI,cs.DS,cs.LG,stat.ML"
"Widget Captioning: Generating Natural Language Description for Mobile User Interface Elements. Natural language descriptions of user interface (UI) elements such as
alternative text are crucial for accessibility and language-based interaction
in general. Yet, these descriptions are constantly missing in mobile UIs. We
propose widget captioning, a novel task for automatically generating language
descriptions for UI elements from multimodal input including both the image and
the structural representations of user interfaces. We collected a large-scale
dataset for widget captioning with crowdsourcing. Our dataset contains 162,859
language phrases created by human workers for annotating 61,285 UI elements
across 21,750 unique UI screens. We thoroughly analyze the dataset, and train
and evaluate a set of deep model configurations to investigate how each feature
modality as well as the choice of learning strategies impact the quality of
predicted captions. The task formulation and the dataset as well as our
benchmark models contribute a solid basis for this novel multimodal captioning
task that connects language and user interfaces.","cs.AI,cs.CL,cs.HC,cs.LG"
"Multi-view Graph Contrastive Representation Learning for Drug-Drug Interaction Prediction. Drug-drug interaction(DDI) prediction is an important task in the medical
health machine learning community. This study presents a new method, multi-view
graph contrastive representation learning for drug-drug interaction prediction,
MIRACLE for brevity, to capture inter-view molecule structure and intra-view
interactions between molecules simultaneously. MIRACLE treats a DDI network as
a multi-view graph where each node in the interaction graph itself is a drug
molecular graph instance. We use GCNs and bond-aware attentive message passing
networks to encode DDI relationships and drug molecular graphs in the MIRACLE
learning stage, respectively. Also, we propose a novel unsupervised contrastive
learning component to balance and integrate the multi-view information.
Comprehensive experiments on multiple real datasets show that MIRACLE
outperforms the state-of-the-art DDI prediction models consistently.","G.2.6; I.2.2,cs.AI,cs.LG"
"Polynomial magic! Hermite polynomials for private data generation. Kernel mean embedding is a useful tool to compare probability measures.
Despite its usefulness, kernel mean embedding considers infinite-dimensional
features, which are challenging to handle in the context of differentially
private data generation. A recent work proposes to approximate the kernel mean
embedding of data distribution using finite-dimensional random features, where
the sensitivity of the features becomes analytically tractable. More
importantly, this approach significantly reduces the privacy cost, compared to
other known privatization methods (e.g., DP-SGD), as the approximate kernel
mean embedding of the data distribution is privatized only once and can then be
repeatedly used during training of a generator without incurring any further
privacy cost. However, the required number of random features is excessively
high, often ten thousand to a hundred thousand, which worsens the sensitivity
of the approximate kernel mean embedding. To improve the sensitivity, we
propose to replace random features with Hermite polynomial features. Unlike the
random features, the Hermite polynomial features are ordered, where the
features at the low orders contain more information on the distribution than
those at the high orders. Hence, a relatively low order of Hermite polynomial
features can more accurately approximate the mean embedding of the data
distribution compared to a significantly higher number of random features. As a
result, using the Hermite polynomial features, we significantly improve the
privacy-accuracy trade-off, reflected in the high quality and diversity of the
generated data, when tested on several heterogeneous tabular datasets, as well
as several image benchmark datasets.","cs.CR,cs.LG,stat.ML"
"A Comparative Study of Using Spatial-Temporal Graph Convolutional Networks for Predicting Availability in Bike Sharing Schemes. Accurately forecasting transportation demand is crucial for efficient urban
traffic guidance, control and management. One solution to enhance the level of
prediction accuracy is to leverage graph convolutional networks (GCN), a neural
network based modelling approach with the ability to process data contained in
graph based structures. As a powerful extension of GCN, a spatial-temporal
graph convolutional network (ST-GCN) aims to capture the relationship of data
contained in the graphical nodes across both spatial and temporal dimensions,
which presents a novel deep learning paradigm for the analysis of complex
time-series data that also involves spatial information as present in
transportation use cases. In this paper, we present an Attention-based ST-GCN
(AST-GCN) for predicting the number of available bikes in bike-sharing systems
in cities, where the attention-based mechanism is introduced to further improve
the performance of an ST-GCN. Furthermore, we also discuss the impacts of
different modelling methods of adjacency matrices on the proposed architecture.
Our experimental results are presented using two real-world datasets,
Dublinbikes and NYC-Citi Bike, to illustrate the efficacy of our proposed model
which outperforms the majority of existing approaches.","cs.LG,cs.SY,eess.SY"
"Complex Momentum for Optimization in Games. We generalize gradient descent with momentum for optimization in
differentiable games to have complex-valued momentum. We give theoretical
motivation for our method by proving convergence on bilinear zero-sum games for
simultaneous and alternating updates. Our method gives real-valued parameter
updates, making it a drop-in replacement for standard optimizers. We
empirically demonstrate that complex-valued momentum can improve convergence in
realistic adversarial games - like generative adversarial networks - by showing
we can find better solutions with an almost identical computational cost. We
also show a practical generalization to a complex-valued Adam variant, which we
use to train BigGAN to better inception scores on CIFAR-10.","cs.GT,cs.LG"
"Small Object Detection for Near Real-Time Egocentric Perception in a Manual Assembly Scenario. Detecting small objects in video streams of head-worn augmented reality
devices in near real-time is a huge challenge: training data is typically
scarce, the input video stream can be of limited quality, and small objects are
notoriously hard to detect. In industrial scenarios, however, it is often
possible to leverage contextual knowledge for the detection of small objects.
Furthermore, CAD data of objects are typically available and can be used to
generate synthetic training data. We describe a near real-time small object
detection pipeline for egocentric perception in a manual assembly scenario: We
generate a training data set based on CAD data and realistic backgrounds in
Unity. We then train a YOLOv4 model for a two-stage detection process: First,
the context is recognized, then the small object of interest is detected. We
evaluate our pipeline on the augmented reality device Microsoft Hololens 2.","cs.AI,cs.CV,cs.HC"
"Convolutional Transformer based Dual Discriminator Generative Adversarial Networks for Video Anomaly Detection. Detecting abnormal activities in real-world surveillance videos is an
important yet challenging task as the prior knowledge about video anomalies is
usually limited or unavailable. Despite that many approaches have been
developed to resolve this problem, few of them can capture the normal
spatio-temporal patterns effectively and efficiently. Moreover, existing works
seldom explicitly consider the local consistency at frame level and global
coherence of temporal dynamics in video sequences. To this end, we propose
Convolutional Transformer based Dual Discriminator Generative Adversarial
Networks (CT-D2GAN) to perform unsupervised video anomaly detection.
Specifically, we first present a convolutional transformer to perform future
frame prediction. It contains three key components, i.e., a convolutional
encoder to capture the spatial information of the input video clips, a temporal
self-attention module to encode the temporal dynamics, and a convolutional
decoder to integrate spatio-temporal features and predict the future frame.
Next, a dual discriminator based adversarial training procedure, which jointly
considers an image discriminator that can maintain the local consistency at
frame-level and a video discriminator that can enforce the global coherence of
temporal dynamics, is employed to enhance the future frame prediction. Finally,
the prediction error is used to identify abnormal video frames. Thoroughly
empirical studies on three public video anomaly detection datasets, i.e., UCSD
Ped2, CUHK Avenue, and Shanghai Tech Campus, demonstrate the effectiveness of
the proposed adversarial spatio-temporal modeling framework.","68T45,I.4.8; I.4.9; I.2.10,cs.AI,cs.CV,cs.LG,cs.MM"
"Energy Efficient Edge Computing: When Lyapunov Meets Distributed Reinforcement Learning. In this work, we study the problem of energy-efficient computation offloading
enabled by edge computing. In the considered scenario, multiple users
simultaneously compete for limited radio and edge computing resources to get
offloaded tasks processed under a delay constraint, with the possibility of
exploiting low power sleep modes at all network nodes. The radio resource
allocation takes into account inter- and intra-cell interference, and the duty
cycles of the radio and computing equipment have to be jointly optimized to
minimize the overall energy consumption. To address this issue, we formulate
the underlying problem as a dynamic long-term optimization. Then, based on
Lyapunov stochastic optimization tools, we decouple the formulated problem into
a CPU scheduling problem and a radio resource allocation problem to be solved
in a per-slot basis. Whereas the first one can be optimally and efficiently
solved using a fast iterative algorithm, the second one is solved using
distributed multi-agent reinforcement learning due to its non-convexity and
NP-hardness. The resulting framework achieves up to 96.5% performance of the
optimal strategy based on exhaustive search, while drastically reducing
complexity. The proposed solution also allows to increase the network's energy
efficiency compared to a benchmark heuristic approach.","cs.AI,cs.LG,cs.MA,cs.NI"
"Functional Nonlinear Sparse Models. Signal processing is rich in inherently continuous and often nonlinear
applications, such as spectral estimation, optical imaging, and
super-resolution microscopy, in which sparsity plays a key role in obtaining
state-of-the-art results. Coping with the infinite dimensionality and
non-convexity of these problems typically involves discretization and convex
relaxations, e.g., using atomic norms. Nevertheless, grid mismatch and other
coherence issues often lead to discretized versions of sparse signals that are
not sparse. Even if they are, recovering sparse solutions using convex
relaxations requires assumptions that may be hard to meet in practice. What is
more, problems involving nonlinear measurements remain non-convex even after
relaxing the sparsity objective. We address these issues by directly tackling
the continuous, nonlinear problem cast as a sparse functional optimization
program. We prove that when these problems are non-atomic, they have no duality
gap and can therefore be solved efficiently using duality and~(stochastic)
convex optimization methods. We illustrate the wide range of applications of
this approach by formulating and solving problems from nonlinear spectral
estimation and robust classification.","cs.LG,eess.SP,math.OC,stat.ML"
"RG-Flow: A hierarchical and explainable flow model based on renormalization group and sparse prior. Flow-based generative models have become an important class of unsupervised
learning approaches. In this work, we incorporate the key idea of
renormalization group (RG) and sparse prior distribution to design a
hierarchical flow-based generative model, called RG-Flow, which can separate
information at different scales of images with disentangled representations at
each scale. We demonstrate our method mainly on the CelebA dataset and show
that the disentangled representations at different scales enable semantic
manipulation and style mixing of the images. To visualize the latent
representations, we introduce receptive fields for flow-based models and find
that the receptive fields learned by RG-Flow are similar to those in
convolutional neural networks. In addition, we replace the widely adopted
Gaussian prior distribution by a sparse prior distribution to further enhance
the disentanglement of representations. From a theoretical perspective, the
proposed method has $O(\log L)$ complexity for image inpainting compared to
previous generative models with $O(L^2)$ complexity.","cond-mat.dis-nn,cs.AI,cs.CV,cs.LG,stat.ML"
"Deep Learning for 2D and 3D Rotatable Data: An Overview of Methods. One of the reasons for the success of convolutional networks is their
equivariance/invariance under translations. However, rotatable data such as
molecules, living cells, everyday objects, or galaxies require processing with
equivariance/invariance under rotations in cases where the rotation of the
coordinate system does not affect the meaning of the data (e.g. object
classification). On the other hand, estimation/processing of rotations is
necessary in cases where rotations are important (e.g. motion estimation).
There has been recent progress in methods and theory in all these regards. Here
we provide an overview of existing methods, both for 2D and 3D rotations (and
translations), and identify commonalities and links between them, in the hope
that our insights will be useful for choosing and perfecting the methods.","62H35,62M45,65D18,68T45,68U10,I.2.6; I.5.1; G.3,cs.CV,cs.LG,cs.NE,stat.ML"
"Understanding Unequal Gender Classification Accuracy from Face Images. Recent work shows unequal performance of commercial face classification
services in the gender classification task across intersectional groups defined
by skin type and gender. Accuracy on dark-skinned females is significantly
worse than on any other group. In this paper, we conduct several analyses to
try to uncover the reason for this gap. The main finding, perhaps surprisingly,
is that skin type is not the driver. This conclusion is reached via stability
experiments that vary an image's skin type via color-theoretic methods, namely
luminance mode-shift and optimal transport. A second suspect, hair length, is
also shown not to be the driver via experiments on face images cropped to
exclude the hair. Finally, using contrastive post-hoc explanation techniques
for neural networks, we bring forth evidence suggesting that differences in
lip, eye and cheek structure across ethnicity lead to the differences. Further,
lip and eye makeup are seen as strong predictors for a female face, which is a
troubling propagation of a gender stereotype.","cs.CV,cs.CY,stat.ML"
"Almost Tight Approximation Algorithms for Explainable Clustering. Recently, due to an increasing interest for transparency in artificial
intelligence, several methods of explainable machine learning have been
developed with the simultaneous goal of accuracy and interpretability by
humans. In this paper, we study a recent framework of explainable clustering
first suggested by Dasgupta et al.~\cite{dasgupta2020explainable}.
Specifically, we focus on the $k$-means and $k$-medians problems and provide
nearly tight upper and lower bounds.
  First, we provide an $O(\log k \log \log k)$-approximation algorithm for
explainable $k$-medians, improving on the best known algorithm of
$O(k)$~\cite{dasgupta2020explainable} and nearly matching the known
$\Omega(\log k)$ lower bound~\cite{dasgupta2020explainable}. In addition, in
low-dimensional spaces $d \ll \log k$, we show that our algorithm also provides
an $O(d \log^2 d)$-approximate solution for explainable $k$-medians. This
improves over the best known bound of $O(d \log k)$ for low
dimensions~\cite{laber2021explainable}, and is a constant for constant
dimensional spaces. To complement this, we show a nearly matching $\Omega(d)$
lower bound. Next, we study the $k$-means problem in this context and provide
an $O(k \log k)$-approximation algorithm for explainable $k$-means, improving
over the $O(k^2)$ bound of Dasgupta et al. and the $O(d k \log k)$ bound of
\cite{laber2021explainable}. To complement this we provide an almost tight
$\Omega(k)$ lower bound, improving over the $\Omega(\log k)$ lower bound of
Dasgupta et al. Given an approximate solution to the classic $k$-means and
$k$-medians, our algorithm for $k$-medians runs in time $O(kd \log^2 k )$ and
our algorithm for $k$-means runs in time $ O(k^2 d)$.","cs.DS,cs.LG"
"FathomNet: An underwater image training database for ocean exploration and discovery. Thousands of hours of marine video data are collected annually from remotely
operated vehicles (ROVs) and other underwater assets. However, current manual
methods of analysis impede the full utilization of collected data for real time
algorithms for ROV and large biodiversity analyses. FathomNet is a novel
baseline image training set, optimized to accelerate development of modern,
intelligent, and automated analysis of underwater imagery. Our seed data set
consists of an expertly annotated and continuously maintained database with
more than 26,000 hours of videotape, 6.8 million annotations, and 4,349 terms
in the knowledge base. FathomNet leverages this data set by providing imagery,
localizations, and class labels of underwater concepts in order to enable
machine learning algorithm development. To date, there are more than 80,000
images and 106,000 localizations for 233 different classes, including midwater
and benthic organisms. Our experiments consisted of training various deep
learning algorithms with approaches to address weakly supervised localization,
image labeling, object detection and classification which prove to be
promising. While we find quality results on prediction for this new dataset,
our results indicate that we are ultimately in need of a larger data set for
ocean exploration.","cs.CV,cs.DB"
"Knowledge-aware Contrastive Molecular Graph Learning. Leveraging domain knowledge including fingerprints and functional groups in
molecular representation learning is crucial for chemical property prediction
and drug discovery. When modeling the relation between graph structure and
molecular properties implicitly, existing works can hardly capture structural
or property changes and complex structure, with much smaller atom vocabulary
and highly frequent atoms. In this paper, we propose the Contrastive
Knowledge-aware GNN (CKGNN) for self-supervised molecular representation
learning to fuse domain knowledge into molecular graph representation. We
explicitly encode domain knowledge via knowledge-aware molecular encoder under
the contrastive learning framework, ensuring that the generated molecular
embeddings equipped with chemical domain knowledge to distinguish molecules
with similar chemical formula but dissimilar functions. Extensive experiments
on 8 public datasets demonstrate the effectiveness of our model with a 6\%
absolute improvement on average against strong competitors. Ablation study and
further investigation also verify the best of both worlds: incorporation of
chemical domain knowledge into self-supervised learning.","cs.LG,q-bio.QM"
"Scale-invariant representation of machine learning. The success of machine learning stems from its structured data
representation. Similar data have close representation as compressed codes for
classification or emerged labels for clustering. We observe that the frequency
of the internal representation follows power laws in both supervised and
unsupervised learning. The scale-invariant distribution implies that machine
learning largely compresses frequent typical data, and at the same time,
differentiates many atypical data as outliers. In this study, we derive how the
power laws can naturally arise in machine learning. In terms of information
theory, the scale-invariant representation corresponds to a maximally uncertain
data grouping among possible representations that guarantee pre-specified
learning accuracy.","cs.IT,cs.LG,math.IT,physics.data-an"
"NEU: A Meta-Algorithm for Universal UAP-Invariant Feature Representation. Effective feature representation is key to the predictive performance of any
algorithm. This paper introduces a meta-procedure, called Non-Euclidean
Upgrading (NEU), which learns feature maps that are expressive enough to embed
the universal approximation property (UAP) into most model classes while only
outputting feature maps that preserve any model class's UAP. We show that NEU
can learn any feature map with these two properties if that feature map is
asymptotically deformable into the identity. We also find that the
feature-representations learned by NEU are always submanifolds of the feature
space. NEU's properties are derived from a new deep neural model that is
universal amongst all orientation-preserving homeomorphisms on the input space.
We derive qualitative and quantitative approximation guarantees for this
architecture. We quantify the number of parameters required for this new
architecture to memorize any set of input-output pairs while simultaneously
fixing every point of the input space lying outside some compact set, and we
quantify the size of this set as a function of our model's depth. Moreover, we
show that no deep feed-forward network with commonly used activation function
has all these properties. NEU's performance is evaluated against competing
machine learning methods on various regression and dimension reduction tasks
both with financial and simulated data.","57-08,62G08,62H25,65D15,68T07,68T30,91G80,cs.LG,cs.NA,math.NA,math.PR,q-fin.CP,stat.ML"
"Baking Neural Radiance Fields for Real-Time View Synthesis. Neural volumetric representations such as Neural Radiance Fields (NeRF) have
emerged as a compelling technique for learning to represent 3D scenes from
images with the goal of rendering photorealistic images of the scene from
unobserved viewpoints. However, NeRF's computational requirements are
prohibitive for real-time applications: rendering views from a trained NeRF
requires querying a multilayer perceptron (MLP) hundreds of times per ray. We
present a method to train a NeRF, then precompute and store (i.e. ""bake"") it as
a novel representation called a Sparse Neural Radiance Grid (SNeRG) that
enables real-time rendering on commodity hardware. To achieve this, we
introduce 1) a reformulation of NeRF's architecture, and 2) a sparse voxel grid
representation with learned feature vectors. The resulting scene representation
retains NeRF's ability to render fine geometric details and view-dependent
appearance, is compact (averaging less than 90 MB per scene), and can be
rendered in real-time (higher than 30 frames per second on a laptop GPU).
Actual screen captures are shown in our video.","cs.CV,cs.GR"
"Towards Learning Controllable Representations of Physical Systems. Learned representations of dynamical systems reduce dimensionality,
potentially supporting downstream reinforcement learning (RL). However, no
established methods predict a representation's suitability for control and
evaluation is largely done via downstream RL performance, slowing
representation design. Towards a principled evaluation of representations for
control, we consider the relationship between the true state and the
corresponding representations, proposing that ideally each representation
corresponds to a unique true state. This motivates two metrics: temporal
smoothness and high mutual information between true state/representation. These
metrics are related to established representation objectives, and studied on
Lagrangian systems where true state, information requirements, and statistical
properties of the state can be formalized for a broad class of systems. These
metrics are shown to predict reinforcement learning performance in a simulated
peg-in-hole task when comparing variants of autoencoder-based representations.","cs.LG,cs.RO,cs.SY,eess.SY"
"Directional Graph Networks. The lack of anisotropic kernels in graph neural networks (GNNs) strongly
limits their expressiveness, contributing to well-known issues such as
over-smoothing. To overcome this limitation, we propose the first globally
consistent anisotropic kernels for GNNs, allowing for graph convolutions that
are defined according to topologicaly-derived directional flows. First, by
defining a vector field in the graph, we develop a method of applying
directional derivatives and smoothing by projecting node-specific messages into
the field. Then, we propose the use of the Laplacian eigenvectors as such
vector field. We show that the method generalizes CNNs on an $n$-dimensional
grid and is provably more discriminative than standard GNNs regarding the
Weisfeiler-Lehman 1-WL test. We evaluate our method on different standard
benchmarks and see a relative error reduction of 8% on the CIFAR10 graph
dataset and 11% to 32% on the molecular ZINC dataset, and a relative increase
in precision of 1.6% on the MolPCBA dataset. An important outcome of this work
is that it enables graph networks to embed directions in an unsupervised way,
thus allowing a better representation of the anisotropic features in different
physical or biological problems.","cs.CG,cs.LG,cs.SI"
"Continuous-in-Depth Neural Networks. Recent work has attempted to interpret residual networks (ResNets) as one
step of a forward Euler discretization of an ordinary differential equation,
focusing mainly on syntactic algebraic similarities between the two systems.
Discrete dynamical integrators of continuous dynamical systems, however, have a
much richer structure. We first show that ResNets fail to be meaningful
dynamical integrators in this richer sense. We then demonstrate that neural
network models can learn to represent continuous dynamical systems, with this
richer structure and properties, by embedding them into higher-order numerical
integration schemes, such as the Runge Kutta schemes. Based on these insights,
we introduce ContinuousNet as a continuous-in-depth generalization of ResNet
architectures. ContinuousNets exhibit an invariance to the particular
computational graph manifestation. That is, the continuous-in-depth model can
be evaluated with different discrete time step sizes, which changes the number
of layers, and different numerical integration schemes, which changes the graph
connectivity. We show that this can be used to develop an incremental-in-depth
training scheme that improves model quality, while significantly decreasing
training time. We also show that, once trained, the number of units in the
computational graph can even be decreased, for faster inference with
little-to-no accuracy drop.","cs.LG,math.DS,stat.ML"
"Visual Sensor Network Reconfiguration with Deep Reinforcement Learning. We present an approach for reconfiguration of dynamic visual sensor networks
with deep reinforcement learning (RL). Our RL agent uses a modified
asynchronous advantage actor-critic framework and the recently proposed
Relational Network module at the foundation of its network architecture. To
address the issue of sample inefficiency in current approaches to model-free
reinforcement learning, we train our system in an abstract simulation
environment that represents inputs from a dynamic scene. Our system is
validated using inputs from a real-world scenario and preexisting object
detection and tracking algorithms.","68T05,cs.AI,cs.CV,cs.LG,stat.ML"
"Characterizing Impacts of Heterogeneity in Federated Learning upon Large-Scale Smartphone Data. Federated learning (FL) is an emerging, privacy-preserving machine learning
paradigm, drawing tremendous attention in both academia and industry. A unique
characteristic of FL is heterogeneity, which resides in the various hardware
specifications and dynamic states across the participating devices.
Theoretically, heterogeneity can exert a huge influence on the FL training
process, e.g., causing a device unavailable for training or unable to upload
its model updates. Unfortunately, these impacts have never been systematically
studied and quantified in existing FL literature.
  In this paper, we carry out the first empirical study to characterize the
impacts of heterogeneity in FL. We collect large-scale data from 136k
smartphones that can faithfully reflect heterogeneity in real-world settings.
We also build a heterogeneity-aware FL platform that complies with the standard
FL protocol but with heterogeneity in consideration. Based on the data and the
platform, we conduct extensive experiments to compare the performance of
state-of-the-art FL algorithms under heterogeneity-aware and
heterogeneity-unaware settings. Results show that heterogeneity causes
non-trivial performance degradation in FL, including up to 9.2% accuracy drop,
2.32x lengthened training time, and undermined fairness. Furthermore, we
analyze potential impact factors and find that device failure and participant
bias are two potential factors for performance degradation. Our study provides
insightful implications for FL practitioners. On the one hand, our findings
suggest that FL algorithm designers consider necessary heterogeneity during the
evaluation. On the other hand, our findings urge system providers to design
specific mechanisms to mitigate the impacts of heterogeneity.","cs.DC,cs.LG,stat.ML"
"Continuum Limit of Lipschitz Learning on Graphs. Tackling semi-supervised learning problems with graph-based methods have
become a trend in recent years since graphs can represent all kinds of data and
provide a suitable framework for studying continuum limits, e.g., of
differential operators. A popular strategy here is $p$-Laplacian learning,
which poses a smoothness condition on the sought inference function on the set
of unlabeled data. For $p<\infty$ continuum limits of this approach were
studied using tools from $\Gamma$-convergence. For the case $p=\infty$, which
is referred to as Lipschitz learning, continuum limits of the related
infinity-Laplacian equation were studied using the concept of viscosity
solutions.
  In this work, we prove continuum limits of Lipschitz learning using
$\Gamma$-convergence. In particular, we define a sequence of functionals which
approximate the largest local Lipschitz constant of a graph function and prove
$\Gamma$-convergence in the $L^\infty$-topology to the supremum norm of the
gradient as the graph becomes denser. Furthermore, we show compactness of the
functionals which implies convergence of minimizers. In our analysis we allow a
varying set of labeled data which converges to a general closed set in the
Hausdorff distance. We apply our results to nonlinear ground states and, as a
by-product, prove convergence of graph distance functions to geodesic distance
functions.","35J20,35R02,65N12,68T05,cs.LG,cs.NA,math.AP,math.NA,stat.ML"
"Foundations of Population-Based SHM, Part IV: The Geometry of Spaces of Structures and their Feature Spaces. One of the requirements of the population-based approach to Structural Health
Monitoring (SHM) proposed in the earlier papers in this sequence, is that
structures be represented by points in an abstract space. Furthermore, these
spaces should be metric spaces in a loose sense; i.e. there should be some
measure of distance applicable to pairs of points; similar structures should
then be close in the metric. However, this geometrical construction is not
enough for the framing of problems in data-based SHM, as it leaves undefined
the notion of feature spaces. Interpreting the feature values on a
structure-by-structure basis as a type of field over the space of structures,
it seems sensible to borrow an idea from modern theoretical physics, and define
feature assignments as sections in a vector bundle over the structure space.
With this idea in place, one can interpret the effect of environmental and
operational variations as gauge degrees of freedom, as in modern gauge field
theories. This paper will discuss the various geometrical structures required
for an abstract theory of feature spaces in SHM, and will draw analogies with
how these structures have shown their power in modern physics. In the second
part of the paper, the problem of determining the normal condition cross
section of a feature bundle is addressed. The solution is provided by the
application of Graph Neural Networks (GNN), a versatile non-Euclidean machine
learning algorithm which is not restricted to inputs and outputs from vector
spaces. In particular, the algorithm is well suited to operating directly on
the sort of graph structures which are an important part of the proposed
framework for PBSHM. The solution of the normal section problem is demonstrated
for a heterogeneous population of truss structures for which the feature of
interest is the first natural frequency.","cs.CE,cs.LG,stat.ML"
"On the Sample Complexity of Learning Sum-Product Networks. Sum-Product Networks (SPNs) can be regarded as a form of deep graphical
models that compactly represent deeply factored and mixed distributions. An SPN
is a rooted directed acyclic graph (DAG) consisting of a set of leaves
(corresponding to base distributions), a set of sum nodes (which represent
mixtures of their children distributions) and a set of product nodes
(representing the products of its children distributions).
  In this work, we initiate the study of the sample complexity of PAC-learning
the set of distributions that correspond to SPNs. We show that the sample
complexity of learning tree structured SPNs with the usual type of leaves
(i.e., Gaussian or discrete) grows at most linearly (up to logarithmic factors)
with the number of parameters of the SPN. More specifically, we show that the
class of distributions that corresponds to tree structured Gaussian SPNs with
$k$ mixing weights and $e$ ($d$-dimensional Gaussian) leaves can be learned
within Total Variation error $\epsilon$ using at most
$\widetilde{O}(\frac{ed^2+k}{\epsilon^2})$ samples. A similar result holds for
tree structured SPNs with discrete leaves.
  We obtain the upper bounds based on the recently proposed notion of
distribution compression schemes. More specifically, we show that if a (base)
class of distributions $\mathcal{F}$ admits an ""efficient"" compression, then
the class of tree structured SPNs with leaves from $\mathcal{F}$ also admits an
efficient compression.","cs.LG,math.ST,stat.ML,stat.TH"
"Multi-Agent Deep Reinforcement Learning with Adaptive Policies. We propose a novel approach to address one aspect of the non-stationarity
problem in multi-agent reinforcement learning (RL), where the other agents may
alter their policies due to environment changes during execution. This violates
the Markov assumption that governs most single-agent RL methods and is one of
the key challenges in multi-agent RL. To tackle this, we propose to train
multiple policies for each agent and postpone the selection of the best policy
at execution time. Specifically, we model the environment non-stationarity with
a finite set of scenarios and train policies fitting each scenario. In addition
to multiple policies, each agent also learns a policy predictor to determine
which policy is the best with its local information. By doing so, each agent is
able to adapt its policy when the environment changes and consequentially the
other agents alter their policies during execution. We empirically evaluated
our method on a variety of common benchmark problems proposed for multi-agent
deep RL in the literature. Our experimental results show that the agents
trained by our algorithm have better adaptiveness in changing environments and
outperform the state-of-the-art methods in all the tested environments.","cs.AI,cs.LG,cs.MA,stat.ML"
"Information Bottleneck and its Applications in Deep Learning. Information Theory (IT) has been used in Machine Learning (ML) from early
days of this field. In the last decade, advances in Deep Neural Networks (DNNs)
have led to surprising improvements in many applications of ML. The result has
been a paradigm shift in the community toward revisiting previous ideas and
applications in this new framework. Ideas from IT are no exception. One of the
ideas which is being revisited by many researchers in this new era, is
Information Bottleneck (IB); a formulation of information extraction based on
IT. The IB is promising in both analyzing and improving DNNs. The goal of this
survey is to review the IB concept and demonstrate its applications in deep
learning. The information theoretic nature of IB, makes it also a good
candidate in showing the more general concept of how IT can be used in ML. Two
important concepts are highlighted in this narrative on the subject, i) the
concise and universal view that IT provides on seemingly unrelated methods of
ML, demonstrated by explaining how IB relates to minimal sufficient statistics,
stochastic gradient descent, and variational auto-encoders, and ii) the common
technical mistakes and problems caused by applying ideas from IT, which is
discussed by a careful study of some recent methods suffering from them.","cs.IT,cs.LG,math.IT,stat.ML"
"An Ode to an ODE. We present a new paradigm for Neural ODE algorithms, called ODEtoODE, where
time-dependent parameters of the main flow evolve according to a matrix flow on
the orthogonal group O(d). This nested system of two flows, where the
parameter-flow is constrained to lie on the compact manifold, provides
stability and effectiveness of training and provably solves the gradient
vanishing-explosion problem which is intrinsically related to training deep
neural network architectures such as Neural ODEs. Consequently, it leads to
better downstream models, as we show on the example of training reinforcement
learning policies with evolution strategies, and in the supervised learning
setting, by comparing with previous SOTA baselines. We provide strong
convergence results for our proposed mechanism that are independent of the
depth of the network, supporting our empirical studies. Our results show an
intriguing connection between the theory of deep neural networks and the field
of matrix flows on compact manifolds.","cs.LG,math.CA,math.DS,math.OC,stat.ML"
"Anomaly Detection and Sampling Cost Control via Hierarchical GANs. Anomaly detection incurs certain sampling and sensing costs and therefore it
is of great importance to strike a balance between the detection accuracy and
these costs. In this work, we study anomaly detection by considering the
detection of threshold crossings in a stochastic time series without the
knowledge of its statistics. To reduce the sampling cost in this detection
process, we propose the use of hierarchical generative adversarial networks
(GANs) to perform nonuniform sampling. In order to improve the detection
accuracy and reduce the delay in detection, we introduce a buffer zone in the
operation of the proposed GAN-based detector. In the experiments, we analyze
the performance of the proposed hierarchical GAN detector considering the
metrics of detection delay, miss rates, average cost of error, and sampling
ratio. We identify the tradeoffs in the performance as the buffer zone sizes
and the number of GAN levels in the hierarchy vary. We also compare the
performance with that of a sampling policy that approximately minimizes the sum
of average costs of sampling and error given the parameters of the stochastic
process. We demonstrate that the proposed GAN-based detector can have
significant performance improvements in terms of detection delay and average
cost of error with a larger buffer zone but at the cost of increased sampling
rates.","cs.IT,cs.LG,math.IT,stat.ML"
"Privacy-preserving Spatiotemporal Scenario Generation of Renewable Energies: A Federated Deep Generative Learning Approach. Scenario generation is a fundamental and crucial tool for decision-making in
power systems with high-penetration renewables. Based on big historical data, a
novel federated deep generative learning framework, called Fed-LSGAN, is
proposed by integrating federated learning and least square generative
adversarial networks (LSGANs) for renewable scenario generation. Specifically,
federated learning learns a shared global model in a central server from
renewable sites at network edges, which enables the Fed-LSGAN to generate
scenarios in a privacy-preserving manner without sacrificing the generation
quality by transferring model parameters, rather than all data. Meanwhile, the
LSGANs-based deep generative model generates scenarios that conform to the
distribution of historical data through fully capturing the spatial-temporal
characteristics of renewable powers, which leverages the least squares loss
function to improve the training stability and generation quality. The
simulation results demonstrate that the proposal manages to generate
high-quality renewable scenarios and outperforms the state-of-the-art
centralized methods. Besides, an experiment with different federated learning
settings is designed and conducted to verify the robustness of our method.","cs.LG,cs.SY,eess.SP,eess.SY"
"Lookahead optimizer improves the performance of Convolutional Autoencoders for reconstruction of natural images. Autoencoders are a class of artificial neural networks which have gained a
lot of attention in the recent past. Using the encoder block of an autoencoder
the input image can be compressed into a meaningful representation. Then a
decoder is employed to reconstruct the compressed representation back to a
version which looks like the input image. It has plenty of applications in the
field of data compression and denoising. Another version of Autoencoders (AE)
exist, called Variational AE (VAE) which acts as a generative model like GAN.
Recently, an optimizer was introduced which is known as lookahead optimizer
which significantly enhances the performances of Adam as well as SGD. In this
paper, we implement Convolutional Autoencoders (CAE) and Convolutional
Variational Autoencoders (CVAE) with lookahead optimizer (with Adam) and
compare them with the Adam (only) optimizer counterparts. For this purpose, we
have used a movie dataset comprising of natural images for the former case and
CIFAR100 for the latter case. We show that lookahead optimizer (with Adam)
improves the performance of CAEs for reconstruction of natural images.","cs.AI,cs.CV,cs.GR,cs.LG,physics.data-an"
"Load Balancing for Ultra-Dense Networks: A Deep Reinforcement Learning Based Approach. In this paper, we propose a deep reinforcement learning (DRL) based mobility
load balancing (MLB) algorithm along with a two-layer architecture to solve the
large-scale load balancing problem for ultra-dense networks (UDNs). Our
contribution is three-fold. First, this work proposes a two-layer architecture
to solve the large-scale load balancing problem in a self-organized manner. The
proposed architecture can alleviate the global traffic variations by
dynamically grouping small cells into self-organized clusters according to
their historical loads, and further adapt to local traffic variations through
intra-cluster load balancing afterwards. Second, for the intra-cluster load
balancing, this paper proposes an off-policy DRL-based MLB algorithm to
autonomously learn the optimal MLB policy under an asynchronous parallel
learning framework, without any prior knowledge assumed over the underlying UDN
environments. Moreover, the algorithm enables joint exploration with multiple
behavior policies, such that the traditional MLB methods can be used to guide
the learning process thereby improving the learning efficiency and stability.
Third, this work proposes an offline-evaluation based safeguard mechanism to
ensure that the online system can always operate with the optimal and
well-trained MLB policy, which not only stabilizes the online performance but
also enables the exploration beyond current policies to make full use of
machine learning in a safe way. Empirical results verify that the proposed
framework outperforms the existing MLB methods in general UDN environments
featured with irregular network topologies, coupled interferences, and random
user movements, in terms of the load balancing performance.","cs.LG,cs.NI,stat.ML"
"BRepNet: A topological message passing system for solid models. Boundary representation (B-rep) models are the standard way 3D shapes are
described in Computer-Aided Design (CAD) applications. They combine lightweight
parametric curves and surfaces with topological information which connects the
geometric entities to describe manifolds. In this paper we introduce BRepNet, a
neural network architecture designed to operate directly on B-rep data
structures, avoiding the need to approximate the model as meshes or point
clouds. BRepNet defines convolutional kernels with respect to oriented coedges
in the data structure. In the neighborhood of each coedge, a small collection
of faces, edges and coedges can be identified and patterns in the feature
vectors from these entities detected by specific learnable parameters. In
addition, to encourage further deep learning research with B-reps, we publish
the Fusion 360 Gallery segmentation dataset. A collection of over 35,000 B-rep
models annotated with information about the modeling operations which created
each face. We demonstrate that BRepNet can segment these models with higher
accuracy than methods working on meshes, and point clouds.","68T07,68T10,I.5.1; I.3.5,cs.CV,cs.LG"
"Quantization and Deployment of Deep Neural Networks on Microcontrollers. Embedding Artificial Intelligence onto low-power devices is a challenging
task that has been partly overcome with recent advances in machine learning and
hardware design. Presently, deep neural networks can be deployed on embedded
targets to perform different tasks such as speech recognition,object detection
or Human Activity Recognition. However, there is still room for optimization of
deep neural networks onto embedded devices. These optimizations mainly address
power consumption,memory and real-time constraints, but also an easier
deployment at the edge. Moreover, there is still a need for a better
understanding of what can be achieved for different use cases. This work
focuses on quantization and deployment of deep neural networks onto low-power
32-bit microcontrollers. The quantization methods, relevant in the context of
an embedded execution onto a microcontroller, are first outlined. Then, a new
framework for end-to-end deep neural networks training, quantization and
deployment is presented. This framework, called MicroAI, is designed as an
alternative to existing inference engines (TensorFlow Lite for Microcontrollers
and STM32CubeAI). Our framework can indeed be easily adjusted and/or extended
for specific use cases. Execution using single precision 32-bit floating-point
as well as fixed-point on 8- and 16-bit integers are supported. The proposed
quantization method is evaluated with three different datasets (UCI-HAR, Spoken
MNIST and GTSRB). Finally, a comparison study between MicroAI and both existing
embedded inference engines is provided in terms of memory and power efficiency.
On-device evaluation is done using ARM Cortex-M4F-based microcontrollers (Ambiq
Apollo3 and STM32L452RE).","cs.LG,eess.SP"
"MM-Deacon: Multimodal molecular domain embedding analysis via contrastive learning. Molecular representation learning plays an essential role in cheminformatics.
Recently, language model-based approaches have been popular as an alternative
to traditional expert-designed features to encode molecules. However, these
approaches only utilize a single modality for representing molecules. Driven by
the fact that a given molecule can be described through different modalities
such as Simplified Molecular Line Entry System (SMILES), The International
Union of Pure and Applied Chemistry (IUPAC), and The IUPAC International
Chemical Identifier (InChI), we propose a multimodal molecular embedding
generation approach called MM-Deacon (multimodal molecular domain embedding
analysis via contrastive learning). MM-Deacon is trained using SMILES and IUPAC
molecule representations as two different modalities. First, SMILES and IUPAC
strings are encoded by using two different transformer-based language models
independently, then the contrastive loss is utilized to bring these encoded
representations from different modalities closer to each other if they belong
to the same molecule, and to push embeddings farther from each other if they
belong to different molecules. We evaluate the robustness of our molecule
embeddings on molecule clustering, cross-modal molecule search, drug similarity
assessment and drug-drug interaction tasks.","cs.CL,cs.LG,physics.chem-ph"
"SaliencyMix: A Saliency Guided Data Augmentation Strategy for Better Regularization. Advanced data augmentation strategies have widely been studied to improve the
generalization ability of deep learning models. Regional dropout is one of the
popular solutions that guides the model to focus on less discriminative parts
by randomly removing image regions, resulting in improved regularization.
However, such information removal is undesirable. On the other hand, recent
strategies suggest to randomly cut and mix patches and their labels among
training images, to enjoy the advantages of regional dropout without having any
pointless pixel in the augmented images. We argue that such random selection
strategies of the patches may not necessarily represent sufficient information
about the corresponding object and thereby mixing the labels according to that
uninformative patch enables the model to learn unexpected feature
representation. Therefore, we propose SaliencyMix that carefully selects a
representative image patch with the help of a saliency map and mixes this
indicative patch with the target image, thus leading the model to learn more
appropriate feature representation. SaliencyMix achieves the best known top-1
error of 21.26% and 20.09% for ResNet-50 and ResNet-101 architectures on
ImageNet classification, respectively, and also improves the model robustness
against adversarial perturbations. Furthermore, models that are trained with
SaliencyMix help to improve the object detection performance. Source code is
available at https://github.com/SaliencyMix/SaliencyMix.","68T07,I.2; I.4,cs.LG,stat.ML"
"A Neural Approach to Ordinal Regression for the Preventive Assessment of Developmental Dyslexia. Developmental Dyslexia (DD) is a learning disability related to the
acquisition of reading skills that affects about 5% of the population. DD can
have an enormous impact on the intellectual and personal development of
affected children, so early detection is key to implementing preventive
strategies for teaching language. Research has shown that there may be
biological underpinnings to DD that affect phoneme processing, and hence these
symptoms may be identifiable before reading ability is acquired, allowing for
early intervention. In this paper we propose a new methodology to assess the
risk of DD before students learn to read. For this purpose, we propose a mixed
neural model that calculates risk levels of dyslexia from tests that can be
completed at the age of 5 years. Our method first trains an auto-encoder, and
then combines the trained encoder with an optimized ordinal regression neural
network devised to ensure consistency of predictions. Our experiments show that
the system is able to detect unaffected subjects two years before it can assess
the risk of DD based mainly on phonological processing, giving a specificity of
0.969 and a correct rate of more than 0.92. In addition, the trained encoder
can be used to transform test results into an interpretable subject spatial
distribution that facilitates risk assessment and validates methodology.","cs.LG,cs.NE,stat.ML"
"Classification with abstention but without disparities. Classification with abstention has gained a lot of attention in recent years
as it allows to incorporate human decision-makers in the process. Yet,
abstention can potentially amplify disparities and lead to discriminatory
predictions. The goal of this work is to build a general purpose classification
algorithm, which is able to abstain from prediction, while avoiding disparate
impact. We formalize this problem as risk minimization under fairness and
abstention constraints for which we derive the form of the optimal classifier.
Building on this result, we propose a post-processing classification algorithm,
which is able to modify any off-the-shelf score-based classifier using only
unlabeled sample. We establish finite sample risk, fairness, and abstention
guarantees for the proposed algorithm. In particular, it is shown that fairness
and abstention constraints can be achieved independently from the initial
classifier as long as sufficiently many unlabeled data is available. The risk
guarantee is established in terms of the quality of the initial classifier. Our
post-processing scheme reduces to a sparse linear program allowing for an
efficient implementation, which we provide. Finally, we validate our method
empirically showing that moderate abstention rates allow to bypass the
risk-fairness trade-off.","cs.CY,cs.LG,stat.ML"
"InfoSSM: Interpretable Unsupervised Learning of Nonparametric State-Space Model for Multi-modal Dynamics. The goal of system identification is to learn about underlying physics
dynamics behind the time-series data. To model the probabilistic and
nonparametric dynamics model, Gaussian process (GP) have been widely used; GP
can estimate the uncertainty of prediction and avoid over-fitting. Traditional
GPSSMs, however, are based on Gaussian transition model, thus often have
difficulty in describing a more complex transition model, e.g. aircraft
motions. To resolve the challenge, this paper proposes a framework using
multiple GP transition models which is capable of describing multi-modal
dynamics. Furthermore, we extend the model to the information-theoretic
framework, the so-called InfoSSM, by introducing a mutual information
regularizer helping the model to learn interpretable and distinguishable
multiple dynamics models. Two illustrative numerical experiments in simple
Dubins vehicle and high-fidelity flight simulator are presented to demonstrate
the performance and interpretability of the proposed model. Finally, this paper
introduces a framework using InfoSSM with Bayesian filtering for air traffic
control tracking.","cs.LG,stat.AP,stat.ML"
"A Hybrid Stochastic Policy Gradient Algorithm for Reinforcement Learning. We propose a novel hybrid stochastic policy gradient estimator by combining
an unbiased policy gradient estimator, the REINFORCE estimator, with another
biased one, an adapted SARAH estimator for policy optimization. The hybrid
policy gradient estimator is shown to be biased, but has variance reduced
property. Using this estimator, we develop a new Proximal Hybrid Stochastic
Policy Gradient Algorithm (ProxHSPGA) to solve a composite policy optimization
problem that allows us to handle constraints or regularizers on the policy
parameters. We first propose a single-looped algorithm then introduce a more
practical restarting variant. We prove that both algorithms can achieve the
best-known trajectory complexity $\mathcal{O}\left(\varepsilon^{-3}\right)$ to
attain a first-order stationary point for the composite problem which is better
than existing REINFORCE/GPOMDP $\mathcal{O}\left(\varepsilon^{-4}\right)$ and
SVRPG $\mathcal{O}\left(\varepsilon^{-10/3}\right)$ in the non-composite
setting. We evaluate the performance of our algorithm on several well-known
examples in reinforcement learning. Numerical results show that our algorithm
outperforms two existing methods on these examples. Moreover, the composite
settings indeed have some advantages compared to the non-composite ones on
certain problems.","cs.LG,math.OC"
"Frequency-Aware Reconstruction of Fluid Simulations with Generative Networks. Convolutional neural networks were recently employed to fully reconstruct
fluid simulation data from a set of reduced parameters. However, since
(de-)convolutions traditionally trained with supervised L1-loss functions do
not discriminate between low and high frequencies in the data, the error is not
minimized efficiently for higher bands. This directly correlates with the
quality of the perceived results, since missing high frequency details are
easily noticeable. In this paper, we analyze the reconstruction quality of
generative networks and present a frequency-aware loss function that is able to
focus on specific bands of the dataset during training time. We show that our
approach improves reconstruction quality of fluid simulation data in
mid-frequency bands, yielding perceptually better results while requiring
comparable training time.","cs.GR,cs.LG,physics.comp-ph,stat.ML"
"Advances in Collaborative Filtering and Ranking. In this dissertation, we cover some recent advances in collaborative
filtering and ranking. In chapter 1, we give a brief introduction of the
history and the current landscape of collaborative filtering and ranking;
chapter 2 we first talk about pointwise collaborative filtering problem with
graph information, and how our proposed new method can encode very deep graph
information which helps four existing graph collaborative filtering algorithms;
chapter 3 is on the pairwise approach for collaborative ranking and how we
speed up the algorithm to near-linear time complexity; chapter 4 is on the new
listwise approach for collaborative ranking and how the listwise approach is a
better choice of loss for both explicit and implicit feedback over pointwise
and pairwise loss; chapter 5 is about the new regularization technique
Stochastic Shared Embeddings (SSE) we proposed for embedding layers and how it
is both theoretically sound and empirically effectively for 6 different tasks
across recommendation and natural language processing; chapter 6 is how we
introduce personalization for the state-of-the-art sequential recommendation
model with the help of SSE, which plays an important role in preventing our
personalized model from overfitting to the training data; chapter 7, we
summarize what we have achieved so far and predict what the future directions
can be; chapter 8 is the appendix to all the chapters.","cs.IR,cs.LG,stat.ML"
"Geometric Surface Image Prediction for Image Recognition Enhancement. This work presents a method to predict a geometric surface image from a
photograph to assist in image recognition. To recognize objects, several images
from different conditions are required for training a model or fine-tuning a
pre-trained model. In this work, a geometric surface image is introduced as a
better representation than its color image counterpart to overcome lighting
conditions. The surface image is predicted from a color image. To do so, the
geometric surface image together with its color photographs are firstly trained
with Generative Adversarial Networks (GAN) model. The trained generator model
is then used to predict the geometric surface image from the input color image.
The evaluation on a case study of an amulet recognition shows that the
predicted geometric surface images contain less ambiguity than their color
images counterpart under different lighting conditions and can be used
effectively for assisting in image recognition task.","cs.CV,cs.GR"
"The Logic of Graph Neural Networks. Graph neural networks (GNNs) are deep learning architectures for machine
learning problems on graphs. It has recently been shown that the expressiveness
of GNNs can be characterised precisely by the combinatorial Weisfeiler-Leman
algorithms and by finite variable counting logics. The correspondence has even
led to new, higher-order GNNs corresponding to the WL algorithm in higher
dimensions.
  The purpose of this paper is to explain these descriptive characterisations
of GNNs.","cs.AI,cs.LG,cs.LO"
"Large-scale nonlinear Granger causality: A data-driven, multivariate approach to recovering directed networks from short time-series data. To gain insight into complex systems it is a key challenge to infer nonlinear
causal directional relations from observational time-series data. Specifically,
estimating causal relationships between interacting components in large systems
with only short recordings over few temporal observations remains an important,
yet unresolved problem. Here, we introduce a large-scale Nonlinear Granger
Causality (lsNGC) approach for inferring directional, nonlinear, multivariate
causal interactions between system components from short high-dimensional
time-series recordings. By modeling interactions with nonlinear state-space
transformations from limited observational data, lsNGC identifies casual
relations with no explicit a priori assumptions on functional interdependence
between component time-series in a computationally efficient manner.
Additionally, our method provides a mathematical formulation revealing
statistical significance of inferred causal relations. We extensively study the
ability of lsNGC to recovering network structure from two-node to thirty-four
node chaotic time-series systems. Our results suggest that lsNGC captures
meaningful interactions from limited observational data, where it performs
favorably when compared to traditionally used methods. Finally, we demonstrate
the applicability of lsNGC to estimating causality in large, real-world systems
by inferring directional nonlinear, multivariate causal relationships among a
large number of relatively short time-series acquired from functional Magnetic
Resonance Imaging (fMRI) data of the human brain.","I.5.1,cs.IT,cs.LG,math.IT,stat.ML"
"Effective and Extensible Feature Extraction Method Using Genetic Algorithm-Based Frequency-Domain Feature Search for Epileptic EEG Multi-classification. In this paper, a genetic algorithm-based frequency-domain feature search
(GAFDS) method is proposed for the electroencephalogram (EEG) analysis of
epilepsy. In this method, frequency-domain features are first searched and then
combined with nonlinear features. Subsequently, these features are selected and
optimized to classify EEG signals. The extracted features are analyzed
experimentally. The features extracted by GAFDS show remarkable independence,
and they are superior to the nonlinear features in terms of the ratio of
inter-class distance and intra-class distance. Moreover, the proposed feature
search method can additionally search for features of instantaneous frequency
in a signal after Hilbert transformation. The classification results achieved
using these features are reasonable, thus, GAFDS exhibits good extensibility.
Multiple classic classifiers (i.e., $k$-nearest neighbor, linear discriminant
analysis, decision tree, AdaBoost, multilayer perceptron, and Na\""ive Bayes)
achieve good results by using the features generated by GAFDS method and the
optimized selection. Specifically, the accuracies for the two-classification
and three-classification problems may reach up to 99% and 97%, respectively.
Results of several cross-validation experiments illustrate that GAFDS is
effective in feature extraction for EEG classification. Therefore, the proposed
feature selection and optimization model can improve classification accuracy.","cs.IT,cs.LG,math.IT,stat.ML"
"Data-Efficient Classification of Birdcall Through Convolutional Neural Networks Transfer Learning. Deep learning Convolutional Neural Network (CNN) models are powerful
classification models but require a large amount of training data. In niche
domains such as bird acoustics, it is expensive and difficult to obtain a large
number of training samples. One method of classifying data with a limited
number of training samples is to employ transfer learning. In this research, we
evaluated the effectiveness of birdcall classification using transfer learning
from a larger base dataset (2814 samples in 46 classes) to a smaller target
dataset (351 samples in 10 classes) using the ResNet-50 CNN. We obtained 79%
average validation accuracy on the target dataset in 5-fold cross-validation.
The methodology of transfer learning from an ImageNet-trained CNN to a
project-specific and a much smaller set of classes and images was extended to
the domain of spectrogram images, where the base dataset effectively played the
role of the ImageNet.","cs.CV,cs.MM,cs.SD,eess.AS,eess.IV"
"Model-Predictive Control via Cross-Entropy and Gradient-Based Optimization. Recent works in high-dimensional model-predictive control and model-based
reinforcement learning with learned dynamics and reward models have resorted to
population-based optimization methods, such as the Cross-Entropy Method (CEM),
for planning a sequence of actions. To decide on an action to take, CEM
conducts a search for the action sequence with the highest return according to
the dynamics model and reward. Action sequences are typically randomly sampled
from an unconditional Gaussian distribution and evaluated on the environment.
This distribution is iteratively updated towards action sequences with higher
returns. However, this planning method can be very inefficient, especially for
high-dimensional action spaces. An alternative line of approaches optimize
action sequences directly via gradient descent, but are prone to local optima.
We propose a method to solve this planning problem by interleaving CEM and
gradient descent steps in optimizing the action sequence. Our experiments show
faster convergence of the proposed hybrid approach, even for high-dimensional
action spaces, avoidance of local minima, and better or equal performance to
CEM. Code accompanying the paper is available here
https://github.com/homangab/gradcem.","cs.AI,cs.LG,cs.RO,stat.ML"
"Capsule Networks for Protein Structure Classification and Prediction. Capsule Networks have great potential to tackle problems in structural
biology because of their attention to hierarchical relationships. This paper
describes the implementation and application of a Capsule Network architecture
to the classification of RAS protein family structures on GPU-based
computational resources. The proposed Capsule Network trained on 2D and 3D
structural encodings can successfully classify HRAS and KRAS structures. The
Capsule Network can also classify a protein-based dataset derived from a
PSI-BLAST search on sequences of KRAS and HRAS mutations. Our results show an
accuracy improvement compared to traditional convolutional networks, while
improving interpretability through visualization of activation vectors.","cs.LG,q-bio.QM,stat.ML"
"Measuring Depression Symptom Severity from Spoken Language and 3D Facial Expressions. With more than 300 million people depressed worldwide, depression is a global
problem. Due to access barriers such as social stigma, cost, and treatment
availability, 60% of mentally-ill adults do not receive any mental health
services. Effective and efficient diagnosis relies on detecting clinical
symptoms of depression. Automatic detection of depressive symptoms would
potentially improve diagnostic accuracy and availability, leading to faster
intervention. In this work, we present a machine learning method for measuring
the severity of depressive symptoms. Our multi-modal method uses 3D facial
expressions and spoken language, commonly available from modern cell phones. It
demonstrates an average error of 3.67 points (15.3% relative) on the
clinically-validated Patient Health Questionnaire (PHQ) scale. For detecting
major depressive disorder, our model demonstrates 83.3% sensitivity and 82.6%
specificity. Overall, this paper shows how speech recognition, computer vision,
and natural language processing can be combined to assist mental health
patients and practitioners. This technology could be deployed to cell phones
worldwide and facilitate low-cost universal access to mental health care.","cs.CV,cs.SD,eess.AS"
"A Convenient Infinite Dimensional Framework for Generative Adversarial Learning. In recent years, generative adversarial networks (GANs) have demonstrated
impressive experimental results while there are only a few works that foster
statistical learning theory for GANs. In this work, we propose an infinite
dimensional theoretical framework for generative adversarial learning. Assuming
the class of uniformly bounded $k$-times $\alpha$-H\""older differentiable and
uniformly positive densities, we show that the Rosenblatt transformation
induces an optimal generator, which is realizable in the hypothesis space of
$\alpha$-H\""older differentiable generators. With a consistent definition of
the hypothesis space of discriminators, we further show that in our framework
the Jensen-Shannon divergence between the distribution induced by the generator
from the adversarial learning procedure and the data generating distribution
converges to zero. Under sufficiently strict regularity assumptions on the
density of the data generating process, we also provide rates of convergence
based on concentration and chaining.","62G20,68T05,cs.LG,math.ST,stat.TH"
"Learning the Structure and Parameters of Large-Population Graphical Games from Behavioral Data. We consider learning, from strictly behavioral data, the structure and
parameters of linear influence games (LIGs), a class of parametric graphical
games introduced by Irfan and Ortiz (2014). LIGs facilitate causal strategic
inference (CSI): Making inferences from causal interventions on stable behavior
in strategic settings. Applications include the identification of the most
influential individuals in large (social) networks. Such tasks can also support
policy-making analysis. Motivated by the computational work on LIGs, we cast
the learning problem as maximum-likelihood estimation (MLE) of a generative
model defined by pure-strategy Nash equilibria (PSNE). Our simple formulation
uncovers the fundamental interplay between goodness-of-fit and model
complexity: good models capture equilibrium behavior within the data while
controlling the true number of equilibria, including those unobserved. We
provide a generalization bound establishing the sample complexity for MLE in
our framework. We propose several algorithms including convex loss minimization
(CLM) and sigmoidal approximations. We prove that the number of exact PSNE in
LIGs is small, with high probability; thus, CLM is sound. We illustrate our
approach on synthetic data and real-world U.S. congressional voting records. We
briefly discuss our learning framework's generality and potential applicability
to general graphical games.","cs.GT,cs.LG,stat.ML"
"Salienteye: Maximizing Engagement While Maintaining Artistic Style on Instagram Using Deep Neural Networks. Instagram has become a great venue for amateur and professional photographers
alike to showcase their work. It has, in other words, democratized photography.
Generally, photographers take thousands of photos in a session, from which they
pick a few to showcase their work on Instagram. Photographers trying to build a
reputation on Instagram have to strike a balance between maximizing their
followers' engagement with their photos, while also maintaining their artistic
style. We used transfer learning to adapt Xception, which is a model for object
recognition trained on the ImageNet dataset, to the task of engagement
prediction and utilized Gram matrices generated from VGG19, another object
recognition model trained on ImageNet, for the task of style similarity
measurement on photos posted on Instagram. Our models can be trained on
individual Instagram accounts to create personalized engagement prediction and
style similarity models. Once trained on their accounts, users can have new
photos sorted based on predicted engagement and style similarity to their
previous work, thus enabling them to upload photos that not only have the
potential to maximize engagement from their followers but also maintain their
style of photography. We trained and validated our models on several Instagram
accounts, showing it to be adept at both tasks, also outperforming several
baseline models and human annotators.","cs.CV,cs.IR,cs.LG"
"Learn molecular representations from large-scale unlabeled molecules for drug discovery. How to produce expressive molecular representations is a fundamental
challenge in AI-driven drug discovery. Graph neural network (GNN) has emerged
as a powerful technique for modeling molecular data. However, previous
supervised approaches usually suffer from the scarcity of labeled data and have
poor generalization capability. Here, we proposed a novel Molecular
Pre-training Graph-based deep learning framework, named MPG, that leans
molecular representations from large-scale unlabeled molecules. In MPG, we
proposed a powerful MolGNet model and an effective self-supervised strategy for
pre-training the model at both the node and graph-level. After pre-training on
11 million unlabeled molecules, we revealed that MolGNet can capture valuable
chemistry insights to produce interpretable representation. The pre-trained
MolGNet can be fine-tuned with just one additional output layer to create
state-of-the-art models for a wide range of drug discovery tasks, including
molecular properties prediction, drug-drug interaction, and drug-target
interaction, involving 13 benchmark datasets. Our work demonstrates that MPG is
promising to become a novel approach in the drug discovery pipeline.","cs.LG,q-bio.BM,q-bio.QM"
"Altruist: Argumentative Explanations through Local Interpretations of Predictive Models. Interpretable machine learning is an emerging field providing solutions on
acquiring insights into machine learning models' rationale. It has been put in
the map of machine learning by suggesting ways to tackle key ethical and
societal issues. However, existing techniques of interpretable machine learning
are far from being comprehensible and explainable to the end user. Another key
issue in this field is the lack of evaluation and selection criteria, making it
difficult for the end user to choose the most appropriate interpretation
technique for its use. In this study, we introduce a meta-explanation
methodology that will provide truthful interpretations, in terms of feature
importance, to the end user through argumentation. At the same time, this
methodology can be used as an evaluation or selection tool for multiple
interpretation techniques based on feature importance.","I.2.0; I.2.6,cs.AI,cs.LG,cs.LO"
"Sparse model selection in the highly under-sampled regime. We propose a method for recovering the structure of a sparse undirected
graphical model when very few samples are available. The method decides about
the presence or absence of bonds between pairs of variable by considering one
pair at a time and using a closed form formula, analytically derived by
calculating the posterior probability for every possible model explaining a two
body system using Jeffreys prior. The approach does not rely on the
optimisation of any cost functions and consequently is much faster than
existing algorithms. Despite this time and computational advantage, numerical
results show that for several sparse topologies the algorithm is comparable to
the best existing algorithms, and is more accurate in the presence of hidden
variables. We apply this approach to the analysis of US stock market data and
to neural data, in order to show its efficiency in recovering robust
statistical dependencies in real data with non stationary correlations in time
and space.","cond-mat.dis-nn,stat.ML"
"One Shot Audio to Animated Video Generation. We consider the challenging problem of audio to animated video generation. We
propose a novel method OneShotAu2AV to generate an animated video of arbitrary
length using an audio clip and a single unseen image of a person as an input.
The proposed method consists of two stages. In the first stage, OneShotAu2AV
generates the talking-head video in the human domain given an audio and a
person's image. In the second stage, the talking-head video from the human
domain is converted to the animated domain. The model architecture of the first
stage consists of spatially adaptive normalization based multi-level generator
and multiple multilevel discriminators along with multiple adversarial and
non-adversarial losses. The second stage leverages attention based
normalization driven GAN architecture along with temporal predictor based
recycle loss and blink loss coupled with lipsync loss, for unsupervised
generation of animated video. In our approach, the input audio clip is not
restricted to any specific language, which gives the method multilingual
applicability. OneShotAu2AV can generate animated videos that have: (a) lip
movements that are in sync with the audio, (b) natural facial expressions such
as blinks and eyebrow movements, (c) head movements. Experimental evaluation
demonstrates superior performance of OneShotAu2AV as compared to U-GAT-IT and
RecycleGan on multiple quantitative metrics including KID(Kernel Inception
Distance), Word error rate, blinks/sec","cs.CV,cs.SD,eess.AS,eess.IV"
"Neural Semantic Encoders. We present a memory augmented neural network for natural language
understanding: Neural Semantic Encoders. NSE is equipped with a novel memory
update rule and has a variable sized encoding memory that evolves over time and
maintains the understanding of input sequences through read}, compose and write
operations. NSE can also access multiple and shared memories. In this paper, we
demonstrated the effectiveness and the flexibility of NSE on five different
natural language tasks: natural language inference, question answering,
sentence classification, document sentiment analysis and machine translation
where NSE achieved state-of-the-art performance when evaluated on publically
available benchmarks. For example, our shared-memory model showed an
encouraging result on neural machine translation, improving an attention-based
baseline by approximately 1.0 BLEU.","cs.CL,cs.LG,stat.ML"
"Online Adversarial Purification based on Self-Supervision. Deep neural networks are known to be vulnerable to adversarial examples,
where a perturbation in the input space leads to an amplified shift in the
latent network representation. In this paper, we combine canonical supervised
learning with self-supervised representation learning, and present
Self-supervised Online Adversarial Purification (SOAP), a novel defense
strategy that uses a self-supervised loss to purify adversarial examples at
test-time. Our approach leverages the label-independent nature of
self-supervised signals and counters the adversarial perturbation with respect
to the self-supervised tasks. SOAP yields competitive robust accuracy against
state-of-the-art adversarial training and purification methods, with
considerably less training complexity. In addition, our approach is robust even
when adversaries are given knowledge of the purification defense strategy. To
the best of our knowledge, our paper is the first that generalizes the idea of
using self-supervised signals to perform online test-time purification.","cs.CR,cs.LG"
"xFraud: Explainable Fraud Transaction Detection on Heterogeneous Graphs. At online retail platforms, it is crucial to actively detect risks of
fraudulent transactions to improve our customer experience, minimize loss, and
prevent unauthorized chargebacks. Traditional rule-based methods and simple
feature-based models are either inefficient or brittle and uninterpretable. The
graph structure that exists among the heterogeneous typed entities of the
transaction logs is informative and difficult to fake. To utilize the
heterogeneous graph relationships and enrich the explainability, we present
xFraud, an explainable Fraud transaction prediction system. xFraud is composed
of a predictor which learns expressive representations for malicious
transaction detection from the heterogeneous transaction graph via a
self-attentive heterogeneous graph neural network, and an explainer that
generates meaningful and human understandable explanations from graphs to
facilitate further process in business unit. In our experiments with xFraud on
two real transaction networks with up to ten millions transactions, we are able
to achieve an area under a curve (AUC) score that outperforms baseline models
and graph embedding methods. In addition, we show how the explainer could
benefit the understanding towards model predictions and enhance model
trustworthiness for real-world fraud transaction cases.","I.2.6,cs.AI,cs.LG,cs.SI"
"Towards Interpretable-AI Policies Induction using Evolutionary Nonlinear Decision Trees for Discrete Action Systems. Black-box AI induction methods such as deep reinforcement learning (DRL) are
increasingly being used to find optimal policies for a given control task.
Although policies represented using a black-box AI are capable of efficiently
executing the underlying control task and achieving optimal closed-loop
performance, the developed control rules are often complex and neither
interpretable nor explainable. In this paper, we use a recently proposed
nonlinear decision-tree (NLDT) approach to find a hierarchical set of control
rules in an attempt to maximize the open-loop performance for approximating and
explaining the pre-trained black-box DRL (oracle) agent using the labelled
state-action dataset. Recent advances in nonlinear optimization approaches
using evolutionary computation facilitates finding a hierarchical set of
nonlinear control rules as a function of state variables using a
computationally fast bilevel optimization procedure at each node of the
proposed NLDT. Additionally, we propose a re-optimization procedure for
enhancing closed-loop performance of an already derived NLDT. We evaluate our
proposed methodologies (open and closed-loop NLDTs) on different control
problems having multiple discrete actions. In all these problems our proposed
approach is able to find relatively simple and interpretable rules involving
one to four non-linear terms per rule, while simultaneously achieving on par
closed-loop performance when compared to a trained black-box DRL agent. A
post-processing approach for simplifying the NLDT is also suggested. The
obtained results are inspiring as they suggest the replacement of complicated
black-box DRL policies involving thousands of parameters (making them
non-interpretable) with relatively simple interpretable policies. Results are
encouraging and motivating to pursue further applications of proposed approach
in solving more complex control tasks.","cs.LG,cs.NE,cs.SY,eess.SY,stat.ML"
"High throughput screening with machine learning. This study assesses the efficiency of several popular machine learning
approaches in the prediction of molecular binding affinity: CatBoost, Graph
Attention Neural Network, and Bidirectional Encoder Representations from
Transformers. The models were trained to predict binding affinities in terms of
inhibition constants $K_i$ for pairs of proteins and small organic molecules.
First two approaches use thoroughly selected physico-chemical features, while
the third one is based on textual molecular representations - it is one of the
first attempts to apply Transformer-based predictors for the binding affinity.
We also discuss the visualization of attention layers within the Transformer
approach in order to highlight the molecular sites responsible for
interactions. All approaches are free from atomic spatial coordinates thus
avoiding bias from known structures and being able to generalize for compounds
with unknown conformations. The achieved accuracy for all suggested approaches
prove their potential in high throughput screening.","cs.LG,physics.chem-ph"
"Provably Efficient Exploration for Reinforcement Learning Using Unsupervised Learning. Motivated by the prevailing paradigm of using unsupervised learning for
efficient exploration in reinforcement learning (RL) problems
[tang2017exploration,bellemare2016unifying], we investigate when this paradigm
is provably efficient. We study episodic Markov decision processes with rich
observations generated from a small number of latent states. We present a
general algorithmic framework that is built upon two components: an
unsupervised learning algorithm and a no-regret tabular RL algorithm.
Theoretically, we prove that as long as the unsupervised learning algorithm
enjoys a polynomial sample complexity guarantee, we can find a near-optimal
policy with sample complexity polynomial in the number of latent states, which
is significantly smaller than the number of observations. Empirically, we
instantiate our framework on a class of hard exploration problems to
demonstrate the practicality of our theory.","cs.AI,cs.LG,math.OC,stat.ML"
"A Dynamical Mean-Field Theory for Learning in Restricted Boltzmann Machines. We define a message-passing algorithm for computing magnetizations in
Restricted Boltzmann machines, which are Ising models on bipartite graphs
introduced as neural network models for probability distributions over spin
configurations. To model nontrivial statistical dependencies between the spins'
couplings, we assume that the rectangular coupling matrix is drawn from an
arbitrary bi-rotation invariant random matrix ensemble. Using the dynamical
functional method of statistical mechanics we exactly analyze the dynamics of
the algorithm in the large system limit. We prove the global convergence of the
algorithm under a stability criterion and compute asymptotic convergence rates
showing excellent agreement with numerical simulations.","cond-mat.dis-nn,cs.LG,stat.ML"
"A Decade Survey of Content Based Image Retrieval using Deep Learning. The content based image retrieval aims to find the similar images from a
large scale dataset against a query image. Generally, the similarity between
the representative features of the query image and dataset images is used to
rank the images for retrieval. In early days, various hand designed feature
descriptors have been investigated based on the visual cues such as color,
texture, shape, etc. that represent the images. However, the deep learning has
emerged as a dominating alternative of hand-designed feature engineering from a
decade. It learns the features automatically from the data. This paper presents
a comprehensive survey of deep learning based developments in the past decade
for content based image retrieval. The categorization of existing
state-of-the-art methods from different perspectives is also performed for
greater understanding of the progress. The taxonomy used in this survey covers
different supervision, different networks, different descriptor type and
different retrieval type. A performance analysis is also performed using the
state-of-the-art methods. The insights are also presented for the benefit of
the researchers to observe the progress and to make the best choices. The
survey presented in this paper will help in further research progress in image
retrieval using deep learning.","cs.AI,cs.CV,cs.MM"
"LiDAR Data Enrichment Using Deep Learning Based on High-Resolution Image: An Approach to Achieve High-Performance LiDAR SLAM Using Low-cost LiDAR. LiDAR-based SLAM algorithms are extensively studied to providing robust and
accurate positioning for autonomous driving vehicles (ADV) in the past decades.
Satisfactory performance can be obtained using high-grade 3D LiDAR with 64
channels, which can provide dense point clouds. Unfortunately, the high price
significantly prevents its extensive commercialization in ADV. The
cost-effective 3D LiDAR with 16 channels is a promising replacement. However,
only limited and sparse point clouds can be provided by the 16 channels LiDAR,
which cannot guarantee sufficient positioning accuracy for ADV in challenging
dynamic environments. The high-resolution image from the low-cost camera can
provide ample information about the surroundings. However, the explicit depth
information is not available from the image. Inspired by the complementariness
of 3D LiDAR and camera, this paper proposes to make use of the high-resolution
images from a camera to enrich the raw 3D point clouds from the low-cost 16
channels LiDAR based on a state-of-the-art deep learning algorithm. An ERFNet
is firstly employed to segment the image with the aid of the raw sparse 3D
point clouds. Meanwhile, the sparse convolutional neural network is employed to
predict the dense point clouds based on raw sparse 3D point clouds. Then, the
predicted dense point clouds are fused with the segmentation outputs from
ERFnet using a novel multi-layer convolutional neural network to refine the
predicted 3D point clouds. Finally, the enriched point clouds are employed to
perform LiDAR SLAM based on the state-of-the-art normal distribution transform
(NDT). We tested our approach on the re-edited KITTI datasets: (1)the sparse 3D
point clouds are significantly enriched with a mean square error of 1.1m MSE.
(2)the map generated from the LiDAR SLAM is denser which includes more details
without significant accuracy loss.","cs.CV,cs.LG,cs.RO"
"Krylov Subspace Recycling for Fast Iterative Least-Squares in Machine Learning. Solving symmetric positive definite linear problems is a fundamental
computational task in machine learning. The exact solution, famously, is
cubicly expensive in the size of the matrix. To alleviate this problem, several
linear-time approximations, such as spectral and inducing-point methods, have
been suggested and are now in wide use. These are low-rank approximations that
choose the low-rank space a priori and do not refine it over time. While this
allows linear cost in the data-set size, it also causes a finite, uncorrected
approximation error. Authors from numerical linear algebra have explored ways
to iteratively refine such low-rank approximations, at a cost of a small number
of matrix-vector multiplications. This idea is particularly interesting in the
many situations in machine learning where one has to solve a sequence of
related symmetric positive definite linear problems. From the machine learning
perspective, such deflation methods can be interpreted as transfer learning of
a low-rank approximation across a time-series of numerical tasks. We study the
use of such methods for our field. Our empirical results show that, on
regression and classification problems of intermediate size, this approach can
interpolate between low computational cost and numerical precision.","cs.LG,math.NA,stat.ML"
"Foley Music: Learning to Generate Music from Videos. In this paper, we introduce Foley Music, a system that can synthesize
plausible music for a silent video clip about people playing musical
instruments. We first identify two key intermediate representations for a
successful video to music generator: body keypoints from videos and MIDI events
from audio recordings. We then formulate music generation from videos as a
motion-to-MIDI translation problem. We present a Graph$-$Transformer framework
that can accurately predict MIDI event sequences in accordance with the body
movements. The MIDI event can then be converted to realistic music using an
off-the-shelf music synthesizer tool. We demonstrate the effectiveness of our
models on videos containing a variety of music performances. Experimental
results show that our model outperforms several existing systems in generating
music that is pleasant to listen to. More importantly, the MIDI representations
are fully interpretable and transparent, thus enabling us to perform music
editing flexibly. We encourage the readers to watch the demo video with audio
turned on to experience the results.","cs.CV,cs.LG,cs.SD,eess.AS"
"Finding Motif Sets in Time Series. Time-series motifs are representative subsequences that occur frequently in a
time series; a motif set is the set of subsequences deemed to be instances of a
given motif. We focus on finding motif sets. Our motivation is to detect motif
sets in household electricity-usage profiles, representing repeated patterns of
household usage.
  We propose three algorithms for finding motif sets. Two are greedy algorithms
based on pairwise comparison, and the third uses a heuristic measure of set
quality to find the motif set directly. We compare these algorithms on
simulated datasets and on electricity-usage data. We show that Scan MK, the
simplest way of using the best-matching pair to find motif sets, is less
accurate on our synthetic data than Set Finder and Cluster MK, although the
latter is very sensitive to parameter settings. We qualitatively analyse the
outputs for the electricity-usage data and demonstrate that both Scan MK and
Set Finder can discover useful motif sets in such data.","cs.DB,cs.LG"
"Knowing What VQA Does Not: Pointing to Error-Inducing Regions to Improve Explanation Helpfulness. Attention maps, a popular heatmap-based explanation method for Visual
Question Answering (VQA), are supposed to help users understand the model by
highlighting portions of the image/question used by the model to infer answers.
However, we see that users are often misled by current attention map
visualizations that point to relevant regions despite the model producing an
incorrect answer. Hence, we propose Error Maps that clarify the error by
highlighting image regions where the model is prone to err. Error maps can
indicate when a correctly attended region may be processed incorrectly leading
to an incorrect answer, and hence, improve users' understanding of those cases.
To evaluate our new explanations, we further introduce a metric that simulates
users' interpretation of explanations to evaluate their potential helpfulness
to understand model correctness. We finally conduct user studies to see that
our new explanations help users understand model correctness better than
baselines by an expected 30% and that our proxy helpfulness metrics correlate
strongly ($\rho$>0.97) with how well users can predict model correctness.","cs.AI,cs.CV,cs.CY,cs.HC"
"Linear Polytree Structural Equation Models: Structural Learning and Inverse Correlation Estimation. We are interested in the problem of learning the directed acyclic graph (DAG)
when data are generated from a linear structural equation model (SEM) and the
causal structure can be characterized by a polytree. Specially, under both
Gaussian and sub-Gaussian models, we study the sample size conditions for the
well-known Chow-Liu algorithm to exactly recover the equivalence class of the
polytree, which is uniquely represented by a CPDAG. We also study the error
rate for the estimation of the inverse correlation matrix under such models.
Our theoretical findings are illustrated by comprehensive numerical
simulations, and experiments on benchmark data also demonstrate the robustness
of the method when the ground truth graphical structure can only be
approximated by a polytree.","cs.LG,math.ST,stat.ME,stat.ML,stat.TH"
"GGNN: Graph-based GPU Nearest Neighbor Search. Approximate nearest neighbor (ANN) search in high dimensions is an integral
part of several computer vision systems and gains importance in deep learning
with explicit memory representations. Since PQT and FAISS started to leverage
the massive parallelism offered by GPUs, GPU-based implementations are a
crucial resource for today's state-of-the-art ANN methods. While most of these
methods allow for faster queries, less emphasis is devoted to accelerate the
construction of the underlying index structures. In this paper, we propose a
novel search structure based on nearest neighbor graphs and information
propagation on graphs. Our method is designed to take advantage of GPU
architectures to accelerate the hierarchical building of the index structure
and for performing the query. Empirical evaluation shows that GGNN
significantly surpasses the state-of-the-art GPU- and CPU-based systems in
terms of build-time, accuracy and search speed.","cs.CV,cs.DB,cs.DS,cs.IR"
"A Quotient Space Formulation for Generative Statistical Analysis of Graphical Data. Complex analyses involving multiple, dependent random quantities often lead
to graphical models - a set of nodes denoting variables of interest, and
corresponding edges denoting statistical interactions between nodes. To develop
statistical analyses for graphical data, especially towards generative
modeling, one needs mathematical representations and metrics for matching and
comparing graphs, and subsequent tools, such as geodesics, means, and
covariances. This paper utilizes a quotient structure to develop efficient
algorithms for computing these quantities, leading to useful statistical tools,
including principal component analysis, statistical testing, and modeling. We
demonstrate the efficacy of this framework using datasets taken from several
problem areas, including letters, biochemical structures, and social networks.","cs.CV,cs.LG,stat.ME"
"Image sequence interpolation using optimal control. The problem of the generation of an intermediate image between two given
images in an image sequence is considered. The problem is formulated as an
optimal control problem governed by a transport equation. This approach bears
similarities with the Horn \& Schunck method for optical flow calculation but
in fact the model is quite different. The images are modelled in $BV$ and an
analysis of solutions of transport equations with values in $BV$ is included.
Moreover, the existence of optimal controls is proven and necessary conditions
are derived. Finally, two algorithms are given and numerical results are
compared with existing methods. The new method is competitive with
state-of-the-art methods and even outperforms several existing methods.","49J20,65D18,68U10,cs.CV,math.AP,math.OC"
"Observer variation-aware medical image segmentation by combining deep learning and surrogate-assisted genetic algorithms. There has recently been great progress in automatic segmentation of medical
images with deep learning algorithms. In most works observer variation is
acknowledged to be a problem as it makes training data heterogeneous but so far
no attempts have been made to explicitly capture this variation. Here, we
propose an approach capable of mimicking different styles of segmentation,
which potentially can improve quality and clinical acceptance of automatic
segmentation methods. In this work, instead of training one neural network on
all available data, we train several neural networks on subgroups of data
belonging to different segmentation variations separately. Because a priori it
may be unclear what styles of segmentation exist in the data and because
different styles do not necessarily map one-on-one to different observers, the
subgroups should be automatically determined. We achieve this by searching for
the best data partition with a genetic algorithm. Therefore, each network can
learn a specific style of segmentation from grouped training data. We provide
proof of principle results for open-sourced prostate segmentation MRI data with
simulated observer variations. Our approach provides an improvement of up to
23% (depending on simulated variations) in terms of Dice and surface Dice
coefficients compared to one network trained on all data.","cs.CV,cs.LG,cs.NE,eess.IV"
"Underestimation Bias and Underfitting in Machine Learning. Often, what is termed algorithmic bias in machine learning will be due to
historic bias in the training data. But sometimes the bias may be introduced
(or at least exacerbated) by the algorithm itself. The ways in which algorithms
can actually accentuate bias has not received a lot of attention with
researchers focusing directly on methods to eliminate bias - no matter the
source. In this paper we report on initial research to understand the factors
that contribute to bias in classification algorithms. We believe this is
important because underestimation bias is inextricably tied to regularization,
i.e. measures to address overfitting can accentuate bias.","cs.LG,stat.ML"
"A Dynamic Penalty Function Approach for Constraints-Handling in Reinforcement Learning. Reinforcement learning (RL) is attracting attention as an effective way to
solve sequential optimization problems that involve high dimensional
state/action space and stochastic uncertainties. Many such problems involve
constraints expressed by inequality constraints. This study focuses on using RL
to solve constrained optimal control problems. Most RL application studies have
dealt with inequality constraints by adding soft penalty terms for violating
the constraints to the reward function. However, while training neural networks
to learn the value (or Q) function, one can run into computational issues
caused by the sharp change in the function value at the constraint boundary due
to the large penalty imposed. This difficulty during training can lead to
convergence problems and ultimately lead to poor closed-loop performance. To
address this issue, this study proposes a dynamic penalty (DP) approach where
the penalty factor is gradually and systematically increased during training as
the iteration episodes proceed. We first examine the ability of a neural
network to represent a value function when uniform, linear, or DP functions are
added to prevent constraint violation. The agent trained by a Deep Q Network
(DQN) algorithm with the DP function approach was compared with agents with
other constant penalty functions in a simple vehicle control problem. Results
show that the proposed approach can improve the neural network approximation
accuracy and provide faster convergence when close to a solution.","cs.LG,cs.SY,eess.SY"
"Theory reconstruction: a representation learning view on predicate invention. With this positional paper we present a representation learning view on
predicate invention. The intention of this proposal is to bridge the relational
and deep learning communities on the problem of predicate invention. We propose
a theory reconstruction approach, a formalism that extends autoencoder approach
to representation learning to the relational settings. Our intention is to
start a discussion to define a unifying framework for predicate invention and
theory revision.","cs.LG,cs.LO,stat.ML"
"Mean Oriented Riesz Features for Micro Expression Classification. Micro-expressions are brief and subtle facial expressions that go on and off
the face in a fraction of a second. This kind of facial expressions usually
occurs in high stake situations and is considered to reflect a human's real
intent. There has been some interest in micro-expression analysis, however, a
great majority of the methods are based on classically established computer
vision methods such as local binary patterns, histogram of gradients and
optical flow. A novel methodology for micro-expression recognition using the
Riesz pyramid, a multi-scale steerable Hilbert transform is presented. In fact,
an image sequence is transformed with this tool, then the image phase
variations are extracted and filtered as proxies for motion. Furthermore, the
dominant orientation constancy from the Riesz transform is exploited to average
the micro-expression sequence into an image pair. Based on that, the Mean
Oriented Riesz Feature description is introduced. Finally the performance of
our methods are tested in two spontaneous micro-expressions databases and
compared to state-of-the-art methods.","68T10,68U10,cs.CV"
"Alleviation of Gradient Exploding in GANs: Fake Can Be Real. In order to alleviate the notorious mode collapse phenomenon in generative
adversarial networks (GANs), we propose a novel training method of GANs in
which certain fake samples are considered as real ones during the training
process. This strategy can reduce the gradient value that generator receives in
the region where gradient exploding happens. We show the process of an
unbalanced generation and a vicious circle issue resulted from gradient
exploding in practical training, which explains the instability of GANs. We
also theoretically prove that gradient exploding can be alleviated by
penalizing the difference between discriminator outputs and fake-as-real
consideration for very close real and fake samples. Accordingly, Fake-As-Real
GAN (FARGAN) is proposed with a more stable training process and a more
faithful generated distribution. Experiments on different datasets verify our
theoretical analysis.","cs.CV,cs.LG,eess.IV"
"Intuitive Physics Guided Exploration for Sample Efficient Sim2real Transfer. Physics-based reinforcement learning tasks can benefit from simplified
physics simulators as they potentially allow near-optimal policies to be
learned in simulation. However, such simulators require the latent factors
(e.g. mass, friction coefficient etc.) of the associated objects and other
environment-specific factors (e.g. wind speed, air density etc.) to be
accurately specified, without which, it could take considerable additional
learning effort to adapt the learned simulation policy to the real environment.
As such a complete specification can be impractical, in this paper, we instead,
focus on learning task-specific estimates of latent factors which allow the
approximation of real world trajectories in an ideal simulation environment.
Specifically, we propose two new concepts: a) action grouping - the idea that
certain types of actions are closely associated with the estimation of certain
latent factors, and; b) partial grounding - the idea that simulation of
task-specific dynamics may not need precise estimation of all the latent
factors. We first introduce intuitive action groupings based on human physics
knowledge and experience, which is then used to design novel strategies for
interacting with the real environment. Next, we describe how prior knowledge of
a task in a given environment can be used to extract the relative importance of
different latent factors, and how this can be used to inform partial grounding,
which enables efficient learning of the task in any arbitrary environment. We
demonstrate our approach in a range of physics based tasks, and show that it
achieves superior performance relative to other baselines, using only a limited
number of real-world interactions.","cs.LG,cs.RO,cs.SY,eess.SY"
"Scheduling optimization of parallel linear algebra algorithms using Supervised Learning. Linear algebra algorithms are used widely in a variety of domains, e.g
machine learning, numerical physics and video games graphics. For all these
applications, loop-level parallelism is required to achieve high performance.
However, finding the optimal way to schedule the workload between threads is a
non-trivial problem because it depends on the structure of the algorithm being
parallelized and the hardware the executable is run on. In the realm of
Asynchronous Many Task runtime systems, a key aspect of the scheduling problem
is predicting the proper chunk-size, where the chunk-size is defined as the
number of iterations of a for-loop assigned to a thread as one task. In this
paper, we study the applications of supervised learning models to predict the
chunk-size which yields maximum performance on multiple parallel linear algebra
operations using the HPX backend of Blaze's linear algebra library. More
precisely, we generate our training and tests sets by measuring performance of
the application with different chunk-sizes for multiple linear algebra
operations; vector-addition, matrix-vector-multiplication, matrix-matrix
addition and matrix-matrix-multiplication. We compare the use of logistic
regression, neural networks and decision trees with a newly developed decision
tree based model in order to predict the optimal value for chunk-size. Our
results show that classical decision trees and our custom decision tree model
are able to forecast a chunk-size which results in good performance for the
linear algebra operations.","cs.DC,cs.LG,stat.ML"
"Multidomain Multimodal Fusion For Human Action Recognition Using Inertial Sensors. One of the major reasons for misclassification of multiplex actions during
action recognition is the unavailability of complementary features that provide
the semantic information about the actions. In different domains these features
are present with different scales and intensities. In existing literature,
features are extracted independently in different domains, but the benefits
from fusing these multidomain features are not realized. To address this
challenge and to extract complete set of complementary information, in this
paper, we propose a novel multidomain multimodal fusion framework that extracts
complementary and distinct features from different domains of the input
modality. We transform input inertial data into signal images, and then make
the input modality multidomain and multimodal by transforming spatial domain
information into frequency and time-spectrum domain using Discrete Fourier
Transform (DFT) and Gabor wavelet transform (GWT) respectively. Features in
different domains are extracted by Convolutional Neural networks (CNNs) and
then fused by Canonical Correlation based Fusion (CCF) for improving the
accuracy of human action recognition. Experimental results on three inertial
datasets show the superiority of the proposed method when compared to the
state-of-the-art.","cs.AI,cs.CV,cs.LG,eess.IV,eess.SP"
"Lung and Pancreatic Tumor Characterization in the Deep Learning Era: Novel Supervised and Unsupervised Learning Approaches. Risk stratification (characterization) of tumors from radiology images can be
more accurate and faster with computer-aided diagnosis (CAD) tools. Tumor
characterization through such tools can also enable non-invasive cancer
staging, prognosis, and foster personalized treatment planning as a part of
precision medicine. In this study, we propose both supervised and unsupervised
machine learning strategies to improve tumor characterization. Our first
approach is based on supervised learning for which we demonstrate significant
gains with deep learning algorithms, particularly by utilizing a 3D
Convolutional Neural Network and Transfer Learning. Motivated by the
radiologists' interpretations of the scans, we then show how to incorporate
task dependent feature representations into a CAD system via a
graph-regularized sparse Multi-Task Learning (MTL) framework. In the second
approach, we explore an unsupervised learning algorithm to address the limited
availability of labeled training data, a common problem in medical imaging
applications. Inspired by learning from label proportion (LLP) approaches in
computer vision, we propose to use proportion-SVM for characterizing tumors. We
also seek the answer to the fundamental question about the goodness of ""deep
features"" for unsupervised tumor classification. We evaluate our proposed
supervised and unsupervised learning algorithms on two different tumor
diagnosis challenges: lung and pancreas with 1018 CT and 171 MRI scans,
respectively, and obtain the state-of-the-art sensitivity and specificity
results in both problems.","cs.AI,cs.CV,cs.LG,q-bio.QM,q-bio.TO"
"GAttANet: Global attention agreement for convolutional neural networks. Transformer attention architectures, similar to those developed for natural
language processing, have recently proved efficient also in vision, either in
conjunction with or as a replacement for convolutional layers. Typically,
visual attention is inserted in the network architecture as a (series of)
feedforward self-attention module(s), with mutual key-query agreement as the
main selection and routing operation. However efficient, this strategy is only
vaguely compatible with the way that attention is implemented in biological
brains: as a separate and unified network of attentional selection regions,
receiving inputs from and exerting modulatory influence on the entire hierarchy
of visual regions. Here, we report experiments with a simple such attention
system that can improve the performance of standard convolutional networks,
with relatively few additional parameters. Each spatial position in each layer
of the network produces a key-query vector pair; all queries are then pooled
into a global attention query. On the next iteration, the match between each
key and the global attention query modulates the network's activations --
emphasizing or silencing the locations that agree or disagree (respectively)
with the global attention system. We demonstrate the usefulness of this
brain-inspired Global Attention Agreement network (GAttANet) for various
convolutional backbones (from a simple 5-layer toy model to a standard ResNet50
architecture) and datasets (CIFAR10, CIFAR100, Imagenet-1k). Each time, our
global attention system improves accuracy over the corresponding baseline.","cs.CV,q-bio.NC"
"Reinforcement Learning Beyond Expectation. The inputs and preferences of human users are important considerations in
situations where these users interact with autonomous cyber or cyber-physical
systems. In these scenarios, one is often interested in aligning behaviors of
the system with the preferences of one or more human users. Cumulative prospect
theory (CPT) is a paradigm that has been empirically shown to model a tendency
of humans to view gains and losses differently. In this paper, we consider a
setting where an autonomous agent has to learn behaviors in an unknown
environment. In traditional reinforcement learning, these behaviors are learned
through repeated interactions with the environment by optimizing an expected
utility. In order to endow the agent with the ability to closely mimic the
behavior of human users, we optimize a CPT-based cost. We introduce the notion
of the CPT-value of an action taken in a state, and establish the convergence
of an iterative dynamic programming-based approach to estimate this quantity.
We develop two algorithms to enable agents to learn policies to optimize the
CPT-vale, and evaluate these algorithms in environments where a target state
has to be reached while avoiding obstacles. We demonstrate that behaviors of
the agent learned using these algorithms are better aligned with that of a
human user who might be placed in the same environment, and is significantly
improved over a baseline that optimizes an expected utility.","cs.AI,cs.LG,cs.SY,eess.SY,math.OC"
"Nonlinear Evolution via Spatially-Dependent Linear Dynamics for Electrophysiology and Calcium Data. Latent variable models have been widely applied for the analysis of time
series resulting from experimental neuroscience techniques. In these datasets,
observations are relatively smooth and possibly nonlinear. We present
Variational Inference for Nonlinear Dynamics (VIND), a variational inference
framework that is able to uncover nonlinear, smooth latent dynamics from
sequential data. The framework is a direct extension of PfLDS; including a
structured approximate posterior describing spatially-dependent linear
dynamics, as well as an algorithm that relies on the fixed-point iteration
method to achieve convergence. We apply VIND to electrophysiology, single-cell
voltage and widefield imaging datasets with state-of-the-art results in
reconstruction error. In single-cell voltage data, VIND finds a 5D latent
space, with variables akin to those of Hodgkin-Huxley-like models. VIND's
learned dynamics are further quantified by predicting future neural activity.
VIND excels in this task, in some cases substantially outperforming current
methods.","cs.LG,q-bio.NC,q-bio.QM,stat.ML"
"Robustness and Diversity Seeking Data-Free Knowledge Distillation. Knowledge distillation (KD) has enabled remarkable progress in model
compression and knowledge transfer. However, KD requires a large volume of
original data or their representation statistics that are not usually available
in practice. Data-free KD has recently been proposed to resolve this problem,
wherein teacher and student models are fed by a synthetic sample generator
trained from the teacher. Nonetheless, existing data-free KD methods rely on
fine-tuning of weights to balance multiple losses, and ignore the diversity of
generated samples, resulting in limited accuracy and robustness. To overcome
this challenge, we propose robustness and diversity seeking data-free KD
(RDSKD) in this paper. The generator loss function is crafted to produce
samples with high authenticity, class diversity, and inter-sample diversity.
Without real data, the objectives of seeking high sample authenticity and class
diversity often conflict with each other, causing frequent loss fluctuations.
We mitigate this by exponentially penalizing loss increments. With MNIST,
CIFAR-10, and SVHN datasets, our experiments show that RDSKD achieves higher
accuracy with more robustness over different hyperparameter settings, compared
to other data-free KD methods such as DAFL, MSKD, ZSKD, and DeepInversion.","cs.CV,cs.LG,cs.SY,eess.SY"
"Molecule Attention Transformer. Designing a single neural network architecture that performs competitively
across a range of molecule property prediction tasks remains largely an open
challenge, and its solution may unlock a widespread use of deep learning in the
drug discovery industry. To move towards this goal, we propose Molecule
Attention Transformer (MAT). Our key innovation is to augment the attention
mechanism in Transformer using inter-atomic distances and the molecular graph
structure. Experiments show that MAT performs competitively on a diverse set of
molecular prediction tasks. Most importantly, with a simple self-supervised
pretraining, MAT requires tuning of only a few hyperparameter values to achieve
state-of-the-art performance on downstream tasks. Finally, we show that
attention weights learned by MAT are interpretable from the chemical point of
view.","cs.LG,physics.comp-ph,stat.ML"
"Discovering conservation laws from trajectories via machine learning. Invariants and conservation laws convey critical information about the
underlying dynamics of a system, yet it is generally infeasible to find them
from large-scale data without any prior knowledge or human insight. We propose
ConservNet to achieve this goal, a neural network that spontaneously discovers
a conserved quantity from grouped data where the members of each group share
invariants, similar to a general experimental setting where trajectories from
different trials are observed. As a neural network trained with a novel and
intuitive loss function called noise-variance loss, ConservNet learns the
hidden invariants in each group of multi-dimensional observables in a
data-driven, end-to-end manner. Our model successfully discovers underlying
invariants from the simulated systems having invariants as well as a real-world
double pendulum trajectory. Since the model is robust to various noises and
data conditions compared to baseline, our approach is directly applicable to
experimental data for discovering hidden conservation laws and further, general
relationships between variables.","cs.LG,physics.class-ph,physics.comp-ph,physics.data-an,stat.ML"
"ERASOR: Egocentric Ratio of Pseudo Occupancy-based Dynamic Object Removal for Static 3D Point Cloud Map Building. Scan data of urban environments often include representations of dynamic
objects, such as vehicles, pedestrians, and so forth. However, when it comes to
constructing a 3D point cloud map with sequential accumulations of the scan
data, the dynamic objects often leave unwanted traces in the map. These traces
of dynamic objects act as obstacles and thus impede mobile vehicles from
achieving good localization and navigation performances. To tackle the problem,
this paper presents a novel static map building method called ERASOR,
Egocentric RAtio of pSeudo Occupancy-based dynamic object Removal, which is
fast and robust to motion ambiguity. Our approach directs its attention to the
nature of most dynamic objects in urban environments being inevitably in
contact with the ground. Accordingly, we propose the novel concept called
pseudo occupancy to express the occupancy of unit space and then discriminate
spaces of varying occupancy. Finally, Region-wise Ground Plane Fitting (R-GPF)
is adopted to distinguish static points from dynamic points within the
candidate bins that potentially contain dynamic points. As experimentally
verified on SemanticKITTI, our proposed method yields promising performance
against state-of-the-art methods overcoming the limitations of existing ray
tracing-based and visibility-based methods.","cs.CV,cs.RO"
"Force myography benchmark data for hand gesture recognition and transfer learning. Force myography has recently gained increasing attention for hand gesture
recognition tasks. However, there is a lack of publicly available benchmark
data, with most existing studies collecting their own data often with custom
hardware and for varying sets of gestures. This limits the ability to compare
various algorithms, as well as the possibility for research to be done without
first needing to collect data oneself. We contribute to the advancement of this
field by making accessible a benchmark dataset collected using a commercially
available sensor setup from 20 persons covering 18 unique gestures, in the hope
of allowing further comparison of results as well as easier entry into this
field of research. We illustrate one use-case for such data, showing how we can
improve gesture recognition accuracy by utilising transfer learning to
incorporate data from multiple other persons. This also illustrates that the
dataset can serve as a benchmark dataset to facilitate research on transfer
learning algorithms.","cs.CV,cs.HC,cs.LG,stat.ML"
"Lagrangian Neural Networks. Accurate models of the world are built upon notions of its underlying
symmetries. In physics, these symmetries correspond to conservation laws, such
as for energy and momentum. Yet even though neural network models see
increasing use in the physical sciences, they struggle to learn these
symmetries. In this paper, we propose Lagrangian Neural Networks (LNNs), which
can parameterize arbitrary Lagrangians using neural networks. In contrast to
models that learn Hamiltonians, LNNs do not require canonical coordinates, and
thus perform well in situations where canonical momenta are unknown or
difficult to compute. Unlike previous approaches, our method does not restrict
the functional form of learned energies and will produce energy-conserving
models for a variety of tasks. We test our approach on a double pendulum and a
relativistic particle, demonstrating energy conservation where a baseline
approach incurs dissipation and modeling relativity without canonical
coordinates where a Hamiltonian approach fails. Finally, we show how this model
can be applied to graphs and continuous systems using a Lagrangian Graph
Network, and demonstrate it on the 1D wave equation.","cs.LG,math.DS,physics.comp-ph,physics.data-an,stat.ML"
"Simplicial Complex Representation Learning. Simplicial complexes form an important class of topological spaces that are
frequently used to in many applications areas such as computer-aided design,
computer graphics, and simulation. The representation learning on graphs, which
are just 1-d simplicial complexes, has witnessed a great attention and success
in the past few years. Due to the additional complexity higher dimensional
simplicial hold, there has not been enough effort to extend representation
learning to these objects especially when it comes to learn entire-simplicial
complex representation. In this work, we propose a method for simplicial
complex-level representation learning that embeds a simplicial complex to a
universal embedding space in a way that complex-to-complex proximity is
preserved. Our method utilizes a simplex-level embedding induced by a
pre-trained simplicial autoencoder to learn an entire simplicial complex
representation. To the best of our knowledge, this work presents the first
method for learning simplicial complex-level representation.","cs.CG,cs.CV,cs.LG,math.AT,stat.ML"
"Generalization in Mean Field Games by Learning Master Policies. Mean Field Games (MFGs) can potentially scale multi-agent systems to
extremely large populations of agents. Yet, most of the literature assumes a
single initial distribution for the agents, which limits the practical
applications of MFGs. Machine Learning has the potential to solve a wider
diversity of MFG problems thanks to generalizations capacities. We study how to
leverage these generalization properties to learn policies enabling a typical
agent to behave optimally against any population distribution. In reference to
the Master equation in MFGs, we coin the term ``Master policies'' to describe
them and we prove that a single Master policy provides a Nash equilibrium,
whatever the initial distribution. We propose a method to learn such Master
policies. Our approach relies on three ingredients: adding the current
population distribution as part of the observation, approximating Master
policies with neural networks, and training via Reinforcement Learning and
Fictitious Play. We illustrate on numerical examples not only the efficiency of
the learned Master policy but also its generalization capabilities beyond the
distributions used for training.","cs.GT,cs.LG,cs.MA,math.OC"
"Normalized and Geometry-Aware Self-Attention Network for Image Captioning. Self-attention (SA) network has shown profound value in image captioning. In
this paper, we improve SA from two aspects to promote the performance of image
captioning. First, we propose Normalized Self-Attention (NSA), a
reparameterization of SA that brings the benefits of normalization inside SA.
While normalization is previously only applied outside SA, we introduce a novel
normalization method and demonstrate that it is both possible and beneficial to
perform it on the hidden activations inside SA. Second, to compensate for the
major limit of Transformer that it fails to model the geometry structure of the
input objects, we propose a class of Geometry-aware Self-Attention (GSA) that
extends SA to explicitly and efficiently consider the relative geometry
relations between the objects in the image. To construct our image captioning
model, we combine the two modules and apply it to the vanilla self-attention
network. We extensively evaluate our proposals on MS-COCO image captioning
dataset and superior results are achieved when comparing to state-of-the-art
approaches. Further experiments on three challenging tasks, i.e. video
captioning, machine translation, and visual question answering, show the
generality of our methods.","cs.CL,cs.CV,cs.MM"
"Learning Mesh-Based Simulation with Graph Networks. Mesh-based simulations are central to modeling complex physical systems in
many disciplines across science and engineering. Mesh representations support
powerful numerical integration methods and their resolution can be adapted to
strike favorable trade-offs between accuracy and efficiency. However,
high-dimensional scientific simulations are very expensive to run, and solvers
and parameters must often be tuned individually to each system studied. Here we
introduce MeshGraphNets, a framework for learning mesh-based simulations using
graph neural networks. Our model can be trained to pass messages on a mesh
graph and to adapt the mesh discretization during forward simulation. Our
results show it can accurately predict the dynamics of a wide range of physical
systems, including aerodynamics, structural mechanics, and cloth. The model's
adaptivity supports learning resolution-independent dynamics and can scale to
more complex state spaces at test time. Our method is also highly efficient,
running 1-2 orders of magnitude faster than the simulation on which it is
trained. Our approach broadens the range of problems on which neural network
simulators can operate and promises to improve the efficiency of complex,
scientific modeling tasks.","cs.CE,cs.LG"
"Variational Hyper RNN for Sequence Modeling. In this work, we propose a novel probabilistic sequence model that excels at
capturing high variability in time series data, both across sequences and
within an individual sequence. Our method uses temporal latent variables to
capture information about the underlying data pattern and dynamically decodes
the latent information into modifications of weights of the base decoder and
recurrent model. The efficacy of the proposed method is demonstrated on a range
of synthetic and real-world sequential data that exhibit large scale
variations, regime shifts, and complex dynamics.","cs.LG,stat.ML"
"CNN-aware Binary Map for General Semantic Segmentation. In this paper we introduce a novel method for general semantic segmentation
that can benefit from general semantics of Convolutional Neural Network (CNN).
Our segmentation proposes visually and semantically coherent image segments. We
use binary encoding of CNN features to overcome the difficulty of the
clustering on the high-dimensional CNN feature space. These binary codes are
very robust against noise and non-semantic changes in the image. These binary
encoding can be embedded into the CNN as an extra layer at the end of the
network. This results in real-time segmentation. To the best of our knowledge
our method is the first attempt on general semantic image segmentation using
CNN. All the previous papers were limited to few number of category of the
images (e.g. PASCAL VOC). Experiments show that our segmentation algorithm
outperform the state-of-the-art non-semantic segmentation methods by large
margin.",cs.CV
"Restrained Generative Adversarial Network against Overfitting in Numeric Data Augmentation. In recent studies, Generative Adversarial Network (GAN) is one of the popular
schemes to augment the image dataset. However, in our study we find the
generator G in the GAN fails to generate numerical data in lower-dimensional
spaces, and we address overfitting in the generation. By analyzing the Directed
Graphical Model (DGM), we propose a theoretical restraint, independence on the
loss function, to suppress the overfitting. Practically, as the Statically
Restrained GAN (SRGAN) and Dynamically Restrained GAN (DRGAN), two frameworks
are proposed to employ the theoretical restraint to the network structure. In
the static structure, we predefined a pair of particular network topologies of
G and D as the restraint, and quantify such restraint by the interpretable
metric Similarity of the Restraint (SR). While for DRGAN we design an
adjustable dropout module for the restraint function. In the widely carried out
20 group experiments, on four public numerical class imbalance datasets and
five classifiers, the static and dynamic methods together produce the best
augmentation results of 19 from 20; and both two methods simultaneously
generate 14 of 20 groups of the top-2 best, proving the effectiveness and
feasibility of the theoretical restraints.","cs.LG,cs.NI"
"Exploiting Hierarchy for Learning and Transfer in KL-regularized RL. As reinforcement learning agents are tasked with solving more challenging and
diverse tasks, the ability to incorporate prior knowledge into the learning
system and to exploit reusable structure in solution space is likely to become
increasingly important. The KL-regularized expected reward objective
constitutes one possible tool to this end. It introduces an additional
component, a default or prior behavior, which can be learned alongside the
policy and as such partially transforms the reinforcement learning problem into
one of behavior modelling. In this work we consider the implications of this
framework in cases where both the policy and default behavior are augmented
with latent variables. We discuss how the resulting hierarchical structures can
be used to implement different inductive biases and how their modularity can
benefit transfer. Empirically we find that they can lead to faster learning and
transfer on a range of continuous control tasks.","cs.LG,stat.ML"
"Convolutional Hashing for Automated Scene Matching. We present a powerful new loss function and training scheme for learning
binary hash functions. In particular, we demonstrate our method by creating for
the first time a neural network that outperforms state-of-the-art Haar wavelets
and color layout descriptors at the task of automated scene matching. By
accurately relating distance on the manifold of network outputs to distance in
Hamming space, we achieve a 100-fold reduction in nontrivial false positive
rate and significantly higher true positive rate. We expect our insights to
provide large wins for hashing models applied to other information retrieval
hashing tasks as well.","68T45,cs.CV,cs.IR,stat.ML"
"Video Segment Copy Detection Using Memory Constrained Hierarchical Batch-Normalized LSTM Autoencoder. In this report, we introduce a video hashing method for scalable video
segment copy detection. The objective of video segment copy detection is to
find the video (s) present in a large database, one of whose segments (cropped
in time) is a (transformed) copy of the given query video. This transformation
may be temporal (for example frame dropping, change in frame rate) or spatial
(brightness and contrast change, addition of noise etc.) in nature although the
primary focus of this report is detecting temporal attacks. The video hashing
method proposed by us uses a deep learning neural network to learn variable
length binary hash codes for the entire video considering both temporal and
spatial features into account. This is in contrast to most existing video
hashing methods, as they use conventional image hashing techniques to obtain
hash codes for a video after extracting features for every frame or certain key
frames, in which case the temporal information present in the video is not
exploited. Our hashing method is specifically resilient to time cropping making
it extremely useful in video segment copy detection. Experimental results
obtained on the large augmented dataset consisting of around 25,000 videos with
segment copies demonstrate the efficacy of our proposed video hashing method.","cs.CV,cs.LG,cs.NI"
"Deep Manifold Prior. We present a prior for manifold structured data, such as surfaces of 3D
shapes, where deep neural networks are adopted to reconstruct a target shape
using gradient descent starting from a random initialization. We show that
surfaces generated this way are smooth, with limiting behavior characterized by
Gaussian processes, and we mathematically derive such properties for
fully-connected as well as convolutional networks. We demonstrate our method in
a variety of manifold reconstruction applications, such as point cloud
denoising and interpolation, achieving considerably better results against
competitive baselines while requiring no training data. We also show that when
training data is available, our method allows developing alternate
parametrizations of surfaces under the framework of AtlasNet, leading to a
compact network architecture and better reconstruction results on standard
image to shape reconstruction benchmarks.","cs.CV,cs.GR"
"A multi-contrast MRI approach to thalamus segmentation. Thalamic alterations are relevant to many neurological disorders including
Alzheimer's disease, Parkinson's disease and multiple sclerosis. Routine
interventions to improve symptom severity in movement disorders, for example,
often consist of surgery or deep brain stimulation to diencephalic nuclei.
Therefore, accurate delineation of grey matter thalamic subregions is of the
upmost clinical importance. MRI is highly appropriate for structural
segmentation as it provides different views of the anatomy from a single
scanning session. Though with several contrasts potentially available, it is
also of increasing importance to develop new image segmentation techniques that
can operate multi-spectrally. We hereby propose a new segmentation method for
use with multi-modality data, which we evaluated for automated segmentation of
major thalamic subnuclear groups using T1-, T2*-weighted and quantitative
susceptibility mapping (QSM) information. The proposed method consists of four
steps: highly iterative image co-registration, manual segmentation on the
average training-data template, supervised learning for pattern recognition,
and a final convex optimisation step imposing further spatial constraints to
refine the solution. This led to solutions in greater agreement with manual
segmentation than the standard Morel atlas based approach. Furthermore, we show
that the multi-contrast approach boosts segmentation performances. We then
investigated whether prior knowledge using the training-template contours could
further improve convex segmentation accuracy and robustness, which led to
highly precise multi-contrast segmentations in single subjects. This approach
can be extended to most 3D imaging data types and any region of interest
discernible in single scans or multi-subject templates.","cs.CV,math.NA"
"A Survey on Deep Learning Architectures for Image-based Depth Reconstruction. Estimating depth from RGB images is a long-standing ill-posed problem, which
has been explored for decades by the computer vision, graphics, and machine
learning communities. In this article, we provide a comprehensive survey of the
recent developments in this field. We will focus on the works which use deep
learning techniques to estimate depth from one or multiple images. Deep
learning, coupled with the availability of large training datasets, have
revolutionized the way the depth reconstruction problem is being approached by
the research community. In this article, we survey more than 100 key
contributions that appeared in the past five years, summarize the most commonly
used pipelines, and discuss their benefits and limitations. In retrospect of
what has been achieved so far, we also conjecture what the future may hold for
learning-based depth reconstruction research.","cs.CV,cs.GR,cs.RO,eess.IV"
"Optimistic Distributionally Robust Policy Optimization. Trust Region Policy Optimization (TRPO) and Proximal Policy Optimization
(PPO), as the widely employed policy based reinforcement learning (RL) methods,
are prone to converge to a sub-optimal solution as they limit the policy
representation to a particular parametric distribution class. To address this
issue, we develop an innovative Optimistic Distributionally Robust Policy
Optimization (ODRPO) algorithm, which effectively utilizes Optimistic
Distributionally Robust Optimization (DRO) approach to solve the trust region
constrained optimization problem without parameterizing the policies. Our
algorithm improves TRPO and PPO with a higher sample efficiency and a better
performance of the final policy while attaining the learning stability.
Moreover, it achieves a globally optimal policy update that is not promised in
the prevailing policy based RL algorithms. Experiments across tabular domains
and robotic locomotion tasks demonstrate the effectiveness of our approach.","cs.LG,math.OC,stat.ML"
"Cross-Resolution Learning for Face Recognition. Convolutional Neural Networks have reached extremely high performances on the
Face Recognition task. Largely used datasets, such as VGGFace2, focus on
gender, pose and age variations trying to balance them to achieve better
results. However, the fact that images have different resolutions is not
usually discussed and resize to 256 pixels before cropping is used. While
specific datasets for very low resolution faces have been proposed, less
attention has been payed on the task of cross-resolution matching. Such
scenarios are of particular interest for forensic and surveillance systems in
which it usually happens that a low-resolution probe has to be matched with
higher-resolution galleries. While it is always possible to either increase the
resolution of the probe image or to reduce the size of the gallery images, to
the best of our knowledge an extensive experimentation of cross-resolution
matching was missing in the recent deep learning based literature. In the
context of low- and cross-resolution Face Recognition, the contributions of our
work are: i) we proposed a training method to fine-tune a state-of-the-art
model in order to make it able to extract resolution-robust deep features; ii)
we tested our models on the benchmark datasets IJB-B/C considering images at
both full and low resolutions in order to show the effectiveness of the
proposed training algorithm. To the best of our knowledge, this is the first
work testing extensively the performance of a FR model in a cross-resolution
scenario; iii) we tested our models on the low resolution and low quality
datasets QMUL-SurvFace and TinyFace and showed their superior performances,
even though we did not train our model on low-resolution faces only and our
main focus was cross-resolution; iv) we showed that our approach can be more
effective with respect to preprocessing faces with super resolution techniques.","I.2.10,I.2.6,I.2.6; I.2.10,cs.CV,cs.LG,eess.IV"
"NICE: An Algorithm for Nearest Instance Counterfactual Explanations. In this paper we suggest NICE: a new algorithm to generate counterfactual
explanations for heterogeneous tabular data. The design of our algorithm
specifically takes into account algorithmic requirements that often emerge in
real-life deployments: the ability to provide an explanation for all
predictions, being efficient in run-time, and being able to handle any
classification model (also non-differentiable ones). More specifically, our
approach exploits information from a nearest instance tospeed up the search
process. We propose four versions of NICE, where three of them optimize the
explanations for one of the following properties: sparsity, proximity or
plausibility. An extensive empirical comparison on 10 datasets shows that our
algorithm performs better on all properties than the current state-of-the-art.
These analyses show a trade-off between on the one hand plausiblity and on the
other hand proximity or sparsity, with our different optimization methods
offering the choice to select the preferred trade-off. An open-source
implementation of NICE can be found at https://github.com/ADMAntwerp/NICE.","cs.AI,cs.LG,cs.LO"
"Learning distributed representations of graphs with Geo2DR. We present Geo2DR (Geometric to Distributed Representations), a GPU ready
Python library for unsupervised learning on graph-structured data using
discrete substructure patterns and neural language models. It contains
efficient implementations of popular graph decomposition algorithms and neural
language models in PyTorch which can be combined to learn representations of
graphs using the distributive hypothesis. Furthermore, Geo2DR comes with
general data processing and loading methods to bring substantial speed-up in
the training of the neural language models. Through this we provide a modular
set of tools and methods to quickly construct systems capable of learning
distributed representations of graphs. This is useful for replication of
existing methods, modification, or development of completely new methods. This
paper serves to present the Geo2DR library and perform a comprehensive
comparative analysis of existing methods re-implemented using Geo2DR across
widely used graph classification benchmarks. Geo2DR displays a high
reproducibility of results in published methods and interoperability with other
libraries useful for distributive language modelling.","cs.LG,stat.ML"
"Multigrid Neural Memory. We introduce a novel approach to endowing neural networks with emergent,
long-term, large-scale memory. Distinct from strategies that connect neural
networks to external memory banks via intricately crafted controllers and
hand-designed attentional mechanisms, our memory is internal, distributed,
co-located alongside computation, and implicitly addressed, while being
drastically simpler than prior efforts. Architecting networks with multigrid
structure and connectivity, while distributing memory cells alongside
computation throughout this topology, we observe the emergence of coherent
memory subsystems. Our hierarchical spatial organization, parameterized
convolutionally, permits efficient instantiation of large-capacity memories,
while multigrid topology provides short internal routing pathways, allowing
convolutional networks to efficiently approximate the behavior of fully
connected networks. Such networks have an implicit capacity for internal
attention; augmented with memory, they learn to read and write specific memory
locations in a dynamic data-dependent manner. We demonstrate these capabilities
on exploration and mapping tasks, where our network is able to self-organize
and retain long-term memory for trajectories of thousands of time steps. On
tasks decoupled from any notion of spatial geometry: sorting, associative
recall, and question answering, our design functions as a truly generic memory
and yields excellent results.","cs.CV,cs.LG,cs.NE"
"Learning Large-Scale Bayesian Networks with the sparsebn Package. Learning graphical models from data is an important problem with wide
applications, ranging from genomics to the social sciences. Nowadays datasets
often have upwards of thousands---sometimes tens or hundreds of thousands---of
variables and far fewer samples. To meet this challenge, we have developed a
new R package called sparsebn for learning the structure of large, sparse
graphical models with a focus on Bayesian networks. While there are many
existing software packages for this task, this package focuses on the unique
setting of learning large networks from high-dimensional data, possibly with
interventions. As such, the methods provided place a premium on scalability and
consistency in a high-dimensional setting. Furthermore, in the presence of
interventions, the methods implemented here achieve the goal of learning a
causal network from data. Additionally, the sparsebn package is fully
compatible with existing software packages for network analysis.","cs.LG,stat.CO,stat.ME,stat.ML"
"BEDS: Bagging ensemble deep segmentation for nucleus segmentation with testing stage stain augmentation. Reducing outcome variance is an essential task in deep learning based medical
image analysis. Bootstrap aggregating, also known as bagging, is a canonical
ensemble algorithm for aggregating weak learners to become a strong learner.
Random forest is one of the most powerful machine learning algorithms before
deep learning era, whose superior performance is driven by fitting bagged
decision trees (weak learners). Inspired by the random forest technique, we
propose a simple bagging ensemble deep segmentation (BEDs) method to train
multiple U-Nets with partial training data to segment dense nuclei on
pathological images. The contributions of this study are three-fold: (1)
developing a self-ensemble learning framework for nucleus segmentation; (2)
aggregating testing stage augmentation with self-ensemble learning; and (3)
elucidating the idea that self-ensemble and testing stage stain augmentation
are complementary strategies for a superior segmentation performance.
Implementation Detail: https://github.com/xingli1102/BEDs.","68T07,68T45,I.4.6,cs.AI,cs.CV,cs.LG"
"SAPE: Spatially-Adaptive Progressive Encoding for Neural Optimization. Multilayer-perceptrons (MLP) are known to struggle with learning functions of
high-frequencies, and in particular cases with wide frequency bands. We present
a spatially adaptive progressive encoding (SAPE) scheme for input signals of
MLP networks, which enables them to better fit a wide range of frequencies
without sacrificing training stability or requiring any domain specific
preprocessing. SAPE gradually unmasks signal components with increasing
frequencies as a function of time and space. The progressive exposure of
frequencies is monitored by a feedback loop throughout the neural optimization
process, allowing changes to propagate at different rates among local spatial
portions of the signal space. We demonstrate the advantage of SAPE on a variety
of domains and applications, including regression of low dimensional signals
and images, representation learning of occupancy networks, and a geometric task
of mesh transfer between 3D shapes.","cs.CV,cs.GR,cs.LG"
"Reading Between the Pixels: Photographic Steganography for Camera Display Messaging. We exploit human color metamers to send light-modulated messages less visible
to the human eye, but recoverable by cameras. These messages are a key
component to camera-display messaging, such as handheld smartphones capturing
information from electronic signage. Each color pixel in the display image is
modified by a particular color gradient vector. The challenge is to find the
color gradient that maximizes camera response, while minimizing human response.
The mismatch in human spectral and camera sensitivity curves creates an
opportunity for hidden messaging. Our approach does not require knowledge of
these sensitivity curves, instead we employ a data-driven method. We learn an
ellipsoidal partitioning of the six-dimensional space of colors and color
gradients. This partitioning creates metamer sets defined by the base color at
the display pixel and the color gradient direction for message encoding. We
sample from the resulting metamer sets to find color steps for each base color
to embed a binary message into an arbitrary image with reduced visible
artifacts. Unlike previous methods that rely on visually obtrusive intensity
modulation, we embed with color so that the message is more hidden. Ordinary
displays and cameras are used without the need for expensive LEDs or high speed
devices. The primary contribution of this work is a framework to map the pixels
in an arbitrary image to a metamer pair for steganographic photo messaging.","I.4.8,cs.CV,cs.GR,cs.MM,cs.NI"
"Deep Reinforcement Learning for Electric Transmission Voltage Control. Today, human operators primarily perform voltage control of the electric
transmission system. As the complexity of the grid increases, so does its
operation, suggesting additional automation could be beneficial. A subset of
machine learning known as deep reinforcement learning (DRL) has recently shown
promise in performing tasks typically performed by humans. This paper applies
DRL to the transmission voltage control problem, presents open-source DRL
environments for voltage control, proposes a novel modification to the ""deep Q
network"" (DQN) algorithm, and performs experiments at scale with systems up to
500 buses. The promise of applying DRL to voltage control is demonstrated,
though more research is needed to enable DRL-based techniques to consistently
outperform conventional methods.","cs.LG,cs.SY,eess.SP,eess.SY,stat.ML"
"Semi-Bagging Based Deep Neural Architecture to Extract Text from High Entropy Images. Extracting texts of various size and shape from images containing multiple
objects is an important problem in many contexts, especially, in connection to
e-commerce, augmented reality assistance system in natural scene, etc. The
existing works (based on only CNN) often perform sub-optimally when the image
contains regions of high entropy having multiple objects. This paper presents
an end-to-end text detection strategy combining a segmentation algorithm and an
ensemble of multiple text detectors of different types to detect text in every
individual image segments independently. The proposed strategy involves a
super-pixel based image segmenter which splits an image into multiple regions.
A convolutional deep neural architecture is developed which works on each of
the segments and detects texts of multiple shapes, sizes, and structures. It
outperforms the competing methods in terms of coverage in detecting texts in
images especially the ones where the text of various types and sizes are
compacted in a small region along with various other objects. Furthermore, the
proposed text detection method along with a text recognizer outperforms the
existing state-of-the-art approaches in extracting text from high entropy
images. We validate the results on a dataset consisting of product images on an
e-commerce website.","cs.CV,cs.LG,eess.IV"
"Batch Size Influence on Performance of Graphic and Tensor Processing Units during Training and Inference Phases. The impact of the maximally possible batch size (for the better runtime) on
performance of graphic processing units (GPU) and tensor processing units (TPU)
during training and inference phases is investigated. The numerous runs of the
selected deep neural network (DNN) were performed on the standard MNIST and
Fashion-MNIST datasets. The significant speedup was obtained even for extremely
low-scale usage of Google TPUv2 units (8 cores only) in comparison to the quite
powerful GPU NVIDIA Tesla K80 card with the speedup up to 10x for training
stage (without taking into account the overheads) and speedup up to 2x for
prediction stage (with and without taking into account overheads). The precise
speedup values depend on the utilization level of TPUv2 units and increase with
the increase of the data volume under processing, but for the datasets used in
this work (MNIST and Fashion-MNIST with images of sizes 28x28) the speedup was
observed for batch sizes >512 images for training phase and >40 000 images for
prediction phase. It should be noted that these results were obtained without
detriment to the prediction accuracy and loss that were equal for both GPU and
TPU runs up to the 3rd significant digit for MNIST dataset, and up to the 2nd
significant digit for Fashion-MNIST dataset.","cs.LG,cs.PF,stat.ML"
"Deep Neural Decision Trees. Deep neural networks have been proven powerful at processing perceptual data,
such as images and audio. However for tabular data, tree-based models are more
popular. A nice property of tree-based models is their natural
interpretability. In this work, we present Deep Neural Decision Trees (DNDT) --
tree models realised by neural networks. A DNDT is intrinsically interpretable,
as it is a tree. Yet as it is also a neural network (NN), it can be easily
implemented in NN toolkits, and trained with gradient descent rather than
greedy splitting. We evaluate DNDT on several tabular datasets, verify its
efficacy, and investigate similarities and differences between DNDT and vanilla
decision trees. Interestingly, DNDT self-prunes at both split and
feature-level.","cs.LG,stat.ML"
"A Novel Approach for Semiconductor Etching Process with Inductive Biases. The etching process is one of the most important processes in semiconductor
manufacturing. We have introduced the state-of-the-art deep learning model to
predict the etching profiles. However, the significant problems violating
physics have been found through various techniques such as explainable
artificial intelligence and representation of prediction uncertainty. To
address this problem, this paper presents a novel approach to apply the
inductive biases for etching process. We demonstrate that our approach fits the
measurement faster than physical simulator while following the physical
behavior. Our approach would bring a new opportunity for better etching process
with higher accuracy and lower cost.","cs.AI,cs.LG,physics.comp-ph,physics.plasm-ph,stat.ML"
"Estimating Vector Fields from Noisy Time Series. While there has been a surge of recent interest in learning differential
equation models from time series, methods in this area typically cannot cope
with highly noisy data. We break this problem into two parts: (i) approximating
the unknown vector field (or right-hand side) of the differential equation, and
(ii) dealing with noise. To deal with (i), we describe a neural network
architecture consisting of tensor products of one-dimensional neural shape
functions. For (ii), we propose an alternating minimization scheme that
switches between vector field training and filtering steps, together with
multiple trajectories of training data. We find that the neural shape function
architecture retains the approximation properties of dense neural networks,
enables effective computation of vector field error, and allows for graphical
interpretability, all for data/systems in any finite dimension $d$. We also
study the combination of either our neural shape function method or existing
differential equation learning methods with alternating minimization and
multiple trajectories. We find that retrofitting any learning method in this
way boosts the method's robustness to noise. While in their raw form the
methods struggle with 1% Gaussian noise, after retrofitting, they learn
accurate vector fields from data with 10% Gaussian noise.","cs.LG,cs.SY,eess.SY,math.DS,math.OC,stat.ML"
"Imperfect ImaGANation: Implications of GANs Exacerbating Biases on Facial Data Augmentation and Snapchat Selfie Lenses. In this paper, we show that popular Generative Adversarial Networks (GANs)
exacerbate biases along the axes of gender and skin tone when given a skewed
distribution of face-shots. While practitioners celebrate synthetic data
generation using GANs as an economical way to augment data for training
data-hungry machine learning models, it is unclear whether they recognize the
perils of such techniques when applied to real world datasets biased along
latent dimensions. Specifically, we show that (1) traditional GANs further skew
the distribution of a dataset consisting of engineering faculty headshots,
generating minority modes less often and of worse quality and (2)
image-to-image translation (conditional) GANs also exacerbate biases by
lightening skin color of non-white faces and transforming female facial
features to be masculine when generating faces of engineering professors. Thus,
our study is meant to serve as a cautionary tale.","cs.CV,cs.LG,eess.IV,stat.ML"
"Restricted Boltzmann Machines: Introduction and Review. The restricted Boltzmann machine is a network of stochastic units with
undirected interactions between pairs of visible and hidden units. This model
was popularized as a building block of deep learning architectures and has
continued to play an important role in applied and theoretical machine
learning. Restricted Boltzmann machines carry a rich structure, with
connections to geometry, applied algebra, probability, statistics, machine
learning, and other areas. The analysis of these models is attractive in its
own right and also as a platform to combine and generalize mathematical tools
for graphical models with hidden variables. This article gives an introduction
to the mathematical analysis of restricted Boltzmann machines, reviews recent
results on the geometry of the sets of probability distributions representable
by these models, and suggests a few directions for further investigation.","cs.IT,cs.LG,math.IT,math.PR,math.ST,stat.ML,stat.TH"
"A Theory of Universal Learning. How quickly can a given class of concepts be learned from examples? It is
common to measure the performance of a supervised machine learning algorithm by
plotting its ""learning curve"", that is, the decay of the error rate as a
function of the number of training examples. However, the classical theoretical
framework for understanding learnability, the PAC model of Vapnik-Chervonenkis
and Valiant, does not explain the behavior of learning curves: the
distribution-free PAC model of learning can only bound the upper envelope of
the learning curves over all possible data distributions. This does not match
the practice of machine learning, where the data source is typically fixed in
any given scenario, while the learner may choose the number of training
examples on the basis of factors such as computational resources and desired
accuracy.
  In this paper, we study an alternative learning model that better captures
such practical aspects of machine learning, but still gives rise to a complete
theory of the learnable in the spirit of the PAC model. More precisely, we
consider the problem of universal learning, which aims to understand the
performance of learning algorithms on every data distribution, but without
requiring uniformity over the distribution. The main result of this paper is a
remarkable trichotomy: there are only three possible rates of universal
learning. More precisely, we show that the learning curves of any given concept
class decay either at an exponential, linear, or arbitrarily slow rates.
Moreover, each of these cases is completely characterized by appropriate
combinatorial parameters, and we exhibit optimal learning algorithms that
achieve the best possible rate in each case.
  For concreteness, we consider in this paper only the realizable case, though
analogous results are expected to extend to more general learning scenarios.","cs.DS,cs.LG,math.ST,stat.ML,stat.TH"
"TiK-means: $K$-means clustering for skewed groups. The $K$-means algorithm is extended to allow for partitioning of skewed
groups. Our algorithm is called TiK-Means and contributes a $K$-means type
algorithm that assigns observations to groups while estimating their
skewness-transformation parameters. The resulting groups and transformation
reveal general-structured clusters that can be explained by inverting the
estimated transformation. Further, a modification of the jump statistic chooses
the number of groups. Our algorithm is evaluated on simulated and real-life
datasets and then applied to a long-standing astronomical dispute regarding the
distinct kinds of gamma ray bursts.","astro-ph.HE,cs.CV,cs.LG,stat.AP,stat.ME,stat.ML"
"Ego-motion Sensor for Unmanned Aerial Vehicles Based on a Single-Board Computer. This paper describes the design and implementation of a ground-related
odometry sensor suitable for micro aerial vehicles. The sensor is based on a
ground-facing camera and a single-board Linux-based embedded computer with a
multimedia System on a Chip (SoC). The SoC features a hardware video encoder
which is used to estimate the optical flow online. The optical flow is then
used in combination with a distance sensor to estimate the vehicle's velocity.
The proposed sensor is compared to a similar existing solution and evaluated in
both indoor and outdoor environments.","cs.CV,cs.RO"
"Augmented Semantic Signatures of Airborne LiDAR Point Clouds for Comparison. LiDAR point clouds provide rich geometric information, which is particularly
useful for the analysis of complex scenes of urban regions. Finding structural
and semantic differences between two different three-dimensional point clouds,
say, of the same region but acquired at different time instances is an
important problem. A comparison of point clouds involves computationally
expensive registration and segmentation. We are interested in capturing the
relative differences in the geometric uncertainty and semantic content of the
point cloud without the registration process. Hence, we propose an
orientation-invariant geometric signature of the point cloud, which integrates
its probabilistic geometric and semantic classifications. We study different
properties of the geometric signature, which are an image-based encoding of
geometric uncertainty and semantic content. We explore different metrics to
determine differences between these signatures, which in turn compare point
clouds without performing point-to-point registration. Our results show that
the differences in the signatures corroborate with the geometric and semantic
differences of the point clouds.","65C50,68U05,68U10,68U20,G.1.3; I.4.10; I.4.8; J.2,cs.CV,cs.GR,eess.IV"
"Tight High Probability Bounds for Linear Stochastic Approximation with Fixed Stepsize. This paper provides a non-asymptotic analysis of linear stochastic
approximation (LSA) algorithms with fixed stepsize. This family of methods
arises in many machine learning tasks and is used to obtain approximate
solutions of a linear system $\bar{A}\theta = \bar{b}$ for which $\bar{A}$ and
$\bar{b}$ can only be accessed through random estimates $\{({\bf A}_n, {\bf
b}_n): n \in \mathbb{N}^*\}$. Our analysis is based on new results regarding
moments and high probability bounds for products of matrices which are shown to
be tight. We derive high probability bounds on the performance of LSA under
weaker conditions on the sequence $\{({\bf A}_n, {\bf b}_n): n \in
\mathbb{N}^*\}$ than previous works. However, in contrast, we establish
polynomial concentration bounds with order depending on the stepsize. We show
that our conclusions cannot be improved without additional assumptions on the
sequence of random matrices $\{{\bf A}_n: n \in \mathbb{N}^*\}$, and in
particular that no Gaussian or exponential high probability bounds can hold.
Finally, we pay a particular attention to establishing bounds with sharp order
with respect to the number of iterations and the stepsize and whose leading
terms contain the covariance matrices appearing in the central limit theorems.","cs.LG,math.PR,math.ST,stat.ML,stat.TH"
"Convolutional Hypercomplex Embeddings for Link Prediction. Knowledge graph embedding research has mainly focused on the two smallest
normed division algebras, $\mathbb{R}$ and $\mathbb{C}$. Recent results suggest
that trilinear products of quaternion-valued embeddings can be a more effective
means to tackle link prediction. In addition, models based on convolutions on
real-valued embeddings often yield state-of-the-art results for link
prediction. In this paper, we investigate a composition of convolution
operations with hypercomplex multiplications. We propose the four approaches
QMult, OMult, ConvQ and ConvO to tackle the link prediction problem. QMult and
OMult can be considered as quaternion and octonion extensions of previous
state-of-the-art approaches, including DistMult and ComplEx. ConvQ and ConvO
build upon QMult and OMult by including convolution operations in a way
inspired by the residual learning framework. We evaluated our approaches on
seven link prediction datasets including WN18RR, FB15K-237 and YAGO3-10.
Experimental results suggest that the benefits of learning hypercomplex-valued
vector representations become more apparent as the size and complexity of the
knowledge graph grows. ConvO outperforms state-of-the-art approaches on
FB15K-237 in MRR, Hit@1 and Hit@3, while QMult, OMult, ConvQ and ConvO
outperform state-of-the-approaches on YAGO3-10 in all metrics. Results also
suggest that link prediction performances can be further improved via
prediction averaging. To foster reproducible research, we provide an
open-source implementation of approaches, including training and evaluation
scripts as well as pretrained models.","cs.IR,cs.LG"
"TeraPipe: Token-Level Pipeline Parallelism for Training Large-Scale Language Models. Model parallelism has become a necessity for training modern large-scale deep
language models. In this work, we identify a new and orthogonal dimension from
existing model parallel approaches: it is possible to perform pipeline
parallelism within a single training sequence for Transformer-based language
models thanks to its autoregressive property. This enables a more fine-grained
pipeline compared with previous work. With this key idea, we design TeraPipe, a
high-performance token-level pipeline parallel algorithm for synchronous
model-parallel training of Transformer-based language models. We develop a
novel dynamic programming-based algorithm to calculate the optimal pipelining
execution scheme given a specific model and cluster configuration. We show that
TeraPipe can speed up the training by 5.0x for the largest GPT-3 model with 175
billion parameters on an AWS cluster with 48 p3.16xlarge instances compared
with state-of-the-art model-parallel methods. The code for reproduction can be
found at https://github.com/zhuohan123/terapipe","cs.CL,cs.DC,cs.LG"
"Machine Learning Approaches For Motor Learning: A Short Review. Machine learning approaches have seen considerable applications in human
movement modeling, but remain limited for motor learning. Motor learning
requires accounting for motor variability, and poses new challenges as the
algorithms need to be able to differentiate between new movements and variation
of known ones. In this short review, we outline existing machine learning
models for motor learning and their adaptation capabilities. We identify and
describe three types of adaptation: Parameter adaptation in probabilistic
models, Transfer and meta-learning in deep neural networks, and Planning
adaptation by reinforcement learning. To conclude, we discuss challenges for
applying these models in the domain of motor learning support systems.","I.2.6; J.4,cs.HC,cs.LG,cs.RO,stat.ML"
"Principled Deep Neural Network Training through Linear Programming. Deep Learning has received significant attention due to its impressive
performance in many state-of-the-art learning tasks. Unfortunately, while very
powerful, Deep Learning is not well understood theoretically and in particular
only recently results for the complexity of training deep neural networks have
been obtained. In this work we show that large classes of deep neural networks
with various architectures (e.g., DNNs, CNNs, Binary Neural Networks, and
ResNets), activation functions (e.g., ReLUs and leaky ReLUs), and loss
functions (e.g., Hinge loss, Euclidean loss, etc) can be trained to near
optimality with desired target accuracy using linear programming in time that
is exponential in the input data and parameter space dimension and polynomial
in the size of the data set; improvements of the dependence in the input
dimension are known to be unlikely assuming $P\neq NP$, and improving the
dependence on the parameter space dimension remains open. In particular, we
obtain polynomial time algorithms for training for a given fixed network
architecture. Our work applies more broadly to empirical risk minimization
problems which allows us to generalize various previous results and obtain new
complexity results for previously unstudied architectures in the proper
learning setting.","cs.LG,math.OC,stat.ML"
"Automatic time-series phenotyping using massive feature extraction. Across a far-reaching diversity of scientific and industrial applications, a
general key problem involves relating the structure of time-series data to a
meaningful outcome, such as detecting anomalous events from sensor recordings,
or diagnosing patients from physiological time-series measurements like heart
rate or brain activity. Currently, researchers must devote considerable effort
manually devising, or searching for, properties of their time series that are
suitable for the particular analysis problem at hand. Addressing this
non-systematic and time-consuming procedure, here we introduce a new tool,
hctsa, that selects interpretable and useful properties of time series
automatically, by comparing implementations over 7700 time-series features
drawn from diverse scientific literatures. Using two exemplar biological
applications, we show how hctsa allows researchers to leverage decades of
time-series research to quantify and understand informative structure in their
time-series data.","cs.LG,physics.data-an,q-bio.QM"
"Who is Afraid of Big Bad Minima? Analysis of Gradient-Flow in a Spiked Matrix-Tensor Model. Gradient-based algorithms are effective for many machine learning tasks, but
despite ample recent effort and some progress, it often remains unclear why
they work in practice in optimising high-dimensional non-convex functions and
why they find good minima instead of being trapped in spurious ones.
  Here we present a quantitative theory explaining this behaviour in a spiked
matrix-tensor model.
  Our framework is based on the Kac-Rice analysis of stationary points and a
closed-form analysis of gradient-flow originating from statistical physics. We
show that there is a well defined region of parameters where the gradient-flow
algorithm finds a good global minimum despite the presence of exponentially
many spurious local minima.
  We show that this is achieved by surfing on saddles that have strong negative
direction towards the global minima, a phenomenon that is connected to a
BBP-type threshold in the Hessian describing the critical points of the
landscapes.","cond-mat.dis-nn,cs.LG,math.ST,stat.ML,stat.TH"
"The Distance Transform and its Computation. Distance transformation is an image processing technique used for many
different applications. Related to a binary image, the general idea is to
determine the distance of all background points to the nearest object point (or
vice versa). In this tutorial, different approaches are explained in detail and
compared using examples. Corresponding source code is provided to facilitate
own investigations. A particular objective of this tutorial is to clarify the
difference between arbitrary distance transforms and exact Euclidean distance
transformations.","cs.CG,cs.CV"
"Fine-Grained Segmentation Networks: Self-Supervised Segmentation for Improved Long-Term Visual Localization. Long-term visual localization is the problem of estimating the camera pose of
a given query image in a scene whose appearance changes over time. It is an
important problem in practice, for example, encountered in autonomous driving.
In order to gain robustness to such changes, long-term localization approaches
often use segmantic segmentations as an invariant scene representation, as the
semantic meaning of each scene part should not be affected by seasonal and
other changes. However, these representations are typically not very
discriminative due to the limited number of available classes. In this paper,
we propose a new neural network, the Fine-Grained Segmentation Network (FGSN),
that can be used to provide image segmentations with a larger number of labels
and can be trained in a self-supervised fashion. In addition, we show how FGSNs
can be trained to output consistent labels across seasonal changes. We
demonstrate through extensive experiments that integrating the fine-grained
segmentations produced by our FGSNs into existing localization algorithms leads
to substantial improvements in localization performance.","68T45,cs.CV"
"Model Uncertainty and Correctability for Directed Graphical Models. Probabilistic graphical models are a fundamental tool in probabilistic
modeling, machine learning and artificial intelligence. They allow us to
integrate in a natural way expert knowledge, physical modeling, heterogeneous
and correlated data and quantities of interest. For exactly this reason,
multiple sources of model uncertainty are inherent within the modular structure
of the graphical model. In this paper we develop information-theoretic, robust
uncertainty quantification methods and non-parametric stress tests for directed
graphical models to assess the effect and the propagation through the graph of
multi-sourced model uncertainties to quantities of interest. These methods
allow us to rank the different sources of uncertainty and correct the graphical
model by targeting its most impactful components with respect to the quantities
of interest. Thus, from a machine learning perspective, we provide a
mathematically rigorous approach to correctability that guarantees a systematic
selection for improvement of components of a graphical model while controlling
potential new errors created in the process in other parts of the model. We
demonstrate our methods in two physico-chemical examples, namely quantum
scale-informed chemical kinetics and materials screening to improve the
efficiency of fuel cells.","62H22,62P30,68T37,80A30,93B35,94A17,cs.IT,cs.LG,math.IT,math.PR,stat.ML"
"A Robust Deep Learning Approach for Automatic Classification of Seizures Against Non-seizures. Identifying epileptic seizures through analysis of the electroencephalography
(EEG) signal becomes a standard method for the diagnosis of epilepsy. Manual
seizure identification on EEG by trained neurologists is time-consuming,
labor-intensive and error-prone, and a reliable automatic seizure/non-seizure
classification method is needed. One of the challenges in automatic
seizure/non-seizure classification is that seizure morphologies exhibit
considerable variabilities. In order to capture essential seizure patterns,
this paper leverages an attention mechanism and a bidirectional long short-term
memory (BiLSTM) to exploit both spatial and temporal discriminating features
and overcome seizure variabilities. The attention mechanism is to capture
spatial features according to the contributions of different brain regions to
seizures. The BiLSTM is to extract discriminating temporal features in the
forward and the backward directions. Cross-validation experiments and
cross-patient experiments over the noisy data of CHB-MIT are performed to
evaluate our proposed approach. The obtained average sensitivity of 87.00%,
specificity of 88.60% and precision of 88.63% in cross-validation experiments
are higher than using the current state-of-the-art methods, and the standard
deviations of our approach are lower. The evaluation results of cross-patient
experiments indicate that, our approach has better performance compared with
the current state-of-the-art methods and is more robust across patients.","cs.LG,q-bio.NC,stat.ML"
"Deep Learning-based Hybrid Graph-Coloring Algorithm for Register Allocation. Register allocation, which is a crucial phase of a good optimizing compiler,
relies on graph coloring. Hence, an efficient graph coloring algorithm is of
paramount importance. In this work we try to learn a good heuristic for
coloring interference graphs that are used in the register allocation phase. We
aim to handle moderate sized interference graphs which have 100 nodes or less.
For such graphs we can get the optimal allocation of colors to the nodes. Such
optimal coloring is then used to train our Deep Learning network which is based
on several layers of LSTM that output a color for each node of the graph.
However, the current network may allocate the same color to the nodes connected
by an edge resulting in an invalid coloring of the interference graph. Since it
is difficult to encode constraints in an LSTM to avoid invalid coloring, we
augment our deep learning network with a color correction phase that runs after
the colors have been allocated by the network. Thus, our algorithm is hybrid in
nature consisting of a mix of a deep learning algorithm followed by a more
traditional correction phase. We have trained our network using several
thousand random graphs of varying sparsity. On application of our hybrid
algorithm to various popular graphs found in literature we see that our
algorithm does very well when compared to the optimal coloring of these graphs.
We have also run our algorithm against LLVMs popular greedy register allocator
for several SPEC CPU 2017 benchmarks and notice that the hybrid algorithm
performs on par or better than such a well-tuned allocator for most of these
benchmarks.","cs.DS,cs.LG,stat.ML"
"SB-GCN: Structured BREP Graph Convolutional Network for Automatic Mating of CAD Assemblies. Assembly modeling is a core task of computer aided design (CAD), comprising
around one third of the work in a CAD workflow. Optimizing this process
therefore represents a huge opportunity in the design of a CAD system, but
current research of assembly based modeling is not directly applicable to
modern CAD systems because it eschews the dominant data structure of modern
CAD: parametric boundary representations (BREPs). CAD assembly modeling defines
assemblies as a system of pairwise constraints, called mates, between parts,
which are defined relative to BREP topology rather than in world coordinates
common to existing work. We propose SB-GCN, a representation learning scheme on
BREPs that retains the topological structure of parts, and use these learned
representations to predict CAD type mates. To train our system, we compiled the
first large scale dataset of BREP CAD assemblies, which we are releasing along
with benchmark mate prediction tasks. Finally, we demonstrate the compatibility
of our model with an existing commercial CAD system by building a tool that
assists users in mate creation by suggesting mate completions, with 72.2%
accuracy.","I.3.5; I.2.10,cs.CV,cs.GR,cs.LG"
"DistDGL: Distributed Graph Neural Network Training for Billion-Scale Graphs. Graph neural networks (GNN) have shown great success in learning from
graph-structured data. They are widely used in various applications, such as
recommendation, fraud detection, and search. In these domains, the graphs are
typically large, containing hundreds of millions of nodes and several billions
of edges. To tackle this challenge, we develop DistDGL, a system for training
GNNs in a mini-batch fashion on a cluster of machines. DistDGL is based on the
Deep Graph Library (DGL), a popular GNN development framework. DistDGL
distributes the graph and its associated data (initial features and embeddings)
across the machines and uses this distribution to derive a computational
decomposition by following an owner-compute rule. DistDGL follows a synchronous
training approach and allows ego-networks forming the mini-batches to include
non-local nodes. To minimize the overheads associated with distributed
computations, DistDGL uses a high-quality and light-weight min-cut graph
partitioning algorithm along with multiple balancing constraints. This allows
it to reduce communication overheads and statically balance the computations.
It further reduces the communication by replicating halo nodes and by using
sparse embedding updates. The combination of these design choices allows
DistDGL to train high-quality models while achieving high parallel efficiency
and memory scalability. We demonstrate our optimizations on both inductive and
transductive GNN models. Our results show that DistDGL achieves linear speedup
without compromising model accuracy and requires only 13 seconds to complete a
training epoch for a graph with 100 million nodes and 3 billion edges on a
cluster with 16 machines. DistDGL is now publicly available as part of
DGL:https://github.com/dmlc/dgl/tree/master/python/dgl/distributed.","cs.DC,cs.LG"
"A Two-Stream Symmetric Network with Bidirectional Ensemble for Aerial Image Matching. In this paper, we propose a novel method to precisely match two aerial images
that were obtained in different environments via a two-stream deep network. By
internally augmenting the target image, the network considers the two-stream
with the three input images and reflects the additional augmented pair in the
training. As a result, the training process of the deep network is regularized
and the network becomes robust for the variance of aerial images. Furthermore,
we introduce an ensemble method that is based on the bidirectional network,
which is motivated by the isomorphic nature of the geometric transformation. We
obtain two global transformation parameters without any additional network or
parameters, which alleviate asymmetric matching results and enable significant
improvement in performance by fusing two outcomes. For the experiment, we adopt
aerial images from Google Earth and the International Society for
Photogrammetry and Remote Sensing (ISPRS). To quantitatively assess our result,
we apply the probability of correct keypoints (PCK) metric, which measures the
degree of matching. The qualitative and quantitative results show the sizable
gap of performance compared to the conventional methods for matching the aerial
images. All code and our trained model, as well as the dataset are available
online.","cs.CV,cs.LG"
"Neural Networks for Modeling Source Code Edits. Programming languages are emerging as a challenging and interesting domain
for machine learning. A core task, which has received significant attention in
recent years, is building generative models of source code. However, to our
knowledge, previous generative models have always been framed in terms of
generating static snapshots of code. In this work, we instead treat source code
as a dynamic object and tackle the problem of modeling the edits that software
developers make to source code files. This requires extracting intent from
previous edits and leveraging it to generate subsequent edits. We develop
several neural networks and use synthetic data to test their ability to learn
challenging edit patterns that require strong generalization. We then collect
and train our models on a large-scale dataset of Google source code, consisting
of millions of fine-grained edits from thousands of Python developers. From the
modeling perspective, our main conclusion is that a new composition of
attentional and pointer network components provides the best overall
performance and scalability. From the application perspective, our results
provide preliminary evidence of the feasibility of developing tools that learn
to predict future edits.","cs.CL,cs.LG,cs.SE,stat.ML"
"RDP-GAN: A Rnyi-Differential Privacy based Generative Adversarial Network. Generative adversarial network (GAN) has attracted increasing attention
recently owing to its impressive ability to generate realistic samples with
high privacy protection. Without directly interactive with training examples,
the generative model can be fully used to estimate the underlying distribution
of an original dataset while the discriminative model can examine the quality
of the generated samples by comparing the label values with the training
examples. However, when GANs are applied on sensitive or private training
examples, such as medical or financial records, it is still probable to divulge
individuals' sensitive and private information. To mitigate this information
leakage and construct a private GAN, in this work we propose a
R\'enyi-differentially private-GAN (RDP-GAN), which achieves differential
privacy (DP) in a GAN by carefully adding random noises on the value of the
loss function during training. Moreover, we derive the analytical results of
the total privacy loss under the subsampling method and cumulated iterations,
which show its effectiveness on the privacy budget allocation. In addition, in
order to mitigate the negative impact brought by the injecting noise, we
enhance the proposed algorithm by adding an adaptive noise tuning step, which
will change the volume of added noise according to the testing accuracy.
Through extensive experimental results, we verify that the proposed algorithm
can achieve a better privacy level while producing high-quality samples
compared with a benchmark DP-GAN scheme based on noise perturbation on training
gradients.","cs.CR,cs.LG,cs.NI,stat.ML"
"GaDei: On Scale-up Training As A Service For Deep Learning. Deep learning (DL) training-as-a-service (TaaS) is an important emerging
industrial workload. The unique challenge of TaaS is that it must satisfy a
wide range of customers who have no experience and resources to tune DL
hyper-parameters, and meticulous tuning for each user's dataset is
prohibitively expensive. Therefore, TaaS hyper-parameters must be fixed with
values that are applicable to all users. IBM Watson Natural Language Classifier
(NLC) service, the most popular IBM cognitive service used by thousands of
enterprise-level clients around the globe, is a typical TaaS service. By
evaluating the NLC workloads, we show that only the conservative
hyper-parameter setup (e.g., small mini-batch size and small learning rate) can
guarantee acceptable model accuracy for a wide range of customers. We further
justify theoretically why such a setup guarantees better model convergence in
general. Unfortunately, the small mini-batch size causes a high volume of
communication traffic in a parameter-server based system. We characterize the
high communication bandwidth requirement of TaaS using representative
industrial deep learning workloads and demonstrate that none of the
state-of-the-art scale-up or scale-out solutions can satisfy such a
requirement. We then present GaDei, an optimized shared-memory based scale-up
parameter server design. We prove that the designed protocol is deadlock-free
and it processes each gradient exactly once. Our implementation is evaluated on
both commercial benchmarks and public benchmarks to demonstrate that it
significantly outperforms the state-of-the-art parameter-server based
implementation while maintaining the required accuracy and our implementation
reaches near the best possible runtime performance, constrained only by the
hardware limitation. Furthermore, to the best of our knowledge, GaDei is the
only scale-up DL system that provides fault-tolerance.","C.1.3; D.1.3; I.2.6; I.2.7; H.3.4,cs.DC,cs.LG,stat.ML"
"Feature Inference Attack on Model Predictions in Vertical Federated Learning. Federated learning (FL) is an emerging paradigm for facilitating multiple
organizations' data collaboration without revealing their private data to each
other. Recently, vertical FL, where the participating organizations hold the
same set of samples but with disjoint features and only one organization owns
the labels, has received increased attention. This paper presents several
feature inference attack methods to investigate the potential privacy leakages
in the model prediction stage of vertical FL. The attack methods consider the
most stringent setting that the adversary controls only the trained vertical FL
model and the model predictions, relying on no background information. We first
propose two specific attacks on the logistic regression (LR) and decision tree
(DT) models, according to individual prediction output. We further design a
general attack method based on multiple prediction outputs accumulated by the
adversary to handle complex models, such as neural networks (NN) and random
forest (RF) models. Experimental evaluations demonstrate the effectiveness of
the proposed attacks and highlight the need for designing private mechanisms to
protect the prediction outputs in vertical FL.","cs.DB,cs.LG"
"DO-GAN: A Double Oracle Framework for Generative Adversarial Networks. In this paper, we propose a new approach to train Generative Adversarial
Networks (GANs) where we deploy a double-oracle framework using the generator
and discriminator oracles. GAN is essentially a two-player zero-sum game
between the generator and the discriminator. Training GANs is challenging as a
pure Nash equilibrium may not exist and even finding the mixed Nash equilibrium
is difficult as GANs have a large-scale strategy space. In DO-GAN, we extend
the double oracle framework to GANs. We first generalize the players'
strategies as the trained models of generator and discriminator from the best
response oracles. We then compute the meta-strategies using a linear program.
For scalability of the framework where multiple generators and discriminator
best responses are stored in the memory, we propose two solutions: 1) pruning
the weakly-dominated players' strategies to keep the oracles from becoming
intractable; 2) applying continual learning to retain the previous knowledge of
the networks. We apply our framework to established GAN architectures such as
vanilla GAN, Deep Convolutional GAN, Spectral Normalization GAN and Stacked
GAN. Finally, we conduct experiments on MNIST, CIFAR-10 and CelebA datasets and
show that DO-GAN variants have significant improvements in both subjective
qualitative evaluation and quantitative metrics, compared with their respective
GAN architectures.","cs.GT,cs.LG"
"Adversarial Attacks on Camera-LiDAR Models for 3D Car Detection. Most autonomous vehicles (AVs) rely on LiDAR and RGB camera sensors for
perception. Using these point cloud and image data, perception models based on
deep neural nets (DNNs) have achieved state-of-the-art performance in 3D
detection. The vulnerability of DNNs to adversarial attacks has been heavily
investigated in the RGB image domain and more recently in the point cloud
domain, but rarely in both domains simultaneously. Multi-modal perception
systems used in AVs can be divided into two broad types: cascaded models which
use each modality independently, and fusion models which learn from different
modalities simultaneously. We propose a universal and physically realizable
adversarial attack for each type, and study and contrast their respective
vulnerabilities to attacks. We place a single adversarial object with specific
shape and texture on top of a car with the objective of making this car evade
detection. Evaluating on the popular KITTI benchmark, our adversarial object
made the host vehicle escape detection by each model type more than 50% of the
time. The dense RGB input contributed more to the success of the adversarial
attacks on both cascaded and fusion models.","cs.CR,cs.CV,cs.GR,cs.LG"
"Unsupervised Representation for EHR Signals and Codes as Patient Status Vector. Effective modeling of electronic health records presents many challenges as
they contain large amounts of irregularity most of which are due to the varying
procedures and diagnosis a patient may have. Despite the recent progress in
machine learning, unsupervised learning remains largely at open, especially in
the healthcare domain. In this work, we present a two-step unsupervised
representation learning scheme to summarize the multi-modal clinical time
series consisting of signals and medical codes into a patient status vector.
First, an auto-encoder step is used to reduce sparse medical codes and clinical
time series into a distributed representation. Subsequently, the concatenation
of the distributed representations is further fine-tuned using a forecasting
task. We evaluate the usefulness of the representation on two downstream tasks:
mortality and readmission. Our proposed method shows improved generalization
performance for both short duration ICU visits and long duration ICU visits.","cs.AI,cs.LG,stat.ML"
"A PAC RL Algorithm for Episodic POMDPs. Many interesting real world domains involve reinforcement learning (RL) in
partially observable environments. Efficient learning in such domains is
important, but existing sample complexity bounds for partially observable RL
are at least exponential in the episode length. We give, to our knowledge, the
first partially observable RL algorithm with a polynomial bound on the number
of episodes on which the algorithm may not achieve near-optimal performance.
Our algorithm is suitable for an important class of episodic POMDPs. Our
approach builds on recent advances in method of moments for latent variable
model estimation.","cs.AI,cs.LG,stat.ML"
"Photothermal-SR-Net: A Customized Deep Unfolding Neural Network for Photothermal Super Resolution Imaging. This paper presents deep unfolding neural networks to handle inverse problems
in photothermal radiometry enabling super resolution (SR) imaging. Photothermal
imaging is a well-known technique in active thermography for nondestructive
inspection of defects in materials such as metals or composites. A grand
challenge of active thermography is to overcome the spatial resolution
limitation imposed by heat diffusion in order to accurately resolve each
defect. The photothermal SR approach enables to extract high-frequency spatial
components based on the deconvolution with the thermal point spread function.
However, stable deconvolution can only be achieved by using the sparse
structure of defect patterns, which often requires tedious, hand-crafted tuning
of hyperparameters and results in computationally intensive algorithms. On this
account, Photothermal-SR-Net is proposed in this paper, which performs
deconvolution by deep unfolding considering the underlying physics. This
enables to super resolve 2D thermal images for nondestructive testing with a
substantially improved convergence rate. Since defects appear sparsely in
materials, Photothermal-SR-Net applies trained block-sparsity thresholding to
the acquired thermal images in each convolutional layer. The performance of the
proposed approach is evaluated and discussed using various deep unfolding and
thresholding approaches applied to 2D thermal images. Subsequently, studies are
conducted on how to increase the reconstruction quality and the computational
performance of Photothermal-SR-Net is evaluated. Thereby, it was found that the
computing time for creating high-resolution images could be significantly
reduced without decreasing the reconstruction quality by using pixel binning as
a preprocessing step.","cs.AI,cs.CV,eess.IV,physics.app-ph,physics.comp-ph"
"Feature Reinforcement Learning: Part I: Unstructured MDPs. General-purpose, intelligent, learning agents cycle through sequences of
observations, actions, and rewards that are complex, uncertain, unknown, and
non-Markovian. On the other hand, reinforcement learning is well-developed for
small finite state Markov decision processes (MDPs). Up to now, extracting the
right state representations out of bare observations, that is, reducing the
general agent setup to the MDP framework, is an art that involves significant
effort by designers. The primary goal of this work is to automate the reduction
process and thereby significantly expand the scope of many existing
reinforcement learning algorithms and the agents that employ them. Before we
can think of mechanizing this search for suitable MDPs, we need a formal
objective criterion. The main contribution of this article is to develop such a
criterion. I also integrate the various parts into one learning algorithm.
Extensions to more realistic dynamic Bayesian networks are developed in Part
II. The role of POMDPs is also considered there.","cs.AI,cs.IT,cs.LG,math.IT"
"Learning Representation over Dynamic Graph using Aggregation-Diffusion Mechanism. Representation learning on graphs that evolve has recently received
significant attention due to its wide application scenarios, such as
bioinformatics, knowledge graphs, and social networks. The propagation of
information in graphs is important in learning dynamic graph representations,
and most of the existing methods achieve this by aggregation. However, relying
only on aggregation to propagate information in dynamic graphs can result in
delays in information propagation and thus affect the performance of the
method. To alleviate this problem, we propose an aggregation-diffusion (AD)
mechanism that actively propagates information to its neighbor by diffusion
after the node updates its embedding through the aggregation mechanism. In
experiments on two real-world datasets in the dynamic link prediction task, the
AD mechanism outperforms the baseline models that only use aggregation to
propagate information. We further conduct extensive experiments to discuss the
influence of different factors in the AD mechanism.","cs.LG,cs.SI"
"Inclusion of Domain-Knowledge into GNNs using Mode-Directed Inverse Entailment. We present a general technique for constructing Graph Neural Networks (GNNs)
capable of using multi-relational domain knowledge. The technique is based on
mode-directed inverse entailment (MDIE) developed in Inductive Logic
Programming (ILP). Given a data instance $e$ and background knowledge $B$, MDIE
identifies a most-specific logical formula $\bot_B(e)$ that contains all the
relational information in $B$ that is related to $e$. We represent $\bot_B(e)$
by a ""bottom-graph"" that can be converted into a form suitable for GNN
implementations. This transformation allows a principled way of incorporating
generic background knowledge into GNNs: we use the term `BotGNN' for this form
of graph neural networks. For several GNN variants, using real-world datasets
with substantial background knowledge, we show that BotGNNs perform
significantly better than both GNNs without background knowledge and a recently
proposed simplified technique for including domain knowledge into GNNs. We also
provide experimental evidence comparing BotGNNs favourably to multi-layer
perceptrons (MLPs) that use features representing a ""propositionalised"" form of
the background knowledge; and BotGNNs to a standard ILP based on the use of
most-specific clauses. Taken together, these results point to BotGNNs as
capable of combining the computational efficacy of GNNs with the
representational versatility of ILP.","68T05,68T07,68T27,68T30,I.2.6,cs.AI,cs.LG,cs.NE"
"Fair-by-design explainable models for prediction of recidivism. Recidivism prediction provides decision makers with an assessment of the
likelihood that a criminal defendant will reoffend that can be used in
pre-trial decision-making. It can also be used for prediction of locations
where crimes most occur, profiles that are more likely to commit violent
crimes. While such instruments are gaining increasing popularity, their use is
controversial as they may present potential discriminatory bias in the risk
assessment. In this paper we propose a new fair-by-design approach to predict
recidivism. It is prototype-based, learns locally and extracts empirically the
data distribution. The results show that the proposed method is able to reduce
the bias and provide human interpretable rules to assist specialists in the
explanation of the given results.","cs.LG,stat.AP,stat.ML"
"Constructing Parsimonious Analytic Models for Dynamic Systems via Symbolic Regression. Developing mathematical models of dynamic systems is central to many
disciplines of engineering and science. Models facilitate simulations, analysis
of the system's behavior, decision making and design of automatic control
algorithms. Even inherently model-free control techniques such as reinforcement
learning (RL) have been shown to benefit from the use of models, typically
learned online. Any model construction method must address the tradeoff between
the accuracy of the model and its complexity, which is difficult to strike. In
this paper, we propose to employ symbolic regression (SR) to construct
parsimonious process models described by analytic equations. We have equipped
our method with two different state-of-the-art SR algorithms which
automatically search for equations that fit the measured data: Single Node
Genetic Programming (SNGP) and Multi-Gene Genetic Programming (MGGP). In
addition to the standard problem formulation in the state-space domain, we show
how the method can also be applied to input-output models of the NARX
(nonlinear autoregressive with exogenous input) type. We present the approach
on three simulated examples with up to 14-dimensional state space: an inverted
pendulum, a mobile robot, and a bipedal walking robot. A comparison with deep
neural networks and local linear regression shows that SR in most cases
outperforms these commonly used alternative methods. We demonstrate on a real
pendulum system that the analytic model found enables a RL controller to
successfully perform the swing-up task, based on a model constructed from only
100 data samples.","cs.LG,cs.NE,cs.RO,cs.SY,stat.ML"
"Fast Reinforcement Learning with Large Action Sets using Error-Correcting Output Codes for MDP Factorization. The use of Reinforcement Learning in real-world scenarios is strongly limited
by issues of scale. Most RL learning algorithms are unable to deal with
problems composed of hundreds or sometimes even dozens of possible actions, and
therefore cannot be applied to many real-world problems. We consider the RL
problem in the supervised classification framework where the optimal policy is
obtained through a multiclass classifier, the set of classes being the set of
actions of the problem. We introduce error-correcting output codes (ECOCs) in
this setting and propose two new methods for reducing complexity when using
rollouts-based approaches. The first method consists in using an ECOC-based
classifier as the multiclass classifier, reducing the learning complexity from
O(A2) to O(Alog(A)). We then propose a novel method that profits from the
ECOC's coding dictionary to split the initial MDP into O(log(A)) seperate
two-action MDPs. This second method reduces learning complexity even further,
from O(A2) to O(log(A)), thus rendering problems with large action sets
tractable. We finish by experimentally demonstrating the advantages of our
approach on a set of benchmark problems, both in speed and performance.","68T05,cs.LG,stat.ML"
"Relation Embedding for Personalised POI Recommendation. Point-of-Interest (POI) recommendation is one of the most important
location-based services helping people discover interesting venues or services.
However, the extreme user-POI matrix sparsity and the varying spatio-temporal
context pose challenges for POI systems, which affects the quality of POI
recommendations. To this end, we propose a translation-based relation embedding
for POI recommendation. Our approach encodes the temporal and geographic
information, as well as semantic contents effectively in a low-dimensional
relation space by using Knowledge Graph Embedding techniques. To further
alleviate the issue of user-POI matrix sparsity, a combined matrix
factorization framework is built on a user-POI graph to enhance the inference
of dynamic personal interests by exploiting the side-information. Experiments
on two real-world datasets demonstrate the effectiveness of our proposed model.","cs.DB,cs.IR,cs.LG,stat.ML"
"Large Scale Image Completion via Co-Modulated Generative Adversarial Networks. Numerous task-specific variants of conditional generative adversarial
networks have been developed for image completion. Yet, a serious limitation
remains that all existing algorithms tend to fail when handling large-scale
missing regions. To overcome this challenge, we propose a generic new approach
that bridges the gap between image-conditional and recent modulated
unconditional generative architectures via co-modulation of both conditional
and stochastic style representations. Also, due to the lack of good
quantitative metrics for image completion, we propose the new Paired/Unpaired
Inception Discriminative Score (P-IDS/U-IDS), which robustly measures the
perceptual fidelity of inpainted images compared to real images via linear
separability in a feature space. Experiments demonstrate superior performance
in terms of both quality and diversity over state-of-the-art methods in
free-form image completion and easy generalization to image-to-image
translation. Code is available at https://github.com/zsyzzsoft/co-mod-gan.","cs.CV,cs.GR,cs.LG"
"What will it take to generate fairness-preserving explanations?. In situations where explanations of black-box models may be useful, the
fairness of the black-box is also often a relevant concern. However, the link
between the fairness of the black-box model and the behavior of explanations
for the black-box is unclear. We focus on explanations applied to tabular
datasets, suggesting that explanations do not necessarily preserve the fairness
properties of the black-box algorithm. In other words, explanation algorithms
can ignore or obscure critical relevant properties, creating incorrect or
misleading explanations. More broadly, we propose future research directions
for evaluating and generating explanations such that they are informative and
relevant from a fairness perspective.","cs.AI,cs.CY,cs.LG"
"Benchmarking Multi-Agent Deep Reinforcement Learning Algorithms in Cooperative Tasks. Multi-agent deep reinforcement learning (MARL) suffers from a lack of
commonly-used evaluation tasks and criteria, making comparisons between
approaches difficult. In this work, we consistently evaluate and compare three
different classes of MARL algorithms (independent learning, centralised
multi-agent policy gradient, value decomposition) in a diverse range of
cooperative multi-agent learning tasks. Our experiments serve as a reference
for the expected performance of algorithms across different learning tasks, and
we provide insights regarding the effectiveness of different learning
approaches. We open-source EPyMARL, which extends the PyMARL
codebase~\citep{samvelyan19smac} to include additional algorithms and allow for
flexible configuration of algorithm implementation details such as parameter
sharing. Finally, we open-source two environments for multi-agent research
which focus on coordination under sparse rewards.","cs.AI,cs.LG,cs.MA,stat.ML"
"Logic Explained Networks. The large and still increasing popularity of deep learning clashes with a
major limit of neural network architectures, that consists in their lack of
capability in providing human-understandable motivations of their decisions. In
situations in which the machine is expected to support the decision of human
experts, providing a comprehensible explanation is a feature of crucial
importance. The language used to communicate the explanations must be formal
enough to be implementable in a machine and friendly enough to be
understandable by a wide audience. In this paper, we propose a general approach
to Explainable Artificial Intelligence in the case of neural architectures,
showing how a mindful design of the networks leads to a family of interpretable
deep learning models called Logic Explained Networks (LENs). LENs only require
their inputs to be human-understandable predicates, and they provide
explanations in terms of simple First-Order Logic (FOL) formulas involving such
predicates. LENs are general enough to cover a large number of scenarios.
Amongst them, we consider the case in which LENs are directly used as special
classifiers with the capability of being explainable, or when they act as
additional networks with the role of creating the conditions for making a
black-box classifier explainable by FOL formulas. Despite supervised learning
problems are mostly emphasized, we also show that LENs can learn and provide
explanations in unsupervised learning settings. Experimental results on several
datasets and tasks show that LENs may yield better classifications than
established white-box models, such as decision trees and Bayesian rule lists,
while providing more compact and meaningful explanations.","cs.AI,cs.LG,cs.LO,cs.NE"
"TilinGNN: Learning to Tile with Self-Supervised Graph Neural Network. We introduce the first neural optimization framework to solve a classical
instance of the tiling problem. Namely, we seek a non-periodic tiling of an
arbitrary 2D shape using one or more types of tiles: the tiles maximally fill
the shape's interior without overlaps or holes. To start, we reformulate tiling
as a graph problem by modeling candidate tile locations in the target shape as
graph nodes and connectivity between tile locations as edges. Further, we build
a graph convolutional neural network, coined TilinGNN, to progressively
propagate and aggregate features over graph edges and predict tile placements.
TilinGNN is trained by maximizing the tiling coverage on target shapes, while
avoiding overlaps and holes between the tiles. Importantly, our network is
self-supervised, as we articulate these criteria as loss terms defined on the
network outputs, without the need of ground-truth tiling solutions. After
training, the runtime of TilinGNN is roughly linear to the number of candidate
tile locations, significantly outperforming traditional combinatorial search.
We conducted various experiments on a variety of shapes to showcase the speed
and versatility of TilinGNN. We also present comparisons to alternative methods
and manual solutions, robustness analysis, and ablation studies to demonstrate
the quality of our approach.","cs.CG,cs.CV,cs.GR,cs.LG"
"An Explainable AI System for the Diagnosis of High Dimensional Biomedical Data. Typical state of the art flow cytometry data samples consists of measures of
more than 100.000 cells in 10 or more features. AI systems are able to diagnose
such data with almost the same accuracy as human experts. However, there is one
central challenge in such systems: their decisions have far-reaching
consequences for the health and life of people, and therefore, the decisions of
AI systems need to be understandable and justifiable by humans. In this work,
we present a novel explainable AI method, called ALPODS, which is able to
classify (diagnose) cases based on clusters, i.e., subpopulations, in the
high-dimensional data. ALPODS is able to explain its decisions in a form that
is understandable for human experts. For the identified subpopulations, fuzzy
reasoning rules expressed in the typical language of domain experts are
generated. A visualization method based on these rules allows human experts to
understand the reasoning used by the AI system. A comparison to a selection of
state of the art explainable AI systems shows that ALPODS operates efficiently
on known benchmark data and also on everyday routine case data.","68T05,I.2; I.5,cs.AI,cs.LG,q-bio.QM,stat.ML"
"A Joint Network for Grasp Detection Conditioned on Natural Language Commands. We consider the task of grasping a target object based on a natural language
command query. Previous work primarily focused on localizing the object given
the query, which requires a separate grasp detection module to grasp it. The
cascaded application of two pipelines incurs errors in overlapping multi-object
cases due to ambiguity in the individual outputs. This work proposes a model
named Command Grasping Network(CGNet) to directly output command satisficing
grasps from RGB image and textual command inputs. A dataset with ground truth
(image, command, grasps) tuple is generated based on the VMRD dataset to train
the proposed network. Experimental results on the generated test set show that
CGNet outperforms a cascaded object-retrieval and grasp detection baseline by a
large margin. Three physical experiments demonstrate the functionality and
performance of CGNet.","cs.CV,cs.RO"
"Convolutional Transformer based Dual Discriminator Generative Adversarial Networks for Video Anomaly Detection. Detecting abnormal activities in real-world surveillance videos is an
important yet challenging task as the prior knowledge about video anomalies is
usually limited or unavailable. Despite that many approaches have been
developed to resolve this problem, few of them can capture the normal
spatio-temporal patterns effectively and efficiently. Moreover, existing works
seldom explicitly consider the local consistency at frame level and global
coherence of temporal dynamics in video sequences. To this end, we propose
Convolutional Transformer based Dual Discriminator Generative Adversarial
Networks (CT-D2GAN) to perform unsupervised video anomaly detection.
Specifically, we first present a convolutional transformer to perform future
frame prediction. It contains three key components, i.e., a convolutional
encoder to capture the spatial information of the input video clips, a temporal
self-attention module to encode the temporal dynamics, and a convolutional
decoder to integrate spatio-temporal features and predict the future frame.
Next, a dual discriminator based adversarial training procedure, which jointly
considers an image discriminator that can maintain the local consistency at
frame-level and a video discriminator that can enforce the global coherence of
temporal dynamics, is employed to enhance the future frame prediction. Finally,
the prediction error is used to identify abnormal video frames. Thoroughly
empirical studies on three public video anomaly detection datasets, i.e., UCSD
Ped2, CUHK Avenue, and Shanghai Tech Campus, demonstrate the effectiveness of
the proposed adversarial spatio-temporal modeling framework.","68T45,I.4.8; I.4.9; I.2.10,cs.AI,cs.CV,cs.LG,cs.MM"
"U2-ONet: A Two-level Nested Octave U-structure with Multiscale Attention Mechanism for Moving Instances Segmentation. Most scenes in practical applications are dynamic scenes containing moving
objects, so segmenting accurately moving objects is crucial for many computer
vision applications. In order to efficiently segment out all moving objects in
the scene, regardless of whether the object has a predefined semantic label, we
propose a two-level nested Octave U-structure network with a multiscale
attention mechanism called U2-ONet. Each stage of U2-ONet is filled with our
newly designed Octave ReSidual U-block (ORSU) to enhance the ability to obtain
more context information at different scales while reducing spatial redundancy
of feature maps. In order to efficiently train our multi-scale deep network, we
introduce a hierarchical training supervision strategy that calculates the loss
at each level while adding a knowledge matching loss to keep the optimization
consistency. Experimental results show that our method achieves
state-of-the-art performance in several general moving objects segmentation
datasets.","cs.CV,cs.LG,eess.IV"
"Which transformer architecture fits my data? A vocabulary bottleneck in self-attention. After their successful debut in natural language processing, Transformer
architectures are now becoming the de-facto standard in many domains. An
obstacle for their deployment over new modalities is the architectural
configuration: the optimal depth-to-width ratio has been shown to dramatically
vary across data types (e.g., $10$x larger over images than over language). We
theoretically predict the existence of an embedding rank bottleneck that limits
the contribution of self-attention width to the Transformer expressivity. We
thus directly tie the input vocabulary size and rank to the optimal
depth-to-width ratio, since a small vocabulary size or rank dictates an added
advantage of depth over width. We empirically demonstrate the existence of this
bottleneck and its implications on the depth-to-width interplay of Transformer
architectures, linking the architecture variability across domains to the often
glossed-over usage of different vocabulary sizes or embedding ranks in
different domains. As an additional benefit, our rank bottlenecking framework
allows us to identify size redundancies of $25\%-50\%$ in leading NLP models
such as ALBERT and T5.","cs.CL,cs.LG"
"FoleyGAN: Visually Guided Generative Adversarial Network-Based Synchronous Sound Generation in Silent Videos. Deep learning based visual to sound generation systems essentially need to be
developed particularly considering the synchronicity aspects of visual and
audio features with time. In this research we introduce a novel task of guiding
a class conditioned generative adversarial network with the temporal visual
information of a video input for visual to sound generation task adapting the
synchronicity traits between audio-visual modalities. Our proposed FoleyGAN
model is capable of conditioning action sequences of visual events leading
towards generating visually aligned realistic sound tracks. We expand our
previously proposed Automatic Foley dataset to train with FoleyGAN and evaluate
our synthesized sound through human survey that shows noteworthy (on average
81\%) audio-visual synchronicity performance. Our approach also outperforms in
statistical experiments compared with other baseline models and audio-visual
datasets.","68T10 (primary) 68T07,68U10(secondary),I.5.4; I.2.10; J.5,cs.AI,cs.CV,cs.LG,cs.MM,cs.SD"
"Interpretable Network Representation Learning with Principal Component Analysis. We consider the problem of interpretable network representation learning for
samples of network-valued data. We propose the Principal Component Analysis for
Networks (PCAN) algorithm to identify statistically meaningful low-dimensional
representations of a network sample via subgraph count statistics. The PCAN
procedure provides an interpretable framework for which one can readily
visualize, explore, and formulate predictive models for network samples. We
furthermore introduce a fast sampling-based algorithm, sPCAN, which is
significantly more computationally efficient than its counterpart, but still
enjoys advantages of interpretability. We investigate the relationship between
these two methods and analyze their large-sample properties under the common
regime where the sample of networks is a collection of kernel-based random
graphs. We show that under this regime, the embeddings of the sPCAN method
enjoy a central limit theorem and moreover that the population level embeddings
of PCAN and sPCAN are equivalent. We assess PCAN's ability to visualize,
cluster, and classify observations in network samples arising in nature,
including functional connectivity network samples and dynamic networks
describing the political co-voting habits of the U.S. Senate. Our analyses
reveal that our proposed algorithm provides informative and discriminatory
features describing the networks in each sample. The PCAN and sPCAN methods
build on the current literature of network representation learning and set the
stage for a new line of research in interpretable learning on network-valued
data. Publicly available software for the PCAN and sPCAN methods are available
at https://www.github.com/jihuilee/.","cs.LG,stat.ME,stat.ML"
"GPU-Accelerated Primal Learning for Extremely Fast Large-Scale Classification. One of the most efficient methods to solve L2-regularized primal problems,
such as logistic regression and linear support vector machine (SVM)
classification, is the widely used trust region Newton algorithm, TRON. While
TRON has recently been shown to enjoy substantial speedups on shared-memory
multi-core systems, exploiting graphical processing units (GPUs) to speed up
the method is significantly more difficult, owing to the highly complex and
heavily sequential nature of the algorithm. In this work, we show that using
judicious GPU-optimization principles, TRON training time for different losses
and feature representations may be drastically reduced. For sparse feature
sets, we show that using GPUs to train logistic regression classifiers in
LIBLINEAR is up to an order-of-magnitude faster than solely using
multithreading. For dense feature sets--which impose far more stringent memory
constraints--we show that GPUs substantially reduce the lengthy SVM learning
times required for state-of-the-art proteomics analysis, leading to dramatic
improvements over recently proposed speedups. Furthermore, we show how GPU
speedups may be mixed with multithreading to enable such speedups when the
dataset is too large for GPU memory requirements; on a massive dense proteomics
dataset of nearly a quarter-billion data instances, these mixed-architecture
speedups reduce SVM analysis time from over half a week to less than a single
day while using limited GPU memory.","cs.DC,cs.LG,q-bio.QM,stat.ML"
"HySTER: A Hybrid Spatio-Temporal Event Reasoner. The task of Video Question Answering (VideoQA) consists in answering natural
language questions about a video and serves as a proxy to evaluate the
performance of a model in scene sequence understanding. Most methods designed
for VideoQA up-to-date are end-to-end deep learning architectures which
struggle at complex temporal and causal reasoning and provide limited
transparency in reasoning steps. We present the HySTER: a Hybrid
Spatio-Temporal Event Reasoner to reason over physical events in videos. Our
model leverages the strength of deep learning methods to extract information
from video frames with the reasoning capabilities and explainability of
symbolic artificial intelligence in an answer set programming framework. We
define a method based on general temporal, causal and physics rules which can
be transferred across tasks. We apply our model to the CLEVRER dataset and
demonstrate state-of-the-art results in question answering accuracy. This work
sets the foundations for the incorporation of inductive logic programming in
the field of VideoQA.","cs.AI,cs.CL,cs.CV,cs.LO"
"Hard and Soft EM in Bayesian Network Learning from Incomplete Data. Incomplete data are a common feature in many domains, from clinical trials to
industrial applications. Bayesian networks (BNs) are often used in these
domains because of their graphical and causal interpretations. BN parameter
learning from incomplete data is usually implemented with the
Expectation-Maximisation algorithm (EM), which computes the relevant sufficient
statistics (""soft EM"") using belief propagation. Similarly, the Structural
Expectation-Maximisation algorithm (Structural EM) learns the network structure
of the BN from those sufficient statistics using algorithms designed for
complete data. However, practical implementations of parameter and structure
learning often impute missing data (""hard EM"") to compute sufficient statistics
instead of using belief propagation, for both ease of implementation and
computational speed. In this paper, we investigate the question: what is the
impact of using imputation instead of belief propagation on the quality of the
resulting BNs? From a simulation study using synthetic data and reference BNs,
we find that it is possible to recommend one approach over the other in several
scenarios based on the characteristics of the data. We then use this
information to build a simple decision tree to guide practitioners in choosing
the EM algorithm best suited to their problem.","cs.LG,stat.ME,stat.ML"
"Metric on Nonlinear Dynamical Systems with Perron-Frobenius Operators. The development of a metric for structural data is a long-term problem in
pattern recognition and machine learning. In this paper, we develop a general
metric for comparing nonlinear dynamical systems that is defined with
Perron-Frobenius operators in reproducing kernel Hilbert spaces. Our metric
includes the existing fundamental metrics for dynamical systems, which are
basically defined with principal angles between some appropriately-chosen
subspaces, as its special cases. We also describe the estimation of our metric
from finite data. We empirically illustrate our metric with an example of
rotation dynamics in a unit disk in a complex plane, and evaluate the
performance with real-world time-series data.","37N99,46E22,47B32,cs.LG,math.DS,math.FA,stat.ML"
"An analysis on the use of autoencoders for representation learning: fundamentals, learning task case studies, explainability and challenges. In many machine learning tasks, learning a good representation of the data
can be the key to building a well-performant solution. This is because most
learning algorithms operate with the features in order to find models for the
data. For instance, classification performance can improve if the data is
mapped to a space where classes are easily separated, and regression can be
facilitated by finding a manifold of data in the feature space. As a general
rule, features are transformed by means of statistical methods such as
principal component analysis, or manifold learning techniques such as Isomap or
locally linear embedding. From a plethora of representation learning methods,
one of the most versatile tools is the autoencoder. In this paper we aim to
demonstrate how to influence its learned representations to achieve the desired
learning behavior. To this end, we present a series of learning tasks: data
embedding for visualization, image denoising, semantic hashing, detection of
abnormal behaviors and instance generation. We model them from the
representation learning perspective, following the state of the art
methodologies in each field. A solution is proposed for each task employing
autoencoders as the only learning method. The theoretical developments are put
into practice using a selection of datasets for the different problems and
implementing each solution, followed by a discussion of the results in each
case study and a brief explanation of other six learning applications. We also
explore the current challenges and approaches to explainability in the context
of autoencoders. All of this helps conclude that, thanks to alterations in
their structure as well as their objective function, autoencoders may be the
core of a possible solution to many problems which can be modeled as a
transformation of the feature space.","68T05,cs.LG,stat.ML"
"Robust 3D Self-portraits in Seconds. In this paper, we propose an efficient method for robust 3D self-portraits
using a single RGBD camera. Benefiting from the proposed PIFusion and
lightweight bundle adjustment algorithm, our method can generate detailed 3D
self-portraits in seconds and shows the ability to handle subjects wearing
extremely loose clothes. To achieve highly efficient and robust reconstruction,
we propose PIFusion, which combines learning-based 3D recovery with volumetric
non-rigid fusion to generate accurate sparse partial scans of the subject.
Moreover, a non-rigid volumetric deformation method is proposed to continuously
refine the learned shape prior. Finally, a lightweight bundle adjustment
algorithm is proposed to guarantee that all the partial scans can not only
""loop"" with each other but also remain consistent with the selected live key
observations. The results and experiments show that the proposed method
achieves more robust and efficient 3D self-portraits compared with
state-of-the-art methods.","cs.CV,cs.GR"
"Balancing Geometry and Density: Path Distances on High-Dimensional Data. New geometric and computational analyses of power-weighted shortest-path
distances (PWSPDs) are presented. By illuminating the way these metrics balance
density and geometry in the underlying data, we clarify their key parameters
and discuss how they may be chosen in practice. Comparisons are made with
related data-driven metrics, which illustrate the broader role of density in
kernel-based unsupervised and semi-supervised machine learning.
Computationally, we relate PWSPDs on complete weighted graphs to their
analogues on weighted nearest neighbor graphs, providing high probability
guarantees on their equivalence that are near-optimal. Connections with
percolation theory are developed to establish estimates on the bias and
variance of PWSPDs in the finite sample setting. The theoretical results are
bolstered by illustrative experiments, demonstrating the versatility of PWSPDs
for a wide range of data settings. Throughout the paper, our results require
only that the underlying data is sampled from a low-dimensional manifold, and
depend crucially on the intrinsic dimension of this manifold, rather than its
ambient dimension.","05C80,05C85,I.5.3,cs.DS,cs.LG,stat.ML"
"Learning Geometric Concepts with Nasty Noise. We study the efficient learnability of geometric concept classes -
specifically, low-degree polynomial threshold functions (PTFs) and
intersections of halfspaces - when a fraction of the data is adversarially
corrupted. We give the first polynomial-time PAC learning algorithms for these
concept classes with dimension-independent error guarantees in the presence of
nasty noise under the Gaussian distribution. In the nasty noise model, an
omniscient adversary can arbitrarily corrupt a small fraction of both the
unlabeled data points and their labels. This model generalizes well-studied
noise models, including the malicious noise model and the agnostic (adversarial
label noise) model. Prior to our work, the only concept class for which
efficient malicious learning algorithms were known was the class of
origin-centered halfspaces.
  Specifically, our robust learning algorithm for low-degree PTFs succeeds
under a number of tame distributions -- including the Gaussian distribution
and, more generally, any log-concave distribution with (approximately) known
low-degree moments. For LTFs under the Gaussian distribution, we give a
polynomial-time algorithm that achieves error $O(\epsilon)$, where $\epsilon$
is the noise rate. At the core of our PAC learning results is an efficient
algorithm to approximate the low-degree Chow-parameters of any bounded function
in the presence of nasty noise. To achieve this, we employ an iterative
spectral method for outlier detection and removal, inspired by recent work in
robust unsupervised learning. Our aforementioned algorithm succeeds for a range
of distributions satisfying mild concentration bounds and moment assumptions.
The correctness of our robust learning algorithm for intersections of
halfspaces makes essential use of a novel robust inverse independence lemma
that may be of broader interest.","cs.CC,cs.DS,cs.LG"
"Learning Dual Semantic Relations with Graph Attention for Image-Text Matching. Image-Text Matching is one major task in cross-modal information processing.
The main challenge is to learn the unified visual and textual representations.
Previous methods that perform well on this task primarily focus on not only the
alignment between region features in images and the corresponding words in
sentences, but also the alignment between relations of regions and relational
words. However, the lack of joint learning of regional features and global
features will cause the regional features to lose contact with the global
context, leading to the mismatch with those non-object words which have global
meanings in some sentences. In this work, in order to alleviate this issue, it
is necessary to enhance the relations between regions and the relations between
regional and global concepts to obtain a more accurate visual representation so
as to be better correlated to the corresponding text. Thus, a novel multi-level
semantic relations enhancement approach named Dual Semantic Relations Attention
Network(DSRAN) is proposed which mainly consists of two modules, separate
semantic relations module and the joint semantic relations module. DSRAN
performs graph attention in both modules respectively for region-level
relations enhancement and regional-global relations enhancement at the same
time. With these two modules, different hierarchies of semantic relations are
learned simultaneously, thus promoting the image-text matching process by
providing more information for the final visual representation. Quantitative
experimental results have been performed on MS-COCO and Flickr30K and our
method outperforms previous approaches by a large margin due to the
effectiveness of the dual semantic relations learning scheme. Codes are
available at https://github.com/kywen1119/DSRAN.","cs.CV,cs.MM"
"Consistent Feature Selection for Analytic Deep Neural Networks. One of the most important steps toward interpretability and explainability of
neural network models is feature selection, which aims to identify the subset
of relevant features. Theoretical results in the field have mostly focused on
the prediction aspect of the problem with virtually no work on feature
selection consistency for deep neural networks due to the model's severe
nonlinearity and unidentifiability. This lack of theoretical foundation casts
doubt on the applicability of deep learning to contexts where correct
interpretations of the features play a central role.
  In this work, we investigate the problem of feature selection for analytic
deep networks. We prove that for a wide class of networks, including deep
feed-forward neural networks, convolutional neural networks, and a major
sub-class of residual neural networks, the Adaptive Group Lasso selection
procedure with Group Lasso as the base estimator is selection-consistent. The
work provides further evidence that Group Lasso might be inefficient for
feature selection with neural networks and advocates the use of Adaptive Group
Lasso over the popular Group Lasso.","cs.LG,math.ST,stat.ML,stat.TH"
"Trek separation for Gaussian graphical models. Gaussian graphical models are semi-algebraic subsets of the cone of positive
definite covariance matrices. Submatrices with low rank correspond to
generalizations of conditional independence constraints on collections of
random variables. We give a precise graph-theoretic characterization of when
submatrices of the covariance matrix have small rank for a general class of
mixed graphs that includes directed acyclic and undirected graphs as special
cases. Our new trek separation criterion generalizes the familiar
$d$-separation criterion. Proofs are based on the trek rule, the resulting
matrix factorizations and classical theorems of algebraic combinatorics on the
expansions of determinants of path polynomials.","math.CO,math.ST,stat.ML,stat.TH"
"Natural Language Person Search Using Deep Reinforcement Learning. Recent success in deep reinforcement learning is having an agent learn how to
play Go and beat the world champion without any prior knowledge of the game. In
that task, the agent has to make a decision on what action to take based on the
positions of the pieces. Person Search is recently explored using natural
language based text description of images for video surveillance applications
(S.Li et.al). We see (Fu.et al) provides an end to end approach for
object-based retrieval using deep reinforcement learning without constraints
placed on which objects are being detected. However, we believe for real-world
applications such as person search defining specific constraints which identify
a person as opposed to starting with a general object detection will have
benefits in terms of performance and computational resources required. In our
task, Deep reinforcement learning would localize the person in an image by
reshaping the sizes of the bounding boxes. Deep Reinforcement learning with
appropriate constraints would look only for the relevant person in the image as
opposed to an unconstrained approach where each individual objects in the image
are ranked. For person search, the agent is trying to form a tight bounding box
around the person in the image who matches the description. The bounding box is
initialized to the full image and at each time step, the agent makes a decision
on how to change the current bounding box so that it has a tighter bound around
the person based on the description of the person and the pixel values of the
current bounding box. After the agent takes an action, it will be given a
reward based on the Intersection over Union (IoU) of the current bounding box
and the ground truth box. Once the agent believes that the bounding box is
covering the person, it will indicate that the person is found.","cs.CV,cs.MM"
"Transfer Learning using Representation Learning in Massive Open Online Courses. In a Massive Open Online Course (MOOC), predictive models of student behavior
can support multiple aspects of learning, including instructor feedback and
timely intervention. Ongoing courses, when the student outcomes are yet
unknown, must rely on models trained from the historical data of previously
offered courses. It is possible to transfer models, but they often have poor
prediction performance. One reason is features that inadequately represent
predictive attributes common to both courses. We present an automated
transductive transfer learning approach that addresses this issue. It relies on
problem-agnostic, temporal organization of the MOOC clickstream data, where,
for each student, for multiple courses, a set of specific MOOC event types is
expressed for each time unit. It consists of two alternative transfer methods
based on representation learning with auto-encoders: a passive approach using
transductive principal component analysis and an active approach that uses a
correlation alignment loss term. With these methods, we investigate the
transferability of dropout prediction across similar and dissimilar MOOCs and
compare with known methods. Results show improved model transferability and
suggest that the methods are capable of automatically learning a feature
representation that expresses common predictive characteristics of MOOCs.","cs.CY,cs.LG,stat.ML"
"P$^2$GNet: Pose-Guided Point Cloud Generating Networks for 6-DoF Object Pose Estimation. Humans are able to perform fast and accurate object pose estimation even
under severe occlusion by exploiting learned object model priors from everyday
life. However, most recently proposed pose estimation algorithms neglect to
utilize the information of object models, often end up with limited accuracy,
and tend to fall short in cluttered scenes. In this paper, we present a novel
learning-based model, \underline{P}ose-Guided \underline{P}oint Cloud
\underline{G}enerating Networks for 6D Object Pose Estimation (P$^2$GNet),
designed to effectively exploit object model priors to facilitate 6D object
pose estimation. We achieve this with an end-to-end estimation-by-generation
workflow that combines the appearance information from the RGB-D image and the
structure knowledge from object point cloud to enable accurate and robust pose
estimation. Experiments on two commonly used benchmarks for 6D pose estimation,
YCB-Video dataset and LineMOD dataset, demonstrate that P$^2$GNet outperforms
the state-of-the-art method by a large margin and shows marked robustness
towards heavy occlusion, while achieving real-time inference.","cs.CV,cs.RO"
"Classification-based Approximate Policy Iteration: Experiments and Extended Discussions. Tackling large approximate dynamic programming or reinforcement learning
problems requires methods that can exploit regularities, or intrinsic
structure, of the problem in hand. Most current methods are geared towards
exploiting the regularities of either the value function or the policy. We
introduce a general classification-based approximate policy iteration (CAPI)
framework, which encompasses a large class of algorithms that can exploit
regularities of both the value function and the policy space, depending on what
is advantageous. This framework has two main components: a generic value
function estimator and a classifier that learns a policy based on the estimated
value function. We establish theoretical guarantees for the sample complexity
of CAPI-style algorithms, which allow the policy evaluation step to be
performed by a wide variety of algorithms (including temporal-difference-style
methods), and can handle nonparametric representations of policies. Our bounds
on the estimation error of the performance loss are tighter than existing
results. We also illustrate this approach empirically on several problems,
including a large HIV control task.","49L20 (Secondary),68T05 (Primary),90C40,93E20,93E35,I.2.6; I.2.8,cs.LG,cs.SY,math.OC,stat.ML"
"Stochastic Proximal Gradient Algorithm with Minibatches. Application to Large Scale Learning Models. Stochastic optimization lies at the core of most statistical learning models.
The recent great development of stochastic algorithmic tools focused
significantly onto proximal gradient iterations, in order to find an efficient
approach for nonsmooth (composite) population risk functions. The complexity of
finding optimal predictors by minimizing regularized risk is largely understood
for simple regularizations such as $\ell_1/\ell_2$ norms. However, more complex
properties desired for the predictor necessitates highly difficult regularizers
as used in grouped lasso or graph trend filtering. In this chapter we develop
and analyze minibatch variants of stochastic proximal gradient algorithm for
general composite objective functions with stochastic nonsmooth components. We
provide iteration complexity for constant and variable stepsize policies
obtaining that, for minibatch size $N$, after
$\mathcal{O}(\frac{1}{N\epsilon})$ iterations $\epsilon-$suboptimality is
attained in expected quadratic distance to optimal solution. The numerical
tests on $\ell_2-$regularized SVMs and parametric sparse representation
problems confirm the theoretical behaviour and surpasses minibatch SGD
performance.","cs.LG,cs.NA,math.NA,math.OC,stat.ML"
"Multi-Scale Contrastive Siamese Networks for Self-Supervised Graph Representation Learning. Graph representation learning plays a vital role in processing
graph-structured data. However, prior arts on graph representation learning
heavily rely on labeling information. To overcome this problem, inspired by the
recent success of graph contrastive learning and Siamese networks in visual
representation learning, we propose a novel self-supervised approach in this
paper to learn node representations by enhancing Siamese self-distillation with
multi-scale contrastive learning. Specifically, we first generate two augmented
views from the input graph based on local and global perspectives. Then, we
employ two objectives called cross-view and cross-network contrastiveness to
maximize the agreement between node representations across different views and
networks. To demonstrate the effectiveness of our approach, we perform
empirical experiments on five real-world datasets. Our method not only achieves
new state-of-the-art results but also surpasses some semi-supervised
counterparts by large margins. Code is made available at
https://github.com/GRAND-Lab/MERIT","cs.LG,cs.SI"
"EnhanceNet: Single Image Super-Resolution Through Automated Texture Synthesis. Single image super-resolution is the task of inferring a high-resolution
image from a single low-resolution input. Traditionally, the performance of
algorithms for this task is measured using pixel-wise reconstruction measures
such as peak signal-to-noise ratio (PSNR) which have been shown to correlate
poorly with the human perception of image quality. As a result, algorithms
minimizing these metrics tend to produce over-smoothed images that lack
high-frequency textures and do not look natural despite yielding high PSNR
values.
  We propose a novel application of automated texture synthesis in combination
with a perceptual loss focusing on creating realistic textures rather than
optimizing for a pixel-accurate reproduction of ground truth images during
training. By using feed-forward fully convolutional neural networks in an
adversarial training setting, we achieve a significant boost in image quality
at high magnification ratios. Extensive experiments on a number of datasets
show the effectiveness of our approach, yielding state-of-the-art results in
both quantitative and qualitative benchmarks.","cs.AI,cs.CV,stat.ML"
"Physics-Informed Graphical Neural Network for Parameter & State Estimations in Power Systems. Parameter Estimation (PE) and State Estimation (SE) are the most wide-spread
tasks in the system engineering. They need to be done automatically, fast and
frequently, as measurements arrive. Deep Learning (DL) holds the promise of
tackling the challenge, however in so far, as PE and SE in power systems is
concerned, (a) DL did not win trust of the system operators because of the lack
of the physics of electricity based, interpretations and (b) DL remained
illusive in the operational regimes were data is scarce. To address this, we
present a hybrid scheme which embeds physics modeling of power systems into
Graphical Neural Networks (GNN), therefore empowering system operators with a
reliable and explainable real-time predictions which can then be used to
control the critical infrastructure. To enable progress towards trustworthy DL
for PE and SE, we build a physics-informed method, named Power-GNN, which
reconstructs physical, thus interpretable, parameters within Effective Power
Flow (EPF) models, such as admittances of effective power lines, and NN
parameters, representing implicitly unobserved elements of the system. In our
experiments, we test the Power-GNN on different realistic power networks,
including these with thousands of loads and hundreds of generators. We show
that the Power-GNN outperforms vanilla NN scheme unaware of the EPF physics.","cs.LG,cs.SY,eess.SY,physics.soc-ph"
"Why does CTC result in peaky behavior?. The peaky behavior of CTC models is well known experimentally. However, an
understanding about why peaky behavior occurs is missing, and whether this is a
good property. We provide a formal analysis of the peaky behavior and gradient
descent convergence properties of the CTC loss and related training criteria.
Our analysis provides a deep understanding why peaky behavior occurs and when
it is suboptimal. On a simple example which should be trivial to learn for any
model, we prove that a feed-forward neural network trained with CTC from
uniform initialization converges towards peaky behavior with a 100% error rate.
Our analysis further explains why CTC only works well together with the blank
label. We further demonstrate that peaky behavior does not occur on other
related losses including a label prior model, and that this improves
convergence.","cs.AI,cs.CL,cs.LG,cs.NE,cs.SD,eess.AS,math.ST,stat.TH"
"Learning Interpretable Representations of Entanglement in Quantum Optics Experiments using Deep Generative Models. Quantum physics experiments produce interesting phenomena such as
interference or entanglement, which is a core property of numerous future
quantum technologies. The complex relationship between a quantum experiment's
structure and its entanglement properties is essential to fundamental research
in quantum optics but is difficult to intuitively understand. We present the
first deep generative model of quantum optics experiments where a variational
autoencoder (QOVAE) is trained on a dataset of experimental setups. In a series
of computational experiments, we investigate the learned representation of the
QOVAE and its internal understanding of the quantum optics world. We
demonstrate that the QOVAE learns an intrepretable representation of quantum
optics experiments and the relationship between experiment structure and
entanglement. We show the QOVAE is able to generate novel experiments for
highly entangled quantum states with specific distributions that match its
training data. Importantly, we are able to fully interpret how the QOVAE
structures its latent space, finding curious patterns that we can entirely
explain in terms of quantum physics. The results demonstrate how we can
successfully use and understand the internal representations of deep generative
models in a complex scientific domain. The QOVAE and the insights from our
investigations can be immediately applied to other physical systems throughout
fundamental scientific research.","cs.LG,quant-ph"
"Deep Learning Applied to Image and Text Matching. The ability to describe images with natural language sentences is the
hallmark for image and language understanding. Such a system has wide ranging
applications such as annotating images and using natural sentences to search
for images.In this project we focus on the task of bidirectional image
retrieval: such asystem is capable of retrieving an image based on a sentence
(image search) andretrieve sentence based on an image query (image annotation).
We present asystem based on a global ranking objective function which uses a
combinationof convolutional neural networks (CNN) and multi layer perceptrons
(MLP).It takes a pair of image and sentence and processes them in different
channels,finally embedding it into a common multimodal vector space. These
embeddingsencode abstract semantic information about the two inputs and can be
comparedusing traditional information retrieval approaches. For each such pair,
the modelreturns a score which is interpretted as a similarity metric. If this
score is high,the image and sentence are likely to convey similar meaning, and
if the score is low then they are likely not to.
  The visual input is modeled via deep convolutional neural network. On
theother hand we explore three models for the textual module. The first one
isbag of words with an MLP. The second one uses n-grams (bigram, trigrams,and a
combination of trigram & skip-grams) with an MLP. The third is morespecialized
deep network specific for modeling variable length sequences (SSE).We report
comparable performance to recent work in the field, even though ouroverall
model is simpler. We also show that the training time choice of how wecan
generate our negative samples has a significant impact on performance, and can
be used to specialize the bi-directional system in one particular task.","cs.CL,cs.CV,cs.LG"
"DeepFlow: History Matching in the Space of Deep Generative Models. The calibration of a reservoir model with observed transient data of fluid
pressures and rates is a key task in obtaining a predictive model of the flow
and transport behaviour of the earth's subsurface. The model calibration task,
commonly referred to as ""history matching"", can be formalised as an ill-posed
inverse problem where we aim to find the underlying spatial distribution of
petrophysical properties that explain the observed dynamic data. We use a
generative adversarial network pretrained on geostatistical object-based models
to represent the distribution of rock properties for a synthetic model of a
hydrocarbon reservoir. The dynamic behaviour of the reservoir fluids is
modelled using a transient two-phase incompressible Darcy formulation. We
invert for the underlying reservoir properties by first modeling property
distributions using the pre-trained generative model then using the adjoint
equations of the forward problem to perform gradient descent on the latent
variables that control the output of the generative model. In addition to the
dynamic observation data, we include well rock-type constraints by introducing
an additional objective function. Our contribution shows that for a synthetic
test case, we are able to obtain solutions to the inverse problem by optimising
in the latent variable space of a deep generative model, given a set of
transient observations of a non-linear forward problem.","68T10 (Primary) 68T20,76T99 (Secondary),86-04,86-08,I.5.1; I.6.3; I.5.4; I.2.8,cs.CV,cs.LG,physics.comp-ph,physics.geo-ph,stat.ML"
"ParNet: Position-aware Aggregated Relation Network for Image-Text matching. Exploring fine-grained relationship between entities(e.g. objects in image or
words in sentence) has great contribution to understand multimedia content
precisely. Previous attention mechanism employed in image-text matching either
takes multiple self attention steps to gather correspondences or uses image
objects (or words) as context to infer image-text similarity. However, they
only take advantage of semantic information without considering that objects'
relative position also contributes to image understanding. To this end, we
introduce a novel position-aware relation module to model both the semantic and
spatial relationship simultaneously for image-text matching in this paper.
Given an image, our method utilizes the location of different objects to
capture spatial relationship innovatively. With the combination of semantic and
spatial relationship, it's easier to understand the content of different
modalities (images and sentences) and capture fine-grained latent
correspondences of image-text pairs. Besides, we employ a two-step aggregated
relation module to capture interpretable alignment of image-text pairs. The
first step, we call it intra-modal relation mechanism, in which we computes
responses between different objects in an image or different words in a
sentence separately; The second step, we call it inter-modal relation
mechanism, in which the query plays a role of textual context to refine the
relationship among object proposals in an image. In this way, our
position-aware aggregated relation network (ParNet) not only knows which
entities are relevant by attending on different objects (words) adaptively, but
also adjust the inter-modal correspondence according to the latent alignments
according to query's content. Our approach achieves the state-of-the-art
results on MS-COCO dataset.","cs.CL,cs.CV,cs.LG,cs.MM"
"Testing Closeness With Unequal Sized Samples. We consider the problem of closeness testing for two discrete distributions
in the practically relevant setting of \emph{unequal} sized samples drawn from
each of them. Specifically, given a target error parameter $\varepsilon > 0$,
$m_1$ independent draws from an unknown distribution $p,$ and $m_2$ draws from
an unknown distribution $q$, we describe a test for distinguishing the case
that $p=q$ from the case that $||p-q||_1 \geq \varepsilon$. If $p$ and $q$ are
supported on at most $n$ elements, then our test is successful with high
probability provided $m_1\geq n^{2/3}/\varepsilon^{4/3}$ and $m_2 =
\Omega(\max\{\frac{n}{\sqrt m_1\varepsilon^2}, \frac{\sqrt
n}{\varepsilon^2}\});$ we show that this tradeoff is optimal throughout this
range, to constant factors. These results extend the recent work of Chan et al.
who established the sample complexity when the two samples have equal sizes,
and tightens the results of Acharya et al. by polynomials factors in both $n$
and $\varepsilon$. As a consequence, we obtain an algorithm for estimating the
mixing time of a Markov chain on $n$ states up to a $\log n$ factor that uses
$\tilde{O}(n^{3/2} \tau_{mix})$ queries to a ""next node"" oracle, improving upon
the $\tilde{O}(n^{5/3}\tau_{mix})$ query algorithm of Batu et al. Finally, we
note that the core of our testing algorithm is a relatively simple statistic
that seems to perform well in practice, both on synthetic data and on natural
language data.","cs.IT,cs.LG,math.IT,math.ST,stat.ML,stat.TH"
"ALReLU: A different approach on Leaky ReLU activation function to improve Neural Networks Performance. Despite the unresolved 'dying ReLU problem', the classical ReLU activation
function (AF) has been extensively applied in Deep Neural Networks (DNN), in
particular Convolutional Neural Networks (CNN), for image classification. The
common gradient issues of ReLU pose challenges in applications on academy and
industry sectors. Recent approaches for improvements are in a similar direction
by just proposing variations of the AF, such as Leaky ReLU (LReLU), while
maintaining the solution within the same unresolved gradient problems. In this
paper, the Absolute Leaky ReLU (ALReLU) AF, a variation of LReLU, is proposed,
as an alternative method to resolve the common 'dying ReLU problem' on NN-based
algorithms for supervised learning. The experimental results demonstrate that
by using the absolute values of LReLU's small negative gradient, has a
significant improvement in comparison with LReLU and ReLU, on image
classification of diseases such as COVID-19, text and tabular data
classification tasks on five different datasets.","68T07,68T10,68T45,68T50,68U35,cs.LG,cs.NE"
"Optimal Stopping via Randomized Neural Networks. This paper presents new machine learning approaches to approximate the
solution of optimal stopping problems. The key idea of these methods is to use
neural networks, where the hidden layers are generated randomly and only the
last layer is trained, in order to approximate the continuation value. Our
approaches are applicable for high dimensional problems where the existing
approaches become increasingly impractical. In addition, since our approaches
can be optimized using a simple linear regression, they are very easy to
implement and theoretical guarantees can be provided. In Markovian examples our
randomized reinforcement learning approach and in non-Markovian examples our
randomized recurrent neural network approach outperform the state-of-the-art
and other relevant machine learning approaches.","cs.LG,cs.NA,math.NA,math.PR,q-fin.CP,stat.ML"
"An Optimized and Energy-Efficient Parallel Implementation of Non-Iteratively Trained Recurrent Neural Networks. Recurrent neural networks (RNN) have been successfully applied to various
sequential decision-making tasks, natural language processing applications, and
time-series predictions. Such networks are usually trained through
back-propagation through time (BPTT) which is prohibitively expensive,
especially when the length of the time dependencies and the number of hidden
neurons increase. To reduce the training time, extreme learning machines (ELMs)
have been recently applied to RNN training, reaching a 99\% speedup on some
applications. Due to its non-iterative nature, ELM training, when parallelized,
has the potential to reach higher speedups than BPTT.
  In this work, we present \opt, an optimized parallel RNN training algorithm
based on ELM that takes advantage of the GPU shared memory and of parallel QR
factorization algorithms to efficiently reach optimal solutions. The
theoretical analysis of the proposed algorithm is presented on six RNN
architectures, including LSTM and GRU, and its performance is empirically
tested on ten time-series prediction applications. \opt~is shown to reach up to
845 times speedup over its sequential counterpart and to require up to 20x less
time to train than parallel BPTT.","cs.DC,cs.LG,stat.ML"
"Adversarial Attack Framework on Graph Embedding Models with Limited Knowledge. With the success of the graph embedding model in both academic and industry
areas, the robustness of graph embedding against adversarial attack inevitably
becomes a crucial problem in graph learning. Existing works usually perform the
attack in a white-box fashion: they need to access the predictions/labels to
construct their adversarial loss. However, the inaccessibility of
predictions/labels makes the white-box attack impractical to a real graph
learning system. This paper promotes current frameworks in a more general and
flexible sense -- we demand to attack various kinds of graph embedding models
with black-box driven. We investigate the theoretical connections between graph
signal processing and graph embedding models and formulate the graph embedding
model as a general graph signal process with a corresponding graph filter.
Therefore, we design a generalized adversarial attacker: GF-Attack. Without
accessing any labels and model predictions, GF-Attack can perform the attack
directly on the graph filter in a black-box fashion. We further prove that
GF-Attack can perform an effective attack without knowing the number of layers
of graph embedding models. To validate the generalization of GF-Attack, we
construct the attacker on four popular graph embedding models. Extensive
experiments validate the effectiveness of GF-Attack on several benchmark
datasets.","cs.CR,cs.LG,cs.SI"
"DDSP: Differentiable Digital Signal Processing. Most generative models of audio directly generate samples in one of two
domains: time or frequency. While sufficient to express any signal, these
representations are inefficient, as they do not utilize existing knowledge of
how sound is generated and perceived. A third approach (vocoders/synthesizers)
successfully incorporates strong domain knowledge of signal processing and
perception, but has been less actively researched due to limited expressivity
and difficulty integrating with modern auto-differentiation-based machine
learning methods. In this paper, we introduce the Differentiable Digital Signal
Processing (DDSP) library, which enables direct integration of classic signal
processing elements with deep learning methods. Focusing on audio synthesis, we
achieve high-fidelity generation without the need for large autoregressive
models or adversarial losses, demonstrating that DDSP enables utilizing strong
inductive biases without losing the expressive power of neural networks.
Further, we show that combining interpretable modules permits manipulation of
each separate model component, with applications such as independent control of
pitch and loudness, realistic extrapolation to pitches not seen during
training, blind dereverberation of room acoustics, transfer of extracted room
acoustics to new environments, and transformation of timbre between disparate
sources. In short, DDSP enables an interpretable and modular approach to
generative modeling, without sacrificing the benefits of deep learning. The
library is publicly available at https://github.com/magenta/ddsp and we welcome
further contributions from the community and domain experts.","cs.LG,cs.SD,eess.AS,eess.SP,stat.ML"
"EvolveGraph: Multi-Agent Trajectory Prediction with Dynamic Relational Reasoning. Multi-agent interacting systems are prevalent in the world, from pure
physical systems to complicated social dynamic systems. In many applications,
effective understanding of the situation and accurate trajectory prediction of
interactive agents play a significant role in downstream tasks, such as
decision making and planning. In this paper, we propose a generic trajectory
forecasting framework (named EvolveGraph) with explicit relational structure
recognition and prediction via latent interaction graphs among multiple
heterogeneous, interactive agents. Considering the uncertainty of future
behaviors, the model is designed to provide multi-modal prediction hypotheses.
Since the underlying interactions may evolve even with abrupt changes, and
different modalities of evolution may lead to different outcomes, we address
the necessity of dynamic relational reasoning and adaptively evolving the
interaction graphs. We also introduce a double-stage training pipeline which
not only improves training efficiency and accelerates convergence, but also
enhances model performance. The proposed framework is evaluated on both
synthetic physics simulations and multiple real-world benchmark datasets in
various areas. The experimental results illustrate that our approach achieves
state-of-the-art performance in terms of prediction accuracy.","cs.CV,cs.LG,cs.MA,cs.RO"
"Markov Random Field Segmentation of Brain MR Images. We describe a fully-automatic 3D-segmentation technique for brain MR images.
Using Markov random fields the segmentation algorithm captures three important
MR features, i.e. non-parametric distributions of tissue intensities,
neighborhood correlations and signal inhomogeneities. Detailed simulations and
real MR images demonstrate the performance of the segmentation algorithm. The
impact of noise, inhomogeneity, smoothing and structure thickness is analyzed
quantitatively. Even single echo MR images are well classified into gray
matter, white matter, cerebrospinal fluid, scalp-bone and background. A
simulated annealing and an iterated conditional modes implementation are
presented.
  Keywords: Magnetic Resonance Imaging, Segmentation, Markov Random Fields","cond-mat.stat-mech,cs.CV,physics.data-an,physics.med-ph"
"Fair Decisions Despite Imperfect Predictions. Consequential decisions are increasingly informed by sophisticated
data-driven predictive models. However, to consistently learn accurate
predictive models, one needs access to ground truth labels. Unfortunately, in
practice, labels may only exist conditional on certain decisions---if a loan is
denied, there is not even an option for the individual to pay back the loan.
Hence, the observed data distribution depends on how decisions are being made.
In this paper, we show that in this selective labels setting, learning a
predictor directly only from available labeled data is suboptimal in terms of
both fairness and utility. To avoid this undesirable behavior, we propose to
directly learn decision policies that maximize utility under fairness
constraints and thereby take into account how decisions affect which data is
observed in the future. Our results suggest the need for a paradigm shift in
the context of fair machine learning from the currently prevalent idea of
simply building predictive models from a single static dataset via risk
minimization, to a more interactive notion of ""learning to decide"". In
particular, such policies should not entirely neglect part of the input space,
drawing connections to explore/exploit tradeoffs in reinforcement learning,
data missingness, and potential outcomes in causal inference. Experiments on
synthetic and real-world data illustrate the favorable properties of learning
to decide in terms of utility and fairness.","cs.CY,cs.LG,stat.ML"
"Sound2Sight: Generating Visual Dynamics from Sound and Context. Learning associations across modalities is critical for robust multimodal
reasoning, especially when a modality may be missing during inference. In this
paper, we study this problem in the context of audio-conditioned visual
synthesis -- a task that is important, for example, in occlusion reasoning.
Specifically, our goal is to generate future video frames and their motion
dynamics conditioned on audio and a few past frames. To tackle this problem, we
present Sound2Sight, a deep variational framework, that is trained to learn a
per frame stochastic prior conditioned on a joint embedding of audio and past
frames. This embedding is learned via a multi-head attention-based audio-visual
transformer encoder. The learned prior is then sampled to further condition a
video forecasting module to generate future frames. The stochastic prior allows
the model to sample multiple plausible futures that are consistent with the
provided audio and the past context. Moreover, to improve the quality and
coherence of the generated frames, we propose a multimodal discriminator that
differentiates between a synthesized and a real audio-visual clip. We
empirically evaluate our approach, vis-\'a-vis closely-related prior methods,
on two new datasets viz. (i) Multimodal Stochastic Moving MNIST with a Surprise
Obstacle, (ii) Youtube Paintings; as well as on the existing Audio-Set Drums
dataset. Our extensive experiments demonstrate that Sound2Sight significantly
outperforms the state of the art in the generated video quality, while also
producing diverse video content.","cs.CV,cs.LG,cs.SD,eess.AS"
"Kernel Distribution Embeddings: Universal Kernels, Characteristic Kernels and Kernel Metrics on Distributions. Kernel mean embeddings have recently attracted the attention of the machine
learning community. They map measures $\mu$ from some set $M$ to functions in a
reproducing kernel Hilbert space (RKHS) with kernel $k$. The RKHS distance of
two mapped measures is a semi-metric $d_k$ over $M$. We study three questions.
(I) For a given kernel, what sets $M$ can be embedded? (II) When is the
embedding injective over $M$ (in which case $d_k$ is a metric)? (III) How does
the $d_k$-induced topology compare to other topologies on $M$? The existing
machine learning literature has addressed these questions in cases where $M$ is
(a subset of) the finite regular Borel measures. We unify, improve and
generalise those results. Our approach naturally leads to continuous and
possibly even injective embeddings of (Schwartz-) distributions, i.e.,
generalised measures, but the reader is free to focus on measures only. In
particular, we systemise and extend various (partly known) equivalences between
different notions of universal, characteristic and strictly positive definite
kernels, and show that on an underlying locally compact Hausdorff space, $d_k$
metrises the weak convergence of probability measures if and only if $k$ is
continuous and characteristic.","G.3,math.FA,math.PR,stat.ML"
"A deep-structured fully-connected random field model for structured inference. There has been significant interest in the use of fully-connected graphical
models and deep-structured graphical models for the purpose of structured
inference. However, fully-connected and deep-structured graphical models have
been largely explored independently, leaving the unification of these two
concepts ripe for exploration. A fundamental challenge with unifying these two
types of models is in dealing with computational complexity. In this study, we
investigate the feasibility of unifying fully-connected and deep-structured
models in a computationally tractable manner for the purpose of structured
inference. To accomplish this, we introduce a deep-structured fully-connected
random field (DFRF) model that integrates a series of intermediate sparse
auto-encoding layers placed between state layers to significantly reduce
computational complexity. The problem of image segmentation was used to
illustrate the feasibility of using the DFRF for structured inference in a
computationally tractable manner. Results in this study show that it is
feasible to unify fully-connected and deep-structured models in a
computationally tractable manner for solving structured inference problems such
as image segmentation.","cs.IT,cs.LG,math.IT,stat.ME,stat.ML"
"Conquering the CNN Over-Parameterization Dilemma: A Volterra Filtering Approach for Action Recognition. The importance of inference in Machine Learning (ML) has led to an explosive
number of different proposals in ML, and particularly in Deep Learning. In an
attempt to reduce the complexity of Convolutional Neural Networks, we propose a
Volterra filter-inspired Network architecture. This architecture introduces
controlled non-linearities in the form of interactions between the delayed
input samples of data. We propose a cascaded implementation of Volterra
Filtering so as to significantly reduce the number of parameters required to
carry out the same classification task as that of a conventional Neural
Network. We demonstrate an efficient parallel implementation of this Volterra
Neural Network (VNN), along with its remarkable performance while retaining a
relatively simpler and potentially more tractable structure. Furthermore, we
show a rather sophisticated adaptation of this network to nonlinearly fuse the
RGB (spatial) information and the Optical Flow (temporal) information of a
video sequence for action recognition. The proposed approach is evaluated on
UCF-101 and HMDB-51 datasets for action recognition, and is shown to outperform
state of the art CNN approaches.","cs.CV,cs.LG,eess.IV"
"Why resampling outperforms reweighting for correcting sampling bias with stochastic gradients. A data set sampled from a certain population is biased if the subgroups of
the population are sampled at proportions that are significantly different from
their underlying proportions. Training machine learning models on biased data
sets requires correction techniques to compensate for the bias. We consider two
commonly-used techniques, resampling and reweighting, that rebalance the
proportions of the subgroups to maintain the desired objective function. Though
statistically equivalent, it has been observed that resampling outperforms
reweighting when combined with stochastic gradient algorithms. By analyzing
illustrative examples, we explain the reason behind this phenomenon using tools
from dynamical stability and stochastic asymptotics. We also present
experiments from regression, classification, and off-policy prediction to
demonstrate that this is a general phenomenon. We argue that it is imperative
to consider the objective function design and the optimization algorithm
together while addressing the sampling bias.","cs.LG,cs.NA,math.NA,stat.ML"
"Context-aware Active Multi-Step Reinforcement Learning. Reinforcement learning has attracted great attention recently, especially
policy gradient algorithms, which have been demonstrated on challenging
decision making and control tasks. In this paper, we propose an active
multi-step TD algorithm with adaptive stepsizes to learn actor and critic.
Specifically, our model consists of two components: active stepsize learning
and adaptive multi-step TD algorithm. Firstly, we divide the time horizon into
chunks and actively select state and action inside each chunk. Then given the
selected samples, we propose the adaptive multi-step TD, which generalizes
TD($\lambda$), but adaptively switch on/off the backups from future returns of
different steps. Particularly, the adaptive multi-step TD introduces a
context-aware mechanism, here a binary classifier, which decides whether or not
to turn on its future backups based on the context changes. Thus, our model is
kind of combination of active learning and multi-step TD algorithm, which has
the capacity for learning off-policy without the need of importance sampling.
We evaluate our approach on both discrete and continuous space tasks in an
off-policy setting respectively, and demonstrate competitive results compared
to other reinforcement learning baselines.","I.2.6,cs.AI,cs.LG,stat.ML"
"Forecasting Emergency Department Capacity Constraints for COVID Isolation Beds. Predicting patient volumes in a hospital setting is a well-studied
application of time series forecasting. Existing tools usually make forecasts
at the daily or weekly level to assist in planning for staffing requirements.
Prompted by new COVID-related capacity constraints placed on our pediatric
hospital's emergency department, we developed an hourly forecasting tool to
make predictions over a 24 hour window. These forecasts would give our hospital
sufficient time to be able to martial resources towards expanding capacity and
augmenting staff (e.g. transforming wards or bringing in physicians on call).
Using Gaussian Process Regressions (GPRs), we obtain strong performance for
both point predictions (average R-squared: 82%) as well as classification
accuracy when predicting the ordinal tiers of our hospital's capacity (average
precision/recall: 82%/74%). Compared to traditional regression approaches, GPRs
not only obtain consistently higher performance, but are also robust to the
dataset shifts that have occurred throughout 2020. Hospital stakeholders are
encouraged by the strength of our results, and we are currently working on
moving our tool to a real-time setting with the goal of augmenting the
capabilities of our healthcare workers.","cs.CY,cs.LG"
"Dual Geometric Graph Network (DG2N) -- Iterative network for deformable shape alignment. We provide a novel new approach for aligning geometric models using a dual
graph structure where local features are mapping probabilities. Alignment of
non-rigid structures is one of the most challenging computer vision tasks due
to the high number of unknowns needed to model the correspondence. We have seen
a leap forward using DNN models in template alignment and functional maps, but
those methods fail for inter-class alignment where nonisometric deformations
exist. Here we propose to rethink this task and use unrolling concepts on a
dual graph structure - one for a forward map and one for a backward map, where
the features are pulled back matching probabilities from the target into the
source. We report state of the art results on stretchable domains alignment in
a rapid and stable solution for meshes and cloud of points.","cs.CV,cs.LG"
"Relation-Shape Convolutional Neural Network for Point Cloud Analysis. Point cloud analysis is very challenging, as the shape implied in irregular
points is difficult to capture. In this paper, we propose RS-CNN, namely,
Relation-Shape Convolutional Neural Network, which extends regular grid CNN to
irregular configuration for point cloud analysis. The key to RS-CNN is learning
from relation, i.e., the geometric topology constraint among points.
Specifically, the convolutional weight for local point set is forced to learn a
high-level relation expression from predefined geometric priors, between a
sampled point from this point set and the others. In this way, an inductive
local representation with explicit reasoning about the spatial layout of points
can be obtained, which leads to much shape awareness and robustness. With this
convolution as a basic operator, RS-CNN, a hierarchical architecture can be
developed to achieve contextual shape-aware learning for point cloud analysis.
Extensive experiments on challenging benchmarks across three tasks verify
RS-CNN achieves the state of the arts.","cs.AI,cs.CG,cs.CV,cs.GR,cs.RO"
"Generation of complex database queries and API calls from natural language utterances. Generating queries corresponding to natural language questions is a long
standing problem. Traditional methods lack language flexibility, while newer
sequence-to-sequence models require large amount of data. Schema-agnostic
sequence-to-sequence models can be fine-tuned for a specific schema using a
small dataset but these models have relatively low accuracy. We present a
method that transforms the query generation problem into an intent
classification and slot filling problem. This method can work using small
datasets. For questions similar to the ones in the training dataset, it
produces complex queries with high accuracy. For other questions, it can use a
template-based approach or predict query pieces to construct the queries, still
at a higher accuracy than sequence-to-sequence models. On a real-world dataset,
a schema fine-tuned state-of-the-art generative model had 60\% exact match
accuracy for the query generation task, while our method resulted in 92\% exact
match accuracy.","cs.CL,cs.DB,cs.LG"
"Deep Learning for Molecular Graphs with Tiered Graph Autoencoders and Graph Prediction. Tiered graph autoencoders provide the architecture and mechanisms for
learning tiered latent representations and latent spaces for molecular graphs
that explicitly represent and utilize groups (e.g., functional groups). This
enables the utilization and exploration of tiered molecular latent spaces,
either individually - the node (atom) tier, the group tier, or the graph
(molecule) tier - or jointly, as well as navigation across the tiers. In this
paper, we discuss the use of tiered graph autoencoders together with graph
prediction for molecular graphs. We show features of molecular graphs used, and
groups in molecular graphs identified for some sample molecules. We briefly
review graph prediction and the QM9 dataset for background information, and
discuss the use of tiered graph embeddings for graph prediction, particularly
weighted group pooling. We find that functional groups and ring groups
effectively capture and represent the chemical essence of molecular graphs
(structures). Further, tiered graph autoencoders and graph prediction together
provide effective, efficient and interpretable deep learning for molecular
graphs, with the former providing unsupervised, transferable learning and the
latter providing supervised, task-optimized learning.","cs.LG,q-bio.BM"
"Optimism in Reinforcement Learning and Kullback-Leibler Divergence. We consider model-based reinforcement learning in finite Markov De- cision
Processes (MDPs), focussing on so-called optimistic strategies. In MDPs,
optimism can be implemented by carrying out extended value it- erations under a
constraint of consistency with the estimated model tran- sition probabilities.
The UCRL2 algorithm by Auer, Jaksch and Ortner (2009), which follows this
strategy, has recently been shown to guarantee near-optimal regret bounds. In
this paper, we strongly argue in favor of using the Kullback-Leibler (KL)
divergence for this purpose. By studying the linear maximization problem under
KL constraints, we provide an ef- ficient algorithm, termed KL-UCRL, for
solving KL-optimistic extended value iteration. Using recent deviation bounds
on the KL divergence, we prove that KL-UCRL provides the same guarantees as
UCRL2 in terms of regret. However, numerical experiments on classical
benchmarks show a significantly improved behavior, particularly when the MDP
has reduced connectivity. To support this observation, we provide elements of
com- parison between the two algorithms based on geometric considerations.","cs.LG,math.ST,stat.ML,stat.TH"
"Contrastive Code Representation Learning. Recent work learns contextual representations of source code by
reconstructing tokens from their context. For downstream semantic understanding
tasks like summarizing code in English, these representations should ideally
capture program functionality. However, we show that the popular
reconstruction-based BERT model is sensitive to source code edits, even when
the edits preserve semantics. We propose ContraCode: a contrastive pre-training
task that learns code functionality, not form. ContraCode pre-trains a neural
network to identify functionally similar variants of a program among many
non-equivalent distractors. We scalably generate these variants using an
automated source-to-source compiler as a form of data augmentation. Contrastive
pre-training improves JavaScript summarization and TypeScript type inference
accuracy by 2% to 13%. We also propose a new zero-shot JavaScript code clone
detection dataset, showing that ContraCode is both more robust and semantically
meaningful. On it, we outperform RoBERTa by 39% AUROC in an adversarial setting
and up to 5% on natural code.","cs.AI,cs.LG,cs.PL,cs.SE,stat.ML"
"Consistent estimation of the max-flow problem: Towards unsupervised image segmentation. Advances in the image-based diagnostics of complex biological and
manufacturing processes have brought unsupervised image segmentation to the
forefront of enabling automated, on the fly decision making. However, most
existing unsupervised segmentation approaches are either computationally
complex or require manual parameter selection (e.g., flow capacities in
max-flow/min-cut segmentation). In this work, we present a fully unsupervised
segmentation approach using a continuous max-flow formulation over the image
domain while optimally estimating the flow parameters from the image
characteristics. More specifically, we show that the maximum a posteriori
estimate of the image labels can be formulated as a continuous max-flow problem
given the flow capacities are known. The flow capacities are then iteratively
obtained by employing a novel Markov random field prior over the image domain.
We present theoretical results to establish the posterior consistency of the
flow capacities. We compare the performance of our approach on two real-world
case studies including brain tumor image segmentation and defect identification
in additively manufactured components using electron microscopic images.
Comparative results with several state-of-the-art supervised as well as
unsupervised methods suggest that the present method performs statistically
similar to the supervised methods, but results in more than 90% improvement in
the Dice score when compared to the state-of-the-art unsupervised methods.","cs.CV,eess.IV"
"Towards Sampling from Nondirected Probabilistic Graphical models using a D-Wave Quantum Annealer. A D-Wave quantum annealer (QA) having a 2048 qubit lattice, with no missing
qubits and couplings, allowed embedding of a complete graph of a Restricted
Boltzmann Machine (RBM). A handwritten digit OptDigits data set having 8x7
pixels of visible units was used to train the RBM using a classical Contrastive
Divergence. Embedding of the classically-trained RBM into the D-Wave lattice
was used to demonstrate that the QA offers a high-efficiency alternative to the
classical Markov Chain Monte Carlo (MCMC) for reconstructing missing labels of
the test images as well as a generative model. At any training iteration, the
D-Wave-based classification had classification error more than two times lower
than MCMC. The main goal of this study was to investigate the quality of the
sample from the RBM model distribution and its comparison to a classical MCMC
sample. For the OptDigits dataset, the states in the D-Wave sample belonged to
about two times more local valleys compared to the MCMC sample. All the
lowest-energy (the highest joint probability) local minima in the MCMC sample
were also found by the D-Wave. The D-Wave missed many of the higher-energy
local valleys, while finding many ""new"" local valleys consistently missed by
the MCMC. It was established that the ""new"" local valleys that the D-Wave finds
are important for the model distribution in terms of the energy of the
corresponding local minima, the width of the local valleys, and the height of
the escape barrier.","cs.LG,quant-ph,stat.ML"
"Leveraging Recent Advances in Deep Learning for Audio-Visual Emotion Recognition. Emotional expressions are the behaviors that communicate our emotional state
or attitude to others. They are expressed through verbal and non-verbal
communication. Complex human behavior can be understood by studying physical
features from multiple modalities; mainly facial, vocal and physical gestures.
Recently, spontaneous multi-modal emotion recognition has been extensively
studied for human behavior analysis. In this paper, we propose a new deep
learning-based approach for audio-visual emotion recognition. Our approach
leverages recent advances in deep learning like knowledge distillation and
high-performing deep architectures. The deep feature representations of the
audio and visual modalities are fused based on a model-level fusion strategy. A
recurrent neural network is then used to capture the temporal dynamics. Our
proposed approach substantially outperforms state-of-the-art approaches in
predicting valence on the RECOLA dataset. Moreover, our proposed visual facial
expression feature extraction network outperforms state-of-the-art results on
the AffectNet and Google Facial Expression Comparison datasets.","cs.CV,cs.LG,cs.SD,eess.AS"
"Learn molecular representations from large-scale unlabeled molecules for drug discovery. How to produce expressive molecular representations is a fundamental
challenge in AI-driven drug discovery. Graph neural network (GNN) has emerged
as a powerful technique for modeling molecular data. However, previous
supervised approaches usually suffer from the scarcity of labeled data and have
poor generalization capability. Here, we proposed a novel Molecular
Pre-training Graph-based deep learning framework, named MPG, that leans
molecular representations from large-scale unlabeled molecules. In MPG, we
proposed a powerful MolGNet model and an effective self-supervised strategy for
pre-training the model at both the node and graph-level. After pre-training on
11 million unlabeled molecules, we revealed that MolGNet can capture valuable
chemistry insights to produce interpretable representation. The pre-trained
MolGNet can be fine-tuned with just one additional output layer to create
state-of-the-art models for a wide range of drug discovery tasks, including
molecular properties prediction, drug-drug interaction, and drug-target
interaction, involving 13 benchmark datasets. Our work demonstrates that MPG is
promising to become a novel approach in the drug discovery pipeline.","cs.LG,q-bio.BM,q-bio.QM"
"Modeling Human Decision-making in Generalized Gaussian Multi-armed Bandits. We present a formal model of human decision-making in explore-exploit tasks
using the context of multi-armed bandit problems, where the decision-maker must
choose among multiple options with uncertain rewards. We address the standard
multi-armed bandit problem, the multi-armed bandit problem with transition
costs, and the multi-armed bandit problem on graphs. We focus on the case of
Gaussian rewards in a setting where the decision-maker uses Bayesian inference
to estimate the reward values. We model the decision-maker's prior knowledge
with the Bayesian prior on the mean reward. We develop the upper credible limit
(UCL) algorithm for the standard multi-armed bandit problem and show that this
deterministic algorithm achieves logarithmic cumulative expected regret, which
is optimal performance for uninformative priors. We show how good priors and
good assumptions on the correlation structure among arms can greatly enhance
decision-making performance, even over short time horizons. We extend to the
stochastic UCL algorithm and draw several connections to human decision-making
behavior. We present empirical data from human experiments and show that human
performance is efficiently captured by the stochastic UCL algorithm with
appropriate parameters. For the multi-armed bandit problem with transition
costs and the multi-armed bandit problem on graphs, we generalize the UCL
algorithm to the block UCL algorithm and the graphical block UCL algorithm,
respectively. We show that these algorithms also achieve logarithmic cumulative
expected regret and require a sub-logarithmic expected number of transitions
among arms. We further illustrate the performance of these algorithms with
numerical examples. NB: Appendix G included in this version details minor
modifications that correct for an oversight in the previously-published proofs.
The remainder of the text reflects the published work.","cs.LG,math.OC,stat.ML"
"Software Engineering Event Modeling using Relative Time in Temporal Knowledge Graphs. We present a multi-relational temporal Knowledge Graph based on the daily
interactions between artifacts in GitHub, one of the largest social coding
platforms. Such representation enables posing many user-activity and project
management questions as link prediction and time queries over the knowledge
graph. In particular, we introduce two new datasets for i) interpolated
time-conditioned link prediction and ii) extrapolated time-conditioned
link/time prediction queries, each with distinguished properties. Our
experiments on these datasets highlight the potential of adapting knowledge
graphs to answer broad software engineering questions. Meanwhile, it also
reveals the unsatisfactory performance of existing temporal models on
extrapolated queries and time prediction queries in general. To overcome these
shortcomings, we introduce an extension to current temporal models using
relative temporal information with regards to past events.","cs.LG,cs.SE,stat.ML"
"SpinNet: Learning a General Surface Descriptor for 3D Point Cloud Registration. Extracting robust and general 3D local features is key to downstream tasks
such as point cloud registration and reconstruction. Existing learning-based
local descriptors are either sensitive to rotation transformations, or rely on
classical handcrafted features which are neither general nor representative. In
this paper, we introduce a new, yet conceptually simple, neural architecture,
termed SpinNet, to extract local features which are rotationally invariant
whilst sufficiently informative to enable accurate registration. A Spatial
Point Transformer is first introduced to map the input local surface into a
carefully designed cylindrical space, enabling end-to-end optimization with
SO(2) equivariant representation. A Neural Feature Extractor which leverages
the powerful point-based and 3D cylindrical convolutional neural layers is then
utilized to derive a compact and representative descriptor for matching.
Extensive experiments on both indoor and outdoor datasets demonstrate that
SpinNet outperforms existing state-of-the-art techniques by a large margin.
More critically, it has the best generalization ability across unseen scenarios
with different sensor modalities. The code is available at
https://github.com/QingyongHu/SpinNet.","cs.AI,cs.CV,cs.LG,cs.RO"
"An Overview on Data Representation Learning: From Traditional Feature Learning to Recent Deep Learning. Since about 100 years ago, to learn the intrinsic structure of data, many
representation learning approaches have been proposed, including both linear
ones and nonlinear ones, supervised ones and unsupervised ones. Particularly,
deep architectures are widely applied for representation learning in recent
years, and have delivered top results in many tasks, such as image
classification, object detection and speech recognition. In this paper, we
review the development of data representation learning methods. Specifically,
we investigate both traditional feature learning algorithms and
state-of-the-art deep learning models. The history of data representation
learning is introduced, while available resources (e.g. online course, tutorial
and book information) and toolboxes are provided. Finally, we conclude this
paper with remarks and some interesting research directions on data
representation learning.","68T05,cs.LG,stat.ML"
"New Benchmarks for Learning on Non-Homophilous Graphs. Much data with graph structures satisfy the principle of homophily, meaning
that connected nodes tend to be similar with respect to a specific attribute.
As such, ubiquitous datasets for graph machine learning tasks have generally
been highly homophilous, rewarding methods that leverage homophily as an
inductive bias. Recent work has pointed out this particular focus, as new
non-homophilous datasets have been introduced and graph representation learning
models better suited for low-homophily settings have been developed. However,
these datasets are small and poorly suited to truly testing the effectiveness
of new methods in non-homophilous settings. We present a series of improved
graph datasets with node label relationships that do not satisfy the homophily
principle. Along with this, we introduce a new measure of the presence or
absence of homophily that is better suited than existing measures in different
regimes. We benchmark a range of simple methods and graph neural networks
across our proposed datasets, drawing new insights for further research. Data
and codes can be found at https://github.com/CUAI/Non-Homophily-Benchmarks.","cs.LG,cs.SI"
"Molecular geometry prediction using a deep generative graph neural network. A molecule's geometry, also known as conformation, is one of a molecule's
most important properties, determining the reactions it participates in, the
bonds it forms, and the interactions it has with other molecules. Conventional
conformation generation methods minimize hand-designed molecular force field
energy functions that are often not well correlated with the true energy
function of a molecule observed in nature. They generate geometrically diverse
sets of conformations, some of which are very similar to the lowest-energy
conformations and others of which are very different. In this paper, we propose
a conditional deep generative graph neural network that learns an energy
function by directly learning to generate molecular conformations that are
energetically favorable and more likely to be observed experimentally in
data-driven manner. On three large-scale datasets containing small molecules,
we show that our method generates a set of conformations that on average is far
more likely to be close to the corresponding reference conformations than are
those obtained from conventional force field methods. Our method maintains
geometrical diversity by generating conformations that are not too similar to
each other, and is also computationally faster. We also show that our method
can be used to provide initial coordinates for conventional force field
methods. On one of the evaluated datasets we show that this combination allows
us to combine the best of both methods, yielding generated conformations that
are on average close to reference conformations with some very similar to
reference conformations.","cs.LG,physics.comp-ph,stat.ML"
"Blind Image Fusion for Hyperspectral Imaging with the Directional Total Variation. Hyperspectral imaging is a cutting-edge type of remote sensing used for
mapping vegetation properties, rock minerals and other materials. A major
drawback of hyperspectral imaging devices is their intrinsic low spatial
resolution. In this paper, we propose a method for increasing the spatial
resolution of a hyperspectral image by fusing it with an image of higher
spatial resolution that was obtained with a different imaging modality. This is
accomplished by solving a variational problem in which the regularization
functional is the directional total variation. To accommodate for possible
mis-registrations between the two images, we consider a non-convex blind
super-resolution problem where both a fused image and the corresponding
convolution kernel are estimated. Using this approach, our model can realign
the given images if needed. Our experimental results indicate that the
non-convexity is negligible in practice and that reliable solutions can be
computed using a variety of different optimization algorithms. Numerical
results on real remote sensing data from plant sciences and urban monitoring
show the potential of the proposed method and suggests that it is robust with
respect to the regularization parameters, mis-registration and the shape of the
kernel.","49M37,65K10,90C30,90C90,cs.CV,cs.NA,math.NA"
"A Survey of Algorithms for Black-Box Safety Validation. Autonomous and semi-autonomous systems for safety-critical applications
require rigorous testing before deployment. Due to the complexity of these
systems, formal verification may be impossible and real-world testing may be
dangerous during development. Therefore, simulation-based techniques have been
developed that treat the system under test as a black box during testing.
Safety validation tasks include finding disturbances to the system that cause
it to fail (falsification), finding the most-likely failure, and estimating the
probability that the system fails. Motivated by the prevalence of
safety-critical artificial intelligence, this work provides a survey of
state-of-the-art safety validation techniques with a focus on applied
algorithms and their modifications for the safety validation problem. We
present and discuss algorithms in the domains of optimization, path planning,
reinforcement learning, and importance sampling. Problem decomposition
techniques are presented to help scale algorithms to large state spaces, and a
brief overview of safety-critical applications is given, including autonomous
vehicles and aircraft collision avoidance systems. Finally, we present a survey
of existing academic and commercially available safety validation tools.","cs.AI,cs.LG,cs.SY,eess.SY,stat.ML"
"A Max-Product EM Algorithm for Reconstructing Markov-tree Sparse Signals from Compressive Samples. We propose a Bayesian expectation-maximization (EM) algorithm for
reconstructing Markov-tree sparse signals via belief propagation. The
measurements follow an underdetermined linear model where the
regression-coefficient vector is the sum of an unknown approximately sparse
signal and a zero-mean white Gaussian noise with an unknown variance. The
signal is composed of large- and small-magnitude components identified by
binary state variables whose probabilistic dependence structure is described by
a Markov tree. Gaussian priors are assigned to the signal coefficients given
their state variables and the Jeffreys' noninformative prior is assigned to the
noise variance. Our signal reconstruction scheme is based on an EM iteration
that aims at maximizing the posterior distribution of the signal and its state
variables given the noise variance. We construct the missing data for the EM
iteration so that the complete-data posterior distribution corresponds to a
hidden Markov tree (HMT) probabilistic graphical model that contains no loops
and implement its maximization (M) step via a max-product algorithm. This EM
algorithm estimates the vector of state variables as well as solves iteratively
a linear system of equations to obtain the corresponding signal estimate. We
select the noise variance so that the corresponding estimated signal and state
variables obtained upon convergence of the EM iteration have the largest
marginal posterior distribution. We compare the proposed and existing
state-of-the-art reconstruction methods via signal and image reconstruction
experiments.","cs.IT,math.IT,stat.ML"
"Deep Graph Infomax. We present Deep Graph Infomax (DGI), a general approach for learning node
representations within graph-structured data in an unsupervised manner. DGI
relies on maximizing mutual information between patch representations and
corresponding high-level summaries of graphs---both derived using established
graph convolutional network architectures. The learnt patch representations
summarize subgraphs centered around nodes of interest, and can thus be reused
for downstream node-wise learning tasks. In contrast to most prior approaches
to unsupervised learning with GCNs, DGI does not rely on random walk
objectives, and is readily applicable to both transductive and inductive
learning setups. We demonstrate competitive performance on a variety of node
classification benchmarks, which at times even exceeds the performance of
supervised learning.","cs.IT,cs.LG,cs.SI,math.IT,stat.ML"
"A Dynamical Perspective on Point Cloud Registration. We provide a dynamical perspective on the classical problem of 3D point cloud
registration with correspondences. A point cloud is considered as a rigid body
consisting of particles. The problem of registering two point clouds is
formulated as a dynamical system, where the dynamic model point cloud
translates and rotates in a viscous environment towards the static scene point
cloud, under forces and torques induced by virtual springs placed between each
pair of corresponding points. We first show that the potential energy of the
system recovers the objective function of the maximum likelihood estimation. We
then adopt Lyapunov analysis, particularly the invariant set theorem, to
analyze the rigid body dynamics and show that the system globally
asymptotically tends towards the set of equilibrium points, where the globally
optimal registration solution lies in. We conjecture that, besides the globally
optimal equilibrium point, the system has either three or infinite ""spurious""
equilibrium points, and these spurious equilibria are all locally unstable. The
case of three spurious equilibria corresponds to generic shape of the point
cloud, while the case of infinite spurious equilibria happens when the point
cloud exhibits symmetry. Therefore, simulating the dynamics with random
perturbations guarantees to obtain the globally optimal registration solution.
Numerical experiments support our analysis and conjecture.","cs.CV,math.DS,math.OC"
"Accelerating Generative Neural Networks on Unmodified Deep Learning Processors -- A Software Approach. Generative neural network is a new category of neural networks and it has
been widely utilized in applications such as content generation, unsupervised
learning, segmentation and pose estimation. It typically involves massive
computing-intensive deconvolution operations that cannot be fitted to
conventional neural network processors directly. However, prior works mainly
investigated specialized hardware architectures through intensive hardware
modifications to the existing deep learning processors to accelerate
deconvolution together with the convolution. In contrast, this work proposes a
novel deconvolution implementation with a software approach and enables fast
and efficient deconvolution execution on the legacy deep learning processors.
Our proposed method reorganizes the computation of deconvolution and allows the
deep learning processors to treat it as the standard convolution by splitting
the original deconvolution filters into multiple small filters. Compared to
prior acceleration schemes, the implemented acceleration scheme achieves 2.41x
- 4.34x performance speedup and reduces the energy consumption by 27.7% - 54.5%
on a set of realistic benchmarks. In addition, we also applied the
deconvolution computing approach to the off-the-shelf commodity deep learning
processors. The performance of deconvolution also exhibits significant
performance speedup over prior deconvolution implementations.","cs.CV,cs.DS,cs.LG,cs.NE"
"A GAN-based Approach for Mitigating Inference Attacks in Smart Home Environment. The proliferation of smart, connected, always listening devices have
introduced significant privacy risks to users in a smart home environment.
Beyond the notable risk of eavesdropping, intruders can adopt machine learning
techniques to infer sensitive information from audio recordings on these
devices, resulting in a new dimension of privacy concerns and attack variables
to smart home users. Techniques such as sound masking and microphone jamming
have been effectively used to prevent eavesdroppers from listening in to
private conversations. In this study, we explore the problem of adversaries
spying on smart home users to infer sensitive information with the aid of
machine learning techniques. We then analyze the role of randomness in the
effectiveness of sound masking for mitigating sensitive information leakage. We
propose a Generative Adversarial Network (GAN) based approach for privacy
preservation in smart homes which generates random noise to distort the
unwanted machine learning-based inference. Our experimental results demonstrate
that GANs can be used to generate more effective sound masking noise signals
which exhibit more randomness and effectively mitigate deep learning-based
inference attacks while preserving the semantics of the audio samples.","cs.CR,cs.LG,cs.NI"
"Pixel-wise Conditioning of Generative Adversarial Networks. Generative Adversarial Networks (GANs) have proven successful for
unsupervised image generation. Several works extended GANs to image inpainting
by conditioning the generation with parts of the image one wants to
reconstruct. However, these methods have limitations in settings where only a
small subset of the image pixels is known beforehand. In this paper, we study
the effectiveness of conditioning GANs by adding an explicit regularization
term to enforce pixel-wise conditions when very few pixel values are provided.
In addition, we also investigate the influence of this regularization term on
the quality of the generated images and the satisfaction of the conditions.
Conducted experiments on MNIST and FashionMNIST show evidence that this
regularization term allows for controlling the trade-off between quality of the
generated images and constraint satisfaction.","cs.CV,cs.LG,eess.IV"
"Transfer Learning and Online Learning for Traffic Forecasting under Different Data Availability Conditions: Alternatives and Pitfalls. This work aims at unveiling the potential of Transfer Learning (TL) for
developing a traffic flow forecasting model in scenarios of absent data.
Knowledge transfer from high-quality predictive models becomes feasible under
the TL paradigm, enabling the generation of new proper models with few data. In
order to explore this capability, we identify three different levels of data
absent scenarios, where TL techniques are applied among Deep Learning (DL)
methods for traffic forecasting. Then, traditional batch learning is compared
against TL based models using real traffic flow data, collected by deployed
loops managed by the City Council of Madrid (Spain). In addition, we apply
Online Learning (OL) techniques, where model receives an update after each
prediction, in order to adapt to traffic flow trend changes and incrementally
learn from new incoming traffic data. The obtained experimental results shed
light on the advantages of transfer and online learning for traffic flow
forecasting, and draw practical insights on their interplay with the amount of
available training data at the location of interest.","68T05,cs.AI,cs.LG"
"Color Texture Image Retrieval Based on Copula Multivariate Modeling in the Shearlet Domain. In this paper, a color texture image retrieval framework is proposed based on
Shearlet domain modeling using Copula multivariate model. In the proposed
framework, Gaussian Copula is used to model the dependencies between different
sub-bands of the Non Subsample Shearlet Transform (NSST) and non-Gaussian
models are used for marginal modeling of the coefficients. Six different
schemes are proposed for modeling NSST coefficients based on the four types of
neighboring defined; moreover, Kullback Leibler Divergence(KLD) close form is
calculated in different situations for the two Gaussian Copula and non Gaussian
functions in order to investigate the similarities in the proposed retrieval
framework. The Jeffery divergence (JD) criterion, which is a symmetrical
version of KLD, is used for investigating similarities in the proposed
framework. We have implemented our experiments on four texture image retrieval
benchmark datasets, the results of which show the superiority of the proposed
framework over the existing state-of-the-art methods. In addition, the
retrieval time of the proposed framework is also analyzed in the two steps of
feature extraction and similarity matching, which also shows that the proposed
framework enjoys an appropriate retrieval time.","68T99,I.4.9,cs.CV,eess.IV"
"Nonseparable Symplectic Neural Networks. Predicting the behaviors of Hamiltonian systems has been drawing increasing
attention in scientific machine learning. However, the vast majority of the
literature was focused on predicting separable Hamiltonian systems with their
kinematic and potential energy terms being explicitly decoupled while building
data-driven paradigms to predict nonseparable Hamiltonian systems that are
ubiquitous in fluid dynamics and quantum mechanics were rarely explored. The
main computational challenge lies in the effective embedding of symplectic
priors to describe the inherently coupled evolution of position and momentum,
which typically exhibits intricate dynamics. To solve the problem, we propose a
novel neural network architecture, Nonseparable Symplectic Neural Networks
(NSSNNs), to uncover and embed the symplectic structure of a nonseparable
Hamiltonian system from limited observation data. The enabling mechanics of our
approach is an augmented symplectic time integrator to decouple the position
and momentum energy terms and facilitate their evolution. We demonstrated the
efficacy and versatility of our method by predicting a wide range of
Hamiltonian systems, both separable and nonseparable, including chaotic
vortical flows. We showed the unique computational merits of our approach to
yield long-term, accurate, and robust predictions for large-scale Hamiltonian
systems by rigorously enforcing symplectomorphism.","cs.CE,cs.LG,stat.ML"
"A Self-Supervised Framework for Function Learning and Extrapolation. Understanding how agents learn to generalize -- and, in particular, to
extrapolate -- in high-dimensional, naturalistic environments remains a
challenge for both machine learning and the study of biological agents. One
approach to this has been the use of function learning paradigms, which allow
peoples' empirical patterns of generalization for smooth scalar functions to be
described precisely. However, to date, such work has not succeeded in
identifying mechanisms that acquire the kinds of general purpose
representations over which function learning can operate to exhibit the
patterns of generalization observed in human empirical studies. Here, we
present a framework for how a learner may acquire such representations, that
then support generalization -- and extrapolation in particular -- in a few-shot
fashion. Taking inspiration from a classic theory of visual processing, we
construct a self-supervised encoder that implements the basic inductive bias of
invariance under topological distortions. We show the resulting representations
outperform those from other models for unsupervised time series learning in
several downstream function learning tasks, including extrapolation.","cs.LG,q-bio.NC"
"Concentration of Contractive Stochastic Approximation and Reinforcement Learning. Using a martingale concentration inequality, concentration bounds `from time
$n_0$ on' are derived for stochastic approximation algorithms with contractive
maps and both martingale difference and Markov noises. These are applied to
reinforcement learning algorithms, in particular to asynchronous Q-learning and
TD(0).","cs.LG,cs.SY,eess.SY"
"Geometry-aware Transformer for molecular property prediction. Recently, graph neural networks (GNNs) have achieved remarkable performances
for quantum mechanical problems. However, a graph convolution can only cover a
localized region, and cannot capture long-range interactions of atoms. This
behavior is contrary to theoretical interatomic potentials, which is a
fundamental limitation of the spatial based GNNs. In this work, we propose a
novel attention-based framework for molecular property prediction tasks. We
represent a molecular conformation as a discrete atomic sequence combined by
atom-atom distance attributes, named Geometry-aware Transformer (GeoT). In
particular, we adopt a Transformer architecture, which has been widely used for
sequential data. Our proposed model trains sequential representations of
molecular graphs based on globally constructed attentions, maintaining all
spatial arrangements of atom pairs. Our method does not suffer from cost
intensive computations, such as angle calculations. The experimental results on
several public benchmarks and visualization maps verified that keeping the
long-range interatomic attributes can significantly improve the model
predictability.","cs.LG,physics.chem-ph"
"Information Potential Auto-Encoders. In this paper, we suggest a framework to make use of mutual information as a
regularization criterion to train Auto-Encoders (AEs). In the proposed
framework, AEs are regularized by minimization of the mutual information
between input and encoding variables of AEs during the training phase. In order
to estimate the entropy of the encoding variables and the mutual information,
we propose a non-parametric method. We also give an information theoretic view
of Variational AEs (VAEs), which suggests that VAEs can be considered as
parametric methods that estimate entropy. Experimental results show that the
proposed non-parametric models have more degree of freedom in terms of
representation learning of features drawn from complex distributions such as
Mixture of Gaussians, compared to methods which estimate entropy using
parametric approaches, such as Variational AEs.","cs.IT,cs.LG,math.IT,stat.ML"
"Using a Deep Reinforcement Learning Agent for Traffic Signal Control. Ensuring transportation systems are efficient is a priority for modern
society. Technological advances have made it possible for transportation
systems to collect large volumes of varied data on an unprecedented scale. We
propose a traffic signal control system which takes advantage of this new, high
quality data, with minimal abstraction compared to other proposed systems. We
apply modern deep reinforcement learning methods to build a truly adaptive
traffic signal control agent in the traffic microsimulator SUMO. We propose a
new state space, the discrete traffic state encoding, which is information
dense. The discrete traffic state encoding is used as input to a deep
convolutional neural network, trained using Q-learning with experience replay.
Our agent was compared against a one hidden layer neural network traffic signal
control agent and reduces average cumulative delay by 82%, average queue length
by 66% and average travel time by 20%.","cs.LG,cs.SY"
"Deep learning-based virtual histology staining using auto-fluorescence of label-free tissue. Histological analysis of tissue samples is one of the most widely used
methods for disease diagnosis. After taking a sample from a patient, it goes
through a lengthy and laborious preparation, which stains the tissue to
visualize different histological features under a microscope. Here, we
demonstrate a label-free approach to create a virtually-stained microscopic
image using a single wide-field auto-fluorescence image of an unlabeled tissue
sample, bypassing the standard histochemical staining process, saving time and
cost. This method is based on deep learning, and uses a convolutional neural
network trained using a generative adversarial network model to transform an
auto-fluorescence image of an unlabeled tissue section into an image that is
equivalent to the bright-field image of the stained-version of the same sample.
We validated this method by successfully creating virtually-stained microscopic
images of human tissue samples, including sections of salivary gland, thyroid,
kidney, liver and lung tissue, also covering three different stains. This
label-free virtual-staining method eliminates cumbersome and costly
histochemical staining procedures, and would significantly simplify tissue
preparation in pathology and histology fields.","62M45,68T01,68T05,68U10,78M32,92C50,92C55,94A08,I.2; I.2.1; I.2.6; I.2.10; I.3; I.3.3; I.4.3; I.4.4; I.4.9; J.3,cs.CV,cs.LG,physics.med-ph"
"Learning Graph Structures with Transformer for Multivariate Time Series Anomaly Detection in IoT. Many real-world IoT systems, which include a variety of internet-connected
sensory devices, produce substantial amounts of multivariate time series data.
Meanwhile, vital IoT infrastructures like smart power grids and water
distribution networks are frequently targeted by cyber-attacks, making anomaly
detection an important study topic. Modeling such relatedness is, nevertheless,
unavoidable for any efficient and effective anomaly detection system, given the
intricate topological and nonlinear connections that are originally unknown
among sensors. Furthermore, detecting anomalies in multivariate time series is
difficult due to their temporal dependency and stochasticity. This paper
presented GTA, a new framework for multivariate time series anomaly detection
that involves automatically learning a graph structure, graph convolution, and
modeling temporal dependency using a Transformer-based architecture. The
connection learning policy, which is based on the Gumbel-softmax sampling
approach to learn bi-directed links among sensors directly, is at the heart of
learning graph structure. To describe the anomaly information flow between
network nodes, we introduced a new graph convolution called Influence
Propagation convolution. In addition, to tackle the quadratic complexity
barrier, we suggested a multi-branch attention mechanism to replace the
original multi-head self-attention method. Extensive experiments on four
publicly available anomaly detection benchmarks further demonstrate the
superiority of our approach over alternative state-of-the-arts.","cs.CR,cs.LG,cs.SY,eess.SY"
"Variational Bayes under Model Misspecification. Variational Bayes (VB) is a scalable alternative to Markov chain Monte Carlo
(MCMC) for Bayesian posterior inference. Though popular, VB comes with few
theoretical guarantees, most of which focus on well-specified models. However,
models are rarely well-specified in practice. In this work, we study VB under
model misspecification. We prove the VB posterior is asymptotically normal and
centers at the value that minimizes the Kullback-Leibler (KL) divergence to the
true data-generating distribution. Moreover, the VB posterior mean centers at
the same value and is also asymptotically normal. These results generalize the
variational Bernstein--von Mises theorem [29] to misspecified models. As a
consequence of these results, we find that the model misspecification error
dominates the variational approximation error in VB posterior predictive
distributions. It explains the widely observed phenomenon that VB achieves
comparable predictive accuracy with MCMC even though VB uses an approximating
family. As illustrations, we study VB under three forms of model
misspecification, ranging from model over-/under-dispersion to latent
dimensionality misspecification. We conduct two simulation studies that
demonstrate the theoretical results.","cs.LG,math.ST,stat.ML,stat.TH"
"Learning the Markov order of paths in a network. We study the problem of learning the Markov order in categorical sequences
that represent paths in a network, i.e. sequences of variable lengths where
transitions between states are constrained to a known graph. Such data pose
challenges for standard Markov order detection methods and demand modelling
techniques that explicitly account for the graph constraint. Adopting a
multi-order modelling framework for paths, we develop a Bayesian learning
technique that (i) more reliably detects the correct Markov order compared to a
competing method based on the likelihood ratio test, (ii) requires considerably
less data compared to methods using AIC or BIC, and (iii) is robust against
partial knowledge of the underlying constraints. We further show that a
recently published method that uses a likelihood ratio test has a tendency to
overfit the true Markov order of paths, which is not the case for our Bayesian
technique. Our method is important for data scientists analyzing patterns in
categorical sequence data that are subject to (partially) known constraints,
e.g. sequences with forbidden words, mobility trajectories and click stream
data, or sequence data in bioinformatics. Addressing the key challenge of model
selection, our work is further relevant for the growing body of research that
emphasizes the need for higher-order models in network analysis.","60J20 (Primary) 62F07,68T05 (Secondary),G.3; I.5.1,cs.LG,cs.SI,stat.ME,stat.ML"
"CNN Detection of GAN-Generated Face Images based on Cross-Band Co-occurrences Analysis. Last-generation GAN models allow to generate synthetic images which are
visually indistinguishable from natural ones, raising the need to develop tools
to distinguish fake and natural images thus contributing to preserve the
trustworthiness of digital images. While modern GAN models can generate very
high-quality images with no visible spatial artifacts, reconstruction of
consistent relationships among colour channels is expectedly more difficult. In
this paper, we propose a method for distinguishing GAN-generated from natural
images by exploiting inconsistencies among spectral bands, with specific focus
on the generation of synthetic face images. Specifically, we use cross-band
co-occurrence matrices, in addition to spatial co-occurrence matrices, as input
to a CNN model, which is trained to distinguish between real and synthetic
faces. The results of our experiments confirm the goodness of our approach
which outperforms a similar detection technique based on intra-band spatial
co-occurrences only. The performance gain is particularly significant with
regard to robustness against post-processing, like geometric transformations,
filtering and contrast manipulations.","cs.CR,cs.CV,cs.LG,eess.IV"
"Accelerating Training in Artificial Neural Networks with Dynamic Mode Decomposition. Training of deep neural networks (DNNs) frequently involves optimizing
several millions or even billions of parameters. Even with modern computing
architectures, the computational expense of DNN training can inhibit, for
instance, network architecture design optimization, hyper-parameter studies,
and integration into scientific research cycles. The key factor limiting
performance is that both the feed-forward evaluation and the back-propagation
rule are needed for each weight during optimization in the update rule. In this
work, we propose a method to decouple the evaluation of the update rule at each
weight. At first, Proper Orthogonal Decomposition (POD) is used to identify a
current estimate of the principal directions of evolution of weights per layer
during training based on the evolution observed with a few backpropagation
steps. Then, Dynamic Mode Decomposition (DMD) is used to learn the dynamics of
the evolution of the weights in each layer according to these principal
directions. The DMD model is used to evaluate an approximate converged state
when training the ANN. Afterward, some number of backpropagation steps are
performed, starting from the DMD estimates, leading to an update to the
principal directions and DMD model. This iterative process is repeated until
convergence. By fine-tuning the number of backpropagation steps used for each
DMD model estimation, a significant reduction in the number of operations
required to train the neural networks can be achieved. In this paper, the DMD
acceleration method will be explained in detail, along with the theoretical
justification for the acceleration provided by DMD. This method is illustrated
using a regression problem of key interest for the scientific machine learning
community: the prediction of a pollutant concentration field in a diffusion,
advection, reaction problem.","cs.CE,cs.LG,eess.SP,physics.comp-ph"
"Gaussian Process Decentralized Data Fusion Meets Transfer Learning in Large-Scale Distributed Cooperative Perception. This paper presents novel Gaussian process decentralized data fusion
algorithms exploiting the notion of agent-centric support sets for distributed
cooperative perception of large-scale environmental phenomena. To overcome the
limitations of scale in existing works, our proposed algorithms allow every
mobile sensing agent to choose a different support set and dynamically switch
to another during execution for encapsulating its own data into a local summary
that, perhaps surprisingly, can still be assimilated with the other agents'
local summaries (i.e., based on their current choices of support sets) into a
globally consistent summary to be used for predicting the phenomenon. To
achieve this, we propose a novel transfer learning mechanism for a team of
agents capable of sharing and transferring information encapsulated in a
summary based on a support set to that utilizing a different support set with
some loss that can be theoretically bounded and analyzed. To alleviate the
issue of information loss accumulating over multiple instances of transfer
learning, we propose a new information sharing mechanism to be incorporated
into our algorithms in order to achieve memory-efficient lazy transfer
learning. Empirical evaluation on real-world datasets show that our algorithms
outperform the state-of-the-art methods.","cs.LG,cs.MA,cs.RO,stat.ML"
"Detection and Tracking of Liquids with Fully Convolutional Networks. Recent advances in AI and robotics have claimed many incredible results with
deep learning, yet no work to date has applied deep learning to the problem of
liquid perception and reasoning. In this paper, we apply fully-convolutional
deep neural networks to the tasks of detecting and tracking liquids. We
evaluate three models: a single-frame network, multi-frame network, and a LSTM
recurrent network. Our results show that the best liquid detection results are
achieved when aggregating data over multiple frames, in contrast to standard
image segmentation. They also show that the LSTM network outperforms the other
two in both tasks. This suggests that LSTM-based neural networks have the
potential to be a key component for enabling robots to handle liquids using
robust, closed-loop controllers.","cs.CV,cs.RO"
"The gap between theory and practice in function approximation with deep neural networks. Deep learning (DL) is transforming industry as decision-making processes are
being automated by deep neural networks (DNNs) trained on real-world data.
Driven partly by rapidly-expanding literature on DNN approximation theory
showing they can approximate a rich variety of functions, such tools are
increasingly being considered for problems in scientific computing. Yet, unlike
traditional algorithms in this field, little is known about DNNs from the
principles of numerical analysis, e.g., stability, accuracy, computational
efficiency and sample complexity. In this paper we introduce a computational
framework for examining DNNs in practice, and use it to study empirical
performance with regard to these issues. We study performance of DNNs of
different widths & depths on test functions in various dimensions, including
smooth and piecewise smooth functions. We also compare DL against best-in-class
methods for smooth function approx. based on compressed sensing (CS). Our main
conclusion from these experiments is that there is a crucial gap between the
approximation theory of DNNs and their practical performance, with trained DNNs
performing relatively poorly on functions for which there are strong
approximation results (e.g. smooth functions), yet performing well in
comparison to best-in-class methods for other functions. To analyze this gap
further, we provide some theoretical insights. We establish a practical
existence theorem, asserting existence of a DNN architecture and training
procedure that offers the same performance as CS. This establishes a key
theoretical benchmark, showing the gap can be closed, albeit via a strategy
guaranteed to perform as well as, but no better than, current best-in-class
schemes. Nevertheless, it demonstrates the promise of practical DNN approx., by
highlighting potential for better schemes through careful design of DNN
architectures and training strategies.","cs.LG,cs.NA,math.NA,stat.ML"
"Adversarial Attack on Graph Structured Data. Deep learning on graph structures has shown exciting results in various
applications. However, few attentions have been paid to the robustness of such
models, in contrast to numerous research work for image or text adversarial
attack and defense. In this paper, we focus on the adversarial attacks that
fool the model by modifying the combinatorial structure of data. We first
propose a reinforcement learning based attack method that learns the
generalizable attack policy, while only requiring prediction labels from the
target classifier. Also, variants of genetic algorithms and gradient methods
are presented in the scenario where prediction confidence or gradients are
available. We use both synthetic and real-world data to show that, a family of
Graph Neural Network models are vulnerable to these attacks, in both
graph-level and node-level classification tasks. We also show such attacks can
be used to diagnose the learned classifiers.","cs.CR,cs.LG,cs.SI,stat.ML"
"Convergence Acceleration via Chebyshev Step: Plausible Interpretation of Deep-Unfolded Gradient Descent. Deep unfolding is a promising deep-learning technique, whose network
architecture is based on expanding the recursive structure of existing
iterative algorithms. Although convergence acceleration is a remarkable
advantage of deep unfolding, its theoretical aspects have not been revealed
yet. The first half of this study details the theoretical analysis of the
convergence acceleration in deep-unfolded gradient descent (DUGD) whose
trainable parameters are step sizes. We propose a plausible interpretation of
the learned step-size parameters in DUGD by introducing the principle of
Chebyshev steps derived from Chebyshev polynomials. The use of Chebyshev steps
in gradient descent (GD) enables us to bound the spectral radius of a matrix
governing the convergence speed of GD, leading to a tight upper bound on the
convergence rate. The convergence rate of GD using Chebyshev steps is shown to
be asymptotically optimal, although it has no momentum terms. We also show that
Chebyshev steps numerically explain the learned step-size parameters in DUGD
well. In the second half of the study, %we apply the theory of Chebyshev steps
and Chebyshev-periodical successive over-relaxation (Chebyshev-PSOR) is
proposed for accelerating linear/nonlinear fixed-point iterations. Theoretical
analysis and numerical experiments indicate that Chebyshev-PSOR exhibits
significantly faster convergence for various examples such as Jacobi method and
proximal gradient methods.","cs.IT,cs.LG,eess.SP,math.IT,stat.ML"
"Representation Learning via Invariant Causal Mechanisms. Self-supervised learning has emerged as a strategy to reduce the reliance on
costly supervised signal by pretraining representations only using unlabeled
data. These methods combine heuristic proxy classification tasks with data
augmentations and have achieved significant success, but our theoretical
understanding of this success remains limited. In this paper we analyze
self-supervised representation learning using a causal framework. We show how
data augmentations can be more effectively utilized through explicit invariance
constraints on the proxy classifiers employed during pretraining. Based on
this, we propose a novel self-supervised objective, Representation Learning via
Invariant Causal Mechanisms (ReLIC), that enforces invariant prediction of
proxy targets across augmentations through an invariance regularizer which
yields improved generalization guarantees. Further, using causality we
generalize contrastive learning, a particular kind of self-supervised method,
and provide an alternative theoretical explanation for the success of these
methods. Empirically, ReLIC significantly outperforms competing methods in
terms of robustness and out-of-distribution generalization on ImageNet, while
also significantly outperforming these methods on Atari achieving above
human-level performance on $51$ out of $57$ games.","cs.CV,cs.LG,stat.ML"
"Deep Shape-from-Template: Wide-Baseline, Dense and Fast Registration and Deformable Reconstruction from a Single Image. We present Deep Shape-from-Template (DeepSfT), a novel Deep Neural Network
(DNN) method for solving real-time automatic registration and 3D reconstruction
of a deformable object viewed in a single monocular image.DeepSfT advances the
state-of-the-art in various aspects. Compared to existing DNN SfT methods, it
is the first fully convolutional real-time approach that handles an arbitrary
object geometry, topology and surface representation. It also does not require
ground truth registration with real data and scales well to very complex object
models with large numbers of elements. Compared to previous non-DNN SfT
methods, it does not involve numerical optimization at run-time, and is a
dense, wide-baseline solution that does not demand, and does not suffer from,
feature-based matching. It is able to process a single image with significant
deformation and viewpoint changes, and handles well the core challenges of
occlusions, weak texture and blur. DeepSfT is based on residual encoder-decoder
structures and refining blocks. It is trained end-to-end with a novel
combination of supervised learning from simulated renderings of the object
model and semi-supervised automatic fine-tuning using real data captured with a
standard RGB-D camera. The cameras used for fine-tuning and run-time can be
different, making DeepSfT practical for real-world use. We show that DeepSfT
significantly outperforms state-of-the-art wide-baseline approaches for
non-trivial templates, with quantitative and qualitative evaluation.","cs.AI,cs.CV,cs.GR,cs.LG,cs.SE"
"Towards interpreting computer vision based on transformation invariant optimization. Interpreting how does deep neural networks (DNNs) make predictions is a vital
field in artificial intelligence, which hinders wide applications of DNNs.
Visualization of learned representations helps we humans understand the vision
of DNNs. In this work, visualized images that can activate the neural network
to the target classes are generated by back-propagation method. Here, rotation
and scaling operations are applied to introduce the transformation invariance
in the image generating process, which we find a significant improvement on
visualization effect. Finally, we show some cases that such method can help us
to gain insight into neural networks.","68T45,I.2.10,cs.CV"
"Linking emotions to behaviors through deep transfer learning. Human behavior refers to the way humans act and interact. Understanding human
behavior is a cornerstone of observational practice, especially in
psychotherapy. An important cue of behavior analysis is the dynamical changes
of emotions during the conversation. Domain experts integrate emotional
information in a highly nonlinear manner, thus, it is challenging to explicitly
quantify the relationship between emotions and behaviors. In this work, we
employ deep transfer learning to analyze their inferential capacity and
contextual importance. We first train a network to quantify emotions from
acoustic signals and then use information from the emotion recognition network
as features for behavior recognition. We treat this emotion-related information
as behavioral primitives and further train higher level layers towards behavior
quantification. Through our analysis, we find that emotion-related information
is an important cue for behavior recognition. Further, we investigate the
importance of emotional-context in the expression of behavior by constraining
(or not) the neural networks' contextual view of the data. This demonstrates
that the sequence of emotions is critical in behavior expression. To achieve
these frameworks we employ hybrid architectures of convolutional networks and
recurrent networks to extract emotion-related behavior primitives and
facilitate automatic behavior recognition from speech.","cs.CL,cs.HC,cs.LG,eess.AS"
"t-viSNE: Interactive Assessment and Interpretation of t-SNE Projections. t-Distributed Stochastic Neighbor Embedding (t-SNE) for the visualization of
multidimensional data has proven to be a popular approach, with successful
applications in a wide range of domains. Despite their usefulness, t-SNE
projections can be hard to interpret or even misleading, which hurts the
trustworthiness of the results. Understanding the details of t-SNE itself and
the reasons behind specific patterns in its output may be a daunting task,
especially for non-experts in dimensionality reduction. In this work, we
present t-viSNE, an interactive tool for the visual exploration of t-SNE
projections that enables analysts to inspect different aspects of their
accuracy and meaning, such as the effects of hyper-parameters, distance and
neighborhood preservation, densities and costs of specific neighborhoods, and
the correlations between dimensions and visual patterns. We propose a coherent,
accessible, and well-integrated collection of different views for the
visualization of t-SNE projections. The applicability and usability of t-viSNE
are demonstrated through hypothetical usage scenarios with real data sets.
Finally, we present the results of a user study where the tool's effectiveness
was evaluated. By bringing to light information that would normally be lost
after running t-SNE, we hope to support analysts in using t-SNE and making its
results better understandable.","cs.HC,cs.LG,stat.ML"
"A Novel Semi-Supervised Data-Driven Method for Chiller Fault Diagnosis with Unlabeled Data. In practical chiller systems, applying efficient fault diagnosis techniques
can significantly reduce energy consumption and improve energy efficiency of
buildings. The success of the existing methods for fault diagnosis of chillers
relies on the condition that sufficient labeled data are available for
training. However, label acquisition is laborious and costly in practice.
Usually, the number of labeled data is limited and most data available are
unlabeled. The existing methods cannot exploit the information contained in
unlabeled data, which significantly limits the improvement of fault diagnosis
performance in chiller systems. To make effective use of unlabeled data to
further improve fault diagnosis performance and reduce the dependency on
labeled data, we proposed a novel semi-supervised data-driven fault diagnosis
method for chiller systems based on the semi-generative adversarial network,
which incorporates both unlabeled and labeled data into learning process. The
semi-generative adversarial network can learn the information of data
distribution from unlabeled data and this information can help to significantly
improve the diagnostic performance. Experimental results demonstrate the
effectiveness of the proposed method. Under the scenario that there are only 80
labeled samples and 16000 unlabeled samples, the proposed method can improve
the diagnostic accuracy to 84%, while the supervised baseline methods only
reach the accuracy of 65% at most. Besides, the minimal required number of
labeled samples can be reduced by about 60% with the proposed method when there
are enough unlabeled samples.","cs.LG,cs.SY,eess.SY"
"Do Large Scale Molecular Language Representations Capture Important Structural Information?. Predicting chemical properties from the structure of a molecule is of great
importance in many applications including drug discovery and material design.
Machine learning based molecular property prediction holds the promise of
enabling accurate predictions at much less complexity, when compared to, for
example Density Functional Theory (DFT) calculations. Features extracted from
molecular graphs, using graph neural nets in a supervised manner, have emerged
as strong baselines for such tasks. However, the vast chemical space together
with the limited availability of labels makes supervised learning challenging,
calling for learning a general-purpose molecular representation. Recently,
pre-trained transformer-based language models (PTLMs) on large unlabeled corpus
have produced state-of-the-art results in many downstream natural language
processing tasks. Inspired by this development, here we present molecular
embeddings obtained by training an efficient transformer encoder model,
referred to as MoLFormer. This model was employed with a linear attention
mechanism and highly paralleized training on 1D SMILES sequences of 1.1 billion
unlabeled molecules from the PubChem and ZINC datasets. Experiments show that
the learned molecular representation performs competitively, when compared to
existing graph-based and fingerprint-based supervised learning baselines, on
the challenging tasks of predicting properties of QM8 and QM9 molecules.
Further task-specific fine-tuning of the MoLFormerr representation improves
performance on several of those property prediction benchmarks. These results
provide encouraging evidence that large-scale molecular language models can
capture sufficient structural information to be able to accurately predict
quantum chemical properties and beyond.","cs.CL,cs.LG,q-bio.BM"
"Atari-fying the Vehicle Routing Problem with Stochastic Service Requests. We present a new general approach to modeling research problems as Atari-like
videogames to make them amenable to recent groundbreaking solution methods from
the deep reinforcement learning community. The approach is flexible, applicable
to a wide range of problems. We demonstrate its application on a well known
vehicle routing problem. Our preliminary results on this problem, though not
transformative, show signs of success and suggest that Atari-fication may be a
useful modeling approach for researchers studying problems involving sequential
decision making under uncertainty.","cs.LG,math.OC,stat.ML"
"Denoising and Regularization via Exploiting the Structural Bias of Convolutional Generators. Convolutional Neural Networks (CNNs) have emerged as highly successful tools
for image generation, recovery, and restoration. A major contributing factor to
this success is that convolutional networks impose strong prior assumptions
about natural images. A surprising experiment that highlights this
architectural bias towards natural images is that one can remove noise and
corruptions from a natural image without using any training data, by simply
fitting (via gradient descent) a randomly initialized, over-parameterized
convolutional generator to the corrupted image. While this over-parameterized
network can fit the corrupted image perfectly, surprisingly after a few
iterations of gradient descent it generates an almost uncorrupted image. This
intriguing phenomenon enables state-of-the-art CNN-based denoising and
regularization of other inverse problems. In this paper, we attribute this
effect to a particular architectural choice of convolutional networks, namely
convolutions with fixed interpolating filters. We then formally characterize
the dynamics of fitting a two-layer convolutional generator to a noisy signal
and prove that early-stopped gradient descent denoises/regularizes. Our proof
relies on showing that convolutional generators fit the structured part of an
image significantly faster than the corrupted portion.","cs.CV,cs.IT,cs.LG,math.IT,stat.ML"
"InfographicVQA. Infographics are documents designed to effectively communicate information
using a combination of textual, graphical and visual elements. In this work, we
explore the automatic understanding of infographic images by using Visual
Question Answering technique.To this end, we present InfographicVQA, a new
dataset that comprises a diverse collection of infographics along with natural
language questions and answers annotations. The collected questions require
methods to jointly reason over the document layout, textual content, graphical
elements, and data visualizations. We curate the dataset with emphasis on
questions that require elementary reasoning and basic arithmetic skills.
Finally, we evaluate two strong baselines based on state of the art multi-modal
VQA models, and establish baseline performance for the new task. The dataset,
code and leaderboard will be made available at http://docvqa.org","cs.CL,cs.CV"
"Change your singer: a transfer learning generative adversarial framework for song to song conversion. Have you ever wondered how a song might sound if performed by a different
artist? In this work, we propose SCM-GAN, an end-to-end non-parallel song
conversion system powered by generative adversarial and transfer learning that
allows users to listen to a selected target singer singing any song. SCM-GAN
first separates songs into vocals and instrumental music using a U-Net network,
then converts the vocal segments to the target singer using advanced
CycleGAN-VC, before merging the converted vocals with their corresponding
background music. SCM-GAN is first initialized with feature representations
learned from a state-of-the-art voice-to-voice conversion and then trained on a
dataset of non-parallel songs. Furthermore, SCM-GAN is evaluated against a set
of metrics including global variance GV and modulation spectra MS on the 24
Mel-cepstral coefficients (MCEPs). Transfer learning improves the GV by 35% and
the MS by 13% on average. A subjective comparison is conducted to test the user
satisfaction with the quality and the naturalness of the conversion. Results
show above par similarity between SCM-GAN's output and the target (70\% on
average) as well as great naturalness of the converted songs.","cs.LG,cs.SD,eess.AS,stat.ML"
"Multi-Output Gaussian Processes with Functional Data: A Study on Coastal Flood Hazard Assessment. Most of the existing coastal flood Forecast and Early-Warning Systems do not
model the flood, but instead, rely on the prediction of hydrodynamic conditions
at the coast and on expert judgment. Recent scientific contributions are now
capable to precisely model flood events, even in situations where wave
overtopping plays a significant role. Such models are nevertheless
costly-to-evaluate and surrogate ones need to be exploited for substantial
computational savings. For the latter models, the hydro-meteorological forcing
conditions (inputs) or flood events (outputs) are conveniently parametrised
into scalar representations. However, they neglect the fact that inputs are
actually functions (more precisely, time series), and that floods spatially
propagate inland. Here, we introduce a multi-output Gaussian process model
accounting for both criteria. On various examples, we test its versatility for
both learning spatial maps and inferring unobserved ones. We demonstrate that
efficient implementations are obtained by considering tensor-structured data
and/or sparse-variational approximations. Finally, the proposed framework is
applied on a coastal application aiming at predicting flood events. We conclude
that accurate predictions are obtained in the order of minutes rather than the
couples of days required by dedicated hydrodynamic simulators.","cs.LG,stat.AP,stat.ML"
"Improving STDP-based Visual Feature Learning with Whitening. In recent years, spiking neural networks (SNNs) emerge as an alternative to
deep neural networks (DNNs). SNNs present a higher computational efficiency
using low-power neuromorphic hardware and require less labeled data for
training using local and unsupervised learning rules such as spike
timing-dependent plasticity (STDP). SNN have proven their effectiveness in
image classification on simple datasets such as MNIST. However, to process
natural images, a pre-processing step is required. Difference-of-Gaussians
(DoG) filtering is typically used together with on-center/off-center coding,
but it results in a loss of information that is detrimental to the
classification performance. In this paper, we propose to use whitening as a
pre-processing step before learning features with STDP. Experiments on CIFAR-10
show that whitening allows STDP to learn visual features that are closer to the
ones learned with standard neural networks, with a significantly increased
classification performance as compared to DoG filtering. We also propose an
approximation of whitening as convolution kernels that is computationally
cheaper to learn and more suited to be implemented on neuromorphic hardware.
Experiments on CIFAR-10 show that it performs similarly to regular whitening.
Cross-dataset experiments on CIFAR-10 and STL-10 also show that it is fairly
stable across datasets, making it possible to learn a single whitening
transformation to process different datasets.","cs.CV,cs.LG,cs.NE"
"GAMesh: Guided and Augmented Meshing for Deep Point Networks. We present a new meshing algorithm called guided and augmented meshing,
GAMesh, which uses a mesh prior to generate a surface for the output points of
a point network. By projecting the output points onto this prior and
simplifying the resulting mesh, GAMesh ensures a surface with the same topology
as the mesh prior but whose geometric fidelity is controlled by the point
network. This makes GAMesh independent of both the density and distribution of
the output points, a common artifact in traditional surface reconstruction
algorithms. We show that such a separation of geometry from topology can have
several advantages especially in single-view shape prediction, fair evaluation
of point networks and reconstructing surfaces for networks which output sparse
point clouds. We further show that by training point networks with GAMesh, we
can directly optimize the vertex positions to generate adaptive meshes with
arbitrary topologies.","cs.CG,cs.CV,cs.GR,cs.LG"
"Reinforcement learning and Bayesian data assimilation for model-informed precision dosing in oncology. Model-informed precision dosing (MIPD) using therapeutic drug/biomarker
monitoring offers the opportunity to significantly improve the efficacy and
safety of drug therapies. Current strategies comprise model-informed dosing
tables or are based on maximum a-posteriori estimates. These approaches,
however, lack a quantification of uncertainty and/or consider only part of the
available patient-specific information. We propose three novel approaches for
MIPD employing Bayesian data assimilation (DA) and/or reinforcement learning
(RL) to control neutropenia, the major dose-limiting side effect in anticancer
chemotherapy. These approaches have the potential to substantially reduce the
incidence of life-threatening grade 4 and subtherapeutic grade 0 neutropenia
compared to existing approaches. We further show that RL allows to gain further
insights by identifying patient factors that drive dose decisions. Due to its
flexibility, the proposed combined DA-RL approach can easily be extended to
integrate multiple endpoints or patient-reported outcomes, thereby promising
important benefits for future personalized therapies.","cs.LG,q-bio.QM,stat.AP,stat.ML"
"Visualizing Deep Graph Generative Models for Drug Discovery. Drug discovery aims at designing novel molecules with specific desired
properties for clinical trials. Over past decades, drug discovery and
development have been a costly and time consuming process. Driven by big
chemical data and AI, deep generative models show great potential to accelerate
the drug discovery process. Existing works investigate different deep
generative frameworks for molecular generation, however, less attention has
been paid to the visualization tools to quickly demo and evaluate model's
results. Here, we propose a visualization framework which provides interactive
visualization tools to visualize molecules generated during the encoding and
decoding process of deep graph generative models, and provide real time
molecular optimization functionalities. Our work tries to empower black box AI
driven drug discovery models with some visual interpretabilities.","I.2.1,cs.HC,cs.LG,stat.ML"
"Mean Oriented Riesz Features for Micro Expression Classification. Micro-expressions are brief and subtle facial expressions that go on and off
the face in a fraction of a second. This kind of facial expressions usually
occurs in high stake situations and is considered to reflect a human's real
intent. There has been some interest in micro-expression analysis, however, a
great majority of the methods are based on classically established computer
vision methods such as local binary patterns, histogram of gradients and
optical flow. A novel methodology for micro-expression recognition using the
Riesz pyramid, a multi-scale steerable Hilbert transform is presented. In fact,
an image sequence is transformed with this tool, then the image phase
variations are extracted and filtered as proxies for motion. Furthermore, the
dominant orientation constancy from the Riesz transform is exploited to average
the micro-expression sequence into an image pair. Based on that, the Mean
Oriented Riesz Feature description is introduced. Finally the performance of
our methods are tested in two spontaneous micro-expressions databases and
compared to state-of-the-art methods.","68T10,68U10,cs.CV"
"Modeling Feature Representations for Affective Speech using Generative Adversarial Networks. Emotion recognition is a classic field of research with a typical setup
extracting features and feeding them through a classifier for prediction. On
the other hand, generative models jointly capture the distributional
relationship between emotions and the feature profiles. Relatively recently,
Generative Adversarial Networks (GANs) have surfaced as a new class of
generative models and have shown considerable success in modeling distributions
in the fields of computer vision and natural language understanding. In this
work, we experiment with variants of GAN architectures to generate feature
vectors corresponding to an emotion in two ways: (i) A generator is trained
with samples from a mixture prior. Each mixture component corresponds to an
emotional class and can be sampled to generate features from the corresponding
emotion. (ii) A one-hot vector corresponding to an emotion can be explicitly
used to generate the features. We perform analysis on such models and also
propose different metrics used to measure the performance of the GAN models in
their ability to generate realistic synthetic samples. Apart from evaluation on
a given dataset of interest, we perform a cross-corpus study where we study the
utility of the synthetic samples as additional training data in low resource
conditions.","cs.LG,cs.SD,eess.AS,stat.ML"
"A geometric perspective on functional outlier detection. We consider functional outlier detection from a geometric perspective,
specifically: for functional data sets drawn from a functional manifold which
is defined by the data's modes of variation in amplitude and phase. Based on
this manifold, we develop a conceptualization of functional outlier detection
that is more widely applicable and realistic than previously proposed. Our
theoretical and experimental analyses demonstrate several important advantages
of this perspective: It considerably improves theoretical understanding and
allows to describe and analyse complex functional outlier scenarios
consistently and in full generality, by differentiating between structurally
anomalous outlier data that are off-manifold and distributionally outlying data
that are on-manifold but at its margins. This improves practical feasibility of
functional outlier detection: We show that simple manifold learning methods can
be used to reliably infer and visualize the geometric structure of functional
data sets. We also show that standard outlier detection methods requiring
tabular data inputs can be applied to functional data very successfully by
simply using their vector-valued representations learned from manifold learning
methods as input features. Our experiments on synthetic and real data sets
demonstrate that this approach leads to outlier detection performances at least
on par with existing functional data-specific methods in a large variety of
settings, without the highly specialized, complex methodology and narrow domain
of application these methods often entail.","cs.LG,stat.CO,stat.ML"
"Central and Non-central Limit Theorems arising from the Scattering Transform and its Neural Activation Generalization. Motivated by analyzing complicated and non-stationary time series, we study a
generalization of the scattering transform (ST) that includes broad neural
activation functions, which is called neural activation ST (NAST). On the
whole, NAST is a transform that comprises a sequence of ``neural processing
units'', each of which applies a high pass filter to the input from the
previous layer followed by a composition with a nonlinear function as the
output to the next neuron. Here, the nonlinear function models how a neuron
gets excited by the input signal. In addition to showing properties like
non-expansion, horizontal translational invariability and insensitivity to
local deformation, the statistical properties of the second order NAST of a
Gaussian process with various dependence and (non-)stationarity structure and
its interaction with the chosen high pass filters and activation functions are
explored and central limit theorem (CLT) and non-CLT results are provided.
Numerical simulations are also provided. The results explain how NAST processes
complicated and non-stationary time series, and pave a way towards statistical
inference based on NAST under the non-null case.","60H05,62M15,Primary 60G60,Secondary 35K15,cs.LG,math.PR,stat.ML"
"Efficient Clustering for Stretched Mixtures: Landscape and Optimality. This paper considers a canonical clustering problem where one receives
unlabeled samples drawn from a balanced mixture of two elliptical distributions
and aims for a classifier to estimate the labels. Many popular methods
including PCA and k-means require individual components of the mixture to be
somewhat spherical, and perform poorly when they are stretched. To overcome
this issue, we propose a non-convex program seeking for an affine transform to
turn the data into a one-dimensional point cloud concentrating around -1 and 1,
after which clustering becomes easy. Our theoretical contributions are
two-fold: (1) we show that the non-convex loss function exhibits desirable
landscape properties as long as the sample size exceeds some constant multiple
of the dimension, and (2) we leverage this to prove that an efficient
first-order algorithm achieves near-optimal statistical precision even without
good initialization. We also propose a general methodology for multi-class
clustering tasks with flexible choices of feature transforms and loss
objectives.","62H30,cs.LG,math.OC,math.ST,stat.ME,stat.ML,stat.TH"
"Unsupervised Learning using Pretrained CNN and Associative Memory Bank. Deep Convolutional features extracted from a comprehensive labeled dataset,
contain substantial representations which could be effectively used in a new
domain. Despite the fact that generic features achieved good results in many
visual tasks, fine-tuning is required for pretrained deep CNN models to be more
effective and provide state-of-the-art performance. Fine tuning using the
backpropagation algorithm in a supervised setting, is a time and resource
consuming process. In this paper, we present a new architecture and an approach
for unsupervised object recognition that addresses the above mentioned problem
with fine tuning associated with pretrained CNN-based supervised deep learning
approaches while allowing automated feature extraction. Unlike existing works,
our approach is applicable to general object recognition tasks. It uses a
pretrained (on a related domain) CNN model for automated feature extraction
pipelined with a Hopfield network based associative memory bank for storing
patterns for classification purposes. The use of associative memory bank in our
framework allows eliminating backpropagation while providing competitive
performance on an unseen dataset.","cs.CV,cs.LG,stat.ML"
"Fantastic Features and Where to Find Them: Detecting Cognitive Impairment with a Subsequence Classification Guided Approach. Despite the widely reported success of embedding-based machine learning
methods on natural language processing tasks, the use of more easily
interpreted engineered features remains common in fields such as cognitive
impairment (CI) detection. Manually engineering features from noisy text is
time and resource consuming, and can potentially result in features that do not
enhance model performance. To combat this, we describe a new approach to
feature engineering that leverages sequential machine learning models and
domain knowledge to predict which features help enhance performance. We provide
a concrete example of this method on a standard data set of CI speech and
demonstrate that CI classification accuracy improves by 2.3% over a strong
baseline when using features produced by this method. This demonstration
provides an ex-ample of how this method can be used to assist classification in
fields where interpretability is important, such as health care.","cs.CL,cs.LG"
"An Information-Theoretic Perspective on Credit Assignment in Reinforcement Learning. How do we formalize the challenge of credit assignment in reinforcement
learning? Common intuition would draw attention to reward sparsity as a key
contributor to difficult credit assignment and traditional heuristics would
look to temporal recency for the solution, calling upon the classic eligibility
trace. We posit that it is not the sparsity of the reward itself that causes
difficulty in credit assignment, but rather the \emph{information sparsity}. We
propose to use information theory to define this notion, which we then use to
characterize when credit assignment is an obstacle to efficient learning. With
this perspective, we outline several information-theoretic mechanisms for
measuring credit under a fixed behavior policy, highlighting the potential of
information theory as a key tool towards provably-efficient credit assignment.","cs.IT,cs.LG,math.IT"
"Induction of Non-Monotonic Rules From Statistical Learning Models Using High-Utility Itemset Mining. We present a fast and scalable algorithm to induce non-monotonic logic
programs from statistical learning models. We reduce the problem of search for
best clauses to instances of the High-Utility Itemset Mining (HUIM) problem. In
the HUIM problem, feature values and their importance are treated as
transactions and utilities respectively. We make use of TreeExplainer, a fast
and scalable implementation of the Explainable AI tool SHAP, to extract locally
important features and their weights from ensemble tree models. Our experiments
with UCI standard benchmarks suggest a significant improvement in terms of
classification evaluation metrics and running time of the training algorithm
compared to ALEPH, a state-of-the-art Inductive Logic Programming (ILP) system.","cs.LG,cs.LO,stat.ML"
"Context-Aware Graph Attention Networks. Graph Neural Networks (GNNs) have been widely studied for graph data
representation and learning. However, existing GNNs generally conduct
context-aware learning on node feature representation only which usually
ignores the learning of edge (weight) representation. In this paper, we propose
a novel unified GNN model, named Context-aware Adaptive Graph Attention Network
(CaGAT). CaGAT aims to learn a context-aware attention representation for each
graph edge by further exploiting the context relationships among different
edges. In particular, CaGAT conducts context-aware learning on both node
feature representation and edge (weight) representation simultaneously and
cooperatively in a unified manner which can boost their respective performance
in network training. We apply CaGAT on semi-supervised learning tasks.
Promising experimental results on several benchmark datasets demonstrate the
effectiveness and benefits of CaGAT.","cs.LG,cs.SI,eess.IV,eess.SP,stat.ML"
"ADD-Lib: Decision Diagrams in Practice. In the paper, we present the ADD-Lib, our efficient and easy to use framework
for Algebraic Decision Diagrams (ADDs). The focus of the ADD-Lib is not so much
on its efficient implementation of individual operations, which are taken by
other established ADD frameworks, but its ease and flexibility, which arise at
two levels: the level of individual ADD-tools, which come with a dedicated
user-friendly web-based graphical user interface, and at the meta level, where
such tools are specified. Both levels are described in the paper: the meta
level by explaining how we can construct an ADD-tool tailored for Random Forest
refinement and evaluation, and the accordingly generated Web-based
domain-specific tool, which we also provide as an artifact for cooperative
experimentation. In particular, the artifact allows readers to combine a given
Random Forest with their own ADDs regarded as expert knowledge and to
experience the corresponding effect.","cs.AI,cs.LG,cs.PL,cs.SE"
"Translating Natural Language Instructions for Behavioral Robot Navigation with a Multi-Head Attention Mechanism. We propose a multi-head attention mechanism as a blending layer in a neural
network model that translates natural language to a high level behavioral
language for indoor robot navigation. We follow the framework established by
(Zang et al., 2018a) that proposes the use of a navigation graph as a knowledge
base for the task. Our results show significant performance gains when
translating instructions on previously unseen environments, therefore,
improving the generalization capabilities of the model.","cs.CL,cs.LG,cs.RO,stat.ML"
"Featurized Bidirectional GAN: Adversarial Defense via Adversarially Learned Semantic Inference. Deep neural networks have been demonstrated to be vulnerable to adversarial
attacks, where small perturbations intentionally added to the original inputs
can fool the classifier. In this paper, we propose a defense method, Featurized
Bidirectional Generative Adversarial Networks (FBGAN), to extract the semantic
features of the input and filter the non-semantic perturbation. FBGAN is
pre-trained on the clean dataset in an unsupervised manner, adversarially
learning a bidirectional mapping between the high-dimensional data space and
the low-dimensional semantic space; also mutual information is applied to
disentangle the semantically meaningful features. After the bidirectional
mapping, the adversarial data can be reconstructed to denoised data, which
could be fed into any pre-trained classifier. We empirically show the quality
of reconstruction images and the effectiveness of defense.","cs.CR,cs.CV,cs.LG,stat.ML"
"Multi-agent Inverse Reinforcement Learning for Certain General-sum Stochastic Games. This paper addresses the problem of multi-agent inverse reinforcement
learning (MIRL) in a two-player general-sum stochastic game framework. Five
variants of MIRL are considered: uCS-MIRL, advE-MIRL, cooE-MIRL, uCE-MIRL, and
uNE-MIRL, each distinguished by its solution concept. Problem uCS-MIRL is a
cooperative game in which the agents employ cooperative strategies that aim to
maximize the total game value. In problem uCE-MIRL, agents are assumed to
follow strategies that constitute a correlated equilibrium while maximizing
total game value. Problem uNE-MIRL is similar to uCE-MIRL in total game value
maximization, but it is assumed that the agents are playing a Nash equilibrium.
Problems advE-MIRL and cooE-MIRL assume agents are playing an adversarial
equilibrium and a coordination equilibrium, respectively. We propose novel
approaches to address these five problems under the assumption that the game
observer either knows or is able to accurate estimate the policies and solution
concepts for players. For uCS-MIRL, we first develop a characteristic set of
solutions ensuring that the observed bi-policy is a uCS and then apply a
Bayesian inverse learning method. For uCE-MIRL, we develop a linear programming
problem subject to constraints that define necessary and sufficient conditions
for the observed policies to be correlated equilibria. The objective is to
choose a solution that not only minimizes the total game value difference
between the observed bi-policy and a local uCS, but also maximizes the scale of
the solution. We apply a similar treatment to the problem of uNE-MIRL. The
remaining two problems can be solved efficiently by taking advantage of
solution uniqueness and setting up a convex optimization problem. Results are
validated on various benchmark grid-world games.","cs.GT,cs.LG,stat.ML"
"Long Short View Feature Decomposition via Contrastive Video Representation Learning. Self-supervised video representation methods typically focus on the
representation of temporal attributes in videos. However, the role of
stationary versus non-stationary attributes is less explored: Stationary
features, which remain similar throughout the video, enable the prediction of
video-level action classes. Non-stationary features, which represent temporally
varying attributes, are more beneficial for downstream tasks involving more
fine-grained temporal understanding, such as action segmentation. We argue that
a single representation to capture both types of features is sub-optimal, and
propose to decompose the representation space into stationary and
non-stationary features via contrastive learning from long and short views,
i.e. long video sequences and their shorter sub-sequences. Stationary features
are shared between the short and long views, while non-stationary features
aggregate the short views to match the corresponding long view. To empirically
verify our approach, we demonstrate that our stationary features work
particularly well on an action recognition downstream task, while our
non-stationary features perform better on action segmentation. Furthermore, we
analyse the learned representations and find that stationary features capture
more temporally stable, static attributes, while non-stationary features
encompass more temporally varying ones.",cs.CV
"GAN for time series prediction, data assimilation and uncertainty quantification. We propose a new method in which a generative adversarial network (GAN) is
used to quantify the uncertainty of forward simulations in the presence of
observed data. Previously, a method has been developed which enables GANs to
make time series predictions and data assimilation by training a GAN with
unconditional simulations of a high-fidelity numerical model. After training,
the GAN can be used to predict the evolution of the spatial distribution of the
simulation states and observed data is assimilated. In this paper, we describe
the process required in order to quantify uncertainty, during which no
additional simulations of the high-fidelity numerical model are required. These
methods take advantage of the adjoint-like capabilities of generative models
and the ability to simulate forwards and backwards in time. Set within a
reduced-order model framework for efficiency, we apply these methods to a
compartmental model in epidemiology to predict the spread of COVID-19 in an
idealised town. The results show that the proposed method can efficiently
quantify uncertainty in the presence of measurements using only unconditional
simulations of the high-fidelity numerical model.","cs.LG,stat.ML"
"The Quantum Version Of Classification Decision Tree Constructing Algorithm C5.0. In the paper, we focus on complexity of C5.0 algorithm for constructing
decision tree classifier that is the models for the classification problem from
machine learning. In classical case the decision tree is constructed in
$O(hd(NM+N \log N))$ running time, where $M$ is a number of classes, $N$ is the
size of a training data set, $d$ is a number of attributes of each element, $h$
is a tree height. Firstly, we improved the classical version, the running time
of the new version is $O(h\cdot d\cdot N\log N)$. Secondly, we suggest a
quantum version of this algorithm, which uses quantum subroutines like the
amplitude amplification and the D{\""u}rr-H{\o}yer minimum search algorithms
that are based on Grover's algorithm. The running time of the quantum algorithm
is $O\big(h\cdot \sqrt{d}\log d \cdot N \log N\big)$ that is better than
complexity of the classical algorithm.","cs.LG,quant-ph,stat.ML"
"Product Graph Learning from Multi-domain Data with Sparsity and Rank Constraints. In this paper, we focus on learning product graphs from multi-domain data. We
assume that the product graph is formed by the Cartesian product of two smaller
graphs, which we refer to as graph factors. We pose the product graph learning
problem as the problem of estimating the graph factor Laplacian matrices. To
capture local interactions in data, we seek sparse graph factors and assume a
smoothness model for data. We propose an efficient iterative solver for
learning sparse product graphs from data. We then extend this solver to infer
multi-component graph factors with applications to product graph clustering by
imposing rank constraints on the graph Laplacian matrices. Although working
with smaller graph factors is computationally more attractive, not all graphs
may readily admit an exact Cartesian product factorization. To this end, we
propose efficient algorithms to approximate a graph by a nearest Cartesian
product of two smaller graphs. The efficacy of the developed framework is
demonstrated using several numerical experiments on synthetic data and real
data.","cs.LG,eess.SP"
"Correlation Priors for Reinforcement Learning. Many decision-making problems naturally exhibit pronounced structures
inherited from the characteristics of the underlying environment. In a Markov
decision process model, for example, two distinct states can have inherently
related semantics or encode resembling physical state configurations. This
often implies locally correlated transition dynamics among the states. In order
to complete a certain task in such environments, the operating agent usually
needs to execute a series of temporally and spatially correlated actions.
Though there exists a variety of approaches to capture these correlations in
continuous state-action domains, a principled solution for discrete
environments is missing. In this work, we present a Bayesian learning framework
based on P\'olya-Gamma augmentation that enables an analogous reasoning in such
cases. We demonstrate the framework on a number of common decision-making
related problems, such as imitation learning, subgoal extraction, system
identification and Bayesian reinforcement learning. By explicitly modeling the
underlying correlation structures of these problems, the proposed approach
yields superior predictive performance compared to correlation-agnostic models,
even when trained on data sets that are an order of magnitude smaller in size.","cs.AI,cs.LG,cs.SY,eess.SY,stat.ML"
"Fairness-aware Classification: Criterion, Convexity, and Bounds. Fairness-aware classification is receiving increasing attention in the
machine learning fields. Recently research proposes to formulate the
fairness-aware classification as constrained optimization problems. However,
several limitations exist in previous works due to the lack of a theoretical
framework for guiding the formulation. In this paper, we propose a general
framework for learning fair classifiers which addresses previous limitations.
The framework formulates various commonly-used fairness metrics as convex
constraints that can be directly incorporated into classic classification
models. Within the framework, we propose a constraint-free criterion on the
training data which ensures that any classifier learned from the data is fair.
We also derive the constraints which ensure that the real fairness metric is
satisfied when surrogate functions are used to achieve convexity. Our framework
can be used to for formulating fairness-aware classification with fairness
guarantee and computational efficiency. The experiments using real-world
datasets demonstrate our theoretical results and show the effectiveness of
proposed framework and methods.","cs.AI,cs.CY,cs.LG,stat.ML"
"Splitting Methods for Convex Bi-Clustering and Co-Clustering. Co-Clustering, the problem of simultaneously identifying clusters across
multiple aspects of a data set, is a natural generalization of clustering to
higher-order structured data. Recent convex formulations of bi-clustering and
tensor co-clustering, which shrink estimated centroids together using a convex
fusion penalty, allow for global optimality guarantees and precise theoretical
analysis, but their computational properties have been less well studied. In
this note, we present three efficient operator-splitting methods for the convex
co-clustering problem: a standard two-block ADMM, a Generalized ADMM which
avoids an expensive tensor Sylvester equation in the primal update, and a
three-block ADMM based on the operator splitting scheme of Davis and Yin.
Theoretical complexity analysis suggests, and experimental evidence confirms,
that the Generalized ADMM is far more efficient for large problems.","stat.CO,stat.ML"
"The Benchmark Lottery. The world of empirical machine learning (ML) strongly relies on benchmarks in
order to determine the relative effectiveness of different algorithms and
methods. This paper proposes the notion of ""a benchmark lottery"" that describes
the overall fragility of the ML benchmarking process. The benchmark lottery
postulates that many factors, other than fundamental algorithmic superiority,
may lead to a method being perceived as superior. On multiple benchmark setups
that are prevalent in the ML community, we show that the relative performance
of algorithms may be altered significantly simply by choosing different
benchmark tasks, highlighting the fragility of the current paradigms and
potential fallacious interpretation derived from benchmarking ML methods. Given
that every benchmark makes a statement about what it perceives to be important,
we argue that this might lead to biased progress in the community. We discuss
the implications of the observed phenomena and provide recommendations on
mitigating them using multiple machine learning domains and communities as use
cases, including natural language processing, computer vision, information
retrieval, recommender systems, and reinforcement learning.","cs.AI,cs.CL,cs.CV,cs.IR,cs.LG"
"A Network Structure to Explicitly Reduce Confusion Errors in Semantic Segmentation. Confusing classes that are ubiquitous in real world often degrade performance
for many vision related applications like object detection, classification, and
segmentation. The confusion errors are not only caused by similar visual
patterns but also amplified by various factors during the training of our
designed models, such as reduced feature resolution in the encoding process or
imbalanced data distributions. A large amount of deep learning based network
structures has been proposed in recent years to deal with these individual
factors and improve network performance. However, to our knowledge, no existing
work in semantic image segmentation is designed to tackle confusion errors
explicitly. In this paper, we present a novel and general network structure
that reduces confusion errors in more direct manner and apply the network for
semantic segmentation. There are two major contributions in our network
structure: 1) We ensemble subnets with heterogeneous output spaces based on the
discriminative confusing groups. The training for each subnet can distinguish
confusing classes within the group without affecting unrelated classes outside
the group. 2) We propose an improved cross-entropy loss function that maximizes
the probability assigned to the correct class and penalizes the probabilities
assigned to the confusing classes at the same time. Our network structure is a
general structure and can be easily adapted to any other networks to further
reduce confusion errors. Without any changes in the feature encoder and
post-processing steps, our experiments demonstrate consistent and significant
improvements on different baseline models on Cityscapes and PASCAL VOC datasets
(e.g., 3.05% over ResNet-101 and 1.30% over ResNet-38).",cs.CV
"Audiovisual Speaker Tracking using Nonlinear Dynamical Systems with Dynamic Stream Weights. Data fusion plays an important role in many technical applications that
require efficient processing of multimodal sensory observations. A prominent
example is audiovisual signal processing, which has gained increasing attention
in automatic speech recognition, speaker localization and related tasks. If
appropriately combined with acoustic information, additional visual cues can
help to improve the performance in these applications, especially under adverse
acoustic conditions. A dynamic weighting of acoustic and visual streams based
on instantaneous sensor reliability measures is an efficient approach to data
fusion in this context. This paper presents a framework that extends the
well-established theory of nonlinear dynamical systems with the notion of
dynamic stream weights for an arbitrary number of sensory observations. It
comprises a recursive state estimator based on the Gaussian filtering paradigm,
which incorporates dynamic stream weights into a framework closely related to
the extended Kalman filter. Additionally, a convex optimization approach to
estimate oracle dynamic stream weights in fully observed dynamical systems
utilizing a Dirichlet prior is presented. This serves as a basis for a generic
parameter learning framework of dynamic stream weight estimators. The proposed
system is application-independent and can be easily adapted to specific tasks
and requirements. A study using audiovisual speaker tracking tasks is
considered as an exemplary application in this work. An improved tracking
performance of the dynamic stream weight-based estimation framework over
state-of-the-art methods is demonstrated in the experiments.","cs.CV,cs.SD,eess.AS"
"Neural Message Passing for Quantum Chemistry. Supervised learning on molecules has incredible potential to be useful in
chemistry, drug discovery, and materials science. Luckily, several promising
and closely related neural network models invariant to molecular symmetries
have already been described in the literature. These models learn a message
passing algorithm and aggregation procedure to compute a function of their
entire input graph. At this point, the next step is to find a particularly
effective variant of this general approach and apply it to chemical prediction
benchmarks until we either solve them or reach the limits of the approach. In
this paper, we reformulate existing models into a single common framework we
call Message Passing Neural Networks (MPNNs) and explore additional novel
variations within this framework. Using MPNNs we demonstrate state of the art
results on an important molecular property prediction benchmark; these results
are strong enough that we believe future work should focus on datasets with
larger molecules or more accurate ground truth labels.","I.2.6,cs.LG"
"Scaled-Time-Attention Robust Edge Network. This paper describes a systematic approach towards building a new family of
neural networks based on a delay-loop version of a reservoir neural network.
The resulting architecture, called Scaled-Time-Attention Robust Edge (STARE)
network, exploits hyper dimensional space and non-multiply-and-add computation
to achieve a simpler architecture, which has shallow layers, is simple to
train, and is better suited for Edge applications, such as Internet of Things
(IoT), over traditional deep neural networks. STARE incorporates new AI
concepts such as Attention and Context, and is best suited for temporal feature
extraction and classification. We demonstrate that STARE is applicable to a
variety of applications with improved performance and lower implementation
complexity. In particular, we showed a novel way of applying a dual-loop
configuration to detection and identification of drone vs bird in a counter
Unmanned Air Systems (UAS) detection application by exploiting both spatial
(video frame) and temporal (trajectory) information. We also demonstrated that
the STARE performance approaches that of a State-of-the-Art deep neural network
in classifying RF modulations, and outperforms Long Short-term Memory (LSTM) in
a special case of Mackey Glass time series prediction. To demonstrate hardware
efficiency, we designed and developed an FPGA implementation of the STARE
algorithm to demonstrate its low-power and high-throughput operations. In
addition, we illustrate an efficient structure for integrating a massively
parallel implementation of the STARE algorithm for ASIC implementation.","68T05,cs.CV"
"Upper Counterfactual Confidence Bounds: a New Optimism Principle for Contextual Bandits. The principle of optimism in the face of uncertainty is one of the most
widely used and successful ideas in multi-armed bandits and reinforcement
learning. However, existing optimistic algorithms (primarily UCB and its
variants) are often unable to deal with large context spaces. Essentially all
existing well performing algorithms for general contextual bandit problems rely
on weighted action allocation schemes; and theoretical guarantees for
optimism-based algorithms are only known for restricted formulations. In this
paper we study general contextual bandits under the realizability condition,
and propose a simple generic principle to design optimistic algorithms, dubbed
""Upper Counterfactual Confidence Bounds"" (UCCB). We show that these algorithms
are provably optimal and efficient in the presence of large context spaces. Key
components of UCCB include: 1) a systematic analysis of confidence bounds in
policy space rather than in action space; and 2) the potential function
perspective that is used to express the power of optimism in the contextual
setting. We further show how the UCCB principle can be extended to infinite
action spaces, by constructing confidence bounds via the newly introduced
notion of ""counterfactual action divergence.""","cs.LG,math.ST,stat.ML,stat.TH"
"How Framelets Enhance Graph Neural Networks. This paper presents a new approach for assembling graph neural networks based
on framelet transforms. The latter provides a multi-scale representation for
graph-structured data. We decompose an input graph into low-pass and high-pass
frequencies coefficients for network training, which then defines a
framelet-based graph convolution. The framelet decomposition naturally induces
a graph pooling strategy by aggregating the graph feature into low-pass and
high-pass spectra, which considers both the feature values and geometry of the
graph data and conserves the total information. The graph neural networks with
the proposed framelet convolution and pooling achieve state-of-the-art
performance in many node and graph prediction tasks. Moreover, we propose
shrinkage as a new activation for the framelet convolution, which thresholds
high-frequency information at different scales. Compared to ReLU, shrinkage
activation improves model performance on denoising and signal compression:
noises in both node and structure can be significantly reduced by accurately
cutting off the high-pass coefficients from framelet decomposition, and the
signal can be compressed to less than half its original size with
well-preserved prediction performance.","05C85,42C40,68T07,I.2.4; I.2.6,cs.AI,cs.LG,cs.NA,math.NA"
"Generalization Error Analysis of Neural networks with Gradient Based Regularization. We study gradient-based regularization methods for neural networks. We mainly
focus on two regularization methods: the total variation and the Tikhonov
regularization. Applying these methods is equivalent to using neural networks
to solve some partial differential equations, mostly in high dimensions in
practical applications. In this work, we introduce a general framework to
analyze the generalization error of regularized networks. The error estimate
relies on two assumptions on the approximation error and the quadrature error.
Moreover, we conduct some experiments on the image classification tasks to show
that gradient-based methods can significantly improve the generalization
ability and adversarial robustness of neural networks. A graphical extension of
the gradient-based methods are also considered in the experiments.","cs.LG,cs.NA,math.NA"
"Figure Captioning with Reasoning and Sequence-Level Training. Figures, such as bar charts, pie charts, and line plots, are widely used to
convey important information in a concise format. They are usually
human-friendly but difficult for computers to process automatically. In this
work, we investigate the problem of figure captioning where the goal is to
automatically generate a natural language description of the figure. While
natural image captioning has been studied extensively, figure captioning has
received relatively little attention and remains a challenging problem. First,
we introduce a new dataset for figure captioning, FigCAP, based on FigureQA.
Second, we propose two novel attention mechanisms. To achieve accurate
generation of labels in figures, we propose Label Maps Attention. To model the
relations between figure labels, we propose Relation Maps Attention. Third, we
use sequence-level training with reinforcement learning in order to directly
optimizes evaluation metrics, which alleviates the exposure bias issue and
further improves the models in generating long captions. Extensive experiments
show that the proposed method outperforms the baselines, thus demonstrating a
significant potential for the automatic captioning of vast repositories of
figures.","cs.CL,cs.CV"
"Reinforcement Learning for Slate-based Recommender Systems: A Tractable Decomposition and Practical Methodology. Most practical recommender systems focus on estimating immediate user
engagement without considering the long-term effects of recommendations on user
behavior. Reinforcement learning (RL) methods offer the potential to optimize
recommendations for long-term user engagement. However, since users are often
presented with slates of multiple items - which may have interacting effects on
user choice - methods are required to deal with the combinatorics of the RL
action space. In this work, we address the challenge of making slate-based
recommendations to optimize long-term value using RL. Our contributions are
three-fold. (i) We develop SLATEQ, a decomposition of value-based
temporal-difference and Q-learning that renders RL tractable with slates. Under
mild assumptions on user choice behavior, we show that the long-term value
(LTV) of a slate can be decomposed into a tractable function of its component
item-wise LTVs. (ii) We outline a methodology that leverages existing myopic
learning-based recommenders to quickly develop a recommender that handles LTV.
(iii) We demonstrate our methods in simulation, and validate the scalability of
decomposed TD-learning using SLATEQ in live experiments on YouTube.","cs.AI,cs.IR,cs.LG,stat.ML"
"Learning Continuous Exponential Families Beyond Gaussian. We address the problem of learning of continuous exponential family
distributions with unbounded support. While a lot of progress has been made on
learning of Gaussian graphical models, we are still lacking scalable algorithms
for reconstructing general continuous exponential families modeling
higher-order moments of the data beyond the mean and the covariance. Here, we
introduce a computationally efficient method for learning continuous graphical
models based on the Interaction Screening approach. Through a series of
numerical experiments, we show that our estimator maintains similar
requirements in terms of accuracy and sample complexity compared to alternative
approaches such as maximization of conditional likelihood, while considerably
improving upon the algorithm's run-time.","cs.LG,physics.data-an,stat.ML"
"Instance-dependent $\ell_\infty$-bounds for policy evaluation in tabular reinforcement learning. Markov reward processes (MRPs) are used to model stochastic phenomena arising
in operations research, control engineering, robotics, and artificial
intelligence, as well as communication and transportation networks. In many of
these cases, such as in the policy evaluation problem encountered in
reinforcement learning, the goal is to estimate the long-term value function of
such a process without access to the underlying population transition and
reward functions. Working with samples generated under the synchronous model,
we study the problem of estimating the value function of an infinite-horizon,
discounted MRP on finitely many states in the $\ell_\infty$-norm. We analyze
both the standard plug-in approach to this problem and a more robust variant,
and establish non-asymptotic bounds that depend on the (unknown) problem
instance, as well as data-dependent bounds that can be evaluated based on the
observations of state-transitions and rewards. We show that these approaches
are minimax-optimal up to constant factors over natural sub-classes of MRPs.
Our analysis makes use of a leave-one-out decoupling argument tailored to the
policy evaluation problem, one which may be of independent interest.","cs.LG,math.OC,math.PR,math.ST,stat.ML,stat.TH"
"Solving Jigsaw Puzzles By the Graph Connection Laplacian. We propose a novel mathematical framework to address the problem of
automatically solving large jigsaw puzzles. This problem assumes a large image,
which is cut into equal square pieces that are arbitrarily rotated and
shuffled, and asks to recover the original image given the transformed pieces.
The main contribution of this work is a method for recovering the rotations of
the pieces when both shuffles and rotations are unknown. A major challenge of
this procedure is estimating the graph connection Laplacian without the
knowledge of shuffles. A careful combination of our proposed method for
estimating rotations with any existing method for estimating shuffles results
in a practical solution for the jigsaw puzzle problem. Our theory guarantees,
in a clean setting, that our basic idea of recovering rotations is robust to
some corruption of the connection graph. Numerical experiments demonstrate the
competitive accuracy of this solution, its robustness to corruption and, its
computational advantage for large puzzles.","90C20,90C27,90C35,90C90,cs.CV,cs.LG,cs.NA,math.NA"
"On Adversarial Robustness of 3D Point Cloud Classification under Adaptive Attacks. 3D point clouds play pivotal roles in various safety-critical applications,
such as autonomous driving, which desires the underlying deep neural networks
to be robust to adversarial perturbations. Though a few defenses against
adversarial point cloud classification have been proposed, it remains unknown
whether they are truly robust to adaptive attacks. To this end, we perform the
first security analysis of state-of-the-art defenses and design adaptive
evaluations on them. Our 100% adaptive attack success rates show that current
countermeasures are still vulnerable. Since adversarial training (AT) is
believed as the most robust defense, we present the first in-depth study
showing how AT behaves in point cloud classification and identify that the
required symmetric function (pooling operation) is paramount to the 3D model's
robustness under AT. Through our systematic analysis, we find that the
default-used fixed pooling (e.g., MAX pooling) generally weakens AT's
effectiveness in point cloud classification. Interestingly, we further discover
that sorting-based parametric pooling can significantly improve the models'
robustness. Based on above insights, we propose DeepSym, a deep symmetric
pooling operation, to architecturally advance the robustness to 47.0% under AT
without sacrificing nominal accuracy, outperforming the original design and a
strong baseline by 28.5% ($\sim 2.6 \times$) and 6.5%, respectively, in
PointNet.","cs.AI,cs.CR,cs.LG"
"NBDT: Neural-Backed Decision Trees. Machine learning applications such as finance and medicine demand accurate
and justifiable predictions, barring most deep learning methods from use. In
response, previous work combines decision trees with deep learning, yielding
models that (1) sacrifice interpretability for accuracy or (2) sacrifice
accuracy for interpretability. We forgo this dilemma by jointly improving
accuracy and interpretability using Neural-Backed Decision Trees (NBDTs). NBDTs
replace a neural network's final linear layer with a differentiable sequence of
decisions and a surrogate loss. This forces the model to learn high-level
concepts and lessens reliance on highly-uncertain decisions, yielding (1)
accuracy: NBDTs match or outperform modern neural networks on CIFAR, ImageNet
and better generalize to unseen classes by up to 16%. Furthermore, our
surrogate loss improves the original model's accuracy by up to 2%. NBDTs also
afford (2) interpretability: improving human trustby clearly identifying model
mistakes and assisting in dataset debugging. Code and pretrained NBDTs are at
https://github.com/alvinwan/neural-backed-decision-trees.","cs.CV,cs.LG,cs.NE"
"Towards a universal neural network encoder for time series. We study the use of a time series encoder to learn representations that are
useful on data set types with which it has not been trained on. The encoder is
formed of a convolutional neural network whose temporal output is summarized by
a convolutional attention mechanism. This way, we obtain a compact,
fixed-length representation from longer, variable-length time series. We
evaluate the performance of the proposed approach on a well-known time series
classification benchmark, considering full adaptation, partial adaptation, and
no adaptation of the encoder to the new data type. Results show that such
strategies are competitive with the state-of-the-art, often outperforming
conceptually-matching approaches. Besides accuracy scores, the facility of
adaptation and the efficiency of pre-trained encoders make them an appealing
option for the processing of scarcely- or non-labeled time series.","cs.LG,cs.NE,stat.ML"
"Modular Deep Reinforcement Learning with Temporal Logic Specifications. We propose an actor-critic, model-free, and online Reinforcement Learning
(RL) framework for continuous-state continuous-action Markov Decision Processes
(MDPs) when the reward is highly sparse but encompasses a high-level temporal
structure. We represent this temporal structure by a finite-state machine and
construct an on-the-fly synchronised product with the MDP and the finite
machine. The temporal structure acts as a guide for the RL agent within the
product, where a modular Deep Deterministic Policy Gradient (DDPG) architecture
is proposed to generate a low-level control policy. We evaluate our framework
in a Mars rover experiment and we present the success rate of the synthesised
policy.","cs.AI,cs.LG,cs.LO,cs.SY,eess.SY,stat.ML"
"Panoptic Segmentation of Satellite Image Time Series with Convolutional Temporal Attention Networks. Unprecedented access to multi-temporal satellite imagery has opened new
perspectives for a variety of Earth observation tasks. Among them,
pixel-precise panoptic segmentation of agricultural parcels has major economic
and environmental implications. While researchers have explored this problem
for single images, we argue that the complex temporal patterns of crop
phenology are better addressed with temporal sequences of images. In this
paper, we present the first end-to-end, single-stage method for panoptic
segmentation of Satellite Image Time Series (SITS). This module can be combined
with our novel image sequence encoding network which relies on temporal
self-attention to extract rich and adaptive multi-scale spatio-temporal
features. We also introduce PASTIS, the first open-access SITS dataset with
panoptic annotations. We demonstrate the superiority of our encoder for
semantic segmentation against multiple competing architectures, and set up the
first state-of-the-art of panoptic segmentation of SITS. Our implementation and
PASTIS are publicly available.","68T07,68T45,I.4.6; I.2.6; J.2,cs.CV"
"e-ViL: A Dataset and Benchmark for Natural Language Explanations in Vision-Language Tasks. Recently, there has been an increasing number of efforts to introduce models
capable of generating natural language explanations (NLEs) for their
predictions on vision-language (VL) tasks. Such models are appealing, because
they can provide human-friendly and comprehensive explanations. However, there
is a lack of comparison between existing methods, which is due to a lack of
re-usable evaluation frameworks and a scarcity of datasets. In this work, we
introduce e-ViL and e-SNLI-VE. e-ViL is a benchmark for explainable
vision-language tasks that establishes a unified evaluation framework and
provides the first comprehensive comparison of existing approaches that
generate NLEs for VL tasks. It spans four models and three datasets and both
automatic metrics and human evaluation are used to assess model-generated
explanations. e-SNLI-VE is currently the largest existing VL dataset with NLEs
(over 430k instances). We also propose a new model that combines UNITER, which
learns joint embeddings of images and text, and GPT-2, a pre-trained language
model that is well-suited for text generation. It surpasses the previous state
of the art by a large margin across all datasets. Code and data are available
here: https://github.com/maximek3/e-ViL.","cs.CL,cs.CV,cs.LG"
"Multi-task Maximum Entropy Inverse Reinforcement Learning. Multi-task Inverse Reinforcement Learning (IRL) is the problem of inferring
multiple reward functions from expert demonstrations. Prior work, built on
Bayesian IRL, is unable to scale to complex environments due to computational
constraints. This paper contributes a formulation of multi-task IRL in the more
computationally efficient Maximum Causal Entropy (MCE) IRL framework.
Experiments show our approach can perform one-shot imitation learning in a
gridworld environment that single-task IRL algorithms need hundreds of
demonstrations to solve. We outline preliminary work using meta-learning to
extend our method to the function approximator setting of modern MCE IRL
algorithms. Evaluating on multi-task variants of common simulated robotics
benchmarks, we discover serious limitations of these IRL algorithms, and
conclude with suggestions for further work.","I.2.6,cs.AI,cs.LG,stat.ML"
"Learning Dynamic Graph Representation of Brain Connectome with Spatio-Temporal Attention. Functional connectivity (FC) between regions of the brain can be assessed by
the degree of temporal correlation measured with functional neuroimaging
modalities. Based on the fact that these connectivities build a network,
graph-based approaches for analyzing the brain connectome have provided
insights into the functions of the human brain. The development of graph neural
networks (GNNs) capable of learning representation from graph structured data
has led to increased interest in learning the graph representation of the brain
connectome. Although recent attempts to apply GNN to the FC network have shown
promising results, there is still a common limitation that they usually do not
incorporate the dynamic characteristics of the FC network which fluctuates over
time. In addition, a few studies that have attempted to use dynamic FC as an
input for the GNN reported a reduction in performance compared to static FC
methods, and did not provide temporal explainability. Here, we propose STAGIN,
a method for learning dynamic graph representation of the brain connectome with
spatio-temporal attention. Specifically, a temporal sequence of brain graphs is
input to the STAGIN to obtain the dynamic graph representation, while novel
READOUT functions and the Transformer encoder provide spatial and temporal
explainability with attention, respectively. Experiments on the HCP-Rest and
the HCP-Task datasets demonstrate exceptional performance of our proposed
method. Analysis of the spatio-temporal attention also provide concurrent
interpretation with the neuroscientific knowledge, which further validates our
method. Code is available at https://github.com/egyptdj/stagin","cs.CV,cs.LG,q-bio.NC"
"A Progressive Sub-Network Searching Framework for Dynamic Inference. Many techniques have been developed, such as model compression, to make Deep
Neural Networks (DNNs) inference more efficiently. Nevertheless, DNNs still
lack excellent run-time dynamic inference capability to enable users trade-off
accuracy and computation complexity (i.e., latency on target hardware) after
model deployment, based on dynamic requirements and environments. Such research
direction recently draws great attention, where one realization is to train the
target DNN through a multiple-term objective function, which consists of
cross-entropy terms from multiple sub-nets. Our investigation in this work show
that the performance of dynamic inference highly relies on the quality of
sub-net sampling. With objective to construct a dynamic DNN and search multiple
high quality sub-nets with minimal searching cost, we propose a progressive
sub-net searching framework, which is embedded with several effective
techniques, including trainable noise ranking, channel group and fine-tuning
threshold setting, sub-nets re-selection. The proposed framework empowers the
target DNN with better dynamic inference capability, which outperforms prior
works on both CIFAR-10 and ImageNet dataset via comprehensive experiments on
different network structures. Taken ResNet18 as an example, our proposed method
achieves much better dynamic inference accuracy compared with prior popular
Universally-Slimmable-Network by 4.4%-maximally and 2.3%-averagely in ImageNet
dataset with the same model size.",cs.CV
"Low Dimensional Embedding of fMRI datasets. We propose a novel method to embed a functional magnetic resonance imaging
(fMRI) dataset in a low-dimensional space. The embedding optimally preserves
the local functional coupling between fMRI time series and provides a
low-dimensional coordinate system for detecting activated voxels. To compute
the embedding, we build a graph of functionally connected voxels. We use the
commute time, instead of the geodesic distance, to measure functional distances
on the graph. Because the commute time can be computed directly from the
eigenvectors of (a symmetric version) the graph probability transition matrix,
we use these eigenvectors to embed the dataset in low dimensions. After
clustering the datasets in low dimensions, coherent structures emerge that can
be easily interpreted. We performed an extensive evaluation of our method
comparing it to linear and nonlinear techniques using synthetic datasets and in
vivo datasets. We analyzed datasets from the EBC competition obtained with
subjects interacting in an urban virtual reality environment. Our exploratory
approach is able to detect independently visual areas (V1/V2, V5/MT), auditory
areas, and language areas. Our method can be used to analyze fMRI collected
during ``natural stimuli''.","q-bio.NC,stat.AP,stat.ML"
"Fair Marriage Principle and Initialization Map for the EM Algorithm. The popular convergence theory of the EM algorithm explains that the observed
incomplete data log-likelihood L and the complete data log-likelihood Q are
positively correlated, and we can maximize L by maximizing Q. The Deterministic
Annealing EM (DAEM) algorithm was hence proposed for avoiding locally maximal
Q. This paper provides different conclusions: 1) The popular convergence theory
is wrong; 2) The locally maximal Q can affect the convergent speed, but cannot
block the global convergence; 3) Like marriage competition, unfair competition
between two components may vastly decrease the globally convergent speed; 4)
Local convergence exists because the sample is too small, and unfair
competition exists; 5) An improved EM algorithm, called the Channel Matching
(CM) EM algorithm, can accelerate the global convergence. This paper provides
an initialization map with two means as two axes for the example of a binary
Gaussian mixture studied by the authors of DAEM algorithm. This map can tell
how fast the convergent speeds are for different initial means and why points
in some areas are not suitable as initial points. A two-dimensional example
indicates that the big sample or the fair initialization can avoid global
convergence. For more complicated mixture models, we need further study to
convert the fair marriage principle to specific methods for the
initializations.","49N30,68T05,93C41,94A17,H.3.3; G.3; I.1.2; I.5.3,cs.LG,stat.ML"
"Attentional Graph Convolutional Networks for Knowledge Concept Recommendation in MOOCs in a Heterogeneous View. Massive open online courses are becoming a modish way for education, which
provides a large-scale and open-access learning opportunity for students to
grasp the knowledge. To attract students' interest, the recommendation system
is applied by MOOCs providers to recommend courses to students. However, as a
course usually consists of a number of video lectures, with each one covering
some specific knowledge concepts, directly recommending courses overlook
students'interest to some specific knowledge concepts. To fill this gap, in
this paper, we study the problem of knowledge concept recommendation. We
propose an end-to-end graph neural network-based approach
calledAttentionalHeterogeneous Graph Convolutional Deep Knowledge
Recommender(ACKRec) for knowledge concept recommendation in MOOCs. Like other
recommendation problems, it suffers from sparsity issues. To address this
issue, we leverage both content information and context information to learn
the representation of entities via graph convolution network. In addition to
students and knowledge concepts, we consider other types of entities (e.g.,
courses, videos, teachers) and construct a heterogeneous information network to
capture the corresponding fruitful semantic relationships among different types
of entities and incorporate them into the representation learning process.
Specifically, we use meta-path on the HIN to guide the propagation of students'
preferences. With the help of these meta-paths, the students' preference
distribution with respect to a candidate knowledge concept can be captured.
Furthermore, we propose an attention mechanism to adaptively fuse the context
information from different meta-paths, in order to capture the different
interests of different students. The promising experiment results show that the
proposedACKRecis able to effectively recommend knowledge concepts to students
pursuing online learning in MOOCs.","cs.CY,cs.IR,cs.LG,stat.ML"
"Toward Universal Testing of Dynamic Network Models. Numerous networks in the real world change over time, in the sense that nodes
and edges enter and leave the networks. Various dynamic random graph models
have been proposed to explain the macroscopic properties of these systems and
to provide a foundation for statistical inferences and predictions. It is of
interest to have a rigorous way to determine how well these models match
observed networks. We thus ask the following goodness of fit question: given a
sequence of observations/snapshots of a growing random graph, along with a
candidate model M, can we determine whether the snapshots came from M or from
some arbitrary alternative model that is well-separated from M in some natural
metric? We formulate this problem precisely and boil it down to goodness of fit
testing for graph-valued, infinite-state Markov processes and exhibit and
analyze a universal test based on non-stationary sampling for a natural class
of models.","cs.LG,cs.SI,math.PR,stat.ML"
"Random deep neural networks are biased towards simple functions. We prove that the binary classifiers of bit strings generated by random wide
deep neural networks with ReLU activation function are biased towards simple
functions. The simplicity is captured by the following two properties. For any
given input bit string, the average Hamming distance of the closest input bit
string with a different classification is at least sqrt(n / (2{\pi} log n)),
where n is the length of the string. Moreover, if the bits of the initial
string are flipped randomly, the average number of flips required to change the
classification grows linearly with n. These results are confirmed by numerical
experiments on deep neural networks with two hidden layers, and settle the
conjecture stating that random deep neural networks are biased towards simple
functions. This conjecture was proposed and numerically explored in [Valle
P\'erez et al., ICLR 2019] to explain the unreasonably good generalization
properties of deep learning algorithms. The probability distribution of the
functions generated by random deep neural networks is a good choice for the
prior probability distribution in the PAC-Bayesian generalization bounds. Our
results constitute a fundamental step forward in the characterization of this
distribution, therefore contributing to the understanding of the generalization
properties of deep learning algorithms.","cond-mat.dis-nn,cs.LG,math-ph,math.MP,quant-ph,stat.ML"
"Fooling Detection Alone is Not Enough: First Adversarial Attack against Multiple Object Tracking. Recent work in adversarial machine learning started to focus on the visual
perception in autonomous driving and studied Adversarial Examples (AEs) for
object detection models. However, in such visual perception pipeline the
detected objects must also be tracked, in a process called Multiple Object
Tracking (MOT), to build the moving trajectories of surrounding obstacles.
Since MOT is designed to be robust against errors in object detection, it poses
a general challenge to existing attack techniques that blindly target objection
detection: we find that a success rate of over 98% is needed for them to
actually affect the tracking results, a requirement that no existing attack
technique can satisfy. In this paper, we are the first to study adversarial
machine learning attacks against the complete visual perception pipeline in
autonomous driving, and discover a novel attack technique, tracker hijacking,
that can effectively fool MOT using AEs on object detection. Using our
technique, successful AEs on as few as one single frame can move an existing
object in to or out of the headway of an autonomous vehicle to cause potential
safety hazards. We perform evaluation using the Berkeley Deep Drive dataset and
find that on average when 3 frames are attacked, our attack can have a nearly
100% success rate while attacks that blindly target object detection only have
up to 25%.","cs.CR,cs.CV"
"Thief, Beware of What Get You There: Towards Understanding Model Extraction Attack. Model extraction increasingly attracts research attentions as keeping
commercial AI models private can retain a competitive advantage. In some
scenarios, AI models are trained proprietarily, where neither pre-trained
models nor sufficient in-distribution data is publicly available. Model
extraction attacks against these models are typically more devastating.
Therefore, in this paper, we empirically investigate the behaviors of model
extraction under such scenarios. We find the effectiveness of existing
techniques significantly affected by the absence of pre-trained models. In
addition, the impacts of the attacker's hyperparameters, e.g. model
architecture and optimizer, as well as the utilities of information retrieved
from queries, are counterintuitive. We provide some insights on explaining the
possible causes of these phenomena. With these observations, we formulate model
extraction attacks into an adaptive framework that captures these factors with
deep reinforcement learning. Experiments show that the proposed framework can
be used to improve existing techniques, and show that model extraction is still
possible in such strict scenarios. Our research can help system designers to
construct better defense strategies based on their scenarios.","cs.CR,cs.CV,cs.LG"
"When Do Extended Physics-Informed Neural Networks (XPINNs) Improve Generalization?. Physics-informed neural networks (PINNs) have become a popular choice for
solving high-dimensional partial differential equations (PDEs) due to their
excellent approximation power and generalization ability. Recently, Extended
PINNs (XPINNs) based on domain decomposition methods have attracted
considerable attention due to their effectiveness in modeling multiscale and
multiphysics problems and their parallelization. However, theoretical
understanding on their convergence and generalization properties remains
unexplored. In this study, we take an initial step towards understanding how
and when XPINNs outperform PINNs. Specifically, for general multi-layer PINNs
and XPINNs, we first provide a prior generalization bound via the complexity of
the target functions in the PDE problem, and a posterior generalization bound
via the posterior matrix norms of the networks after optimization. Moreover,
based on our bounds, we analyze the conditions under which XPINNs improve
generalization. Concretely, our theory shows that the key building block of
XPINN, namely the domain decomposition, introduces a tradeoff for
generalization. On the one hand, XPINNs decompose the complex PDE solution into
several simple parts, which decreases the complexity needed to learn each part
and boosts generalization. On the other hand, decomposition leads to less
training data being available in each subdomain, and hence such model is
typically prone to overfitting and may become less generalizable. Empirically,
we choose five PDEs to show when XPINNs perform better than, similar to, or
worse than PINNs, hence demonstrating and justifying our new theory.","cs.LG,cs.NA,math.DS,math.NA,stat.ML"
"Mask Combination of Multi-layer Graphs for Global Structure Inference. Structure inference is an important task for network data processing and
analysis in data science. In recent years, quite a few approaches have been
developed to learn the graph structure underlying a set of observations
captured in a data space. Although real-world data is often acquired in
settings where relationships are influenced by a priori known rules, such
domain knowledge is still not well exploited in structure inference problems.
In this paper, we identify the structure of signals defined in a data space
whose inner relationships are encoded by multi-layer graphs. We aim at properly
exploiting the information originating from each layer to infer the global
structure underlying the signals. We thus present a novel method for combining
the multiple graphs into a global graph using mask matrices, which are
estimated through an optimization problem that accommodates the multi-layer
graph information and a signal representation model. The proposed mask
combination method also estimates the contribution of each graph layer in the
structure of signals. The experiments conducted both on synthetic and
real-world data suggest that integrating the multi-layer graph representation
of the data in the structure inference framework enhances the learning
procedure considerably by adapting to the quality and the quantity of the input
data.","cs.LG,eess.SP,stat.ML"
"Power-SLIC: Diagram-based superpixel generation. Superpixel algorithms, which group pixels similar in color and other
low-level properties, are increasingly used for pre-processing in image
segmentation. Commonly important criteria for the computation of superpixels
are boundary adherence, speed, and regularity.
  Boundary adherence and regularity are typically contradictory goals. Most
recent algorithms have focused on improving boundary adherence. Motivated by
improving superpixel regularity, we propose a diagram-based superpixel
generation method called Power-SLIC.
  On the BSDS500 data set, Power-SLIC outperforms other state-of-the-art
algorithms in terms of compactness and boundary precision, and its boundary
adherence is the most robust against varying levels of Gaussian noise. In terms
of speed, Power-SLIC is competitive with SLIC.","cs.CG,cs.CV,cs.LG"
"Structured Diversification Emergence via Reinforced Organization Control and Hierarchical Consensus Learning. When solving a complex task, humans will spontaneously form teams and to
complete different parts of the whole task, respectively. Meanwhile, the
cooperation between teammates will improve efficiency. However, for current
cooperative MARL methods, the cooperation team is constructed through either
heuristics or end-to-end blackbox optimization. In order to improve the
efficiency of cooperation and exploration, we propose a structured
diversification emergence MARL framework named {\sc{Rochico}} based on
reinforced organization control and hierarchical consensus learning.
{\sc{Rochico}} first learns an adaptive grouping policy through the
organization control module, which is established by independent multi-agent
reinforcement learning. Further, the hierarchical consensus module based on the
hierarchical intentions with consensus constraint is introduced after team
formation. Simultaneously, utilizing the hierarchical consensus module and a
self-supervised intrinsic reward enhanced decision module, the proposed
cooperative MARL algorithm {\sc{Rochico}} can output the final diversified
multi-agent cooperative policy. All three modules are organically combined to
promote the structured diversification emergence. Comparative experiments on
four large-scale cooperation tasks show that {\sc{Rochico}} is significantly
better than the current SOTA algorithms in terms of exploration efficiency and
cooperation strength.","cs.AI,cs.GT,cs.LG,cs.MA"
"Exathlon: A Benchmark for Explainable Anomaly Detection over Time Series. Access to high-quality data repositories and benchmarks have been
instrumental in advancing the state of the art in many experimental research
domains. While advanced analytics tasks over time series data have been gaining
lots of attention, lack of such community resources severely limits scientific
progress. In this paper, we present Exathlon, the first comprehensive public
benchmark for explainable anomaly detection over high-dimensional time series
data. Exathlon has been systematically constructed based on real data traces
from repeated executions of large-scale stream processing jobs on an Apache
Spark cluster. Some of these executions were intentionally disturbed by
introducing instances of six different types of anomalous events (e.g.,
misbehaving inputs, resource contention, process failures). For each of the
anomaly instances, ground truth labels for the root cause interval as well as
those for the extended effect interval are provided, supporting the development
and evaluation of a wide range of anomaly detection (AD) and explanation
discovery (ED) tasks. We demonstrate the practical utility of Exathlon's
dataset, evaluation methodology, and end-to-end data science pipeline design
through an experimental study with three state-of-the-art AD and ED techniques.","cs.DB,cs.LG"
"On distances, paths and connections for hyperspectral image segmentation. The present paper introduces the $\eta$ and {\eta} connections in order to
add regional information on $\lambda$-flat zones, which only take into account
a local information. A top-down approach is considered. First $\lambda$-flat
zones are built in a way leading to a sub-segmentation. Then a finer
segmentation is obtained by computing $\eta$-bounded regions and $\mu$-geodesic
balls inside the $\lambda$-flat zones. The proposed algorithms for the
construction of new partitions are based on queues with an ordered selection of
seeds using the cumulative distance. $\eta$-bounded regions offers a control on
the variations of amplitude in the class from a point, called center, and
$\mu$-geodesic balls controls the ""size"" of the class. These results are
applied to hyperspectral images.","cs.CV,math.NA"
"Trade When Opportunity Comes: Price Movement Forecasting via Locality-Aware Attention and Adaptive Refined Labeling. Price movement forecasting aims at predicting the future trends of financial
assets based on the current market conditions and other relevant information.
Recently, machine learning(ML) methods have become increasingly popular and
achieved promising results for price movement forecasting in both academia and
industry. Most existing ML solutions formulate the forecasting problem as a
classification(to predict the direction) or a regression(to predict the return)
problem in the entire set of training data. However, due to the extremely low
signal-to-noise ratio and stochastic nature of financial data, good trading
opportunities are extremely scarce. As a result, without careful selection of
potentially profitable samples, such ML methods are prone to capture the
patterns of noises instead of real signals. To address the above issues, we
propose a novel framework-LARA(Locality-Aware Attention and Adaptive Refined
Labeling), which contains the following three components: 1)Locality-aware
attention automatically extracts the potentially profitable samples by
attending to their label information in order to construct a more accurate
classifier on these selected samples. 2)Adaptive refined labeling further
iteratively refines the labels, alleviating the noise of samples. 3)Equipped
with metric learning techniques, Locality-aware attention enjoys task-specific
distance metrics and distributes attention on potentially profitable samples in
a more effective way. To validate our method, we conduct comprehensive
experiments on three real-world financial markets: ETFs, the China's A-share
stock market, and the cryptocurrency market. LARA achieves superior performance
compared with the time-series analysis methods and a set of machine learning
based competitors on the Qlib platform. Extensive ablation studies and
experiments demonstrate that LARA indeed captures more reliable trading
opportunities.","cs.AI,cs.CE,cs.LG,q-fin.ST"
"Disparity Image Segmentation For ADAS. We present a simple solution for segmenting grayscale images using existing
Connected Component Labeling (CCL) algorithms (which are generally applied to
binary images), which was efficient enough to be implemented in a constrained
(embedded automotive) architecture. Our solution customizes the region growing
and merging approach, and is primarily targeted for stereoscopic disparity
images where nearer objects carry more relevance. We provide results from a
standard OpenCV implementation for some basic cases and an image from the
Tsukuba stereo-pair dataset.","68U10,68W99,I.4.6; I.4.5,cs.CV"
"An Experimental Evaluation of Large Scale GBDT Systems. Gradient boosting decision tree (GBDT) is a widely-used machine learning
algorithm in both data analytic competitions and real-world industrial
applications. Further, driven by the rapid increase in data volume, efforts
have been made to train GBDT in a distributed setting to support large-scale
workloads. However, we find it surprising that the existing systems manage the
training dataset in different ways, but none of them have studied the impact of
data management. To that end, this paper aims to study the pros and cons of
different data management methods regarding the performance of distributed
GBDT. We first introduce a quadrant categorization of data management policies
based on data partitioning and data storage. Then we conduct an in-depth
systematic analysis and summarize the advantageous scenarios of the quadrants.
Based on the analysis, we further propose a novel distributed GBDT system named
Vero, which adopts the unexplored composition of vertical partitioning and
row-store and suits for many large-scale cases. To validate our analysis
empirically, we implement different quadrants in the same code base and compare
them under extensive workloads, and finally compare Vero with other
state-of-the-art systems over a wide range of datasets. Our theoretical and
experimental results provide a guideline on choosing a proper data management
policy for a given workload.","cs.DB,cs.LG,stat.ML"
"Generating Realistic Geology Conditioned on Physical Measurements with Generative Adversarial Networks. An important problem in geostatistics is to build models of the subsurface of
the Earth given physical measurements at sparse spatial locations. Typically,
this is done using spatial interpolation methods or by reproducing patterns
from a reference image. However, these algorithms fail to produce realistic
patterns and do not exhibit the wide range of uncertainty inherent in the
prediction of geology. In this paper, we show how semantic inpainting with
Generative Adversarial Networks can be used to generate varied realizations of
geology which honor physical measurements while matching the expected
geological patterns. In contrast to other algorithms, our method scales well
with the number of data points and mimics a distribution of patterns as opposed
to a single pattern or image. The generated conditional samples are state of
the art.","physics.comp-ph,physics.geo-ph,stat.ML"
"Improved Deep Point Cloud Geometry Compression. Point clouds have been recognized as a crucial data structure for 3D content
and are essential in a number of applications such as virtual and mixed
reality, autonomous driving, cultural heritage, etc. In this paper, we propose
a set of contributions to improve deep point cloud compression, i.e.: using a
scale hyperprior model for entropy coding; employing deeper transforms; a
different balancing weight in the focal loss; optimal thresholding for
decoding; and sequential model training. In addition, we present an extensive
ablation study on the impact of each of these factors, in order to provide a
better understanding about why they improve RD performance. An optimal
combination of the proposed improvements achieves BD-PSNR gains over G-PCC
trisoup and octree of 5.50 (6.48) dB and 6.84 (5.95) dB, respectively, when
using the point-to-point (point-to-plane) metric. Code is available at
https://github.com/mauriceqch/pcc_geo_cnn_v2 .","cs.CV,cs.LG,eess.IV,eess.SP,stat.ML"
"Parallel and Flexible Sampling from Autoregressive Models via Langevin Dynamics. This paper introduces an alternative approach to sampling from autoregressive
models. Autoregressive models are typically sampled sequentially, according to
the transition dynamics defined by the model. Instead, we propose a sampling
procedure that initializes a sequence with white noise and follows a Markov
chain defined by Langevin dynamics on the global log-likelihood of the
sequence. This approach parallelizes the sampling process and generalizes to
conditional sampling. Using an autoregressive model as a Bayesian prior, we can
steer the output of a generative model using a conditional likelihood or
constraints. We apply these techniques to autoregressive models in the visual
and audio domains, with competitive results for audio source separation,
super-resolution, and inpainting.","cs.LG,cs.SD,eess.AS,stat.ML"
"Q-learning with Uniformly Bounded Variance: Large Discounting is Not a Barrier to Fast Learning. Sample complexity bounds are a common performance metric in the Reinforcement
Learning literature. In the discounted cost, infinite horizon setting, all of
the known bounds have a factor that is a polynomial in $1/(1-\gamma)$, where
$\gamma < 1$ is the discount factor. For a large discount factor, these bounds
seem to imply that a very large number of samples is required to achieve an
$\varepsilon$-optimal policy. The objective of the present work is to introduce
a new class of algorithms that have sample complexity uniformly bounded for all
$\gamma < 1$. One may argue that this is impossible, due to a recent min-max
lower bound. The explanation is that this previous lower bound is for a
specific problem, which we modify, without compromising the ultimate objective
of obtaining an $\varepsilon$-optimal policy. Specifically, we show that the
asymptotic covariance of the Q-learning algorithm with an optimized step-size
sequence is a quadratic function of $1/(1-\gamma)$; an expected, and
essentially known result. The new relative Q-learning algorithm proposed here
is shown to have asymptotic covariance that is a quadratic in $1/(1- \rho^*
\gamma)$, where $1 - \rho^* > 0$ is an upper bound on the spectral gap of an
optimal transition matrix.","cs.LG,cs.SY,eess.SY,stat.ML"
"Sparse Graphical Memory for Robust Planning. To operate effectively in the real world, agents should be able to act from
high-dimensional raw sensory input such as images and achieve diverse goals
across long time-horizons. Current deep reinforcement and imitation learning
methods can learn directly from high-dimensional inputs but do not scale well
to long-horizon tasks. In contrast, classical graphical methods like A* search
are able to solve long-horizon tasks, but assume that the state space is
abstracted away from raw sensory input. Recent works have attempted to combine
the strengths of deep learning and classical planning; however, dominant
methods in this domain are still quite brittle and scale poorly with the size
of the environment. We introduce Sparse Graphical Memory (SGM), a new data
structure that stores states and feasible transitions in a sparse memory. SGM
aggregates states according to a novel two-way consistency objective, adapting
classic state aggregation criteria to goal-conditioned RL: two states are
redundant when they are interchangeable both as goals and as starting states.
Theoretically, we prove that merging nodes according to two-way consistency
leads to an increase in shortest path lengths that scales only linearly with
the merging threshold. Experimentally, we show that SGM significantly
outperforms current state of the art methods on long horizon, sparse-reward
visual navigation tasks. Project video and code are available at
https://mishalaskin.github.io/sgm/","cs.AI,cs.CV,cs.LG,cs.RO,stat.ML"
"Wasserstein Dictionary Learning: Optimal Transport-based unsupervised non-linear dictionary learning. This paper introduces a new nonlinear dictionary learning method for
histograms in the probability simplex. The method leverages optimal transport
theory, in the sense that our aim is to reconstruct histograms using so-called
displacement interpolations (a.k.a. Wasserstein barycenters) between dictionary
atoms; such atoms are themselves synthetic histograms in the probability
simplex. Our method simultaneously estimates such atoms, and, for each
datapoint, the vector of weights that can optimally reconstruct it as an
optimal transport barycenter of such atoms. Our method is computationally
tractable thanks to the addition of an entropic regularization to the usual
optimal transportation problem, leading to an approximation scheme that is
efficient, parallel and simple to differentiate. Both atoms and weights are
learned using a gradient-based descent method. Gradients are obtained by
automatic differentiation of the generalized Sinkhorn iterations that yield
barycenters with entropic smoothing. Because of its formulation relying on
Wasserstein barycenters instead of the usual matrix product between dictionary
and codes, our method allows for nonlinear relationships between atoms and the
reconstruction of input data. We illustrate its application in several
different image processing settings.","cs.GR,math.OC,stat.ML"
"A Latent Source Model for Nonparametric Time Series Classification. For classifying time series, a nearest-neighbor approach is widely used in
practice with performance often competitive with or better than more elaborate
methods such as neural networks, decision trees, and support vector machines.
We develop theoretical justification for the effectiveness of
nearest-neighbor-like classification of time series. Our guiding hypothesis is
that in many applications, such as forecasting which topics will become trends
on Twitter, there aren't actually that many prototypical time series to begin
with, relative to the number of time series we have access to, e.g., topics
become trends on Twitter only in a few distinct manners whereas we can collect
massive amounts of Twitter data. To operationalize this hypothesis, we propose
a latent source model for time series, which naturally leads to a ""weighted
majority voting"" classification rule that can be approximated by a
nearest-neighbor classifier. We establish nonasymptotic performance guarantees
of both weighted majority voting and nearest-neighbor classification under our
model accounting for how much of the time series we observe and the model
complexity. Experimental results on synthetic data show weighted majority
voting achieving the same misclassification rate as nearest-neighbor
classification while observing less of the time series. We then use weighted
majority to forecast which news topics on Twitter become trends, where we are
able to detect such ""trending topics"" in advance of Twitter 79% of the time,
with a mean early advantage of 1 hour and 26 minutes, a true positive rate of
95%, and a false positive rate of 4%.","cs.LG,cs.SI,stat.ML"
"High-Resolution Image Harmonization via Collaborative Dual Transformations. Given a composite image, image harmonization aims to adjust the foreground to
make it compatible with the background. High-resolution image harmonization is
in high demand, but still remains unexplored. Conventional image harmonization
methods learn global RGB-to-RGB transformation which could effortlessly scale
to high resolution, but ignore diverse local context. Recent deep learning
methods learn the dense pixel-to-pixel transformation which could generate
harmonious outputs, but are highly constrained in low resolution. In this work,
we propose a high-resolution image harmonization network with Collaborative
Dual Transformation (CDTNet) to combine pixel-to-pixel transformation and
RGB-to-RGB transformation coherently in an end-to-end framework. Our CDTNet
consists of a low-resolution generator for pixel-to-pixel transformation, a
color mapping module for RGB-to-RGB transformation, and a refinement module to
take advantage of both. Extensive experiments on high-resolution image
harmonization dataset demonstrate that our CDTNet strikes a good balance
between efficiency and effectiveness.",cs.CV
"Why Do Adversarial Attacks Transfer? Explaining Transferability of Evasion and Poisoning Attacks. Transferability captures the ability of an attack against a machine-learning
model to be effective against a different, potentially unknown, model.
Empirical evidence for transferability has been shown in previous work, but the
underlying reasons why an attack transfers or not are not yet well understood.
In this paper, we present a comprehensive analysis aimed to investigate the
transferability of both test-time evasion and training-time poisoning attacks.
We provide a unifying optimization framework for evasion and poisoning attacks,
and a formal definition of transferability of such attacks. We highlight two
main factors contributing to attack transferability: the intrinsic adversarial
vulnerability of the target model, and the complexity of the surrogate model
used to optimize the attack. Based on these insights, we define three metrics
that impact an attack's transferability. Interestingly, our results derived
from theoretical analysis hold for both evasion and poisoning attacks, and are
confirmed experimentally using a wide range of linear and non-linear
classifiers and datasets.","68T10,68T45,cs.CR,cs.LG,stat.ML"
"Necessary and sufficient graphical conditions for optimal adjustment sets in causal graphical models with hidden variables. The problem of selecting optimal valid backdoor adjustment sets to estimate
causal effects in graphical models with hidden and conditioned variables is
addressed. Previous work has defined optimality as achieving the smallest
asymptotic variance compared to other adjustment sets and identified a
graphical criterion for an optimal set for the case without hidden variables.
For the case with hidden variables currently a sufficient graphical criterion
and a corresponding construction algorithm exists. Here optimality is
characterized by an information-theoretic approach based on the conditional
mutual informations among cause, effect, adjustment set, and conditioned
variables. This characterization allows to derive the main contributions of
this paper: A necessary and sufficient graphical criterion for the existence of
an optimal adjustment set and a definition and algorithm to construct it.
Further, the optimal set is valid if and only if a valid adjustment set exists
and has smaller (or equal) asymptotic variance compared to the Adjust-set
proposed in Perkovic et al. (2018) (arXiv:1606.06903) for any graph, whether
graphical optimality holds or not. The results are valid for a class of
estimators whose asymptotic variance follows a certain information-theoretic
relation. Numerical experiments indicate that the asymptotic results also hold
for relatively small sample sizes. For estimators outside of the class studied
here none of the considered adjustment sets outperforms all others, but a
minimized variant of the optimal set proposed here tends to have lower
variance. Surprisingly, among the randomly created setups more than 80\%
fulfill the optimality conditions indicating that also in many real-world
scenarios graphical optimality may hold. Code is available as part of the
python package \url{https://github.com/jakobrunge/tigramite}.","cs.IT,cs.LG,math.IT,math.ST,stat.TH"
"Spatio-Temporal Facial Expression Recognition Using Convolutional Neural Networks and Conditional Random Fields. Automated Facial Expression Recognition (FER) has been a challenging task for
decades. Many of the existing works use hand-crafted features such as LBP, HOG,
LPQ, and Histogram of Optical Flow (HOF) combined with classifiers such as
Support Vector Machines for expression recognition. These methods often require
rigorous hyperparameter tuning to achieve good results. Recently Deep Neural
Networks (DNN) have shown to outperform traditional methods in visual object
recognition. In this paper, we propose a two-part network consisting of a
DNN-based architecture followed by a Conditional Random Field (CRF) module for
facial expression recognition in videos. The first part captures the spatial
relation within facial images using convolutional layers followed by three
Inception-ResNet modules and two fully-connected layers. To capture the
temporal relation between the image frames, we use linear chain CRF in the
second part of our network. We evaluate our proposed network on three publicly
available databases, viz. CK+, MMI, and FERA. Experiments are performed in
subject-independent and cross-database manners. Our experimental results show
that cascading the deep network architecture with the CRF module considerably
increases the recognition of facial expressions in videos and in particular it
outperforms the state-of-the-art methods in the cross-database experiments and
yields comparable results in the subject-independent experiments.",cs.CV
"Handwritten Digit Recognition with a Committee of Deep Neural Nets on GPUs. The competitive MNIST handwritten digit recognition benchmark has a long
history of broken records since 1998. The most recent substantial improvement
by others dates back 7 years (error rate 0.4%) . Recently we were able to
significantly improve this result, using graphics cards to greatly speed up
training of simple but deep MLPs, which achieved 0.35%, outperforming all the
previous more complex methods. Here we report another substantial improvement:
0.31% obtained using a committee of MLPs.","cs.AI,cs.CV,cs.LG,cs.NE"
"RL-Scope: Cross-Stack Profiling for Deep Reinforcement Learning Workloads. Deep reinforcement learning (RL) has made groundbreaking advancements in
robotics, data center management and other applications. Unfortunately,
system-level bottlenecks in RL workloads are poorly understood; we observe
fundamental structural differences in RL workloads that make them inherently
less GPU-bound than supervised learning (SL). To explain where training time is
spent in RL workloads, we propose RL-Scope, a cross-stack profiler that scopes
low-level CPU/GPU resource usage to high-level algorithmic operations, and
provides accurate insights by correcting for profiling overhead. Using
RL-Scope, we survey RL workloads across its major dimensions including ML
backend, RL algorithm, and simulator. For ML backends, we explain a $2.3\times$
difference in runtime between equivalent PyTorch and TensorFlow algorithm
implementations, and identify a bottleneck rooted in overly abstracted
algorithm implementations. For RL algorithms and simulators, we show that
on-policy algorithms are at least $3.5\times$ more simulation-bound than
off-policy algorithms. Finally, we profile a scale-up workload and demonstrate
that GPU utilization metrics reported by commonly used tools dramatically
inflate GPU usage, whereas RL-Scope reports true GPU-bound time. RL-Scope is an
open-source tool available at https://github.com/UofT-EcoSystem/rlscope .","cs.LG,cs.SE"
"Hybrid Quantum-Classical Neural Network for Incident Detection. The efficiency and reliability of real-time incident detection models
directly impact the affected corridors' traffic safety and operational
conditions. The recent emergence of cloud-based quantum computing
infrastructure and innovations in noisy intermediate-scale quantum devices have
revealed a new era of quantum-enhanced algorithms that can be leveraged to
improve real-time incident detection accuracy. In this research, a hybrid
machine learning model, which includes classical and quantum machine learning
(ML) models, is developed to identify incidents using the connected vehicle
(CV) data. The incident detection performance of the hybrid model is evaluated
against baseline classical ML models. The framework is evaluated using data
from a microsimulation tool for different incident scenarios. The results
indicate that a hybrid neural network containing a 4-qubit quantum layer
outperforms all other baseline models when there is a lack of training data. We
have created three datasets; DS-1 with sufficient training data, and DS-2 and
DS-3 with insufficient training data. The hybrid model achieves a recall of
98.9%, 98.3%, and 96.6% for DS-1, DS-2, and DS-3, respectively. For DS-2 and
DS-3, the average improvement in F2-score (measures model's performance to
correctly identify incidents) achieved by the hybrid model is 1.9% and 7.8%,
respectively, compared to the classical models. It shows that with insufficient
data, which may be common for CVs, the hybrid ML model will perform better than
the classical models. With the continuing improvements of quantum computing
infrastructure, the quantum ML models could be a promising alternative for
CV-related applications when the available data is insufficient.","cs.LG,cs.SY,eess.SY,quant-ph"
"Deep Generative Markov State Models. We propose a deep generative Markov State Model (DeepGenMSM) learning
framework for inference of metastable dynamical systems and prediction of
trajectories. After unsupervised training on time series data, the model
contains (i) a probabilistic encoder that maps from high-dimensional
configuration space to a small-sized vector indicating the membership to
metastable (long-lived) states, (ii) a Markov chain that governs the
transitions between metastable states and facilitates analysis of the long-time
dynamics, and (iii) a generative part that samples the conditional distribution
of configurations in the next time step. The model can be operated in a
recursive fashion to generate trajectories to predict the system evolution from
a defined starting state and propose new configurations. The DeepGenMSM is
demonstrated to provide accurate estimates of the long-time kinetics and
generate valid distributions for molecular dynamics (MD) benchmark systems.
Remarkably, we show that DeepGenMSMs are able to make long time-steps in
molecular configuration space and generate physically realistic structures in
regions that were not seen in training data.","cs.LG,math.DS,math.PR,physics.data-an,stat.ML"
"DDet: Dual-path Dynamic Enhancement Network for Real-World Image Super-Resolution. Different from traditional image super-resolution task, real image
super-resolution(Real-SR) focus on the relationship between real-world
high-resolution(HR) and low-resolution(LR) image. Most of the traditional image
SR obtains the LR sample by applying a fixed down-sampling operator. Real-SR
obtains the LR and HR image pair by incorporating different quality optical
sensors. Generally, Real-SR has more challenges as well as broader application
scenarios. Previous image SR methods fail to exhibit similar performance on
Real-SR as the image data is not aligned inherently. In this article, we
propose a Dual-path Dynamic Enhancement Network(DDet) for Real-SR, which
addresses the cross-camera image mapping by realizing a dual-way dynamic
sub-pixel weighted aggregation and refinement. Unlike conventional methods
which stack up massive convolutional blocks for feature representation, we
introduce a content-aware framework to study non-inherently aligned image pair
in image SR issue. First, we use a content-adaptive component to exhibit the
Multi-scale Dynamic Attention(MDA). Second, we incorporate a long-term skip
connection with a Coupled Detail Manipulation(CDM) to perform collaborative
compensation and manipulation. The above dual-path model is joint into a
unified model and works collaboratively. Extensive experiments on the
challenging benchmarks demonstrate the superiority of our model.","cs.CV,cs.MM,eess.IV"
"Statistical and Computational Trade-Offs in Kernel K-Means. We investigate the efficiency of k-means in terms of both statistical and
computational requirements. More precisely, we study a Nystr\""om approach to
kernel k-means. We analyze the statistical properties of the proposed method
and show that it achieves the same accuracy of exact kernel k-means with only a
fraction of computations. Indeed, we prove under basic assumptions that
sampling $\sqrt{n}$ Nystr\""om landmarks allows to greatly reduce computational
costs without incurring in any loss of accuracy. To the best of our knowledge
this is the first result of this kind for unsupervised learning.","cs.DS,cs.LG,stat.ML"
"A Novel Adaptive Deep Network for Building Footprint Segmentation. Building footprint segmentations for high resolution images are increasingly
demanded for many remote sensing applications. By the emerging deep learning
approaches, segmentation networks have made significant advances in the
semantic segmentation of objects. However, these advances and the increased
access to satellite images require the generation of accurate object boundaries
in satellite images. In the current paper, we propose a novel network-based on
Pix2Pix methodology to solve the problem of inaccurate boundaries obtained by
converting satellite images into maps using segmentation networks in order to
segment building footprints. To define the new network named G2G, our framework
includes two generators where the first generator extracts localization
features in order to merge them with the boundary features extracted from the
second generator to segment all detailed building edges. Moreover, different
strategies are implemented to enhance the quality of the proposed networks'
results, implying that the proposed network outperforms state-of-the-art
networks in segmentation accuracy with a large margin for all evaluation
metrics. The implementation is available at
https://github.com/A2Amir/A-Novel-Adaptive-Deep-Network-for-Building-Footprint-Segmentation.","cs.CV,cs.LG,cs.NA,eess.IV,math.NA"
"Partial Recovery in the Graph Alignment Problem. In this paper, we consider the graph alignment problem, which is the problem
of recovering, given two graphs, a one-to-one mapping between nodes that
maximizes edge overlap. This problem can be viewed as a noisy version of the
well-known graph isomorphism problem and appears in many applications,
including social network deanonymization and cellular biology. Our focus here
is on partial recovery, i.e., we look for a one-to-one mapping which is correct
on a fraction of the nodes of the graph rather than on all of them, and we
assume that the two input graphs to the problem are correlated
Erd\H{o}s-R\'enyi graphs of parameters $(n,q,s)$. Our main contribution is then
to give necessary and sufficient conditions on $(n,q,s)$ under which partial
recovery is possible with high probability as the number of nodes $n$ goes to
infinity. In particular, we show that it is possible to achieve partial
recovery in the $nqs=\Theta(1)$ regime under certain additional assumptions. An
interesting byproduct of the analysis techniques we develop to obtain the
sufficiency result in the partial recovery setting is a tighter analysis of the
maximum likelihood estimator for the graph alignment problem, which leads to
improved sufficient conditions for exact recovery.","cs.DS,cs.LG,cs.SI,math.PR,stat.ML"
"Integrated Decision and Control at Multi-Lane Intersections with Mixed Traffic Flow. Autonomous driving at intersections is one of the most complicated and
accident-prone traffic scenarios, especially with mixed traffic participants
such as vehicles, bicycles and pedestrians. The driving policy should make safe
decisions to handle the dynamic traffic conditions and meet the requirements of
on-board computation. However, most of the current researches focuses on
simplified intersections considering only the surrounding vehicles and
idealized traffic lights. This paper improves the integrated decision and
control framework and develops a learning-based algorithm to deal with complex
intersections with mixed traffic flows, which can not only take account of
realistic characteristics of traffic lights, but also learn a safe policy under
different safety constraints. We first consider different velocity models for
green and red lights in the training process and use a finite state machine to
handle different modes of light transformation. Then we design different types
of distance constraints for vehicles, traffic lights, pedestrians, bicycles
respectively and formulize the constrained optimal control problems (OCPs) to
be optimized. Finally, reinforcement learning (RL) with value and policy
networks is adopted to solve the series of OCPs. In order to verify the safety
and efficiency of the proposed method, we design a multi-lane intersection with
the existence of large-scale mixed traffic participants and set practical
traffic light phases. The simulation results indicate that the trained decision
and control policy can well balance safety and tracking performance. Compared
with model predictive control (MPC), the computational time is three orders of
magnitude lower.","cs.AI,cs.LG,cs.SY,eess.SY"
"Logic Explained Networks. The large and still increasing popularity of deep learning clashes with a
major limit of neural network architectures, that consists in their lack of
capability in providing human-understandable motivations of their decisions. In
situations in which the machine is expected to support the decision of human
experts, providing a comprehensible explanation is a feature of crucial
importance. The language used to communicate the explanations must be formal
enough to be implementable in a machine and friendly enough to be
understandable by a wide audience. In this paper, we propose a general approach
to Explainable Artificial Intelligence in the case of neural architectures,
showing how a mindful design of the networks leads to a family of interpretable
deep learning models called Logic Explained Networks (LENs). LENs only require
their inputs to be human-understandable predicates, and they provide
explanations in terms of simple First-Order Logic (FOL) formulas involving such
predicates. LENs are general enough to cover a large number of scenarios.
Amongst them, we consider the case in which LENs are directly used as special
classifiers with the capability of being explainable, or when they act as
additional networks with the role of creating the conditions for making a
black-box classifier explainable by FOL formulas. Despite supervised learning
problems are mostly emphasized, we also show that LENs can learn and provide
explanations in unsupervised learning settings. Experimental results on several
datasets and tasks show that LENs may yield better classifications than
established white-box models, such as decision trees and Bayesian rule lists,
while providing more compact and meaningful explanations.","cs.AI,cs.LG,cs.LO,cs.NE"
"End-to-End Video-To-Speech Synthesis using Generative Adversarial Networks. Video-to-speech is the process of reconstructing the audio speech from a
video of a spoken utterance. Previous approaches to this task have relied on a
two-step process where an intermediate representation is inferred from the
video, and is then decoded into waveform audio using a vocoder or a waveform
reconstruction algorithm. In this work, we propose a new end-to-end
video-to-speech model based on Generative Adversarial Networks (GANs) which
translates spoken video to waveform end-to-end without using any intermediate
representation or separate waveform synthesis algorithm. Our model consists of
an encoder-decoder architecture that receives raw video as input and generates
speech, which is then fed to a waveform critic and a power critic. The use of
an adversarial loss based on these two critics enables the direct synthesis of
raw audio waveform and ensures its realism. In addition, the use of our three
comparative losses helps establish direct correspondence between the generated
audio and the input video. We show that this model is able to reconstruct
speech with remarkable realism for constrained datasets such as GRID, and that
it is the first end-to-end model to produce intelligible speech for LRW (Lip
Reading in the Wild), featuring hundreds of speakers recorded entirely `in the
wild'. We evaluate the generated samples in two different scenarios -- seen and
unseen speakers -- using four objective metrics which measure the quality and
intelligibility of artificial speech. We demonstrate that the proposed approach
outperforms all previous works in most metrics on GRID and LRW.","cs.CV,cs.LG,cs.SD,eess.AS"
"RAIN: Reinforced Hybrid Attention Inference Network for Motion Forecasting. Motion forecasting plays a significant role in various domains (e.g.,
autonomous driving, human-robot interaction), which aims to predict future
motion sequences given a set of historical observations. However, the observed
elements may be of different levels of importance. Some information may be
irrelevant or even distracting to the forecasting in certain situations. To
address this issue, we propose a generic motion forecasting framework (named
RAIN) with dynamic key information selection and ranking based on a hybrid
attention mechanism. The general framework is instantiated to handle
multi-agent trajectory prediction and human motion forecasting tasks,
respectively. In the former task, the model learns to recognize the relations
between agents with a graph representation and to determine their relative
significance. In the latter task, the model learns to capture the temporal
proximity and dependency in long-term human motions. We also propose an
effective double-stage training pipeline with an alternating training strategy
to optimize the parameters in different modules of the framework. We validate
the framework on both synthetic simulations and motion forecasting benchmarks
in different domains, demonstrating that our method not only achieves
state-of-the-art forecasting performance, but also provides interpretable and
reasonable hybrid attention weights.","cs.AI,cs.CV,cs.LG,cs.MA,cs.RO"
"Model Extraction Attacks on Graph Neural Networks: Taxonomy and Realization. Graph neural networks (GNNs) have been widely used to analyze the
graph-structured data in various application domains, e.g., social networks,
molecular biology, and anomaly detection. With great power, the GNN models,
usually as valuable Intellectual Properties of their owners, also become
attractive targets of the attacker. Recent studies show that machine learning
models are facing a severe threat called Model Extraction Attacks, where a
well-trained private model owned by a service provider can be stolen by the
attacker pretending as a client. Unfortunately, existing works focus on the
models trained on the Euclidean space, e.g., images and texts, while how to
extract a GNN model that contains a graph structure and node features is yet to
be explored. In this paper, we explore and develop model extraction attacks
against GNN models. Given only black-box access to a target GNN model, the
attacker aims to reconstruct a duplicated one via several nodes he obtained
(called attacker nodes). We first systematically formalise the threat modeling
in the context of GNN model extraction and classify the adversarial threats
into seven categories by considering different background knowledge of the
attacker, e.g., attributes and/or neighbor connectives of the attacker nodes.
Then we present the detailed methods which utilize the accessible knowledge in
each threat to implement the attacks. By evaluating over three real-world
datasets, our attacks are shown to extract duplicated models effectively, i.e.,
more than 89% inputs in the target domain have the same output predictions as
the victim model.","cs.CR,cs.LG"
"DeepFlow: History Matching in the Space of Deep Generative Models. The calibration of a reservoir model with observed transient data of fluid
pressures and rates is a key task in obtaining a predictive model of the flow
and transport behaviour of the earth's subsurface. The model calibration task,
commonly referred to as ""history matching"", can be formalised as an ill-posed
inverse problem where we aim to find the underlying spatial distribution of
petrophysical properties that explain the observed dynamic data. We use a
generative adversarial network pretrained on geostatistical object-based models
to represent the distribution of rock properties for a synthetic model of a
hydrocarbon reservoir. The dynamic behaviour of the reservoir fluids is
modelled using a transient two-phase incompressible Darcy formulation. We
invert for the underlying reservoir properties by first modeling property
distributions using the pre-trained generative model then using the adjoint
equations of the forward problem to perform gradient descent on the latent
variables that control the output of the generative model. In addition to the
dynamic observation data, we include well rock-type constraints by introducing
an additional objective function. Our contribution shows that for a synthetic
test case, we are able to obtain solutions to the inverse problem by optimising
in the latent variable space of a deep generative model, given a set of
transient observations of a non-linear forward problem.","68T10 (Primary) 68T20,76T99 (Secondary),86-04,86-08,I.5.1; I.6.3; I.5.4; I.2.8,cs.CV,cs.LG,physics.comp-ph,physics.geo-ph,stat.ML"
"MMLSpark: Unifying Machine Learning Ecosystems at Massive Scales. We introduce Microsoft Machine Learning for Apache Spark (MMLSpark), an
ecosystem of enhancements that expand the Apache Spark distributed computing
library to tackle problems in Deep Learning, Micro-Service Orchestration,
Gradient Boosting, Model Interpretability, and other areas of modern
computation. Furthermore, we present a novel system called Spark Serving that
allows users to run any Apache Spark program as a distributed, sub-millisecond
latency web service backed by their existing Spark Cluster. All MMLSpark
contributions have the same API to enable simple composition across frameworks
and usage across batch, streaming, and RESTful web serving scenarios on static,
elastic, or serverless clusters. We showcase MMLSpark by creating a method for
deep object detection capable of learning without human labeled data and
demonstrate its effectiveness for Snow Leopard conservation.","cs.AI,cs.DC,cs.LG,stat.ML"
"Relative Positional Encoding for Transformers with Linear Complexity. Recent advances in Transformer models allow for unprecedented sequence
lengths, due to linear space and time complexity. In the meantime, relative
positional encoding (RPE) was proposed as beneficial for classical Transformers
and consists in exploiting lags instead of absolute positions for inference.
Still, RPE is not available for the recent linear-variants of the Transformer,
because it requires the explicit computation of the attention matrix, which is
precisely what is avoided by such methods. In this paper, we bridge this gap
and present Stochastic Positional Encoding as a way to generate PE that can be
used as a replacement to the classical additive (sinusoidal) PE and provably
behaves like RPE. The main theoretical contribution is to make a connection
between positional encoding and cross-covariance structures of correlated
Gaussian processes. We illustrate the performance of our approach on the
Long-Range Arena benchmark and on music generation.","cs.CL,cs.LG,cs.SD,eess.AS,stat.ML"
"Depth Reconstruction from Sparse Samples: Representation, Algorithm, and Sampling. The rapid development of 3D technology and computer vision applications have
motivated a thrust of methodologies for depth acquisition and estimation.
However, most existing hardware and software methods have limited performance
due to poor depth precision, low resolution and high computational cost. In
this paper, we present a computationally efficient method to recover dense
depth maps from sparse measurements. We make three contributions. First, we
provide empirical evidence that depth maps can be encoded much more sparsely
than natural images by using common dictionaries such as wavelets and
contourlets. We also show that a combined wavelet-contourlet dictionary
achieves better performance than using either dictionary alone. Second, we
propose an alternating direction method of multipliers (ADMM) to achieve fast
reconstruction. A multi-scale warm start procedure is proposed to speed up the
convergence. Third, we propose a two-stage randomized sampling scheme to
optimally choose the sampling locations, thus maximizing the reconstruction
performance for any given sampling budget. Experimental results show that the
proposed method produces high quality dense depth estimates, and is robust to
noisy measurements. Applications to real data in stereo matching are
demonstrated.",cs.CV
"An Interpretable Compression and Classification System: Theory and Applications. This study proposes a low-complexity interpretable classification system. The
proposed system contains three main modules including feature extraction,
feature reduction, and classification. All of them are linear. Thanks to the
linear property, the extracted and reduced features can be inversed to original
data, like a linear transform such as Fourier transform, so that one can
quantify and visualize the contribution of individual features towards the
original data. Also, the reduced features and reversibility naturally endure
the proposed system ability of data compression. This system can significantly
compress data with a small percent deviation between the compressed and the
original data. At the same time, when the compressed data is used for
classification, it still achieves high testing accuracy. Furthermore, we
observe that the extracted features of the proposed system can be approximated
to uncorrelated Gaussian random variables. Hence, classical theory in
estimation and detection can be applied for classification. This motivates us
to propose using a MAP (maximum a posteriori) based classification method. As a
result, the extracted features and the corresponding performance have
statistical meaning and mathematically interpretable. Simulation results show
that the proposed classification system not only enjoys significant reduced
training and testing time but also high testing accuracy compared to the
conventional schemes.","cs.CV,eess.SP"
"Fast And Efficient Boolean Matrix Factorization By Geometric Segmentation. Boolean matrix has been used to represent digital information in many fields,
including bank transaction, crime records, natural language processing,
protein-protein interaction, etc. Boolean matrix factorization (BMF) aims to
find an approximation of a binary matrix as the Boolean product of two low rank
Boolean matrices, which could generate vast amount of information for the
patterns of relationships between the features and samples. Inspired by binary
matrix permutation theories and geometric segmentation, we developed a fast and
efficient BMF approach called MEBF (Median Expansion for Boolean
Factorization). Overall, MEBF adopted a heuristic approach to locate binary
patterns presented as submatrices that are dense in 1's. At each iteration,
MEBF permutates the rows and columns such that the permutated matrix is
approximately Upper Triangular-Like (UTL) with so-called Simultaneous
Consecutive-ones Property (SC1P). The largest submatrix dense in 1 would lies
on the upper triangular area of the permutated matrix, and its location was
determined based on a geometric segmentation of a triangular. We compared MEBF
with other state of the art approaches on data scenarios with different
sparsity and noise levels. MEBF demonstrated superior performances in lower
reconstruction error, and higher computational efficiency, as well as more
accurate sparse patterns than popular methods such as ASSO, PANDA and MP. We
demonstrated the application of MEBF on both binary and non-binary data sets,
and revealed its further potential in knowledge retrieving and data denoising.","cs.CG,cs.LG,stat.ML"
"Affine and Regional Dynamic Time Warpng. Pointwise matches between two time series are of great importance in time
series analysis, and dynamic time warping (DTW) is known to provide generally
reasonable matches. There are situations where time series alignment should be
invariant to scaling and offset in amplitude or where local regions of the
considered time series should be strongly reflected in pointwise matches. Two
different variants of DTW, affine DTW (ADTW) and regional DTW (RDTW), are
proposed to handle scaling and offset in amplitude and provide regional
emphasis respectively. Furthermore, ADTW and RDTW can be combined in two
different ways to generate alignments that incorporate advantages from both
methods, where the affine model can be applied either globally to the entire
time series or locally to each region. The proposed alignment methods
outperform DTW on specific simulated datasets, and one-nearest-neighbor
classifiers using their associated difference measures are competitive with the
difference measures associated with state-of-the-art alignment methods on real
datasets.","cs.CE,cs.CV,cs.LG"
"ATOM3D: Tasks On Molecules in Three Dimensions. Computational methods that operate on three-dimensional molecular structure
have the potential to solve important questions in biology and chemistry. In
particular, deep neural networks have gained significant attention, but their
widespread adoption in the biomolecular domain has been limited by a lack of
either systematic performance benchmarks or a unified toolkit for interacting
with molecular data. To address this, we present ATOM3D, a collection of both
novel and existing benchmark datasets spanning several key classes of
biomolecules. We implement several classes of three-dimensional molecular
learning methods for each of these tasks and show that they consistently
improve performance relative to methods based on one- and two-dimensional
representations. The specific choice of architecture proves to be critical for
performance, with three-dimensional convolutional networks excelling at tasks
involving complex geometries, graph networks performing well on systems
requiring detailed positional information, and the more recently developed
equivariant networks showing significant promise. Our results indicate that
many molecular problems stand to gain from three-dimensional molecular
learning, and that there is potential for improvement on many tasks which
remain underexplored. To lower the barrier to entry and facilitate further
developments in the field, we also provide a comprehensive suite of tools for
dataset processing, model training, and evaluation in our open-source atom3d
Python package. All datasets are available for download from
https://www.atom3d.ai .","cs.LG,physics.bio-ph,physics.comp-ph,q-bio.BM"
"Attentive Adversarial Learning for Domain-Invariant Training. Adversarial domain-invariant training (ADIT) proves to be effective in
suppressing the effects of domain variability in acoustic modeling and has led
to improved performance in automatic speech recognition (ASR). In ADIT, an
auxiliary domain classifier takes in equally-weighted deep features from a deep
neural network (DNN) acoustic model and is trained to improve their
domain-invariance by optimizing an adversarial loss function. In this work, we
propose an attentive ADIT (AADIT) in which we advance the domain classifier
with an attention mechanism to automatically weight the input deep features
according to their importance in domain classification. With this attentive
re-weighting, AADIT can focus on the domain normalization of phonetic
components that are more susceptible to domain variability and generates deep
features with improved domain-invariance and senone-discriminativity over ADIT.
Most importantly, the attention block serves only as an external component to
the DNN acoustic model and is not involved in ASR, so AADIT can be used to
improve the acoustic modeling with any DNN architectures. More generally, the
same methodology can improve any adversarial learning system with an auxiliary
discriminator. Evaluated on CHiME-3 dataset, the AADIT achieves 13.6% and 9.3%
relative WER improvements, respectively, over a multi-conditional model and a
strong ADIT baseline.","cs.CL,cs.LG,cs.SD,eess.AS,stat.ML"
"Optimizing Large-Scale Fleet Management on a Road Network using Multi-Agent Deep Reinforcement Learning with Graph Neural Network. We propose a novel approach to optimize fleet management by combining
multi-agent reinforcement learning with graph neural network. To provide
ride-hailing service, one needs to optimize dynamic resources and demands over
spatial domain. While the spatial structure was previously approximated with a
regular grid, our approach represents the road network with a graph, which
better reflects the underlying geometric structure. Dynamic resource allocation
is formulated as multi-agent reinforcement learning, whose action-value
function (Q function) is approximated with graph neural networks. We use
stochastic policy update rule over the graph with deep Q-networks (DQN), and
achieve superior results over the greedy policy update. We design a realistic
simulator that emulates the empirical taxi call data, and confirm the
effectiveness of the proposed model under various conditions.","cs.LG,cs.SY,eess.SY"
"Automated Vulnerability Detection in Source Code Using Deep Representation Learning. Increasing numbers of software vulnerabilities are discovered every year
whether they are reported publicly or discovered internally in proprietary
code. These vulnerabilities can pose serious risk of exploit and result in
system compromise, information leaks, or denial of service. We leveraged the
wealth of C and C++ open-source code available to develop a large-scale
function-level vulnerability detection system using machine learning. To
supplement existing labeled vulnerability datasets, we compiled a vast dataset
of millions of open-source functions and labeled it with carefully-selected
findings from three different static analyzers that indicate potential
exploits. The labeled dataset is available at: https://osf.io/d45bw/. Using
these datasets, we developed a fast and scalable vulnerability detection tool
based on deep feature representation learning that directly interprets lexed
source code. We evaluated our tool on code from both real software packages and
the NIST SATE IV benchmark dataset. Our results demonstrate that deep feature
representation learning on source code is a promising approach for automated
software vulnerability detection.","cs.AI,cs.LG,cs.SE,stat.ML"
"Exact and approximate inference in graphical models: variable elimination and beyond. Probabilistic graphical models offer a powerful framework to account for the
dependence structure between variables, which is represented as a graph.
However, the dependence between variables may render inference tasks
intractable. In this paper we review techniques exploiting the graph structure
for exact inference, borrowed from optimisation and computer science. They are
built on the principle of variable elimination whose complexity is dictated in
an intricate way by the order in which variables are eliminated. The so-called
treewidth of the graph characterises this algorithmic complexity: low-treewidth
graphs can be processed efficiently. The first message that we illustrate is
therefore the idea that for inference in graphical model, the number of
variables is not the limiting factor, and it is worth checking for the
treewidth before turning to approximate methods. We show how algorithms
providing an upper bound of the treewidth can be exploited to derive a 'good'
elimination order enabling to perform exact inference. The second message is
that when the treewidth is too large, algorithms for approximate inference
linked to the principle of variable elimination, such as loopy belief
propagation and variational approaches, can lead to accurate results while
being much less time consuming than Monte-Carlo approaches. We illustrate the
techniques reviewed in this article on benchmarks of inference problems in
genetic linkage analysis and computer vision, as well as on hidden variables
restoration in coupled Hidden Markov Models.","cs.AI,cs.LG,stat.ML"
"ODVICE: An Ontology-Driven Visual Analytic Tool for Interactive Cohort Extraction. Increased availability of electronic health records (EHR) has enabled
researchers to study various medical questions. Cohort selection for the
hypothesis under investigation is one of the main consideration for EHR
analysis. For uncommon diseases, cohorts extracted from EHRs contain very
limited number of records - hampering the robustness of any analysis. Data
augmentation methods have been successfully applied in other domains to address
this issue mainly using simulated records. In this paper, we present ODVICE, a
data augmentation framework that leverages the medical concept ontology to
systematically augment records using a novel ontologically guided Monte-Carlo
graph spanning algorithm. The tool allows end users to specify a small set of
interactive controls to control the augmentation process. We analyze the
importance of ODVICE by conducting studies on MIMIC-III dataset for two
learning tasks. Our results demonstrate the predictive performance of ODVICE
augmented cohorts, showing ~30% improvement in area under the curve (AUC) over
the non-augmented dataset and other data augmentation strategies.","cs.IR,cs.LG,stat.ML"
"Survival Prediction of Heart Failure Patients using Stacked Ensemble Machine Learning Algorithm. Cardiovascular disease, especially heart failure is one of the major health
hazard issues of our time and is a leading cause of death worldwide.
Advancement in data mining techniques using machine learning (ML) models is
paving promising prediction approaches. Data mining is the process of
converting massive volumes of raw data created by the healthcare institutions
into meaningful information that can aid in making predictions and crucial
decisions. Collecting various follow-up data from patients who have had heart
failures, analyzing those data, and utilizing several ML models to predict the
survival possibility of cardiovascular patients is the key aim of this study.
Due to the imbalance of the classes in the dataset, Synthetic Minority
Oversampling Technique (SMOTE) has been implemented. Two unsupervised models
(K-Means and Fuzzy C-Means clustering) and three supervised classifiers (Random
Forest, XGBoost and Decision Tree) have been used in our study. After thorough
investigation, our results demonstrate a superior performance of the supervised
ML algorithms over unsupervised models. Moreover, we designed and propose a
supervised stacked ensemble learning model that can achieve an accuracy,
precision, recall and F1 score of 99.98%. Our study shows that only certain
attributes collected from the patients are imperative to successfully predict
the surviving possibility post heart failure, using supervised ML algorithms.","cs.AI,cs.LG,q-bio.BM,stat.ML"
"Augmenting Molecular Images with Vector Representations as a Featurization Technique for Drug Classification. One of the key steps in building deep learning systems for drug
classification and generation is the choice of featurization for the molecules.
Previous featurization methods have included molecular images, binary strings,
graphs, and SMILES strings. This paper proposes the creation of molecular
images captioned with binary vectors that encode information not contained in
or easily understood from a molecular image alone. Specifically, we use Morgan
fingerprints, which encode higher level structural information, and MACCS keys,
which encode yes or no questions about a molecules properties and structure. We
tested our method on the HIV dataset published by the Pande lab, which consists
of 41,127 molecules labeled by if they inhibit the HIV virus. Our final model
achieved a state of the art AUC ROC on the HIV dataset, outperforming all other
methods. Moreover, the model converged significantly faster than most other
methods, requiring dramatically less computational power than unaugmented
images.","cs.CV,q-bio.BM"
"Robust Change Captioning. Describing what has changed in a scene can be useful to a user, but only if
generated text focuses on what is semantically relevant. It is thus important
to distinguish distractors (e.g. a viewpoint change) from relevant changes
(e.g. an object has moved). We present a novel Dual Dynamic Attention Model
(DUDA) to perform robust Change Captioning. Our model learns to distinguish
distractors from semantic changes, localize the changes via Dual Attention over
""before"" and ""after"" images, and accurately describe them in natural language
via Dynamic Speaker, by adaptively focusing on the necessary visual inputs
(e.g. ""before"" or ""after"" image). To study the problem in depth, we collect a
CLEVR-Change dataset, built off the CLEVR engine, with 5 types of scene
changes. We benchmark a number of baselines on our dataset, and systematically
study different change types and robustness to distractors. We show the
superiority of our DUDA model in terms of both change captioning and
localization. We also show that our approach is general, obtaining
state-of-the-art results on the recent realistic Spot-the-Diff dataset which
has no distractors.","cs.AI,cs.CV"
"MDA-Net: Multi-Dimensional Attention-Based Neural Network for 3D Image Segmentation. Segmenting an entire 3D image often has high computational complexity and
requires large memory consumption; by contrast, performing volumetric
segmentation in a slice-by-slice manner is efficient but does not fully
leverage the 3D data. To address this challenge, we propose a multi-dimensional
attention network (MDA-Net) to efficiently integrate slice-wise, spatial, and
channel-wise attention into a U-Net based network, which results in high
segmentation accuracy with a low computational cost. We evaluate our model on
the MICCAI iSeg and IBSR datasets, and the experimental results demonstrate
consistent improvements over existing methods.","cs.CV,eess.IV"
"Unsupervised Feature Selection for Tumor Profiles using Autoencoders and Kernel Methods. Molecular data from tumor profiles is high dimensional. Tumor profiles can be
characterized by tens of thousands of gene expression features. Due to the size
of the gene expression feature set machine learning methods are exposed to
noisy variables and complexity. Tumor types present heterogeneity and can be
subdivided in tumor subtypes. In many cases tumor data does not include tumor
subtype labeling thus unsupervised learning methods are necessary for tumor
subtype discovery. This work aims to learn meaningful and low dimensional
representations of tumor samples and find tumor subtype clusters while keeping
biological signatures without using tumor labels. The proposed method named
Latent Kernel Feature Selection (LKFS) is an unsupervised approach for gene
selection in tumor gene expression profiles. By using Autoencoders a low
dimensional and denoised latent space is learned as a target representation to
guide a Multiple Kernel Learning model that selects a subset of genes. By using
the selected genes a clustering method is used to group samples. In order to
evaluate the performance of the proposed unsupervised feature selection method
the obtained features and clusters are analyzed by clinical significance. The
proposed method has been applied on three tumor datasets which are Brain, Renal
and Lung, each one composed by two tumor subtypes. When compared with benchmark
unsupervised feature selection methods the results obtained by the proposed
method reveal lower redundancy in the selected features and a better clustering
performance.","cs.LG,q-bio.GN,q-bio.QM,stat.ML"
"Light Field Segmentation From Super-pixel Graph Representation. Efficient and accurate segmentation of light field is an important task in
computer vision and graphics. The large volume of input data and the redundancy
of light field make it an open challenge. In the paper, we propose a novel
graph representation for interactive light field segmentation based on light
field super-pixel (LFSP). The LFSP not only maintains light field redundancy,
but also greatly reduces the graph size. These advantages make LFSP useful to
improve segmentation efficiency. Based on LFSP graph structure, we present an
efficient light field segmentation algorithm using graph-cuts. Experimental
results on both synthetic and real dataset demonstrate that our method is
superior to previous light field segmentation algorithms with respect to
accuracy and efficiency.","68U10,I.4.6,cs.CV"
"Decimated Framelet System on Graphs and Fast G-Framelet Transforms. Graph representation learning has many real-world applications, from
super-resolution imaging, 3D computer vision to drug repurposing, protein
classification, social networks analysis. An adequate representation of graph
data is vital to the learning performance of a statistical or machine learning
model for graph-structured data. In this paper, we propose a novel multiscale
representation system for graph data, called decimated framelets, which form a
localized tight frame on the graph. The decimated framelet system allows
storage of the graph data representation on a coarse-grained chain and
processes the graph data at multi scales where at each scale, the data is
stored at a subgraph. Based on this, we then establish decimated G-framelet
transforms for the decomposition and reconstruction of the graph data at multi
resolutions via a constructive data-driven filter bank. The graph framelets are
built on a chain-based orthonormal basis that supports fast graph Fourier
transforms. From this, we give a fast algorithm for the decimated G-framelet
transforms, or FGT, that has linear computational complexity O(N) for a graph
of size N. The theory of decimated framelets and FGT is verified with numerical
examples for random graphs. The effectiveness is demonstrated by real-world
applications, including multiresolution analysis for traffic network, and graph
neural networks for graph classification tasks.","cs.LG,cs.NA,math.NA"
"Central and Non-central Limit Theorems arising from the Scattering Transform and its Neural Activation Generalization. Motivated by analyzing complicated and non-stationary time series, we study a
generalization of the scattering transform (ST) that includes broad neural
activation functions, which is called neural activation ST (NAST). On the
whole, NAST is a transform that comprises a sequence of ``neural processing
units'', each of which applies a high pass filter to the input from the
previous layer followed by a composition with a nonlinear function as the
output to the next neuron. Here, the nonlinear function models how a neuron
gets excited by the input signal. In addition to showing properties like
non-expansion, horizontal translational invariability and insensitivity to
local deformation, the statistical properties of the second order NAST of a
Gaussian process with various dependence and (non-)stationarity structure and
its interaction with the chosen high pass filters and activation functions are
explored and central limit theorem (CLT) and non-CLT results are provided.
Numerical simulations are also provided. The results explain how NAST processes
complicated and non-stationary time series, and pave a way towards statistical
inference based on NAST under the non-null case.","60H05,62M15,Primary 60G60,Secondary 35K15,cs.LG,math.PR,stat.ML"
"On the Optimization Landscape of Tensor Decompositions. Non-convex optimization with local search heuristics has been widely used in
machine learning, achieving many state-of-art results. It becomes increasingly
important to understand why they can work for these NP-hard problems on typical
data. The landscape of many objective functions in learning has been
conjectured to have the geometric property that ""all local optima are
(approximately) global optima"", and thus they can be solved efficiently by
local search algorithms. However, establishing such property can be very
difficult.
  In this paper, we analyze the optimization landscape of the random
over-complete tensor decomposition problem, which has many applications in
unsupervised learning, especially in learning latent variable models. In
practice, it can be efficiently solved by gradient ascent on a non-convex
objective. We show that for any small constant $\epsilon > 0$, among the set of
points with function values $(1+\epsilon)$-factor larger than the expectation
of the function, all the local maxima are approximate global maxima.
Previously, the best-known result only characterizes the geometry in small
neighborhoods around the true components. Our result implies that even with an
initialization that is barely better than the random guess, the gradient ascent
algorithm is guaranteed to solve this problem.
  Our main technique uses Kac-Rice formula and random matrix theory. To our
best knowledge, this is the first time when Kac-Rice formula is successfully
applied to counting the number of local minima of a highly-structured random
polynomial with dependent coefficients.","cs.DS,cs.LG,math.OC,math.PR,stat.ML"
"Enhancing Feature Tracking With Gyro Regularization. We present a deeply integrated method of exploiting low-cost gyroscopes to
improve general purpose feature tracking. Most previous methods use gyroscopes
to initialize and bound the search for features. In contrast, we use them to
regularize the tracking energy function so that they can directly assist in the
tracking of ambiguous and poor-quality features. We demonstrate that our simple
technique offers significant improvements in performance over conventional
template-based tracking methods, and is in fact competitive with more complex
and computationally expensive state-of-the-art trackers, but at a fraction of
the computational cost. Additionally, we show that the practice of initializing
template-based feature trackers like KLT (Kanade-Lucas-Tomasi) using
gyro-predicted optical flow offers no advantage over using a careful
optical-only initialization method, suggesting that some deeper level of
integration, like the method we propose, is needed in order to realize a
genuine improvement in tracking performance from these inertial sensors.","68T45,cs.CV"
"N-GCN: Multi-scale Graph Convolution for Semi-supervised Node Classification. Graph Convolutional Networks (GCNs) have shown significant improvements in
semi-supervised learning on graph-structured data. Concurrently, unsupervised
learning of graph embeddings has benefited from the information contained in
random walks. In this paper, we propose a model: Network of GCNs (N-GCN), which
marries these two lines of work. At its core, N-GCN trains multiple instances
of GCNs over node pairs discovered at different distances in random walks, and
learns a combination of the instance outputs which optimizes the classification
objective. Our experiments show that our proposed N-GCN model improves
state-of-the-art baselines on all of the challenging node classification tasks
we consider: Cora, Citeseer, Pubmed, and PPI. In addition, our proposed method
has other desirable properties, including generalization to recently proposed
semi-supervised learning methods such as GraphSAGE, allowing us to propose
N-SAGE, and resilience to adversarial input perturbations.","cs.LG,cs.SI,stat.ML"
"Times series averaging and denoising from a probabilistic perspective on time-elastic kernels. In the light of regularized dynamic time warping kernels, this paper
re-considers the concept of time elastic centroid for a setof time series. We
derive a new algorithm based on a probabilistic interpretation of kernel
alignment matrices. This algorithm expressesthe averaging process in terms of a
stochastic alignment automata. It uses an iterative agglomerative heuristic
method for averagingthe aligned samples, while also averaging the times of
occurrence of the aligned samples. By comparing classification accuracies for45
heterogeneous time series datasets obtained by first nearest centroid/medoid
classifiers we show that: i) centroid-basedapproaches significantly outperform
medoid-based approaches, ii) for the considered datasets, our algorithm that
combines averagingin the sample space and along the time axes, emerges as the
most significantly robust model for time-elastic averaging with apromising
noise reduction capability. We also demonstrate its benefit in an isolated
gesture recognition experiment and its ability tosignificantly reduce the size
of training instance sets. Finally we highlight its denoising capability using
demonstrative synthetic data:we show that it is possible to retrieve, from few
noisy instances, a signal whose components are scattered in a wide spectral
band.","cs.IR,cs.LG"
"Bayesian Optimisation for Sequential Experimental Design with Applications in Additive Manufacturing. Bayesian optimization (BO) is an approach to globally optimizing black-box
objective functions that are expensive to evaluate. BO-powered experimental
design has found wide application in materials science, chemistry, experimental
physics, drug development, etc. This work aims to bring attention to the
benefits of applying BO in designing experiments and to provide a BO manual,
covering both methodology and software, for the convenience of anyone who wants
to apply or learn BO. In particular, we briefly explain the BO technique,
review all the applications of BO in additive manufacturing, compare and
exemplify the features of different open BO libraries, unlock new potential
applications of BO to other types of data (e.g., preferential output). This
article is aimed at readers with some understanding of Bayesian methods, but
not necessarily with knowledge of additive manufacturing; the software
performance overview and implementation instructions are instrumental for any
experimental-design practitioner. Moreover, our review in the field of additive
manufacturing highlights the current knowledge and technological trends of BO.","cs.CE,cs.LG"
"Categorical Representation Learning: Morphism is All You Need. We provide a construction for categorical representation learning and
introduce the foundations of ""$\textit{categorifier}$"". The central theme in
representation learning is the idea of $\textbf{everything to vector}$. Every
object in a dataset $\mathcal{S}$ can be represented as a vector in
$\mathbb{R}^n$ by an $\textit{encoding map}$ $E:
\mathcal{O}bj(\mathcal{S})\to\mathbb{R}^n$. More importantly, every morphism
can be represented as a matrix $E:
\mathcal{H}om(\mathcal{S})\to\mathbb{R}^{n}_{n}$. The encoding map $E$ is
generally modeled by a $\textit{deep neural network}$. The goal of
representation learning is to design appropriate tasks on the dataset to train
the encoding map (assuming that an encoding is optimal if it universally
optimizes the performance on various tasks). However, the latter is still a
$\textit{set-theoretic}$ approach. The goal of the current article is to
promote the representation learning to a new level via a
$\textit{category-theoretic}$ approach. As a proof of concept, we provide an
example of a text translator equipped with our technology, showing that our
categorical learning model outperforms the current deep learning models by 17
times. The content of the current article is part of the recent US patent
proposal (patent application number: 63110906).","03-04,03B70,03D10,11Y16,cond-mat.dis-nn,cs.AI,cs.LG,math.CT"
"Clustering multilayer graphs with missing nodes. Relationship between agents can be conveniently represented by graphs. When
these relationships have different modalities, they are better modelled by
multilayer graphs where each layer is associated with one modality. Such graphs
arise naturally in many contexts including biological and social networks.
Clustering is a fundamental problem in network analysis where the goal is to
regroup nodes with similar connectivity profiles. In the past decade, various
clustering methods have been extended from the unilayer setting to multilayer
graphs in order to incorporate the information provided by each layer. While
most existing works assume - rather restrictively - that all layers share the
same set of nodes, we propose a new framework that allows for layers to be
defined on different sets of nodes. In particular, the nodes not recorded in a
layer are treated as missing. Within this paradigm, we investigate several
generalizations of well-known clustering methods in the complete setting to the
incomplete one and prove some consistency results under the Multi-Layer
Stochastic Block Model assumption. Our theoretical results are complemented by
thorough numerical comparisons between our proposed algorithms on synthetic
data, and also on real datasets, thus highlighting the promising behaviour of
our methods in various settings.","cs.LG,math.ST,stat.ML,stat.TH"
"Learning Gradient Fields for Molecular Conformation Generation. We study a fundamental problem in computational chemistry known as molecular
conformation generation, trying to predict stable 3D structures from 2D
molecular graphs. Existing machine learning approaches usually first predict
distances between atoms and then generate a 3D structure satisfying the
distances, where noise in predicted distances may induce extra errors during 3D
coordinate generation. Inspired by the traditional force field methods for
molecular dynamics simulation, in this paper, we propose a novel approach
called ConfGF by directly estimating the gradient fields of the log density of
atomic coordinates. The estimated gradient fields allow directly generating
stable conformations via Langevin dynamics. However, the problem is very
challenging as the gradient fields are roto-translation equivariant. We notice
that estimating the gradient fields of atomic coordinates can be translated to
estimating the gradient fields of interatomic distances, and hence develop a
novel algorithm based on recent score-based generative models to effectively
estimate these gradients. Experimental results across multiple tasks show that
ConfGF outperforms previous state-of-the-art baselines by a significant margin.","cs.LG,physics.chem-ph,q-bio.BM"
"Depth by Poking: Learning to Estimate Depth from Self-Supervised Grasping. Accurate depth estimation remains an open problem for robotic manipulation;
even state of the art techniques including structured light and LiDAR sensors
fail on reflective or transparent surfaces. We address this problem by training
a neural network model to estimate depth from RGB-D images, using labels from
physical interactions between a robot and its environment. Our network
predicts, for each pixel in an input image, the z position that a robot's end
effector would reach if it attempted to grasp or poke at the corresponding
position. Given an autonomous grasping policy, our approach is self-supervised
as end effector position labels can be recovered through forward kinematics,
without human annotation. Although gathering such physical interaction data is
expensive, it is necessary for training and routine operation of state of the
art manipulation systems. Therefore, this depth estimator comes ``for free''
while collecting data for other tasks (e.g., grasping, pushing, placing). We
show our approach achieves significantly lower root mean squared error than
traditional structured light sensors and unsupervised deep learning methods on
difficult, industry-scale jumbled bin datasets.","cs.CV,cs.LG,cs.RO,eess.IV"
"Algorithms for ridge estimation with convergence guarantees. The extraction of filamentary structure from a point cloud is discussed. The
filaments are modeled as ridge lines or higher dimensional ridges of an
underlying density. We propose two novel algorithms, and provide theoretical
guarantees for their convergences. We consider the new algorithms as
alternatives to the Subspace Constraint Mean Shift (SCMS) algorithm that do not
suffer from a shortcoming of the SCMS that is also revealed in this paper.","62G05,cs.LG,math.ST,stat.ML,stat.TH"
"Two-Stage Framework for Seasonal Time Series Forecasting. Seasonal time series Forecasting remains a challenging problem due to the
long-term dependency from seasonality. In this paper, we propose a two-stage
framework to forecast univariate seasonal time series. The first stage
explicitly learns the long-range time series structure in a time window beyond
the forecast horizon. By incorporating the learned long-range structure, the
second stage can enhance the prediction accuracy in the forecast horizon. In
both stages, we integrate the auto-regressive model with neural networks to
capture both linear and non-linear characteristics in time series. Our
framework achieves state-of-the-art performance on M4 Competition Hourly
datasets. In particular, we show that incorporating the intermediate results
generated in the first stage to existing forecast models can effectively
enhance their prediction performance.","cs.AI,cs.LG,stat.AP"
"Classifying sequences by the optimized dissimilarity space embedding approach: a case study on the solubility analysis of the E. coli proteome. We evaluate a version of the recently-proposed classification system named
Optimized Dissimilarity Space Embedding (ODSE) that operates in the input space
of sequences of generic objects. The ODSE system has been originally presented
as a classification system for patterns represented as labeled graphs. However,
since ODSE is founded on the dissimilarity space representation of the input
data, the classifier can be easily adapted to any input domain where it is
possible to define a meaningful dissimilarity measure. Here we demonstrate the
effectiveness of the ODSE classifier for sequences by considering an
application dealing with the recognition of the solubility degree of the
Escherichia coli proteome. Solubility, or analogously aggregation propensity,
is an important property of protein molecules, which is intimately related to
the mechanisms underlying the chemico-physical process of folding. Each protein
of our dataset is initially associated with a solubility degree and it is
represented as a sequence of symbols, denoting the 20 amino acid residues. The
herein obtained computational results, which we stress that have been achieved
with no context-dependent tuning of the ODSE system, confirm the validity and
generality of the ODSE-based approach for structured data classification.","I.5,cs.AI,cs.CV,physics.bio-ph,q-bio.BM"
"Optimizing the Long-Term Average Reward for Continuing MDPs: A Technical Report. Recently, we have struck the balance between the information freshness, in
terms of age of information (AoI), experienced by users and energy consumed by
sensors, by appropriately activating sensors to update their current status in
caching enabled Internet of Things (IoT) networks [1]. To solve this problem,
we cast the corresponding status update procedure as a continuing Markov
Decision Process (MDP) (i.e., without termination states), where the number of
state-action pairs increases exponentially with respect to the number of
considered sensors and users. Moreover, to circumvent the curse of
dimensionality, we have established a methodology for designing deep
reinforcement learning (DRL) algorithms to maximize (resp. minimize) the
average reward (resp. cost), by integrating R-learning, a tabular reinforcement
learning (RL) algorithm tailored for maximizing the long-term average reward,
and traditional DRL algorithms, initially developed to optimize the discounted
long-term cumulative reward rather than the average one. In this technical
report, we would present detailed discussions on the technical contributions of
this methodology.","cs.IT,cs.LG,cs.NI,math.IT"
"Analysis of the Optimization Landscapes for Overcomplete Representation Learning. We study nonconvex optimization landscapes for learning overcomplete
representations, including learning (i) sparsely used overcomplete dictionaries
and (ii) convolutional dictionaries, where these unsupervised learning problems
find many applications in high-dimensional data analysis. Despite the empirical
success of simple nonconvex algorithms, theoretical justifications of why these
methods work so well are far from satisfactory. In this work, we show these
problems can be formulated as $\ell^4$-norm optimization problems with
spherical constraint, and study the geometric properties of their nonconvex
optimization landscapes. For both problems, we show the nonconvex objectives
have benign (global) geometric structures, in the sense that every local
minimizer is close to one of the target solutions and every saddle point
exhibits negative curvature. This discovery enables the development of
guaranteed global optimization methods using simple initializations. For both
problems, we show the nonconvex objectives have benign geometric structures --
every local minimizer is close to one of the target solutions and every saddle
point exhibits negative curvature -- either in the entire space or within a
sufficiently large region. This discovery ensures local search algorithms (such
as Riemannian gradient descent) with simple initializations approximately find
the target solutions. Finally, numerical experiments justify our theoretical
discoveries.","cs.IT,cs.LG,eess.SP,math.IT,math.OC,stat.ML"
"Model-based clustering with Hidden Markov Model regression for time series with regime changes. This paper introduces a novel model-based clustering approach for clustering
time series which present changes in regime. It consists of a mixture of
polynomial regressions governed by hidden Markov chains. The underlying hidden
process for each cluster activates successively several polynomial regimes
during time. The parameter estimation is performed by the maximum likelihood
method through a dedicated Expectation-Maximization (EM) algorithm. The
proposed approach is evaluated using simulated time series and real-world time
series issued from a railway diagnosis application. Comparisons with existing
approaches for time series clustering, including the stand EM for Gaussian
mixtures, $K$-means clustering, the standard mixture of regression models and
mixture of Hidden Markov Models, demonstrate the effectiveness of the proposed
approach.","cs.LG,stat.ME,stat.ML"
"DADA: Deep Adversarial Data Augmentation for Extremely Low Data Regime Classification. Deep learning has revolutionized the performance of classification, but
meanwhile demands sufficient labeled data for training. Given insufficient
data, while many techniques have been developed to help combat overfitting, the
challenge remains if one tries to train deep networks, especially in the
ill-posed extremely low data regimes: only a small set of labeled data are
available, and nothing -- including unlabeled data -- else. Such regimes arise
from practical situations where not only data labeling but also data collection
itself is expensive. We propose a deep adversarial data augmentation (DADA)
technique to address the problem, in which we elaborately formulate data
augmentation as a problem of training a class-conditional and supervised
generative adversarial network (GAN). Specifically, a new discriminator loss is
proposed to fit the goal of data augmentation, through which both real and
augmented samples are enforced to contribute to and be consistent in finding
the decision boundaries. Tailored training techniques are developed
accordingly. To quantitatively validate its effectiveness, we first perform
extensive simulations to show that DADA substantially outperforms both
traditional data augmentation and a few GAN-based options. We then extend
experiments to three real-world small labeled datasets where existing data
augmentation and/or transfer learning strategies are either less effective or
infeasible. All results endorse the superior capability of DADA in enhancing
the generalization ability of deep networks trained in practical extremely low
data regimes. Source code is available at
https://github.com/SchafferZhang/DADA.",cs.CV
"ResOT: Resource-Efficient Oblique Trees for Neural Signal Classification. Classifiers that can be implemented on chip with minimal computational and
memory resources are essential for edge computing in emerging applications such
as medical and IoT devices. This paper introduces a machine learning model
based on oblique decision trees to enable resource-efficient classification on
a neural implant. By integrating model compression with probabilistic routing
and implementing cost-aware learning, our proposed model could significantly
reduce the memory and hardware cost compared to state-of-the-art models, while
maintaining the classification accuracy. We trained the resource-efficient
oblique tree with power-efficient regularization (ResOT-PE) on three neural
classification tasks to evaluate the performance, memory, and hardware
requirements. On seizure detection task, we were able to reduce the model size
by 3.4X and the feature extraction cost by 14.6X compared to the ensemble of
boosted trees, using the intracranial EEG from 10 epilepsy patients. In a
second experiment, we tested the ResOT-PE model on tremor detection for
Parkinson's disease, using the local field potentials from 12 patients
implanted with a deep-brain stimulation (DBS) device. We achieved a comparable
classification performance as the state-of-the-art boosted tree ensemble, while
reducing the model size and feature extraction cost by 10.6X and 6.8X,
respectively. We also tested on a 6-class finger movement detection task using
ECoG recordings from 9 subjects, reducing the model size by 17.6X and feature
computation cost by 5.1X. The proposed model can enable a low-power and
memory-efficient implementation of classifiers for real-time neurological
disease detection and motor decoding.","cs.LG,eess.SP,stat.ML"
"Flickr30k Entities: Collecting Region-to-Phrase Correspondences for Richer Image-to-Sentence Models. The Flickr30k dataset has become a standard benchmark for sentence-based
image description. This paper presents Flickr30k Entities, which augments the
158k captions from Flickr30k with 244k coreference chains, linking mentions of
the same entities across different captions for the same image, and associating
them with 276k manually annotated bounding boxes. Such annotations are
essential for continued progress in automatic image description and grounded
language understanding. They enable us to define a new benchmark for
localization of textual entity mentions in an image. We present a strong
baseline for this task that combines an image-text embedding, detectors for
common objects, a color classifier, and a bias towards selecting larger
objects. While our baseline rivals in accuracy more complex state-of-the-art
models, we show that its gains cannot be easily parlayed into improvements on
such tasks as image-sentence retrieval, thus underlining the limitations of
current methods and the need for further research.","cs.CL,cs.CV"
"Option Discovery in Hierarchical Reinforcement Learning using Spatio-Temporal Clustering. This paper introduces an automated skill acquisition framework in
reinforcement learning which involves identifying a hierarchical description of
the given task in terms of abstract states and extended actions between
abstract states. Identifying such structures present in the task provides ways
to simplify and speed up reinforcement learning algorithms. These structures
also help to generalize such algorithms over multiple tasks without relearning
policies from scratch. We use ideas from dynamical systems to find metastable
regions in the state space and associate them with abstract states. The
spectral clustering algorithm PCCA+ is used to identify suitable abstractions
aligned to the underlying structure. Skills are defined in terms of the
sequence of actions that lead to transitions between such abstract states. The
connectivity information from PCCA+ is used to generate these skills or
options. These skills are independent of the learning task and can be
efficiently reused across a variety of tasks defined over the same model. This
approach works well even without the exact model of the environment by using
sample trajectories to construct an approximate estimate. We also present our
approach to scaling the skill acquisition framework to complex tasks with large
state spaces for which we perform state aggregation using the representation
learned from an action conditional video prediction network and use the skill
acquisition framework on the aggregated state space.","cs.AI,cs.CV,cs.LG,cs.NE"
"Fast-Slow Streamflow Model Using Mass-Conserving LSTM. Streamflow forecasting is key to effectively managing water resources and
preparing for the occurrence of natural calamities being exacerbated by climate
change. Here we use the concept of fast and slow flow components to create a
new mass-conserving Long Short-Term Memory (LSTM) neural network model. It uses
hydrometeorological time series and catchment attributes to predict daily river
discharges. Preliminary results evidence improvement in skills for different
scores compared to the recent literature.","68T07,J.2; I.2.m,cs.LG,physics.ao-ph"
"Real-time Prediction for Mechanical Ventilation in COVID-19 Patients using A Multi-task Gaussian Process Multi-objective Self-attention Network. We propose a robust in-time predictor for in-hospital COVID-19 patient's
probability of requiring mechanical ventilation. A challenge in the risk
prediction for COVID-19 patients lies in the great variability and irregular
sampling of patient's vitals and labs observed in the clinical setting.
Existing methods have strong limitations in handling time-dependent features'
complex dynamics, either oversimplifying temporal data with summary statistics
that lose information or over-engineering features that lead to less robust
outcomes. We propose a novel in-time risk trajectory predictive model to handle
the irregular sampling rate in the data, which follows the dynamics of risk of
performing mechanical ventilation for individual patients. The model
incorporates the Multi-task Gaussian Process using observed values to learn the
posterior joint multi-variant conditional probability and infer the missing
values on a unified time grid. The temporal imputed data is fed into a
multi-objective self-attention network for the prediction task. A novel
positional encoding layer is proposed and added to the network for producing
in-time predictions. The positional layer outputs a risk score at each
user-defined time point during the entire hospital stay of an inpatient. We
frame the prediction task into a multi-objective learning framework, and the
risk scores at all time points are optimized altogether, which adds robustness
and consistency to the risk score trajectory prediction. Our experimental
evaluation on a large database with nationwide in-hospital patients with
COVID-19 also demonstrates that it improved the state-of-the-art performance in
terms of AUC (Area Under the receiver operating characteristic Curve) and AUPRC
(Area Under the Precision-Recall Curve) performance metrics, especially at
early times after hospital admission.","cs.LG,stat.AP"
"Attention-Based Learning on Molecular Ensembles. The three-dimensional shape and conformation of small-molecule ligands are
critical for biomolecular recognition, yet encoding 3D geometry has not
improved ligand-based virtual screening approaches. We describe an end-to-end
deep learning approach that operates directly on small-molecule conformational
ensembles and identifies key conformational poses of small-molecules. Our
networks leverage two levels of representation learning: 1) individual
conformers are first encoded as spatial graphs using a graph neural network,
and 2) sampled conformational ensembles are represented as sets using an
attention mechanism to aggregate over individual instances. We demonstrate the
feasibility of this approach on a simple task based on bidentate coordination
of biaryl ligands, and show how attention-based pooling can elucidate key
conformational poses in tasks based on molecular geometry. This work
illustrates how set-based learning approaches may be further developed for
small molecule-based virtual screening.","cs.LG,physics.chem-ph"
"A Brief Introduction to Machine Learning for Engineers. This monograph aims at providing an introduction to key concepts, algorithms,
and theoretical results in machine learning. The treatment concentrates on
probabilistic models for supervised and unsupervised learning problems. It
introduces fundamental concepts and algorithms by building on first principles,
while also exposing the reader to more advanced topics with extensive pointers
to the literature, within a unified notation and mathematical framework. The
material is organized according to clearly defined categories, such as
discriminative and generative models, frequentist and Bayesian approaches,
exact and approximate inference, as well as directed and undirected models.
This monograph is meant as an entry point for researchers with a background in
probability and linear algebra.","cs.IT,cs.LG,math.IT,stat.ML"
"Learning to Shape Rewards using a Game of Switching Controls. Reward shaping (RS) is a powerful method in reinforcement learning (RL) for
overcoming the problem of sparse or uninformative rewards. However, RS
typically relies on manually engineered shaping-reward functions whose
construction is time-consuming and error-prone. It also requires domain
knowledge which runs contrary to the goal of autonomous learning. We introduce
Reinforcement Learning Optimal Shaping Algorithm (ROSA), an automated RS
framework in which the shaping-reward function is constructed in a novel Markov
game between two agents. A reward-shaping agent (Shaper) uses switching
controls to determine which states to add shaping rewards and their optimal
values while the other agent (Controller) learns the optimal policy for the
task using these shaped rewards. We prove that ROSA, which easily adopts
existing RL algorithms, learns to construct a shaping-reward function that is
tailored to the task thus ensuring efficient convergence to high performance
policies. We demonstrate ROSA's congenial properties in three carefully
designed experiments and show its superior performance against state-of-the-art
RS algorithms in challenging sparse reward environments.","cs.AI,cs.GT,cs.LG"
"Causal Mediation Analysis with Hidden Confounders. An important problem in causal inference is to break down the total effect of
treatment into different causal pathways and quantify the causal effect in each
pathway. Causal mediation analysis (CMA) is a formal statistical approach for
identifying and estimating these causal effects. Central to CMA is the
sequential ignorability assumption that implies all pre-treatment confounders
are measured and they can capture different types of confounding, e.g.,
post-treatment confounders and hidden confounders. Typically unverifiable in
observational studies, this assumption restrains both the coverage and
practicality of conventional methods. This work, therefore, aims to circumvent
the stringent assumption by following a causal graph with a unified confounder
and its proxy variables. Our core contribution is an algorithm that combines
deep latent-variable models and proxy strategy to jointly infer a unified
surrogate confounder and estimate different causal effects in CMA from observed
variables. Empirical evaluations using both synthetic and semi-synthetic
datasets validate the effectiveness of the proposed method.","cs.AI,cs.CY,cs.LG,stat.ML"
"Context-aware demand prediction in bike sharing systems: incorporating spatial, meteorological and calendrical context. Bike sharing demand is increasing in large cities worldwide. The proper
functioning of bike-sharing systems is, nevertheless, dependent on a balanced
geographical distribution of bicycles throughout a day. In this context,
understanding the spatiotemporal distribution of check-ins and check-outs is
key for station balancing and bike relocation initiatives. Still, recent
contributions from deep learning and distance-based predictors show limited
success on forecasting bike sharing demand. This consistent observation is
hypothesized to be driven by: i) the strong dependence between demand and the
meteorological and situational context of stations; and ii) the absence of
spatial awareness as most predictors are unable to model the effects of
high-low station load on nearby stations.
  This work proposes a comprehensive set of new principles to incorporate both
historical and prospective sources of spatial, meteorological, situational and
calendrical context in predictive models of station demand. To this end, a new
recurrent neural network layering composed by serial long-short term memory
(LSTM) components is proposed with two major contributions: i) the feeding of
multivariate time series masks produced from historical context data at the
input layer, and ii) the time-dependent regularization of the forecasted time
series using prospective context data. This work further assesses the impact of
incorporating different sources of context, showing the relevance of the
proposed principles for the community even though not all improvements from the
context-aware predictors yield statistical significance.","68T07,I.2.6; I.5.1,cs.LG"
"ExKMC: Expanding Explainable $k$-Means Clustering. Despite the popularity of explainable AI, there is limited work on effective
methods for unsupervised learning. We study algorithms for $k$-means
clustering, focusing on a trade-off between explainability and accuracy.
Following prior work, we use a small decision tree to partition a dataset into
$k$ clusters. This enables us to explain each cluster assignment by a short
sequence of single-feature thresholds. While larger trees produce more accurate
clusterings, they also require more complex explanations. To allow flexibility,
we develop a new explainable $k$-means clustering algorithm, ExKMC, that takes
an additional parameter $k' \geq k$ and outputs a decision tree with $k'$
leaves. We use a new surrogate cost to efficiently expand the tree and to label
the leaves with one of $k$ clusters. We prove that as $k'$ increases, the
surrogate cost is non-increasing, and hence, we trade explainability for
accuracy. Empirically, we validate that ExKMC produces a low cost clustering,
outperforming both standard decision tree methods and other algorithms for
explainable clustering. Implementation of ExKMC available at
https://github.com/navefr/ExKMC.","cs.CG,cs.DS,cs.LG,stat.ML"
"Fast computation of mutual information in the frequency domain with applications to global multimodal image alignment. Multimodal image alignment is the process of finding spatial correspondences
between images formed by different imaging techniques or under different
conditions, to facilitate heterogeneous data fusion and correlative analysis.
The information-theoretic concept of mutual information (MI) is widely used as
a similarity measure to guide multimodal alignment processes, where most works
have focused on local maximization of MI that typically works well only for
small displacements; this points to a need for global maximization of MI, which
has previously been computationally infeasible due to the high run-time
complexity of existing algorithms. We propose an efficient algorithm for
computing MI for all discrete displacements (formalized as the cross-mutual
information function (CMIF)), which is based on cross-correlation computed in
the frequency domain. We show that the algorithm is equivalent to a direct
method while asymptotically superior in terms of run-time. Furthermore, we
propose a method for multimodal image alignment for transformation models with
few degrees of freedom (e.g. rigid) based on the proposed CMIF-algorithm. We
evaluate the efficacy of the proposed method on three distinct benchmark
datasets, of aerial images, cytological images, and histological images, and we
observe excellent success-rates (in recovering known rigid transformations),
overall outperforming alternative methods, including local optimization of MI
as well as several recent deep learning-based approaches. We also evaluate the
run-times of a GPU implementation of the proposed algorithm and observe
speed-ups from 100 to more than 10,000 times for realistic image sizes compared
to a GPU implementation of a direct method. Code is shared as open-source at
\url{github.com/MIDA-group/globalign}.","68U10,68W01,92C55,94A08,94A15,94A17,cs.CV"
"Preservation of Anomalous Subgroups On Machine Learning Transformed Data. In this paper, we investigate the effect of machine learning based
anonymization on anomalous subgroup preservation. In particular, we train a
binary classifier to discover the most anomalous subgroup in a dataset by
maximizing the bias between the group's predicted odds ratio from the model and
observed odds ratio from the data. We then perform anonymization using a
variational autoencoder (VAE) to synthesize an entirely new dataset that would
ideally be drawn from the distribution of the original data. We repeat the
anomalous subgroup discovery task on the new data and compare it to what was
identified pre-anonymization. We evaluated our approach using publicly
available datasets from the financial industry. Our evaluation confirmed that
the approach was able to produce synthetic datasets that preserved a high level
of subgroup differentiation as identified initially in the original dataset.
Such a distinction was maintained while having distinctly different records
between the synthetic and original dataset. Finally, we packed the above end to
end process into what we call Utility Guaranteed Deep Privacy (UGDP) system.
UGDP can be easily extended to onboard alternative generative approaches such
as GANs to synthesize tabular data.","cs.CR,cs.LG,stat.ML"
"Bayesian stochastic blockmodeling. This chapter provides a self-contained introduction to the use of Bayesian
inference to extract large-scale modular structures from network data, based on
the stochastic blockmodel (SBM), as well as its degree-corrected and
overlapping generalizations. We focus on nonparametric formulations that allow
their inference in a manner that prevents overfitting, and enables model
selection. We discuss aspects of the choice of priors, in particular how to
avoid underfitting via increased Bayesian hierarchies, and we contrast the task
of sampling network partitions from the posterior distribution with finding the
single point estimate that maximizes it, while describing efficient algorithms
to perform either one. We also show how inferring the SBM can be used to
predict missing and spurious links, and shed light on the fundamental
limitations of the detectability of modular structures in networks.","cond-mat.stat-mech,physics.data-an,stat.ML"
"Natural Color Image Enhancement based on Modified Multiscale Retinex Algorithm and Performance Evaluation usingWavelet Energy. This paper presents a new color image enhancement technique based on modified
MultiScale Retinex(MSR) algorithm and visual quality of the enhanced images are
evaluated using a new metric, namely, wavelet energy. The color image
enhancement is achieved by down sampling the value component of HSV color space
converted image into three scales (normal, medium and fine) following the
contrast stretching operation. These down sampled value components are enhanced
using the MSR algorithm. The value component is reconstructed by averaging each
pixels of the lower scale image with that of the upper scale image subsequent
to up sampling the lower scale image. This process replaces dark pixel by the
average pixels of both the lower scale and upper scale, while retaining the
bright pixels. The quality of the reconstructed images in the proposed method
is found to be good and far better then the other researchers method. The
performance of the proposed scheme is evaluated using new wavelet domain based
assessment criterion, referred as wavelet energy. This scheme computes the
energy of both original and enhanced image in wavelet domain. The number of
edge details as well as wavelet energy is less in a poor quality image compared
with naturally enhanced image. Experimental results presented confirms that the
proposed wavelet energy based color image quality assessment technique
efficiently characterizes both the local and global details of enhanced image.","68U10,I.4.3,cs.CV"
"Quantum Energy Regression using Scattering Transforms. We present a novel approach to the regression of quantum mechanical energies
based on a scattering transform of an intermediate electron density
representation. A scattering transform is a deep convolution network computed
with a cascade of multiscale wavelet transforms. It possesses appropriate
invariant and stability properties for quantum energy regression. This new
framework removes fundamental limitations of Coulomb matrix based energy
regressions, and numerical experiments give state-of-the-art accuracy over
planar molecules.","cs.CV,cs.LG,physics.chem-ph,physics.comp-ph,quant-ph"
"MeshWalker: Deep Mesh Understanding by Random Walks. Most attempts to represent 3D shapes for deep learning have focused on
volumetric grids, multi-view images and point clouds. In this paper we look at
the most popular representation of 3D shapes in computer graphics - a
triangular mesh - and ask how it can be utilized within deep learning. The few
attempts to answer this question propose to adapt convolutions & pooling to
suit Convolutional Neural Networks (CNNs). This paper proposes a very different
approach, termed MeshWalker, to learn the shape directly from a given mesh. The
key idea is to represent the mesh by random walks along the surface, which
""explore"" the mesh's geometry and topology. Each walk is organized as a list of
vertices, which in some manner imposes regularity on the mesh. The walk is fed
into a Recurrent Neural Network (RNN) that ""remembers"" the history of the walk.
We show that our approach achieves state-of-the-art results for two fundamental
shape analysis tasks: shape classification and semantic segmentation.
Furthermore, even a very small number of examples suffices for learning. This
is highly important, since large datasets of meshes are difficult to acquire.","cs.CG,cs.CV,cs.LG"
"Mix and Match: A Novel FPGA-Centric Deep Neural Network Quantization Framework. Deep Neural Networks (DNNs) have achieved extraordinary performance in
various application domains. To support diverse DNN models, efficient
implementations of DNN inference on edge-computing platforms, e.g., ASICs,
FPGAs, and embedded systems, are extensively investigated. Due to the huge
model size and computation amount, model compression is a critical step to
deploy DNN models on edge devices. This paper focuses on weight quantization, a
hardware-friendly model compression approach that is complementary to weight
pruning. Unlike existing methods that use the same quantization scheme for all
weights, we propose the first solution that applies different quantization
schemes for different rows of the weight matrix. It is motivated by (1) the
distribution of the weights in the different rows are not the same; and (2) the
potential of achieving better utilization of heterogeneous FPGA hardware
resources. To achieve that, we first propose a hardware-friendly quantization
scheme named sum-of-power-of-2 (SP2) suitable for Gaussian-like weight
distribution, in which the multiplication arithmetic can be replaced with logic
shifter and adder, thereby enabling highly efficient implementations with the
FPGA LUT resources. In contrast, the existing fixed-point quantization is
suitable for Uniform-like weight distribution and can be implemented
efficiently by DSP. Then to fully explore the resources, we propose an
FPGA-centric mixed scheme quantization (MSQ) with an ensemble of the proposed
SP2 and the fixed-point schemes. Combining the two schemes can maintain, or
even increase accuracy due to better matching with weight distributions.","68T07,cs.AR,cs.LG"
"Putting NeRF on a Diet: Semantically Consistent Few-Shot View Synthesis. We present DietNeRF, a 3D neural scene representation estimated from a few
images. Neural Radiance Fields (NeRF) learn a continuous volumetric
representation of a scene through multi-view consistency, and can be rendered
from novel viewpoints by ray casting. While NeRF has an impressive ability to
reconstruct geometry and fine details given many images, up to 100 for
challenging 360{\deg} scenes, it often finds a degenerate solution to its image
reconstruction objective when only a few input views are available. To improve
few-shot quality, we propose DietNeRF. We introduce an auxiliary semantic
consistency loss that encourages realistic renderings at novel poses. DietNeRF
is trained on individual scenes to (1) correctly render given input views from
the same pose, and (2) match high-level semantic attributes across different,
random poses. Our semantic loss allows us to supervise DietNeRF from arbitrary
poses. We extract these semantics using a pre-trained visual encoder such as
CLIP, a Vision Transformer trained on hundreds of millions of diverse
single-view, 2D photographs mined from the web with natural language
supervision. In experiments, DietNeRF improves the perceptual quality of
few-shot view synthesis when learned from scratch, can render novel views with
as few as one observed image when pre-trained on a multi-view dataset, and
produces plausible completions of completely unobserved regions.","cs.AI,cs.CV,cs.GR,cs.LG"
"Holographic Neural Architectures. Representation learning is at the heart of what makes deep learning
effective. In this work, we introduce a new framework for representation
learning that we call ""Holographic Neural Architectures"" (HNAs). In the same
way that an observer can experience the 3D structure of a holographed object by
looking at its hologram from several angles, HNAs derive Holographic
Representations from the training set. These representations can then be
explored by moving along a continuous bounded single dimension. We show that
HNAs can be used to make generative networks, state-of-the-art regression
models and that they are inherently highly resistant to noise. Finally, we
argue that because of their denoising abilities and their capacity to
generalize well from very few examples, models based upon HNAs are particularly
well suited for biological applications where training examples are rare or
noisy.","62-07l,68T05,68T30,I.2.0; I.2.4; I.2.6; G.3,cs.AI,cs.LG,q-bio.GN,q-bio.TO,stat.ML"
"Efficient, Interpretable Atomistic Graph Neural Network Representation for Angle-dependent Properties and its Application to Optical Spectroscopy Prediction. Graph neural networks (GNNs) are attractive for learning properties of atomic
structures thanks to their intuitive, physically informed graph encoding of
atoms and bonds. However, conventional GNN encodings do not account for angular
information, which is critical for describing complex atomic arrangements in
disordered materials, interfaces, and molecular distortions. In this work, we
extend the recently proposed ALIGNN encoding, which incorporates bond angles,
to also include dihedral angles (ALIGNN-d), and we apply the model to capture
the structures of aqua copper complexes for spectroscopy prediction. This
simple extension is shown to lead to a memory-efficient graph representation
capable of capturing the full geometric information of atomic structures.
Specifically, the ALIGNN-d encoding is a sparse yet equally expressive
representation compared to the dense, maximally-connected graph, in which all
bonds are encoded. We also explore model interpretability based on ALIGNN-d by
elucidating the relative contributions of individual structural components to
the optical response of the copper complexes. Lastly, we briefly discuss future
developments to validate the computational efficiency and to extend the
interpretability of ALIGNN-d.","cs.LG,physics.comp-ph"
"Heterogeneous relational message passing networks for molecular dynamics simulations. With many frameworks based on message passing neural networks proposed to
predict molecular and bulk properties, machine learning methods have
tremendously shifted the paradigms of computational sciences underpinning
physics, material science, chemistry, and biology. While existing machine
learning models have yielded superior performances in many occasions, most of
them model and process molecular systems in terms of homogeneous graph, which
severely limits the expressive power for representing diverse interactions. In
practice, graph data with multiple node and edge types is ubiquitous and more
appropriate for molecular systems. Thus, we propose the heterogeneous
relational message passing network (HermNet), an end-to-end heterogeneous graph
neural networks, to efficiently express multiple interactions in a single model
with {\it ab initio} accuracy. HermNet performs impressively against many
top-performing models on both molecular and extended systems. Specifically,
HermNet outperforms other tested models in nearly 75\%, 83\% and 94\% of tasks
on MD17, QM9 and extended systems datasets, respectively. Finally, we elucidate
how the design of HermNet is compatible with quantum mechanics from the
perspective of the density functional theory. Besides, HermNet is a universal
framework, whose sub-networks could be replaced by other advanced models.","cond-mat.dis-nn,cs.LG"
"Quantifying and Alleviating the Language Prior Problem in Visual Question Answering. Benefiting from the advancement of computer vision, natural language
processing and information retrieval techniques, visual question answering
(VQA), which aims to answer questions about an image or a video, has received
lots of attentions over the past few years. Although some progress has been
achieved so far, several studies have pointed out that current VQA models are
heavily affected by the \emph{language prior problem}, which means they tend to
answer questions based on the co-occurrence patterns of question keywords
(e.g., how many) and answers (e.g., 2) instead of understanding images and
questions. Existing methods attempt to solve this problem by either balancing
the biased datasets or forcing models to better understand images. However,
only marginal effects and even performance deterioration are observed for the
first and second solution, respectively. In addition, another important issue
is the lack of measurement to quantitatively measure the extent of the language
prior effect, which severely hinders the advancement of related techniques.
  In this paper, we make contributions to solve the above problems from two
perspectives. Firstly, we design a metric to quantitatively measure the
language prior effect of VQA models. The proposed metric has been demonstrated
to be effective in our empirical studies. Secondly, we propose a regularization
method (i.e., score regularization module) to enhance current VQA models by
alleviating the language prior problem as well as boosting the backbone model
performance. The proposed score regularization module adopts a pair-wise
learning strategy, which makes the VQA models answer the question based on the
reasoning of the image (upon this question) instead of basing on
question-answer patterns observed in the biased training set. The score
regularization module is flexible to be integrated into various VQA models.","cs.CL,cs.CV,cs.IR"
"General solutions for nonlinear differential equations: a rule-based self-learning approach using deep reinforcement learning. A universal rule-based self-learning approach using deep reinforcement
learning (DRL) is proposed for the first time to solve nonlinear ordinary
differential equations and partial differential equations. The solver consists
of a deep neural network-structured actor that outputs candidate solutions, and
a critic derived only from physical rules (governing equations and boundary and
initial conditions). Solutions in discretized time are treated as multiple
tasks sharing the same governing equation, and the current step parameters
provide an ideal initialization for the next owing to the temporal continuity
of the solutions, which shows a transfer learning characteristic and indicates
that the DRL solver has captured the intrinsic nature of the equation. The
approach is verified through solving the Schr\""odinger, Navier-Stokes,
Burgers', Van der Pol, and Lorenz equations and an equation of motion. The
results indicate that the approach gives solutions with high accuracy, and the
solution process promises to get faster.","cs.LG,math.NA,stat.ML"
"Dual Transformer for Point Cloud Analysis. Following the tremendous success of transformer in natural language
processing and image understanding tasks, in this paper, we present a novel
point cloud representation learning architecture, named Dual Transformer
Network (DTNet), which mainly consists of Dual Point Cloud Transformer (DPCT)
module. Specifically, by aggregating the well-designed point-wise and
channel-wise multi-head self-attention models simultaneously, DPCT module can
capture much richer contextual dependencies semantically from the perspective
of position and channel. With the DPCT module as a fundamental component, we
construct the DTNet for performing point cloud analysis in an end-to-end
manner. Extensive quantitative and qualitative experiments on publicly
available benchmarks demonstrate the effectiveness of our proposed transformer
framework for the tasks of 3D point cloud classification and segmentation,
achieving highly competitive performance in comparison with the
state-of-the-art approaches.","cs.CV,cs.MM"
"Provably Safe PAC-MDP Exploration Using Analogies. A key challenge in applying reinforcement learning to safety-critical domains
is understanding how to balance exploration (needed to attain good performance
on the task) with safety (needed to avoid catastrophic failure). Although a
growing line of work in reinforcement learning has investigated this area of
""safe exploration,"" most existing techniques either 1) do not guarantee safety
during the actual exploration process; and/or 2) limit the problem to a priori
known and/or deterministic transition dynamics with strong smoothness
assumptions. Addressing this gap, we propose Analogous Safe-state Exploration
(ASE), an algorithm for provably safe exploration in MDPs with unknown,
stochastic dynamics. Our method exploits analogies between state-action pairs
to safely learn a near-optimal policy in a PAC-MDP sense. Additionally, ASE
also guides exploration towards the most task-relevant states, which
empirically results in significant improvements in terms of sample efficiency,
when compared to existing methods.","cs.AI,cs.LG,stat.ML"
"Learning Multi-Agent Coordination through Graph-driven Communication. We discuss the problem of learning collaborative behaviour through
communication in multi-agent systems using deep reinforcement learning. A
connectivity-driven communication (CDC) algorithm is proposed to address three
key aspects: what agents to involve in the communication, what information
content to share, and how often to share it. The multi-agent system is modelled
as a weighted graph with nodes representing agents. The unknown edge weights
reflect the degree of communication between pairs of agents, which depends on a
diffusion process on the graph - the heat kernel. An optimal communication
strategy, tightly coupled with overall graph topology, is learned end-to-end
concurrently with the agents' policy so as to maximise future expected returns.
Empirical results show that CDC is capable of superior performance over
alternative algorithms for a range of cooperative navigation tasks, and that
the learned graph structures can be interpretable.","cs.AI,cs.LG,cs.MA,stat.ML"
"Integrating Image Captioning with Rule-based Entity Masking. Given an image, generating its natural language description (i.e., caption)
is a well studied problem. Approaches proposed to address this problem usually
rely on image features that are difficult to interpret. Particularly, these
image features are subdivided into global and local features, where global
features are extracted from the global representation of the image, while local
features are extracted from the objects detected locally in an image. Although,
local features extract rich visual information from the image, existing models
generate captions in a blackbox manner and humans have difficulty interpreting
which local objects the caption is aimed to represent. Hence in this paper, we
propose a novel framework for the image captioning with an explicit object
(e.g., knowledge graph entity) selection process while still maintaining its
end-to-end training ability. The model first explicitly selects which local
entities to include in the caption according to a human-interpretable mask,
then generate proper captions by attending to selected entities. Experiments
conducted on the MSCOCO dataset demonstrate that our method achieves good
performance in terms of the caption quality and diversity with a more
interpretable generating process than previous counterparts.","cs.CL,cs.CV,cs.LG"
"Co-training for Deep Object Detection: Comparing Single-modal and Multi-modal Approaches. Top-performing computer vision models are powered by convolutional neural
networks (CNNs). Training an accurate CNN highly depends on both the raw sensor
data and their associated ground truth (GT). Collecting such GT is usually done
through human labeling, which is time-consuming and does not scale as we wish.
This data labeling bottleneck may be intensified due to domain shifts among
image sensors, which could force per-sensor data labeling. In this paper, we
focus on the use of co-training, a semi-supervised learning (SSL) method, for
obtaining self-labeled object bounding boxes (BBs), i.e., the GT to train deep
object detectors. In particular, we assess the goodness of multi-modal
co-training by relying on two different views of an image, namely, appearance
(RGB) and estimated depth (D). Moreover, we compare appearance-based
single-modal co-training with multi-modal. Our results suggest that in a
standard SSL setting (no domain shift, a few human-labeled data) and under
virtual-to-real domain shift (many virtual-world labeled data, no human-labeled
data) multi-modal co-training outperforms single-modal. In the latter case, by
performing GAN-based domain translation both co-training modalities are on
pair; at least, when using an off-the-shelf depth estimation model not
specifically trained on the translated images.","cs.CV,cs.LG"
"Out-of-Distribution Dynamics Detection: RL-Relevant Benchmarks and Results. We study the problem of out-of-distribution dynamics (OODD) detection, which
involves detecting when the dynamics of a temporal process change compared to
the training-distribution dynamics. This is relevant to applications in
control, reinforcement learning (RL), and multi-variate time-series, where
changes to test time dynamics can impact the performance of learning
controllers/predictors in unknown ways. This problem is particularly important
in the context of deep RL, where learned controllers often overfit to the
training environment. Currently, however, there is a lack of established OODD
benchmarks for the types of environments commonly used in RL research. Our
first contribution is to design a set of OODD benchmarks derived from common RL
environments with varying types and intensities of OODD. Our second
contribution is to design a strong OODD baseline approach based on recurrent
implicit quantile networks (RIQNs), which monitors autoregressive prediction
errors for OODD detection. Our final contribution is to evaluate the RIQN
approach on the benchmarks to provide baseline results for future comparison.","cs.AI,cs.LG"
"Dissecting FLOPs along input dimensions for GreenAI cost estimations. The term GreenAI refers to a novel approach to Deep Learning, that is more
aware of the ecological impact and the computational efficiency of its methods.
The promoters of GreenAI suggested the use of Floating Point Operations (FLOPs)
as a measure of the computational cost of Neural Networks; however, that
measure does not correlate well with the energy consumption of hardware
equipped with massively parallel processing units like GPUs or TPUs. In this
article, we propose a simple refinement of the formula used to compute floating
point operations for convolutional layers, called {\alpha}-FLOPs, explaining
and correcting the traditional discrepancy with respect to different layers,
and closer to reality. The notion of {\alpha}-FLOPs relies on the crucial
insight that, in case of inputs with multiple dimensions, there is no reason to
believe that the speedup offered by parallelism will be uniform along all
different axes.","68T07,I.2,cs.LG"
"Deep Transform and Metric Learning Network: Wedding Deep Dictionary Learning and Neural Networks. On account of its many successes in inference tasks and denoising
applications, Dictionary Learning (DL) and its related sparse optimization
problems have garnered a lot of research interest. While most solutions have
focused on single layer dictionaries, the improved recently proposed Deep DL
(DDL) methods have also fallen short on a number of issues. We propose herein,
a novel DDL approach where each DL layer can be formulated as a combination of
one linear layer and a Recurrent Neural Network (RNN). The RNN is shown to
flexibly account for the layer-associated and learned metric. Our proposed work
unveils new insights into Neural Networks and DDL and provides a new, efficient
and competitive approach to jointly learn a deep transform and a metric for
inference applications. Extensive experiments are carried out to demonstrate
that the proposed method can not only outperform existing DDL but also
state-of-the-art generic CNNs.","cs.LG,stat.ML"
"Deep Neural Networks Motivated by Partial Differential Equations. Partial differential equations (PDEs) are indispensable for modeling many
physical phenomena and also commonly used for solving image processing tasks.
In the latter area, PDE-based approaches interpret image data as
discretizations of multivariate functions and the output of image processing
algorithms as solutions to certain PDEs. Posing image processing problems in
the infinite dimensional setting provides powerful tools for their analysis and
solution. Over the last few decades, the reinterpretation of classical image
processing problems through the PDE lens has been creating multiple celebrated
approaches that benefit a vast area of tasks including image segmentation,
denoising, registration, and reconstruction.
  In this paper, we establish a new PDE-interpretation of a class of deep
convolutional neural networks (CNN) that are commonly used to learn from
speech, image, and video data. Our interpretation includes convolution residual
neural networks (ResNet), which are among the most promising approaches for
tasks such as image classification having improved the state-of-the-art
performance in prestigious benchmark challenges. Despite their recent
successes, deep ResNets still face some critical challenges associated with
their design, immense computational costs and memory requirements, and lack of
understanding of their reasoning.
  Guided by well-established PDE theory, we derive three new ResNet
architectures that fall into two new classes: parabolic and hyperbolic CNNs. We
demonstrate how PDE theory can provide new insights and algorithms for deep
learning and demonstrate the competitiveness of three new CNN architectures
using numerical experiments.","65K10,68T45,cs.LG,math.OC,stat.ML"
"Learning who is in the market from time series: market participant discovery through adversarial calibration of multi-agent simulators. In electronic trading markets often only the price or volume time series,
that result from interaction of multiple market participants, are directly
observable. In order to test trading strategies before deploying them to
real-time trading, multi-agent market environments calibrated so that the time
series that result from interaction of simulated agents resemble historical are
often used. To ensure adequate testing, one must test trading strategies in a
variety of market scenarios -- which includes both scenarios that represent
ordinary market days as well as stressed markets (most recently observed due to
the beginning of COVID pandemic). In this paper, we address the problem of
multi-agent simulator parameter calibration to allow simulator capture
characteristics of different market regimes. We propose a novel two-step method
to train a discriminator that is able to distinguish between ""real"" and ""fake""
price and volume time series as a part of GAN with self-attention, and then
utilize it within an optimization framework to tune parameters of a simulator
model with known agent archetypes to represent a market scenario. We conclude
with experimental results that demonstrate effectiveness of our method.","cs.LG,cs.MA,q-fin.TR"
"A meta-algorithm for classification using random recursive tree ensembles: A high energy physics application. The aim of this work is to propose a meta-algorithm for automatic
classification in the presence of discrete binary classes. Classifier learning
in the presence of overlapping class distributions is a challenging problem in
machine learning. Overlapping classes are described by the presence of
ambiguous areas in the feature space with a high density of points belonging to
both classes. This often occurs in real-world datasets, one such example is
numeric data denoting properties of particle decays derived from high-energy
accelerators like the Large Hadron Collider (LHC). A significant body of
research targeting the class overlap problem use ensemble classifiers to boost
the performance of algorithms by using them iteratively in multiple stages or
using multiple copies of the same model on different subsets of the input
training data. The former is called boosting and the latter is called bagging.
The algorithm proposed in this thesis targets a challenging classification
problem in high energy physics - that of improving the statistical significance
of the Higgs discovery. The underlying dataset used to train the algorithm is
experimental data built from the official ATLAS full-detector simulation with
Higgs events (signal) mixed with different background events (background) that
closely mimic the statistical properties of the signal generating class
overlap. The algorithm proposed is a variant of the classical boosted decision
tree which is known to be one of the most successful analysis techniques in
experimental physics. The algorithm utilizes a unified framework that combines
two meta-learning techniques - bagging and boosting. The results show that this
combination only works in the presence of a randomization trick in the base
learners.","cs.LG,stat.AP,stat.ML"
"Factorized Graph Representations for Semi-Supervised Learning from Sparse Data. Node classification is an important problem in graph data management. It is
commonly solved by various label propagation methods that work iteratively
starting from a few labeled seed nodes. For graphs with arbitrary
compatibilities between classes, these methods crucially depend on knowing the
compatibility matrix that must be provided by either domain experts or
heuristics. Can we instead directly estimate the correct compatibilities from a
sparsely labeled graph in a principled and scalable way? We answer this
question affirmatively and suggest a method called distant compatibility
estimation that works even on extremely sparsely labeled graphs (e.g., 1 in
10,000 nodes is labeled) in a fraction of the time it later takes to label the
remaining nodes. Our approach first creates multiple factorized graph
representations (with size independent of the graph) and then performs
estimation on these smaller graph sketches. We define algebraic amplification
as the more general idea of leveraging algebraic properties of an algorithm's
update equations to amplify sparse signals. We show that our estimator is by
orders of magnitude faster than an alternative approach and that the end-to-end
classification accuracy is comparable to using gold standard compatibilities.
This makes it a cheap preprocessing step for any existing label propagation
method and removes the current dependence on heuristics.","cs.DB,cs.LG,cs.SI,stat.ML"
"Finding Non-overlapping Clusters for Generalized Inference Over Graphical Models. Graphical models use graphs to compactly capture stochastic dependencies
amongst a collection of random variables. Inference over graphical models
corresponds to finding marginal probability distributions given joint
probability distributions. In general, this is computationally intractable,
which has led to a quest for finding efficient approximate inference
algorithms. We propose a framework for generalized inference over graphical
models that can be used as a wrapper for improving the estimates of approximate
inference algorithms. Instead of applying an inference algorithm to the
original graph, we apply the inference algorithm to a block-graph, defined as a
graph in which the nodes are non-overlapping clusters of nodes from the
original graph. This results in marginal estimates of a cluster of nodes, which
we further marginalize to get the marginal estimates of each node. Our proposed
block-graph construction algorithm is simple, efficient, and motivated by the
observation that approximate inference is more accurate on graphs with longer
cycles. We present extensive numerical simulations that illustrate our
block-graph framework with a variety of inference algorithms (e.g., those in
the libDAI software package). These simulations show the improvements provided
by our framework.","cs.IT,math.IT,stat.ML"
"Causal Inference Q-Network: Toward Resilient Reinforcement Learning. Deep reinforcement learning (DRL) has demonstrated impressive performance in
various gaming simulators and real-world applications. In practice, however, a
DRL agent may receive faulty observation by abrupt interferences such as
black-out, frozen-screen, and adversarial perturbation. How to design a
resilient DRL algorithm against these rare but mission-critical and
safety-crucial scenarios is an important yet challenging task. In this paper,
we consider a generative DRL framework training with an auxiliary task of
observational interferences such as artificial noises. Under this framework, we
discuss the importance of the causal relation and propose a causal inference
based DRL algorithm called causal inference Q-network (CIQ). We evaluate the
performance of CIQ in several benchmark DRL environments with different types
of interferences as auxiliary labels. Our experimental results show that the
proposed CIQ method could achieve higher performance and more resilience
against observational interferences.","cs.AI,cs.LG,cs.NE,cs.RO,cs.SY,eess.SY"
"Vision: A Deep Learning Approach to provide walking assistance to the visually impaired. Blind people face a lot of problems in their daily routines. They have to
struggle a lot just to do their day-to-day chores. In this paper, we have
proposed a system with the objective to help the visually impaired by providing
audio aid guiding them to avoid obstacles, which will assist them to move in
their surroundings. Object Detection using YOLO will help them detect the
nearby objects and Depth Estimation using monocular vision will tell the
approximate distance of the detected objects from the user. Despite a higher
accuracy, stereo vision has many hardware constraints, which makes monocular
vision the preferred choice for this application.","68T45,cs.CV,cs.LG"
"Physics-informed Dyna-Style Model-Based Deep Reinforcement Learning for Dynamic Control. Model-based reinforcement learning (MBRL) is believed to have much higher
sample efficiency compared to model-free algorithms by learning a predictive
model of the environment. However, the performance of MBRL highly relies on the
quality of the learned model, which is usually built in a black-box manner and
may have poor predictive accuracy outside of the data distribution. The
deficiencies of the learned model may prevent the policy from being fully
optimized. Although some uncertainty analysis-based remedies have been proposed
to alleviate this issue, model bias still poses a great challenge for MBRL. In
this work, we propose to leverage the prior knowledge of underlying physics of
the environment, where the governing laws are (partially) known. In particular,
we developed a physics-informed MBRL framework, where governing equations and
physical constraints are utilized to inform the model learning and policy
search. By incorporating the prior information of the environment, the quality
of the learned model can be notably improved, while the required interactions
with the environment are significantly reduced, leading to better sample
efficiency and learning performance. The effectiveness and merit have been
demonstrated over a handful of classic control problems, where the environments
are governed by canonical ordinary/partial differential equations.","cs.LG,math.OC"
"Sampling Techniques in Bayesian Target Encoding. Target encoding is an effective encoding technique of categorical variables
and is often used in machine learning systems for processing tabular data sets
with mixed numeric and categorical variables. Recently en enhanced version of
this encoding technique was proposed by using conjugate Bayesian modeling. This
paper presents a further development of Bayesian encoding method by using
sampling techniques, which helps in extracting information from intra-category
distribution of the target variable, improves generalization and reduces target
leakage.","cs.DM,cs.LG,stat.ML"
"On Multimarginal Partial Optimal Transport: Equivalent Forms and Computational Complexity. We study the multi-marginal partial optimal transport (POT) problem between
$m$ discrete (unbalanced) measures with at most $n$ supports. We first prove
that we can obtain two equivalence forms of the multimarginal POT problem in
terms of the multimarginal optimal transport problem via novel extensions of
cost tensor. The first equivalence form is derived under the assumptions that
the total masses of each measure are sufficiently close while the second
equivalence form does not require any conditions on these masses but at the
price of more sophisticated extended cost tensor. Our proof techniques for
obtaining these equivalence forms rely on novel procedures of moving mass in
graph theory to push transportation plan into appropriate regions. Finally,
based on the equivalence forms, we develop optimization algorithm, named
ApproxMPOT algorithm, that builds upon the Sinkhorn algorithm for solving the
entropic regularized multimarginal optimal transport. We demonstrate that the
ApproxMPOT algorithm can approximate the optimal value of multimarginal POT
problem with a computational complexity upper bound of the order
$\tilde{\mathcal{O}}(m^3(n+1)^{m}/ \varepsilon^2)$ where $\varepsilon > 0$
stands for the desired tolerance.","cs.DS,cs.LG,math.OC,stat.CO,stat.ML"
"Unsupervised Heterogeneous Coupling Learning for Categorical Representation. Complex categorical data is often hierarchically coupled with heterogeneous
relationships between attributes and attribute values and the couplings between
objects. Such value-to-object couplings are heterogeneous with complementary
and inconsistent interactions and distributions. Limited research exists on
unlabeled categorical data representations, ignores the heterogeneous and
hierarchical couplings, underestimates data characteristics and complexities,
and overuses redundant information, etc. The deep representation learning of
unlabeled categorical data is challenging, overseeing such value-to-object
couplings, complementarity and inconsistency, and requiring large data,
disentanglement, and high computational power. This work introduces a shallow
but powerful UNsupervised heTerogeneous couplIng lEarning (UNTIE) approach for
representing coupled categorical data by untying the interactions between
couplings and revealing heterogeneous distributions embedded in each type of
couplings. UNTIE is efficiently optimized w.r.t. a kernel k-means objective
function for unsupervised representation learning of heterogeneous and
hierarchical value-to-object couplings. Theoretical analysis shows that UNTIE
can represent categorical data with maximal separability while effectively
represent heterogeneous couplings and disclose their roles in categorical data.
The UNTIE-learned representations make significant performance improvement
against the state-of-the-art categorical representations and deep
representation models on 25 categorical data sets with diversified
characteristics.","cs.LG,stat.ML"
"High throughput screening with machine learning. This study assesses the efficiency of several popular machine learning
approaches in the prediction of molecular binding affinity: CatBoost, Graph
Attention Neural Network, and Bidirectional Encoder Representations from
Transformers. The models were trained to predict binding affinities in terms of
inhibition constants $K_i$ for pairs of proteins and small organic molecules.
First two approaches use thoroughly selected physico-chemical features, while
the third one is based on textual molecular representations - it is one of the
first attempts to apply Transformer-based predictors for the binding affinity.
We also discuss the visualization of attention layers within the Transformer
approach in order to highlight the molecular sites responsible for
interactions. All approaches are free from atomic spatial coordinates thus
avoiding bias from known structures and being able to generalize for compounds
with unknown conformations. The achieved accuracy for all suggested approaches
prove their potential in high throughput screening.","cs.LG,physics.chem-ph"
"PhyCRNet: Physics-informed Convolutional-Recurrent Network for Solving Spatiotemporal PDEs. Partial differential equations (PDEs) play a fundamental role in modeling and
simulating problems across a wide range of disciplines. Recent advances in deep
learning have shown the great potential of physics-informed neural networks
(PINNs) to solve PDEs as a basis for data-driven modeling and inverse analysis.
However, the majority of existing PINN methods, based on fully-connected NNs,
pose intrinsic limitations to low-dimensional spatiotemporal parameterizations.
Moreover, since the initial/boundary conditions (I/BCs) are softly imposed via
penalty, the solution quality heavily relies on hyperparameter tuning. To this
end, we propose the novel physics-informed convolutional-recurrent learning
architectures (PhyCRNet and PhyCRNet-s) for solving PDEs without any labeled
data. Specifically, an encoder-decoder convolutional long short-term memory
network is proposed for low-dimensional spatial feature extraction and temporal
evolution learning. The loss function is defined as the aggregated discretized
PDE residuals, while the I/BCs are hard-encoded in the network to ensure
forcible satisfaction (e.g., periodic boundary padding). The networks are
further enhanced by autoregressive and residual connections that explicitly
simulate time marching. The performance of our proposed methods has been
assessed by solving three nonlinear PDEs (e.g., 2D Burgers' equations, the
$\lambda$-$\omega$ and FitzHugh Nagumo reaction-diffusion equations), and
compared against the start-of-the-art baseline algorithms. The numerical
results demonstrate the superiority of our proposed methodology in the context
of solution accuracy, extrapolability and generalizability.","cs.CL,cs.LG,cs.NA,math.NA"
"Relax, no need to round: integrality of clustering formulations. We study exact recovery conditions for convex relaxations of point cloud
clustering problems, focusing on two of the most common optimization problems
for unsupervised clustering: $k$-means and $k$-median clustering. Motivations
for focusing on convex relaxations are: (a) they come with a certificate of
optimality, and (b) they are generic tools which are relatively parameter-free,
not tailored to specific assumptions over the input. More precisely, we
consider the distributional setting where there are $k$ clusters in
$\mathbb{R}^m$ and data from each cluster consists of $n$ points sampled from a
symmetric distribution within a ball of unit radius. We ask: what is the
minimal separation distance between cluster centers needed for convex
relaxations to exactly recover these $k$ clusters as the optimal integral
solution? For the $k$-median linear programming relaxation we show a tight
bound: exact recovery is obtained given arbitrarily small pairwise separation
$\epsilon > 0$ between the balls. In other words, the pairwise center
separation is $\Delta > 2+\epsilon$. Under the same distributional model, the
$k$-means LP relaxation fails to recover such clusters at separation as large
as $\Delta = 4$. Yet, if we enforce PSD constraints on the $k$-means LP, we get
exact cluster recovery at center separation $\Delta > 2\sqrt2(1+\sqrt{1/m})$.
In contrast, common heuristics such as Lloyd's algorithm (a.k.a. the $k$-means
algorithm) can fail to recover clusters in this setting; even with arbitrarily
large cluster separation, k-means++ with overseeding by any constant factor
fails with high probability at exact cluster recovery. To complement the
theoretical analysis, we provide an experimental study of the recovery
guarantees for these various methods, and discuss several open problems which
these experiments suggest.","cs.DS,cs.LG,math.ST,stat.ML,stat.TH"
"AdaFlow: Domain-Adaptive Density Estimator with Application to Anomaly Detection and Unpaired Cross-Domain Translation. We tackle unsupervised anomaly detection (UAD), a problem of detecting data
that significantly differ from normal data. UAD is typically solved by using
density estimation. Recently, deep neural network (DNN)-based density
estimators, such as Normalizing Flows, have been attracting attention. However,
one of their drawbacks is the difficulty in adapting them to the change in the
normal data's distribution. To address this difficulty, we propose AdaFlow, a
new DNN-based density estimator that can be easily adapted to the change of the
distribution. AdaFlow is a unified model of a Normalizing Flow and Adaptive
Batch-Normalizations, a module that enables DNNs to adapt to new distributions.
AdaFlow can be adapted to a new distribution by just conducting forward
propagation once per sample; hence, it can be used on devices that have limited
computational resources. We have confirmed the effectiveness of the proposed
model through an anomaly detection in a sound task. We also propose a method of
applying AdaFlow to the unpaired cross-domain translation problem, in which one
has to train a cross-domain translation model with only unpaired samples. We
have confirmed that our model can be used for the cross-domain translation
problem through experiments on image datasets.","cs.LG,cs.SD,eess.AS,stat.ML"
"Modelling the influence of data structure on learning in neural networks: the hidden manifold model. Understanding the reasons for the success of deep neural networks trained
using stochastic gradient-based methods is a key open problem for the nascent
theory of deep learning. The types of data where these networks are most
successful, such as images or sequences of speech, are characterised by
intricate correlations. Yet, most theoretical work on neural networks does not
explicitly model training data, or assumes that elements of each data sample
are drawn independently from some factorised probability distribution. These
approaches are thus by construction blind to the correlation structure of
real-world data sets and their impact on learning in neural networks. Here, we
introduce a generative model for structured data sets that we call the hidden
manifold model (HMM). The idea is to construct high-dimensional inputs that lie
on a lower-dimensional manifold, with labels that depend only on their position
within this manifold, akin to a single layer decoder or generator in a
generative adversarial network. We demonstrate that learning of the hidden
manifold model is amenable to an analytical treatment by proving a ""Gaussian
Equivalence Property"" (GEP), and we use the GEP to show how the dynamics of
two-layer neural networks trained using one-pass stochastic gradient descent is
captured by a set of integro-differential equations that track the performance
of the network at all times. This permits us to analyse in detail how a neural
network learns functions of increasing complexity during training, how its
performance depends on its size and how it is impacted by parameters such as
the learning rate or the dimension of the hidden manifold.","cond-mat.dis-nn,cond-mat.stat-mech,cs.LG,stat.ML"
"Feature Selection for Learning to Predict Outcomes of Compute Cluster Jobs with Application to Decision Support. We present a machine learning framework and a new test bed for data mining
from the Slurm Workload Manager for high-performance computing (HPC) clusters.
The focus was to find a method for selecting features to support decisions:
helping users decide whether to resubmit failed jobs with boosted CPU and
memory allocations or migrate them to a computing cloud. This task was cast as
both supervised classification and regression learning, specifically,
sequential problem solving suitable for reinforcement learning. Selecting
relevant features can improve training accuracy, reduce training time, and
produce a more comprehensible model, with an intelligent system that can
explain predictions and inferences. We present a supervised learning model
trained on a Simple Linux Utility for Resource Management (Slurm) data set of
HPC jobs using three different techniques for selecting features: linear
regression, lasso, and ridge regression. Our data set represented both HPC jobs
that failed and those that succeeded, so our model was reliable, less likely to
overfit, and generalizable. Our model achieved an R^2 of 95\% with 99\%
accuracy. We identified five predictors for both CPU and memory properties.","I.2.11,I.2.6,I.2.6; I.2.11,cs.AI,cs.LG"
"Data-driven control of room temperature and bidirectional EV charging using deep reinforcement learning: simulations and experiments. This work presents a fully data-driven, black-box pipeline to obtain an
optimal control policy for a multi-loop building control problem based on
historical building and weather data, thus without the need for complex
physics-based modelling. We demonstrate the method for joint control of room
temperature and bidirectional EV charging to maximize the occupant thermal
comfort and energy savings while leaving enough energy in the EV battery for
the next trip. We modelled the room temperature with a recurrent neural network
and EV charging with a piece-wise linear function. Using these models as a
simulation environment, we applied a deep reinforcement learning (DRL)
algorithm to obtain an optimal control policy. The learnt policy achieves on
average 17% energy savings over the heating season and 19% better comfort
satisfaction than a standard RB room temperature controller. When a
bidirectional EV is additionally connected and a two-tariff electricity pricing
is applied, the MIMO DRL policy successfully leverages the battery and
decreases the overall cost of electricity compared to two standard RB
controllers, one controlling the room temperature and another controlling the
bidirectional EV (dis-)charging. Finally, we demonstrate a successful transfer
of the learnt DRL policy from simulation onto a real building, the DFAB HOUSE
at Empa Duebendorf in Switzerland, achieving up to 30% energy savings while
maintaining similar comfort levels compared to a conventional RB room
temperature controller over three weeks during the heating season.","cs.LG,cs.SY,eess.SY"
"Deep Neural Networks Are Congestion Games: From Loss Landscape to Wardrop Equilibrium and Beyond. The theoretical analysis of deep neural networks (DNN) is arguably among the
most challenging research directions in machine learning (ML) right now, as it
requires from scientists to lay novel statistical learning foundations to
explain their behaviour in practice. While some success has been achieved
recently in this endeavour, the question on whether DNNs can be analyzed using
the tools from other scientific fields outside the ML community has not
received the attention it may well have deserved. In this paper, we explore the
interplay between DNNs and game theory (GT), and show how one can benefit from
the classic readily available results from the latter when analyzing the
former. In particular, we consider the widely studied class of congestion
games, and illustrate their intrinsic relatedness to both linear and non-linear
DNNs and to the properties of their loss surface. Beyond retrieving the
state-of-the-art results from the literature, we argue that our work provides a
very promising novel tool for analyzing the DNNs and support this claim by
proposing concrete open problems that can advance significantly our
understanding of DNNs when solved.","cs.GT,cs.LG,stat.ML"
"CORSAIR: Convolutional Object Retrieval and Symmetry-AIded Registration. This paper considers online object-level mapping using partial point-cloud
observations obtained online in an unknown environment. We develop and approach
for fully Convolutional Object Retrieval and Symmetry-AIded Registration
(CORSAIR). Our model extends the Fully Convolutional Geometric Features model
to learn a global object-shape embedding in addition to local point-wise
features from the point-cloud observations. The global feature is used to
retrieve a similar object from a category database, and the local features are
used for robust pose registration between the observed and the retrieved
object. Our formulation also leverages symmetries, present in the object
shapes, to obtain promising local-feature pairs from different symmetry classes
for matching. We present results from synthetic and real-world datasets with
different object categories to verify the robustness of our method.","cs.CV,cs.RO"
"Social Determinants of Recidivism: A Machine Learning Solution. In criminal justice analytics, the widely-studied problem of recidivism
prediction (forecasting re-offenses after release or parole) is fraught with
ethical missteps. In particular, Machine Learning (ML) models rely on
historical patterns of behavior to predict future outcomes, engendering a
vicious feedback loop of recidivism and incarceration. This paper repurposes ML
to instead identify social factors that can serve as levers to prevent
recidivism. Our contributions are along three dimensions. (1) Recidivism models
typically agglomerate individuals into one dataset, but we invoke unsupervised
learning to extract homogeneous subgroups with similar features. (2) We then
apply subgroup-level supervised learning to determine factors correlated to
recidivism. (3) We therefore shift the focus from predicting which individuals
will re-offend to identifying broader underlying factors that explain
recidivism, with the goal of informing preventative policy intervention. We
demonstrate that this approach can guide the ethical application of ML using
real-world data.","cs.CY,cs.LG,stat.AP"
"Strong Black-box Adversarial Attacks on Unsupervised Machine Learning Models. Machine Learning (ML) and Deep Learning (DL) models have achieved
state-of-the-art performance on multiple learning tasks, from vision to natural
language modelling. With the growing adoption of ML and DL to many areas of
computer science, recent research has also started focusing on the security
properties of these models. There has been a lot of work undertaken to
understand if (deep) neural network architectures are resilient to black-box
adversarial attacks which craft perturbed input samples that fool the
classifier without knowing the architecture used. Recent work has also focused
on the transferability of adversarial attacks and found that adversarial
attacks are generally easily transferable between models, datasets, and
techniques. However, such attacks and their analysis have not been covered from
the perspective of unsupervised machine learning algorithms. In this paper, we
seek to bridge this gap through multiple contributions. We first provide a
strong (iterative) black-box adversarial attack that can craft adversarial
samples which will be incorrectly clustered irrespective of the choice of
clustering algorithm. We choose 4 prominent clustering algorithms, and a
real-world dataset to show the working of the proposed adversarial algorithm.
Using these clustering algorithms we also carry out a simple study of
cross-technique adversarial attack transferability.","cs.CR,cs.LG,stat.ML"
"Imperceptible Adversarial Attacks on Tabular Data. Security of machine learning models is a concern as they may face adversarial
attacks for unwarranted advantageous decisions. While research on the topic has
mainly been focusing on the image domain, numerous industrial applications, in
particular in finance, rely on standard tabular data. In this paper, we discuss
the notion of adversarial examples in the tabular domain. We propose a
formalization based on the imperceptibility of attacks in the tabular domain
leading to an approach to generate imperceptible adversarial examples.
Experiments show that we can generate imperceptible adversarial examples with a
high fooling rate.","cs.CR,cs.LG,stat.ML"
"A Matrix Chernoff Bound for Markov Chains and Its Application to Co-occurrence Matrices. We prove a Chernoff-type bound for sums of matrix-valued random variables
sampled via a regular (aperiodic and irreducible) finite Markov chain.
Specially, consider a random walk on a regular Markov chain and a Hermitian
matrix-valued function on its state space. Our result gives exponentially
decreasing bounds on the tail distributions of the extreme eigenvalues of the
sample mean matrix. Our proof is based on the matrix expander (regular
undirected graph) Chernoff bound [Garg et al. STOC '18] and scalar
Chernoff-Hoeffding bounds for Markov chains [Chung et al. STACS '12].
  Our matrix Chernoff bound for Markov chains can be applied to analyze the
behavior of co-occurrence statistics for sequential data, which have been
common and important data signals in machine learning. We show that given a
regular Markov chain with $n$ states and mixing time $\tau$, we need a
trajectory of length $O(\tau (\log{(n)}+\log{(\tau)})/\epsilon^2)$ to achieve
an estimator of the co-occurrence matrix with error bound $\epsilon$. We
conduct several experiments and the experimental results are consistent with
the exponentially fast convergence rate from theoretical analysis. Our result
gives the first bound on the convergence rate of the co-occurrence matrix and
the first sample complexity analysis in graph representation learning.","cs.LG,math.PR,stat.ML"
"Minimax Lower Bounds for Transfer Learning with Linear and One-hidden Layer Neural Networks. Transfer learning has emerged as a powerful technique for improving the
performance of machine learning models on new domains where labeled training
data may be scarce. In this approach a model trained for a source task, where
plenty of labeled training data is available, is used as a starting point for
training a model on a related target task with only few labeled training data.
Despite recent empirical success of transfer learning approaches, the benefits
and fundamental limits of transfer learning are poorly understood. In this
paper we develop a statistical minimax framework to characterize the
fundamental limits of transfer learning in the context of regression with
linear and one-hidden layer neural network models. Specifically, we derive a
lower-bound for the target generalization error achievable by any algorithm as
a function of the number of labeled source and target data as well as
appropriate notions of similarity between the source and target tasks. Our
lower bound provides new insights into the benefits and limitations of transfer
learning. We further corroborate our theoretical finding with various
experiments.","cs.IT,cs.LG,math.IT,stat.ML"
"Penalty Constraints and Kernelization of M-Estimation Based Fuzzy C-Means. A framework of M-estimation based fuzzy C-means clustering (MFCM) algorithm
is proposed with iterative reweighted least squares (IRLS) algorithm, and
penalty constraint and kernelization extensions of MFCM algorithms are also
developed. Introducing penalty information to the object functions of MFCM
algorithms, the spatially constrained fuzzy C-means (SFCM) is extended to
penalty constraints MFCM algorithms(abbr. pMFCM).Substituting the Euclidean
distance with kernel method, the MFCM and pMFCM algorithms are extended to
kernelized MFCM (abbr. KMFCM) and kernelized pMFCM (abbr.pKMFCM) algorithms.
The performances of MFCM, pMFCM, KMFCM and pKMFCM algorithms are evaluated in
three tasks: pattern recognition on 10 standard data sets from UCI Machine
Learning databases, noise image segmentation performances on a synthetic image,
a magnetic resonance brain image (MRI), and image segmentation of a standard
images from Berkeley Segmentation Dataset and Benchmark. The experimental
results demonstrate the effectiveness of our proposed algorithms in pattern
recognition and image segmentation.","cs.CV,stat.CO"
"Logarithmic landscape and power-law escape rate of SGD. Stochastic gradient descent (SGD) undergoes complicated multiplicative noise
for the mean-square loss. We use this property of the SGD noise to derive a
stochastic differential equation (SDE) with simpler additive noise by
performing a non-uniform transformation of the time variable. In the SDE, the
gradient of the loss is replaced by that of the logarithmized loss.
Consequently, we show that, near a local or global minimum, the stationary
distribution $P_\mathrm{ss}(\theta)$ of the network parameters $\theta$ follows
a power-law with respect to the loss function $L(\theta)$, i.e.
$P_\mathrm{ss}(\theta)\propto L(\theta)^{-\phi}$ with the exponent $\phi$
specified by the mini-batch size, the learning rate, and the Hessian at the
minimum. We obtain the escape rate formula from a local minimum, which is
determined not by the loss barrier height $\Delta L=L(\theta^s)-L(\theta^*)$
between a minimum $\theta^*$ and a saddle $\theta^s$ but by the logarithmized
loss barrier height $\Delta\log L=\log[L(\theta^s)/L(\theta^*)]$. Our
escape-rate formula explains an empirical fact that SGD prefers flat minima
with low effective dimensions.","cond-mat.dis-nn,cond-mat.stat-mech,cs.LG,stat.ML"
"Do Gradient-based Explanations Tell Anything About Adversarial Robustness to Android Malware?. While machine-learning algorithms have demonstrated a strong ability in
detecting Android malware, they can be evaded by sparse evasion attacks crafted
by injecting a small set of fake components, e.g., permissions and system
calls, without compromising intrusive functionality. Previous work has shown
that, to improve robustness against such attacks, learning algorithms should
avoid overemphasizing few discriminant features, providing instead decisions
that rely upon a large subset of components. In this work, we investigate
whether gradient-based attribution methods, used to explain classifiers'
decisions by identifying the most relevant features, can be used to help
identify and select more robust algorithms. To this end, we propose to exploit
two different metrics that represent the evenness of explanations, and a new
compact security measure called Adversarial Robustness Metric. Our experiments
conducted on two different datasets and five classification algorithms for
Android malware detection show that a strong connection exists between the
uniformity of explanations and adversarial robustness. In particular, we found
that popular techniques like Gradient*Input and Integrated Gradients are
strongly correlated to security when applied to both linear and nonlinear
detectors, while more elementary explanation techniques like the simple
Gradient do not provide reliable information about the robustness of such
classifiers.","cs.CR,cs.LG,stat.ML"
"Analysis of the Optimization Landscapes for Overcomplete Representation Learning. We study nonconvex optimization landscapes for learning overcomplete
representations, including learning (i) sparsely used overcomplete dictionaries
and (ii) convolutional dictionaries, where these unsupervised learning problems
find many applications in high-dimensional data analysis. Despite the empirical
success of simple nonconvex algorithms, theoretical justifications of why these
methods work so well are far from satisfactory. In this work, we show these
problems can be formulated as $\ell^4$-norm optimization problems with
spherical constraint, and study the geometric properties of their nonconvex
optimization landscapes. For both problems, we show the nonconvex objectives
have benign (global) geometric structures, in the sense that every local
minimizer is close to one of the target solutions and every saddle point
exhibits negative curvature. This discovery enables the development of
guaranteed global optimization methods using simple initializations. For both
problems, we show the nonconvex objectives have benign geometric structures --
every local minimizer is close to one of the target solutions and every saddle
point exhibits negative curvature -- either in the entire space or within a
sufficiently large region. This discovery ensures local search algorithms (such
as Riemannian gradient descent) with simple initializations approximately find
the target solutions. Finally, numerical experiments justify our theoretical
discoveries.","cs.IT,cs.LG,eess.SP,math.IT,math.OC,stat.ML"
"Molecule Attention Transformer. Designing a single neural network architecture that performs competitively
across a range of molecule property prediction tasks remains largely an open
challenge, and its solution may unlock a widespread use of deep learning in the
drug discovery industry. To move towards this goal, we propose Molecule
Attention Transformer (MAT). Our key innovation is to augment the attention
mechanism in Transformer using inter-atomic distances and the molecular graph
structure. Experiments show that MAT performs competitively on a diverse set of
molecular prediction tasks. Most importantly, with a simple self-supervised
pretraining, MAT requires tuning of only a few hyperparameter values to achieve
state-of-the-art performance on downstream tasks. Finally, we show that
attention weights learned by MAT are interpretable from the chemical point of
view.","cs.LG,physics.comp-ph,stat.ML"
"3D for Free: Crossmodal Transfer Learning using HD Maps. 3D object detection is a core perceptual challenge for robotics and
autonomous driving. However, the class-taxonomies in modern autonomous driving
datasets are significantly smaller than many influential 2D detection datasets.
In this work, we address the long-tail problem by leveraging both the large
class-taxonomies of modern 2D datasets and the robustness of state-of-the-art
2D detection methods. We proceed to mine a large, unlabeled dataset of images
and LiDAR, and estimate 3D object bounding cuboids, seeded from an
off-the-shelf 2D instance segmentation model. Critically, we constrain this
ill-posed 2D-to-3D mapping by using high-definition maps and object size
priors. The result of the mining process is 3D cuboids with varying confidence.
This mining process is itself a 3D object detector, although not especially
accurate when evaluated as such. However, we then train a 3D object detection
model on these cuboids, consistent with other recent observations in the deep
learning literature, we find that the resulting model is fairly robust to the
noisy supervision that our mining process provides. We mine a collection of
1151 unlabeled, multimodal driving logs from an autonomous vehicle and use the
discovered objects to train a LiDAR-based object detector. We show that
detector performance increases as we mine more unlabeled data. With our full,
unlabeled dataset, our method performs competitively with fully supervised
methods, even exceeding the performance for certain object categories, without
any human 3D annotations.","cs.AI,cs.CV,cs.LG"
"Automatic design of novel potential 3CL$^{\text{pro}}$ and PL$^{\text{pro}}$ inhibitors. With the goal of designing novel inhibitors for SARS-CoV-1 and SARS-CoV-2, we
propose the general molecule optimization framework, Molecular Neural Assay
Search (MONAS), consisting of three components: a property predictor which
identifies molecules with specific desirable properties, an energy model which
approximates the statistical similarity of a given molecule to known training
molecules, and a molecule search method. In this work, these components are
instantiated with graph neural networks (GNNs), Deep Energy Estimator Networks
(DEEN) and Monte Carlo tree search (MCTS), respectively. This implementation is
used to identify 120K molecules (out of 40-million explored) which the GNN
determined to be likely SARS-CoV-1 inhibitors, and, at the same time, are
statistically close to the dataset used to train the GNN.","cs.AI,cs.LG,q-bio.QM"
"Video Exploration via Video-Specific Autoencoders. We present simple video-specific autoencoders that enables human-controllable
video exploration. This includes a wide variety of analytic tasks such as (but
not limited to) spatial and temporal super-resolution, spatial and temporal
editing, object removal, video textures, average video exploration, and
correspondence estimation within and across videos. Prior work has
independently looked at each of these problems and proposed different
formulations. In this work, we observe that a simple autoencoder trained (from
scratch) on multiple frames of a specific video enables one to perform a large
variety of video processing and editing tasks. Our tasks are enabled by two key
observations: (1) latent codes learned by the autoencoder capture spatial and
temporal properties of that video and (2) autoencoders can project
out-of-sample inputs onto the video-specific manifold. For e.g. (1)
interpolating latent codes enables temporal super-resolution and
user-controllable video textures; (2) manifold reprojection enables spatial
super-resolution, object removal, and denoising without training for any of the
tasks. Importantly, a two-dimensional visualization of latent codes via
principal component analysis acts as a tool for users to both visualize and
intuitively control video edits. Finally, we quantitatively contrast our
approach with the prior art and found that without any supervision and
task-specific knowledge, our approach can perform comparably to supervised
approaches specifically trained for a task.","cs.CV,cs.GR,cs.HC,cs.LG"
"Comparison of Multi-agent and Single-agent Inverse Learning on a Simulated Soccer Example. We compare the performance of Inverse Reinforcement Learning (IRL) with the
relative new model of Multi-agent Inverse Reinforcement Learning (MIRL). Before
comparing the methods, we extend a published Bayesian IRL approach that is only
applicable to the case where the reward is only state dependent to a general
one capable of tackling the case where the reward depends on both state and
action. Comparison between IRL and MIRL is made in the context of an abstract
soccer game, using both a game model in which the reward depends only on state
and one in which it depends on both state and action. Results suggest that the
IRL approach performs much worse than the MIRL approach. We speculate that the
underperformance of IRL is because it fails to capture equilibrium information
in the manner possible in MIRL.","cs.GT,cs.LG"
"Reinforcement Learning for on-line Sequence Transformation. A number of problems in the processing of sound and natural language, as well
as in other areas, can be reduced to simultaneously reading an input sequence
and writing an output sequence of generally different length. There are well
developed methods that produce the output sequence based on the entirely known
input. However, efficient methods that enable such transformations on-line do
not exist. In this paper we introduce an architecture that learns with
reinforcement to make decisions about whether to read a token or write another
token. This architecture is able to transform potentially infinite sequences
on-line. In an experimental study we compare it with state-of-the-art methods
for neural machine translation. While it produces slightly worse translations
than Transformer, it outperforms the autoencoder with attention, even though
our architecture translates texts on-line thereby solving a more difficult
problem than both reference methods.","I.2.6,cs.CL,cs.LG"
"Quantum Continual Learning Overcoming Catastrophic Forgetting. Catastrophic forgetting describes the fact that machine learning models will
likely forget the knowledge of previously learned tasks after the learning
process of a new one. It is a vital problem in the continual learning scenario
and recently has attracted tremendous concern across different communities. In
this paper, we explore the catastrophic forgetting phenomena in the context of
quantum machine learning. We find that, similar to those classical learning
models based on neural networks, quantum learning systems likewise suffer from
such forgetting problem in classification tasks emerging from various
application scenes. We show that based on the local geometrical information in
the loss function landscape of the trained model, a uniform strategy can be
adapted to overcome the forgetting problem in the incremental learning setting.
Our results uncover the catastrophic forgetting phenomena in quantum machine
learning and offer a practical method to overcome this problem, which opens a
new avenue for exploring potential quantum advantages towards continual
learning.","cond-mat.mes-hall,cs.LG,quant-ph"
"A Practical Method for Constructing Equivariant Multilayer Perceptrons for Arbitrary Matrix Groups. Symmetries and equivariance are fundamental to the generalization of neural
networks on domains such as images, graphs, and point clouds. Existing work has
primarily focused on a small number of groups, such as the translation,
rotation, and permutation groups. In this work we provide a completely general
algorithm for solving for the equivariant layers of matrix groups. In addition
to recovering solutions from other works as special cases, we construct
multilayer perceptrons equivariant to multiple groups that have never been
tackled before, including $\mathrm{O}(1,3)$, $\mathrm{O}(5)$, $\mathrm{Sp}(n)$,
and the Rubik's cube group. Our approach outperforms non-equivariant baselines,
with applications to particle physics and dynamical systems. We release our
software library to enable researchers to construct equivariant layers for
arbitrary matrix groups.","cs.LG,math.DS,stat.ML"
"Towards interpreting computer vision based on transformation invariant optimization. Interpreting how does deep neural networks (DNNs) make predictions is a vital
field in artificial intelligence, which hinders wide applications of DNNs.
Visualization of learned representations helps we humans understand the vision
of DNNs. In this work, visualized images that can activate the neural network
to the target classes are generated by back-propagation method. Here, rotation
and scaling operations are applied to introduce the transformation invariance
in the image generating process, which we find a significant improvement on
visualization effect. Finally, we show some cases that such method can help us
to gain insight into neural networks.","68T45,I.2.10,cs.CV"
"DistGNN: Scalable Distributed Training for Large-Scale Graph Neural Networks. Full-batch training on Graph Neural Networks (GNN) to learn the structure of
large graphs is a critical problem that needs to scale to hundreds of compute
nodes to be feasible. It is challenging due to large memory capacity and
bandwidth requirements on a single compute node and high communication volumes
across multiple nodes. In this paper, we present DistGNN that optimizes the
well-known Deep Graph Library (DGL) for full-batch training on CPU clusters via
an efficient shared memory implementation, communication reduction using a
minimum vertex-cut graph partitioning algorithm and communication avoidance
using a family of delayed-update algorithms. Our results on four common GNN
benchmark datasets: Reddit, OGB-Products, OGB-Papers and Proteins, show up to
3.7x speed-up using a single CPU socket and up to 97x speed-up using 128 CPU
sockets, respectively, over baseline DGL implementations running on a single
CPU socket","cs.DC,cs.LG"
"Molecule Edit Graph Attention Network: Modeling Chemical Reactions as Sequences of Graph Edits. The central challenge in automated synthesis planning is to be able to
generate and predict outcomes of a diverse set of chemical reactions. In
particular, in many cases, the most likely synthesis pathway cannot be applied
due to additional constraints, which requires proposing alternative chemical
reactions. With this in mind, we present Molecule Edit Graph Attention Network
(MEGAN), an end-to-end encoder-decoder neural model. MEGAN is inspired by
models that express a chemical reaction as a sequence of graph edits, akin to
the arrow pushing formalism. We extend this model to retrosynthesis prediction
(predicting substrates given the product of a chemical reaction) and scale it
up to large datasets. We argue that representing the reaction as a sequence of
edits enables MEGAN to efficiently explore the space of plausible chemical
reactions, maintaining the flexibility of modeling the reaction in an
end-to-end fashion, and achieving state-of-the-art accuracy in standard
benchmarks. Code and trained models are made available online at
https://github.com/molecule-one/megan.","cs.LG,physics.chem-ph,stat.ML"
"Efficient Algorithms for Outlier-Robust Regression. We give the first polynomial-time algorithm for performing linear or
polynomial regression resilient to adversarial corruptions in both examples and
labels.
  Given a sufficiently large (polynomial-size) training set drawn i.i.d. from
distribution D and subsequently corrupted on some fraction of points, our
algorithm outputs a linear function whose squared error is close to the squared
error of the best-fitting linear function with respect to D, assuming that the
marginal distribution of D over the input space is \emph{certifiably
hypercontractive}. This natural property is satisfied by many well-studied
distributions such as Gaussian, strongly log-concave distributions and, uniform
distribution on the hypercube among others. We also give a simple statistical
lower bound showing that some distributional assumption is necessary to succeed
in this setting.
  These results are the first of their kind and were not known to be even
information-theoretically possible prior to our work.
  Our approach is based on the sum-of-squares (SoS) method and is inspired by
the recent applications of the method for parameter recovery problems in
unsupervised learning. Our algorithm can be seen as a natural convex relaxation
of the following conceptually simple non-convex optimization problem: find a
linear function and a large subset of the input corrupted sample such that the
least squares loss of the function over the subset is minimized over all
possible large subsets.","cs.AI,cs.DS,cs.LG,stat.ML"
"BatVision: Learning to See 3D Spatial Layout with Two Ears. Many species have evolved advanced non-visual perception while artificial
systems fall behind. Radar and ultrasound complement camera-based vision but
they are often too costly and complex to set up for very limited information
gain. In nature, sound is used effectively by bats, dolphins, whales, and
humans for navigation and communication. However, it is unclear how to best
harness sound for machine perception. Inspired by bats' echolocation mechanism,
we design a low-cost BatVision system that is capable of seeing the 3D spatial
layout of space ahead by just listening with two ears. Our system emits short
chirps from a speaker and records returning echoes through microphones in an
artificial human pinnae pair. During training, we additionally use a stereo
camera to capture color images for calculating scene depths. We train a model
to predict depth maps and even grayscale images from the sound alone. During
testing, our trained BatVision provides surprisingly good predictions of 2D
visual scenes from two 1D audio signals. Such a sound to vision system would
benefit robot navigation and machine vision, especially in low-light or
no-light conditions. Our code and data are publicly available.","cs.CV,cs.RO,cs.SD,eess.AS"
"CnGAN: Generative Adversarial Networks for Cross-network user preference generation for non-overlapped users. A major drawback of cross-network recommender solutions is that they can only
be applied to users that are overlapped across networks. Thus, the
non-overlapped users, which form the majority of users are ignored. As a
solution, we propose CnGAN, a novel multi-task learning based,
encoder-GAN-recommender architecture. The proposed model synthetically
generates source network user preferences for non-overlapped users by learning
the mapping from target to source network preference manifolds. The resultant
user preferences are used in a Siamese network based neural recommender
architecture. Furthermore, we propose a novel user based pairwise loss function
for recommendations using implicit interactions to better guide the generation
process in the multi-task learning environment.We illustrate our solution by
generating user preferences on the Twitter source network for recommendations
on the YouTube target network. Extensive experiments show that the generated
preferences can be used to improve recommendations for non-overlapped users.
The resultant recommendations achieve superior performance compared to the
state-of-the-art cross-network recommender solutions in terms of accuracy,
novelty and diversity.","cs.AI,cs.IR,cs.LG,stat.ML"
"Constraining Implicit Space with Minimum Description Length: An Unsupervised Attention Mechanism across Neural Network Layers. Inspired by the adaptation phenomenon of neuronal firing, we propose the
regularity normalization (RN) as an unsupervised attention mechanism (UAM)
which computes the statistical regularity in the implicit space of neural
networks under the Minimum Description Length (MDL) principle. Treating the
neural network optimization process as a partially observable model selection
problem, UAM constrains the implicit space by a normalization factor, the
universal code length. We compute this universal code incrementally across
neural network layers and demonstrated the flexibility to include data priors
such as top-down attention and other oracle information. Empirically, our
approach outperforms existing normalization methods in tackling limited,
imbalanced and non-stationary input distribution in image classification,
classic control, procedurally-generated reinforcement learning, generative
modeling, handwriting generation and question answering tasks with various
neural network architectures. Lastly, UAM tracks dependency and critical
learning stages across layers and recurrent time steps of deep networks.","cs.CV,cs.IT,cs.LG,math.IT,q-bio.NC,stat.ML"
"Video Influencers: Unboxing the Mystique. Influencer marketing is being used increasingly as a tool to reach customers
because of the growing popularity of social media stars who primarily reach
their audience(s) via custom videos. Despite the rapid growth in influencer
marketing, there has been little research on the design and effectiveness of
influencer videos. Using publicly available data on YouTube influencer videos,
we implement novel interpretable deep learning architectures, supported by
transfer learning, to identify significant relationships between advertising
content in videos (across text, audio, and images) and video views, interaction
rates and sentiment. By avoiding ex-ante feature engineering and instead using
ex-post interpretation, our approach avoids making a trade-off between
interpretability and predictive ability. We filter out relationships that are
affected by confounding factors unassociated with an increase in attention to
video elements, thus facilitating the generation of plausible causal
relationships between video elements and marketing outcomes which can be tested
in the field. A key finding is that brand mentions in the first 30 seconds of a
video are on average associated with a significant increase in attention to the
brand but a significant decrease in sentiment expressed towards the video. We
illustrate the learnings from our approach for both influencers and brands.","cs.CL,cs.CV,cs.LG,cs.SD,eess.AS"
"Prediction of severe thunderstorm events with ensemble deep learning and radar data. The problem of nowcasting extreme weather events can be addressed by applying
either numerical methods for the solution of dynamic model equations or
data-driven artificial intelligence algorithms. Within this latter framework,
the present paper illustrates how a deep learning method, exploiting videos of
radar reflectivity frames as input, can be used to realize a warning machine
able to sound timely alarms of possible severe thunderstorm events. From a
technical viewpoint, the computational core of this approach is the use of a
value-weighted skill score for both transforming the probabilistic outcomes of
the deep neural network into binary classification and assessing the
forecasting performances. The warning machine has been validated against
weather radar data recorded in the Liguria region, in Italy,","68T07,86A10,cs.AI,cs.LG"
"$Q$-learning with Logarithmic Regret. This paper presents the first non-asymptotic result showing that a model-free
algorithm can achieve a logarithmic cumulative regret for episodic tabular
reinforcement learning if there exists a strictly positive sub-optimality gap
in the optimal $Q$-function. We prove that the optimistic $Q$-learning studied
in [Jin et al. 2018] enjoys a ${\mathcal{O}}\left(\frac{SA\cdot
\mathrm{poly}\left(H\right)}{\Delta_{\min}}\log\left(SAT\right)\right)$
cumulative regret bound, where $S$ is the number of states, $A$ is the number
of actions, $H$ is the planning horizon, $T$ is the total number of steps, and
$\Delta_{\min}$ is the minimum sub-optimality gap. This bound matches the
information theoretical lower bound in terms of $S,A,T$ up to a
$\log\left(SA\right)$ factor. We further extend our analysis to the discounted
setting and obtain a similar logarithmic cumulative regret bound.","cs.LG,math.OC,stat.ML"
"Apprentice: Using Knowledge Distillation Techniques To Improve Low-Precision Network Accuracy. Deep learning networks have achieved state-of-the-art accuracies on computer
vision workloads like image classification and object detection. The performant
systems, however, typically involve big models with numerous parameters. Once
trained, a challenging aspect for such top performing models is deployment on
resource constrained inference systems - the models (often deep networks or
wide networks or both) are compute and memory intensive. Low-precision numerics
and model compression using knowledge distillation are popular techniques to
lower both the compute requirements and memory footprint of these deployed
models. In this paper, we study the combination of these two techniques and
show that the performance of low-precision networks can be significantly
improved by using knowledge distillation techniques. Our approach, Apprentice,
achieves state-of-the-art accuracies using ternary precision and 4-bit
precision for variants of ResNet architecture on ImageNet dataset. We present
three schemes using which one can apply knowledge distillation techniques to
various stages of the train-and-deploy pipeline.","cs.CV,cs.LG,cs.NE"
"On Solving Cooperative MARL Problems with a Few Good Experiences. Cooperative Multi-agent Reinforcement Learning (MARL) is crucial for
cooperative decentralized decision learning in many domains such as search and
rescue, drone surveillance, package delivery and fire fighting problems. In
these domains, a key challenge is learning with a few good experiences, i.e.,
positive reinforcements are obtained only in a few situations (e.g., on
extinguishing a fire or tracking a crime or delivering a package) and in most
other situations there is zero or negative reinforcement. Learning decisions
with a few good experiences is extremely challenging in cooperative MARL
problems due to three reasons. First, compared to the single agent case,
exploration is harder as multiple agents have to be coordinated to receive a
good experience. Second, environment is not stationary as all the agents are
learning at the same time (and hence change policies). Third, scale of problem
increases significantly with every additional agent.
  Relevant existing work is extensive and has focussed on dealing with a few
good experiences in single-agent RL problems or on scalable approaches for
handling non-stationarity in MARL problems. Unfortunately, neither of these
approaches (or their extensions) are able to address the problem of sparse good
experiences effectively. Therefore, we provide a novel fictitious self
imitation approach that is able to simultaneously handle non-stationarity and
sparse good experiences in a scalable manner. Finally, we provide a thorough
comparison (experimental or descriptive) against relevant cooperative MARL
algorithms to demonstrate the utility of our approach.","cs.AI,cs.LG,cs.MA"
"Interpretable Approximation of High-Dimensional Data. In this paper we apply the previously introduced approximation method based
on the ANOVA (analysis of variance) decomposition and Grouped Transformations
to synthetic and real data. The advantage of this method is the
interpretability of the approximation, i.e., the ability to rank the importance
of the attribute interactions or the variable couplings. Moreover, we are able
to generate an attribute ranking to identify unimportant variables and reduce
the dimensionality of the problem. We compare the method to other approaches on
publicly available benchmark datasets.","cs.LG,cs.NA,math.NA,stat.ML"
"Multi-dimensional Parametric Mincuts for Constrained MAP Inference. In this paper, we propose novel algorithms for inferring the Maximum a
Posteriori (MAP) solution of discrete pairwise random field models under
multiple constraints. We show how this constrained discrete optimization
problem can be formulated as a multi-dimensional parametric mincut problem via
its Lagrangian dual, and prove that our algorithm isolates all constraint
instances for which the problem can be solved exactly. These multiple solutions
enable us to even deal with `soft constraints' (higher order penalty
functions). Moreover, we propose two practical variants of our algorithm to
solve problems with hard constraints. We also show how our method can be
applied to solve various constrained discrete optimization problems such as
submodular minimization and shortest path computation. Experimental evaluation
using the foreground-background image segmentation problem with statistic
constraints reveals that our method is faster and its results are closer to the
ground truth labellings compared with the popular continuous relaxation based
methods.","cs.AI,cs.LG"
"Conditionally Parameterized, Discretization-Aware Neural Networks for Mesh-Based Modeling of Physical Systems. The numerical simulations of physical systems are heavily dependent on
mesh-based models. While neural networks have been extensively explored to
assist such tasks, they often ignore the interactions or hierarchical relations
between input features, and process them as concatenated mixtures. In this
work, we generalize the idea of conditional parametrization -- using trainable
functions of input parameters to generate the weights of a neural network, and
extend them in a flexible way to encode information critical to the numerical
simulations. Inspired by discretized numerical methods, choices of the
parameters include physical quantities and mesh topology features. The
functional relation between the modeled features and the parameters are built
into the network architecture. The method is implemented on different networks,
which are applied to several frontier scientific machine learning tasks,
including the discovery of unmodeled physics, super-resolution of coarse
fields, and the simulation of unsteady flows with chemical reactions. The
results show that the conditionally parameterized networks provide superior
performance compared to their traditional counterparts. A network architecture
named CP-GNet is also proposed as the first deep learning model capable of
standalone prediction of reacting flows on irregular meshes.","cs.LG,cs.NE,physics.comp-ph"
"Model-based versus Model-free Deep Reinforcement Learning for Autonomous Racing Cars. Despite the rich theoretical foundation of model-based deep reinforcement
learning (RL) agents, their effectiveness in real-world robotics-applications
is less studied and understood. In this paper, we, therefore, investigate how
such agents generalize to real-world autonomous-vehicle control-tasks, where
advanced model-free deep RL algorithms fail. In particular, we set up a series
of time-lap tasks for an F1TENTH racing robot, equipped with high-dimensional
LiDAR sensors, on a set of test tracks with a gradual increase in their
complexity. In this continuous-control setting, we show that model-based agents
capable of learning in imagination, substantially outperform model-free agents
with respect to performance, sample efficiency, successful task completion, and
generalization. Moreover, we show that the generalization ability of
model-based agents strongly depends on the observation-model choice. Finally,
we provide extensive empirical evidence for the effectiveness of model-based
agents provided with long enough memory horizons in sim2real tasks.","cs.AI,cs.LG,cs.NE,cs.RO"
"Hybrid modeling of the human cardiovascular system using NeuralFMUs. Hybrid modeling, the combination of first principle and machine learning
models, is an emerging research field that gathers more and more attention.
Even if hybrid models produce formidable results for academic examples, there
are still different technical challenges that hinder the use of hybrid modeling
in real-world applications. By presenting NeuralFMUs, the fusion of a FMU, a
numerical ODE solver and an ANN, we are paving the way for the use of a variety
of first principle models from different modeling tools as parts of hybrid
models. This contribution handles the hybrid modeling of a complex, real-world
example: Starting with a simplified 1D-fluid model of the human cardiovascular
system (arterial side), the aim is to learn neglected physical effects like
arterial elasticity from data. We will show that the hybrid modeling process is
more comfortable, needs less system knowledge and is therefore less error-prone
compared to modeling solely based on first principle. Further, the resulting
hybrid model has improved in computation performance, compared to a pure first
principle white-box model, while still fulfilling the requirements regarding
accuracy of the considered hemodynamic quantities. The use of the presented
techniques is explained in a general manner and the considered use-case can
serve as example for other modeling and simulation applications in and beyond
the medical domain.","cs.CE,cs.LG"
"GAN-Based Interactive Reinforcement Learning from Demonstration and Human Evaluative Feedback. Deep reinforcement learning (DRL) has achieved great successes in many
simulated tasks. The sample inefficiency problem makes applying traditional DRL
methods to real-world robots a great challenge. Generative Adversarial
Imitation Learning (GAIL) -- a general model-free imitation learning method,
allows robots to directly learn policies from expert trajectories in large
environments. However, GAIL shares the limitation of other imitation learning
methods that they can seldom surpass the performance of demonstrations. In this
paper, to address the limit of GAIL, we propose GAN-Based Interactive
Reinforcement Learning (GAIRL) from demonstration and human evaluative feedback
by combining the advantages of GAIL and interactive reinforcement learning. We
tested our proposed method in six physics-based control tasks, ranging from
simple low-dimensional control tasks -- Cart Pole and Mountain Car, to
difficult high-dimensional tasks -- Inverted Double Pendulum, Lunar Lander,
Hopper and HalfCheetah. Our results suggest that with both optimal and
suboptimal demonstrations, a GAIRL agent can always learn a more stable policy
with optimal or close to optimal performance, while the performance of the GAIL
agent is upper bounded by the performance of demonstrations or even worse than
it. In addition, our results indicate the reason that GAIRL is superior over
GAIL is the complementary effect of demonstrations and human evaluative
feedback.","cs.AI,cs.LG,cs.RO"
"Impact of Ground Truth Annotation Quality on Performance of Semantic Image Segmentation of Traffic Conditions. Preparation of high-quality datasets for the urban scene understanding is a
labor-intensive task, especially, for datasets designed for the autonomous
driving applications. The application of the coarse ground truth (GT)
annotations of these datasets without detriment to the accuracy of semantic
image segmentation (by the mean intersection over union - mIoU) could simplify
and speedup the dataset preparation and model fine tuning before its practical
application. Here the results of the comparative analysis for semantic
segmentation accuracy obtained by PSPNet deep learning architecture are
presented for fine and coarse annotated images from Cityscapes dataset. Two
scenarios were investigated: scenario 1 - the fine GT images for training and
prediction, and scenario 2 - the fine GT images for training and the coarse GT
images for prediction. The obtained results demonstrated that for the most
important classes the mean accuracy values of semantic image segmentation for
coarse GT annotations are higher than for the fine GT ones, and the standard
deviation values are vice versa. It means that for some applications some
unimportant classes can be excluded and the model can be tuned further for some
classes and specific regions on the coarse GT dataset without loss of the
accuracy even. Moreover, this opens the perspectives to use deep neural
networks for the preparation of such coarse GT datasets.","cs.CV,cs.LG,stat.ML"
"Sketch-Guided Object Localization in Natural Images. We introduce the novel problem of localizing all the instances of an object
(seen or unseen during training) in a natural image via sketch query. We refer
to this problem as sketch-guided object localization. This problem is
distinctively different from the traditional sketch-based image retrieval task
where the gallery set often contains images with only one object. The
sketch-guided object localization proves to be more challenging when we
consider the following: (i) the sketches used as queries are abstract
representations with little information on the shape and salient attributes of
the object, (ii) the sketches have significant variability as they are
hand-drawn by a diverse set of untrained human subjects, and (iii) there exists
a domain gap between sketch queries and target natural images as these are
sampled from very different data distributions. To address the problem of
sketch-guided object localization, we propose a novel cross-modal attention
scheme that guides the region proposal network (RPN) to generate object
proposals relevant to the sketch query. These object proposals are later scored
against the query to obtain final localization. Our method is effective with as
little as a single sketch query. Moreover, it also generalizes well to object
categories not seen during training and is effective in localizing multiple
object instances present in the image. Furthermore, we extend our framework to
a multi-query setting using novel feature fusion and attention fusion
strategies introduced in this paper. The localization performance is evaluated
on publicly available object detection benchmarks, viz. MS-COCO and PASCAL-VOC,
with sketch queries obtained from `Quick, Draw!'. The proposed method
significantly outperforms related baselines on both single-query and
multi-query localization tasks.","cs.CV,cs.IT,cs.LG,math.IT"
"Domain Adaptation for Autoencoder-Based End-to-End Communication Over Wireless Channels. The problem of domain adaptation conventionally considers the setting where a
source domain has plenty of labeled data, and a target domain (with a different
data distribution) has plenty of unlabeled data but none or very limited
labeled data. In this paper, we address the setting where the target domain has
only limited labeled data from a distribution that is expected to change
frequently. We first propose a fast and light-weight method for adapting a
Gaussian mixture density network (MDN) using only a small set of target domain
samples. This method is well-suited for the setting where the distribution of
target data changes rapidly (e.g., a wireless channel), making it challenging
to collect a large number of samples and retrain. We then apply the proposed
MDN adaptation method to the problem of end-of-end learning of a wireless
communication autoencoder. A communication autoencoder models the encoder,
decoder, and the channel using neural networks, and learns them jointly to
minimize the overall decoding error rate. However, the error rate of an
autoencoder trained on a particular (source) channel distribution can degrade
as the channel distribution changes frequently, not allowing enough time for
data collection and retraining of the autoencoder to the target channel
distribution. We propose a method for adapting the autoencoder without
modifying the encoder and decoder neural networks, and adapting only the MDN
model of the channel. The method utilizes feature transformations at the
decoder to compensate for changes in the channel distribution, and effectively
present to the decoder samples close to the source distribution. Experimental
evaluation on simulated datasets and real mmWave wireless channels demonstrate
that the proposed methods can quickly adapt the MDN model, and improve or
maintain the error rate of the autoencoder under changing channel conditions.","cs.IT,cs.LG,eess.SP,math.IT,stat.ML"
"From Static to Dynamic Node Embeddings. We introduce a general framework for leveraging graph stream data for
temporal prediction-based applications. Our proposed framework includes novel
methods for learning an appropriate graph time-series representation, modeling
and weighting the temporal dependencies, and generalizing existing embedding
methods for such data. While previous work on dynamic modeling and embedding
has focused on representing a stream of timestamped edges using a time-series
of graphs based on a specific time-scale (e.g., 1 month), we propose the notion
of an $\epsilon$-graph time-series that uses a fixed number of edges for each
graph, and show its superiority over the time-scale representation used in
previous work. In addition, we propose a number of new temporal models based on
the notion of temporal reachability graphs and weighted temporal summary
graphs. These temporal models are then used to generalize existing base
(static) embedding methods by enabling them to incorporate and appropriately
model temporal dependencies in the data. From the 6 temporal network models
investigated (for each of the 7 base embedding methods), we find that the top-3
temporal models are always those that leverage the new $\epsilon$-graph
time-series representation. Furthermore, the dynamic embedding methods from the
framework almost always achieve better predictive performance than existing
state-of-the-art dynamic node embedding methods that are developed specifically
for such temporal prediction tasks. Finally, the findings of this work are
useful for designing better dynamic embedding methods.","cs.AI,cs.LG,cs.SI,stat.ML"
"Learning with Density Matrices and Random Features. A density matrix describes the statistical state of a quantum system. It is a
powerful formalism to represent both the quantum and classical uncertainty of
quantum systems and to express different statistical operations such as
measurement, system combination and expectations as linear algebra operations.
This paper explores how density matrices can be used as a building block to
build machine learning models exploiting their ability to straightforwardly
combine linear algebra and probability. One of the main results of the paper is
to show that density matrices coupled with random Fourier features could
approximate arbitrary probability distributions over $\mathbb{R}^n$. Based on
this finding the paper builds different models for density estimation,
classification and regression. These models are differentiable, so it is
possible to integrate them with other differentiable components, such as deep
learning architectures and to learn their parameters using gradient-based
optimization. In addition, the paper presents optimization-less training
strategies based on estimation and model averaging. The models are evaluated in
benchmark tasks and the results are reported and discussed.","cs.AI,cs.LG,quant-ph"
"M3D-GAN: Multi-Modal Multi-Domain Translation with Universal Attention. Generative adversarial networks have led to significant advances in
cross-modal/domain translation. However, typically these networks are designed
for a specific task (e.g., dialogue generation or image synthesis, but not
both). We present a unified model, M3D-GAN, that can translate across a wide
range of modalities (e.g., text, image, and speech) and domains (e.g.,
attributes in images or emotions in speech). Our model consists of modality
subnets that convert data from different modalities into unified
representations, and a unified computing body where data from different
modalities share the same network architecture. We introduce a universal
attention module that is jointly trained with the whole network and learns to
encode a large range of domain information into a highly structured latent
space. We use this to control synthesis in novel ways, such as producing
diverse realistic pictures from a sketch or varying the emotion of synthesized
speech. We evaluate our approach on extensive benchmark tasks, including
image-to-image, text-to-image, image captioning, text-to-speech, speech
recognition, and machine translation. Our results show state-of-the-art
performance on some of the tasks.","cs.CL,cs.CV,cs.LG,eess.AS,eess.IV"
"Direct Estimation of Appearance Models for Segmentation. Image segmentation algorithms often depend on appearance models that
characterize the distribution of pixel values in different image regions. We
describe a new approach for estimating appearance models directly from an
image, without explicit consideration of the pixels that make up each region.
Our approach is based on novel algebraic expressions that relate local image
statistics to the appearance of spatially coherent regions. We describe two
algorithms that can use the aforementioned algebraic expressions to estimate
appearance models directly from an image. The first algorithm solves a system
of linear and quadratic equations using a least squares formulation. The second
algorithm is a spectral method based on an eigenvector computation. We present
experimental results that demonstrate the proposed methods work well in
practice and lead to effective image segmentation algorithms.","62H30,62M05,65C20,68U10,cs.CV"
"Convergence of Batch Asynchronous Stochastic Approximation With Applications to Reinforcement Learning. The stochastic approximation (SA) algorithm is a widely used probabilistic
method for finding a solution to an equation of the form
$\mathbf{f}(\boldsymbol{\theta}) = \mathbf{0}$ where $\mathbf{f} : \mathbb{R}^d
\rightarrow \mathbb{R}^d$, when only noisy measurements of $\mathbf{f}(\cdot)$
are available. In the literature to date, one can make a distinction between
""synchronous"" updating, whereby the entire vector of the current guess
$\boldsymbol{\theta}_t$ is updated at each time, and ""asynchronous"" updating,
whereby ony one component of $\boldsymbol{\theta}_t$ is updated. In convex and
nonconvex optimization, there is also the notion of ""batch"" updating, whereby
some but not all components of $\boldsymbol{\theta}_t$ are updated at each time
$t$. In addition, there is also a distinction between using a ""local"" clock
versus a ""global"" clock. In the literature to date, convergence proofs when a
local clock is used make the assumption that the measurement noise is an i.i.d\
sequence, an assumption that does not hold in Reinforcement Learning (RL).
  In this note, we provide a general theory of convergence for batch
asymchronous stochastic approximation (BASA), that works whether the updates
use a local clock or a global clock, for the case where the measurement noises
form a martingale difference sequence. This is the most general result to date
and encompasses all others.","cs.AI,cs.LG,cs.SY,eess.SY,math.PR,stat.ML"
"Transfer Learning-based Road Damage Detection for Multiple Countries. Many municipalities and road authorities seek to implement automated
evaluation of road damage. However, they often lack technology, know-how, and
funds to afford state-of-the-art equipment for data collection and analysis of
road damages. Although some countries, like Japan, have developed less
expensive and readily available Smartphone-based methods for automatic road
condition monitoring, other countries still struggle to find efficient
solutions. This work makes the following contributions in this context.
Firstly, it assesses the usability of the Japanese model for other countries.
Secondly, it proposes a large-scale heterogeneous road damage dataset
comprising 26620 images collected from multiple countries using smartphones.
Thirdly, we propose generalized models capable of detecting and classifying
road damages in more than one country. Lastly, we provide recommendations for
readers, local agencies, and municipalities of other countries when one other
country publishes its data and model for automatic road damage detection and
classification. Our dataset is available at
(https://github.com/sekilab/RoadDamageDetector/).","cs.CV,cs.CY"
"DeepFreight: A Model-free Deep-reinforcement-learning-based Algorithm for Multi-transfer Freight Delivery. With the freight delivery demands and shipping costs increasing rapidly,
intelligent control of fleets to enable efficient and cost-conscious solutions
becomes an important problem. In this paper, we propose DeepFreight, a
model-free deep-reinforcement-learning-based algorithm for multi-transfer
freight delivery, which includes two closely-collaborative components:
truck-dispatch and package-matching. Specifically, a deep multi-agent
reinforcement learning framework called QMIX is leveraged to learn a dispatch
policy, with which we can obtain the multi-step joint dispatch decisions for
the fleet with respect to the delivery requests. Then an efficient
multi-transfer matching algorithm is executed to assign the delivery requests
to the trucks. Also, DeepFreight is integrated with a Mixed-Integer Linear
Programming optimizer for further optimization. The evaluation results show
that the proposed system is highly scalable and ensures a 100% delivery success
while maintaining low delivery time and fuel consumption.","cs.LG,cs.MA"
"BELT: Block-wise Missing Embedding Learning Transformer. Matrix completion has attracted attention in many fields, including
statistics, applied mathematics, and electrical engineering. Most of the works
focus on the independent sampling models under which the observed entries are
sampled independently. Motivated by applications in the integration of multiple
Electronic Health Record (EHR) datasets, we propose the method {\bf B}lock-wise
missing {\bf E}mbedding {\bf L}earning {\bf T}ransformer (BELT) to treat
row-wise/column-wise missingness. Specifically, BELT can recover block-wise
missing matrices efficiently when every pair of matrices has an overlap. Our
idea is to exploit the orthogonal Procrustes problem to align the eigenspace of
the two sub-matrices using their overlap, then complete the missing blocks by
the inner product of the two low-rank components. Besides, we prove the
statistical rate for the eigenspace of the underlying matrix, which is
comparable to the rate under the independently missing assumption. Simulation
studies show that the method performs well under a variety of configurations.
In the real data analysis, the method is applied to two tasks: (i) the
integrating of several point-wise mutual information matrices built by English
EHR and Chinese medical text data, and (ii) the machine translation between
English and Chinese medical concepts. Our method shows an advantage over
existing methods.","cs.LG,math.ST,stat.AP,stat.ME,stat.ML,stat.TH"
"Exchangeable Input Representations for Reinforcement Learning. Poor sample efficiency is a major limitation of deep reinforcement learning
in many domains. This work presents an attention-based method to project neural
network inputs into an efficient representation space that is invariant under
changes to input ordering. We show that our proposed representation results in
an input space that is a factor of $m!$ smaller for inputs of $m$ objects. We
also show that our method is able to represent inputs over variable numbers of
objects. Our experiments demonstrate improvements in sample efficiency for
policy gradient methods on a variety of tasks. We show that our representation
allows us to solve problems that are otherwise intractable when using na\""ive
approaches.","I.2.6,cs.AI,cs.LG,stat.ML"
"Reinforcement Learning for Joint Optimization of Multiple Rewards. Reinforcement Learning (RL) algorithms such as DQN owe their success to
Markov Decision Processes, and the fact that maximizing the sum of rewards
allows using backward induction and reduce to the Bellman optimality equation.
However, many real-world problems require optimization of an objective that is
non-linear in cumulative rewards for which dynamic programming cannot be
applied directly. For example, in a resource allocation problem, one of the
objectives is to maximize long-term fairness among the users. We notice that
when the function of the sum of rewards is considered, the problem loses its
Markov nature. This paper addresses and formalizes the problem of optimizing a
non-linear function of the long term average of rewards. We propose model-based
and model-free algorithms to learn the policy, where the model-based policy is
shown to achieve a regret of $\Tilde{O}\left(KDSA\sqrt{\frac{A}{T}}\right)$ for
$K$ users. Further, using the fairness in cellular base-station scheduling, and
queueing system scheduling as examples, the proposed algorithm is shown to
significantly outperform the conventional RL approaches.","cs.AI,cs.GT,cs.IT,cs.LG,cs.MA,math.IT,stat.ML"
"Tensor Programs IIb: Architectural Universality of Neural Tangent Kernel Training Dynamics. Yang (2020a) recently showed that the Neural Tangent Kernel (NTK) at
initialization has an infinite-width limit for a large class of architectures
including modern staples such as ResNet and Transformers. However, their
analysis does not apply to training. Here, we show the same neural networks (in
the so-called NTK parametrization) during training follow a kernel gradient
descent dynamics in function space, where the kernel is the infinite-width NTK.
This completes the proof of the *architectural universality* of NTK behavior.
To achieve this result, we apply the Tensor Programs technique: Write the
entire SGD dynamics inside a Tensor Program and analyze it via the Master
Theorem. To facilitate this proof, we develop a graphical notation for Tensor
Programs.","cs.LG,cs.NE,math.PR"
"Robust Unsupervised Multi-Object Tracking in Noisy Environments. Physical processes, camera movement, and unpredictable environmental
conditions like the presence of dust can induce noise and artifacts in video
feeds. We observe that popular unsupervised MOT methods are dependent on
noise-free inputs. We show that the addition of a small amount of artificial
random noise causes a sharp degradation in model performance on benchmark
metrics. We resolve this problem by introducing a robust unsupervised
multi-object tracking (MOT) model: AttU-Net. The proposed single-head attention
model helps limit the negative impact of noise by learning visual
representations at different segment scales. AttU-Net shows better unsupervised
MOT tracking performance over variational inference-based state-of-the-art
baselines. We evaluate our method in the MNIST-MOT and the Atari game video
benchmark. We also provide two extended video datasets: ``Kuzushiji-MNIST MOT''
which consists of moving Japanese characters and ``Fashion-MNIST MOT'' to
validate the effectiveness of the MOT models.","cs.AI,cs.CV,cs.LG,cs.MM,cs.NE"
"What goes around comes around: Cycle-Consistency-based Short-Term Motion Prediction for Anomaly Detection using Generative Adversarial Networks. Anomaly detection plays in many fields of research, along with the strongly
related task of outlier detection, a very important role. Especially within the
context of the automated analysis of video material recorded by surveillance
cameras, abnormal situations can be of very different nature. For this purpose
this work investigates Generative-Adversarial-Network-based methods (GAN) for
anomaly detection related to surveillance applications. The focus is on the
usage of static camera setups, since this kind of camera is one of the most
often used and belongs to the lower price segment. In order to address this
task, multiple subtasks are evaluated, including the influence of existing
optical flow methods for the incorporation of short-term temporal information,
different forms of network setups and losses for GANs, and the use of
morphological operations for further performance improvement. With these
extension we achieved up to 2.4% better results. Furthermore, the final method
reduced the anomaly detection error for GAN-based methods by about 42.8%.",cs.CV
"Learning by Sampling and Compressing: Efficient Graph Representation Learning with Extremely Limited Annotations. Graph convolution network (GCN) attracts intensive research interest with
broad applications. While existing work mainly focused on designing novel GCN
architectures for better performance, few of them studied a practical yet
challenging problem: How to learn GCNs from data with extremely limited
annotation? In this paper, we propose a new learning method by sampling
strategy and model compression to overcome this challenge. Our approach has
multifold advantages: 1) the adaptive sampling strategy largely suppresses the
GCN training deviation over uniform sampling; 2) compressed GCN-based methods
with a smaller scale of parameters need fewer labeled data to train; 3) the
smaller scale of training data is beneficial to reduce the human resource cost
to label them. We choose six popular GCN baselines and conduct extensive
experiments on three real-world datasets. The results show that by applying our
method, all GCN baselines cut down the annotation requirement by as much as
90$\%$ and compress the scale of parameters more than 6$\times$ without
sacrificing their strong performance. It verifies that the training method
could extend the existing semi-supervised GCN-based methods to the scenarios
with the extremely small scale of labeled data.","cs.LG,cs.SI,stat.ML"
"Using state space differential geometry for nonlinear blind source separation. Given a time series of multicomponent measurements of an evolving stimulus,
nonlinear blind source separation (BSS) seeks to find a ""source"" time series,
comprised of statistically independent combinations of the measured components.
In this paper, we seek a source time series with local velocity cross
correlations that vanish everywhere in stimulus state space. However, in an
earlier paper the local velocity correlation matrix was shown to constitute a
metric on state space. Therefore, nonlinear BSS maps onto a problem of
differential geometry: given the metric observed in the measurement coordinate
system, find another coordinate system in which the metric is diagonal
everywhere. We show how to determine if the observed data are separable in this
way, and, if they are, we show how to construct the required transformation to
the source coordinate system, which is essentially unique except for an unknown
rotation that can be found by applying the methods of linear BSS. Thus, the
proposed technique solves nonlinear BSS in many situations or, at least,
reduces it to linear BSS, without the use of probabilistic, parametric, or
iterative procedures. This paper also describes a generalization of this
methodology that performs nonlinear independent subspace separation. In every
case, the resulting decomposition of the observed data is an intrinsic property
of the stimulus' evolution in the sense that it does not depend on the way the
observer chooses to view it (e.g., the choice of the observing machine's
sensors). In other words, the decomposition is a property of the evolution of
the ""real"" stimulus that is ""out there"" broadcasting energy to the observer.
The technique is illustrated with analytic and numerical examples.","I.2.6; J.2; I.2.7; I.5,cs.LG,cs.SD"
"Towards Robust Visual Information Extraction in Real World: New Dataset and Novel Solution. Visual information extraction (VIE) has attracted considerable attention
recently owing to its various advanced applications such as document
understanding, automatic marking and intelligent education. Most existing works
decoupled this problem into several independent sub-tasks of text spotting
(text detection and recognition) and information extraction, which completely
ignored the high correlation among them during optimization. In this paper, we
propose a robust visual information extraction system (VIES) towards real-world
scenarios, which is a unified end-to-end trainable framework for simultaneous
text detection, recognition and information extraction by taking a single
document image as input and outputting the structured information.
Specifically, the information extraction branch collects abundant visual and
semantic representations from text spotting for multimodal feature fusion and
conversely, provides higher-level semantic clues to contribute to the
optimization of text spotting. Moreover, regarding the shortage of public
benchmarks, we construct a fully-annotated dataset called EPHOIE
(https://github.com/HCIILAB/EPHOIE), which is the first Chinese benchmark for
both text spotting and visual information extraction. EPHOIE consists of 1,494
images of examination paper head with complex layouts and background, including
a total of 15,771 Chinese handwritten or printed text instances. Compared with
the state-of-the-art methods, our VIES shows significant superior performance
on the EPHOIE dataset and achieves a 9.01% F-score gain on the widely used
SROIE dataset under the end-to-end scenario.","cs.AI,cs.CV,cs.IR,cs.LG,cs.MM"
"Entity Context Graph: Learning Entity Representations fromSemi-Structured Textual Sources on the Web. Knowledge is captured in the form of entities and their relationships and
stored in knowledge graphs. Knowledge graphs enhance the capabilities of
applications in many different areas including Web search, recommendation, and
natural language understanding. This is mainly because, entities enable
machines to understand things that go beyond simple tokens. Many modern
algorithms use learned entity embeddings from these structured representations.
However, building a knowledge graph takes time and effort, hence very costly
and nontrivial. On the other hand, many Web sources describe entities in some
structured format and therefore, finding ways to get them into useful entity
knowledge is advantageous. We propose an approach that processes entity centric
textual knowledge sources to learn entity embeddings and in turn avoids the
need for a traditional knowledge graph. We first extract triples into the new
representation format that does not use traditional complex triple extraction
methods defined by pre-determined relationship labels. Then we learn entity
embeddings through this new type of triples. We show that the embeddings
learned from our approach are: (i) high quality and comparable to a known
knowledge graph-based embeddings and can be used to improve them further, (ii)
better than a contextual language model-based entity embeddings, and (iii) easy
to compute and versatile in domain-specific applications where a knowledge
graph is not readily available","cs.CL,cs.IR,cs.LG"
"Inference with Deep Generative Priors in High Dimensions. Deep generative priors offer powerful models for complex-structured data,
such as images, audio, and text. Using these priors in inverse problems
typically requires estimating the input and/or hidden signals in a multi-layer
deep neural network from observation of its output. While these approaches have
been successful in practice, rigorous performance analysis is complicated by
the non-convex nature of the underlying optimization problems. This paper
presents a novel algorithm, Multi-Layer Vector Approximate Message Passing
(ML-VAMP), for inference in multi-layer stochastic neural networks. ML-VAMP can
be configured to compute maximum a priori (MAP) or approximate minimum
mean-squared error (MMSE) estimates for these networks. We show that the
performance of ML-VAMP can be exactly predicted in a certain high-dimensional
random limit. Furthermore, under certain conditions, ML-VAMP yields estimates
that achieve the minimum (i.e., Bayes-optimal) MSE as predicted by the replica
method. In this way, ML-VAMP provides a computationally efficient method for
multi-layer inference with an exact performance characterization and testable
conditions for optimality in the large-system limit.","cs.IT,cs.LG,cs.NE,eess.SP,math.IT,stat.ML"
"Cascade of Phase Transitions for Multi-Scale Clustering. We present a novel framework exploiting the cascade of phase transitions
occurring during a simulated annealing of the Expectation-Maximisation
algorithm to cluster datasets with multi-scale structures. Using the weighted
local covariance, we can extract, a posteriori and without any prior knowledge,
information on the number of clusters at different scales together with their
size. We also study the linear stability of the iterative scheme to derive the
threshold at which the first transition occurs and show how to approximate the
next ones. Finally, we combine simulated annealing together with recent
developments of regularised Gaussian mixture models to learn a principal graph
from spatially structured datasets that can also exhibit many scales.","cond-mat.dis-nn,cond-mat.stat-mech,cs.LG,stat.ML"
"Logarithmic landscape and power-law escape rate of SGD. Stochastic gradient descent (SGD) undergoes complicated multiplicative noise
for the mean-square loss. We use this property of the SGD noise to derive a
stochastic differential equation (SDE) with simpler additive noise by
performing a non-uniform transformation of the time variable. In the SDE, the
gradient of the loss is replaced by that of the logarithmized loss.
Consequently, we show that, near a local or global minimum, the stationary
distribution $P_\mathrm{ss}(\theta)$ of the network parameters $\theta$ follows
a power-law with respect to the loss function $L(\theta)$, i.e.
$P_\mathrm{ss}(\theta)\propto L(\theta)^{-\phi}$ with the exponent $\phi$
specified by the mini-batch size, the learning rate, and the Hessian at the
minimum. We obtain the escape rate formula from a local minimum, which is
determined not by the loss barrier height $\Delta L=L(\theta^s)-L(\theta^*)$
between a minimum $\theta^*$ and a saddle $\theta^s$ but by the logarithmized
loss barrier height $\Delta\log L=\log[L(\theta^s)/L(\theta^*)]$. Our
escape-rate formula explains an empirical fact that SGD prefers flat minima
with low effective dimensions.","cond-mat.dis-nn,cond-mat.stat-mech,cs.LG,stat.ML"
"ProstateGAN: Mitigating Data Bias via Prostate Diffusion Imaging Synthesis with Generative Adversarial Networks. Generative Adversarial Networks (GANs) have shown considerable promise for
mitigating the challenge of data scarcity when building machine learning-driven
analysis algorithms. Specifically, a number of studies have shown that
GAN-based image synthesis for data augmentation can aid in improving
classification accuracy in a number of medical image analysis tasks, such as
brain and liver image analysis. However, the efficacy of leveraging GANs for
tackling prostate cancer analysis has not been previously explored. Motivated
by this, in this study we introduce ProstateGAN, a GAN-based model for
synthesizing realistic prostate diffusion imaging data. More specifically, in
order to generate new diffusion imaging data corresponding to a particular
cancer grade (Gleason score), we propose a conditional deep convolutional GAN
architecture that takes Gleason scores into consideration during the training
process. Experimental results show that high-quality synthetic prostate
diffusion imaging data can be generated using the proposed ProstateGAN for
specified Gleason scores.","cs.AI,cs.CV,cs.NE"
"Reflection methods for user-friendly submodular optimization. Recently, it has become evident that submodularity naturally captures widely
occurring concepts in machine learning, signal processing and computer vision.
Consequently, there is need for efficient optimization procedures for
submodular functions, especially for minimization problems. While general
submodular minimization is challenging, we propose a new method that exploits
existing decomposability of submodular functions. In contrast to previous
approaches, our method is neither approximate, nor impractical, nor does it
need any cumbersome parameter tuning. Moreover, it is easy to implement and
parallelize. A key component of our method is a formulation of the discrete
submodular minimization problem as a continuous best approximation problem that
is solved through a sequence of reflections, and its solution can be easily
thresholded to obtain an optimal discrete solution. This method solves both the
continuous and discrete formulations of the problem, and therefore has
applications in learning, inference, and reconstruction. In our experiments, we
illustrate the benefits of our method on two image segmentation tasks.","cs.LG,cs.NA,cs.RO,math.OC"
"A reinforcement learning approach to improve communication performance and energy utilization in fog-based IoT. Recent research has shown the potential of using available mobile fog devices
(such as smartphones, drones, domestic and industrial robots) as relays to
minimize communication outages between sensors and destination devices, where
localized Internet-of-Things services (e.g., manufacturing process control,
health and security monitoring) are delivered. However, these mobile relays
deplete energy when they move and transmit to distant destinations. As such,
power-control mechanisms and intelligent mobility of the relay devices are
critical in improving communication performance and energy utilization. In this
paper, we propose a Q-learning-based decentralized approach where each mobile
fog relay agent (MFRA) is controlled by an autonomous agent which uses
reinforcement learning to simultaneously improve communication performance and
energy utilization. Each autonomous agent learns based on the feedback from the
destination and its own energy levels whether to remain active and forward the
message, or become passive for that transmission phase. We evaluate the
approach by comparing with the centralized approach, and observe that with
lesser number of MFRAs, our approach is able to ensure reliable delivery of
data and reduce overall energy cost by 56.76\% -- 88.03\%.","cs.LG,cs.NI,eess.SP"
"Backward Feature Correction: How Deep Learning Performs Deep Learning. How does a 110-layer ResNet learn a high-complexity classifier using
relatively few training examples and short training time? We present a theory
towards explaining this in terms of Hierarchical Learning. We refer
hierarchical learning as the learner learns to represent a complicated target
function by decomposing it into a sequence of simpler functions to reduce
sample and time complexity. We formally analyze how multi-layer neural networks
can perform such hierarchical learning efficiently and automatically by
applying SGD.
  On the conceptual side, we present, to the best of our knowledge, the FIRST
theory result indicating how deep neural networks can still be sample and time
efficient using SGD on certain hierarchical learning tasks, when NO KNOWN
existing algorithm is efficient. We establish a new principle called ""backward
feature correction"", where training higher-level layers in the network can
improve the features of lower-level ones. We believe this is the key to
understand the deep learning process in multi-layer neural networks.
  On the technical side, we show for regression and even binary classification,
for every input dimension $d>0$, there is a concept class of degree $\omega(1)$
polynomials so that, using $\omega(1)$-layer neural networks as learners, SGD
can learn any function from this class in $\mathsf{poly}(d)$ time and sample
complexity to any $\frac{1}{\mathsf{poly}(d)}$ error, through learning to
represent it as a composition of $\omega(1)$ layers of quadratic functions. In
contrast, we do not know any other simple algorithm (including layer-wise
training or applying kernel method sequentially) that can learn this concept
class in $\mathsf{poly}(d)$ time even to any $d^{-0.01}$ error. As a side
result, we prove $d^{\omega(1)}$ lower bounds for several non-hierarchical
learners, including any kernel methods, neural tangent or neural compositional
kernels.","cs.DS,cs.LG,cs.NE,math.OC,stat.ML"
"Knowledge-Based Video Question Answering with Unsupervised Scene Descriptions. To understand movies, humans constantly reason over the dialogues and actions
shown in specific scenes and relate them to the overall storyline already seen.
Inspired by this behaviour, we design ROLL, a model for knowledge-based video
story question answering that leverages three crucial aspects of movie
understanding: dialog comprehension, scene reasoning, and storyline recalling.
In ROLL, each of these tasks is in charge of extracting rich and diverse
information by 1) processing scene dialogues, 2) generating unsupervised video
scene descriptions, and 3) obtaining external knowledge in a weakly supervised
fashion. To answer a given question correctly, the information generated by
each inspired-cognitive task is encoded via Transformers and fused through a
modality weighting mechanism, which balances the information from the different
sources. Exhaustive evaluation demonstrates the effectiveness of our approach,
which yields a new state-of-the-art on two challenging video question answering
datasets: KnowIT VQA and TVQA+.","cs.CL,cs.CV"
"Adversarial Immunization for Certifiable Robustness on Graphs. Despite achieving strong performance in semi-supervised node classification
task, graph neural networks (GNNs) are vulnerable to adversarial attacks,
similar to other deep learning models. Existing researches focus on developing
either robust GNN models or attack detection methods against adversarial
attacks on graphs. However, little research attention is paid to the potential
and practice of immunization to adversarial attacks on graphs. In this paper,
we propose and formulate the graph adversarial immunization problem, i.e.,
vaccinating an affordable fraction of node pairs, connected or unconnected, to
improve the certifiable robustness of graph against any admissible adversarial
attack. We further propose an effective algorithm, called AdvImmune, which
optimizes with meta-gradient in a discrete way to circumvent the
computationally expensive combinatorial optimization when solving the
adversarial immunization problem. Experiments are conducted on two citation
networks and one social network. Experimental results demonstrate that the
proposed AdvImmune method remarkably improves the ratio of robust nodes by 12%,
42%, 65%, with an affordable immune budget of only 5% edges.","cs.CR,cs.LG,cs.SI,stat.ML"
"Neural-Attention-Based Deep Learning Architectures for Modeling Traffic Dynamics on Lane Graphs. Deep neural networks can be powerful tools, but require careful
application-specific design to ensure that the most informative relationships
in the data are learnable. In this paper, we apply deep neural networks to the
nonlinear spatiotemporal physics problem of vehicle traffic dynamics. We
consider problems of estimating macroscopic quantities (e.g., the queue at an
intersection) at a lane level. First-principles modeling at the lane scale has
been a challenge due to complexities in modeling social behaviors like lane
changes, and those behaviors' resultant macro-scale effects. Following domain
knowledge that upstream/downstream lanes and neighboring lanes affect each
others' traffic flows in distinct ways, we apply a form of neural attention
that allows the neural network layers to aggregate information from different
lanes in different manners. Using a microscopic traffic simulator as a testbed,
we obtain results showing that an attentional neural network model can use
information from nearby lanes to improve predictions, and, that explicitly
encoding the lane-to-lane relationship types significantly improves
performance. We also demonstrate the transfer of our learned neural network to
a more complex road network, discuss how its performance degradation may be
attributable to new traffic behaviors induced by increased topological
complexity, and motivate learning dynamics models from many road network
topologies.","cs.LG,cs.SY,stat.ML"
"Realistic Image Generation using Region-phrase Attention. The Generative Adversarial Network (GAN) has recently been applied to
generate synthetic images from text. Despite significant advances, most current
state-of-the-art algorithms are regular-grid region based; when attention is
used, it is mainly applied between individual regular-grid regions and a word.
These approaches are sufficient to generate images that contain a single object
in its foreground, such as a ""bird"" or ""flower"". However, natural languages
often involve complex foreground objects and the background may also constitute
a variable portion of the generated image. Therefore, the regular-grid based
image attention weights may not necessarily concentrate on the intended
foreground region(s), which in turn, results in an unnatural looking image.
Additionally, individual words such as ""a"", ""blue"" and ""shirt"" do not
necessarily provide a full visual context unless they are applied together. For
this reason, in our paper, we proposed a novel method in which we introduced an
additional set of attentions between true-grid regions and word phrases. The
true-grid region is derived using a set of auxiliary bounding boxes. These
auxiliary bounding boxes serve as superior location indicators to where the
alignment and attention should be drawn with the word phrases. Word phrases are
derived from analysing Part-of-Speech (POS) results. We perform experiments on
this novel network architecture using the Microsoft Common Objects in Context
(MSCOCO) dataset and the model generates $256 \times 256$ conditioned on a
short sentence description. Our proposed approach is capable of generating more
realistic images compared with the current state-of-the-art algorithms.","cs.CV,cs.GR,cs.LG,stat.ML"
"MNEW: Multi-domain Neighborhood Embedding and Weighting for Sparse Point Clouds Segmentation. Point clouds have been widely adopted in 3D semantic scene understanding.
However, point clouds for typical tasks such as 3D shape segmentation or indoor
scenario parsing are much denser than outdoor LiDAR sweeps for the application
of autonomous driving perception. Due to the spatial property disparity, many
successful methods designed for dense point clouds behave depreciated
effectiveness on the sparse data. In this paper, we focus on the semantic
segmentation task of sparse outdoor point clouds. We propose a new method
called MNEW, including multi-domain neighborhood embedding, and attention
weighting based on their geometry distance, feature similarity, and
neighborhood sparsity. The network architecture inherits PointNet which
directly process point clouds to capture pointwise details and global
semantics, and is improved by involving multi-scale local neighborhoods in
static geometry domain and dynamic feature space. The distance/similarity
attention and sparsity-adapted weighting mechanism of MNEW enable its
capability for a wide range of data sparsity distribution. With experiments
conducted on virtual and real KITTI semantic datasets, MNEW achieves the top
performance for sparse point clouds, which is important to the application of
LiDAR-based automated driving perception.","cs.CV,cs.LG,eess.IV"
"Benchmarking Energy-Conserving Neural Networks for Learning Dynamics from Data. The last few years have witnessed an increased interest in incorporating
physics-informed inductive bias in deep learning frameworks. In particular, a
growing volume of literature has been exploring ways to enforce energy
conservation while using neural networks for learning dynamics from observed
time-series data. In this work, we survey ten recently proposed
energy-conserving neural network models, including HNN, LNN, DeLaN, SymODEN,
CHNN, CLNN and their variants. We provide a compact derivation of the theory
behind these models and explain their similarities and differences. Their
performance are compared in 4 physical systems. We point out the possibility of
leveraging some of these energy-conserving models to design energy-based
controllers.","cs.AI,cs.LG,cs.SY,eess.SY,math.DS"
"Convergence of Gradient Methods on Bilinear Zero-Sum Games. Min-max formulations have attracted great attention in the ML community due
to the rise of deep generative models and adversarial methods, while
understanding the dynamics of gradient algorithms for solving such formulations
has remained a grand challenge. As a first step, we restrict to bilinear
zero-sum games and give a systematic analysis of popular gradient updates, for
both simultaneous and alternating versions. We provide exact conditions for
their convergence and find the optimal parameter setup and convergence rates.
In particular, our results offer formal evidence that alternating updates
converge ""better"" than simultaneous ones.","cs.GT,cs.LG,math.OC,stat.ML"
"DISSECT: Disentangled Simultaneous Explanations via Concept Traversals. Explaining deep learning model inferences is a promising venue for scientific
understanding, improving safety, uncovering hidden biases, evaluating fairness,
and beyond, as argued by many scholars. One of the principal benefits of
counterfactual explanations is allowing users to explore ""what-if"" scenarios
through what does not and cannot exist in the data, a quality that many other
forms of explanation such as heatmaps and influence functions are inherently
incapable of doing. However, most previous work on generative explainability
cannot disentangle important concepts effectively, produces unrealistic
examples, or fails to retain relevant information. We propose a novel approach,
DISSECT, that jointly trains a generator, a discriminator, and a concept
disentangler to overcome such challenges using little supervision. DISSECT
generates Concept Traversals (CTs), defined as a sequence of generated examples
with increasing degrees of concepts that influence a classifier's decision. By
training a generative model from a classifier's signal, DISSECT offers a way to
discover a classifier's inherent ""notion"" of distinct concepts automatically
rather than rely on user-predefined concepts. We show that DISSECT produces CTs
that (1) disentangle several concepts, (2) are influential to a classifier's
decision and are coupled to its reasoning due to joint training (3), are
realistic, (4) preserve relevant information, and (5) are stable across similar
inputs. We validate DISSECT on several challenging synthetic and realistic
datasets where previous methods fall short of satisfying desirable criteria for
interpretability and show that it performs consistently well and better than
existing methods. Finally, we present experiments showing applications of
DISSECT for detecting potential biases of a classifier and identifying spurious
artifacts that impact predictions.","cs.AI,cs.LG"
"Involutive MCMC: a Unifying Framework. Markov Chain Monte Carlo (MCMC) is a computational approach to fundamental
problems such as inference, integration, optimization, and simulation. The
field has developed a broad spectrum of algorithms, varying in the way they are
motivated, the way they are applied and how efficiently they sample. Despite
all the differences, many of them share the same core principle, which we unify
as the Involutive MCMC (iMCMC) framework. Building upon this, we describe a
wide range of MCMC algorithms in terms of iMCMC, and formulate a number of
""tricks"" which one can use as design principles for developing new MCMC
algorithms. Thus, iMCMC provides a unified view of many known MCMC algorithms,
which facilitates the derivation of powerful extensions. We demonstrate the
latter with two examples where we transform known reversible MCMC algorithms
into more efficient irreversible ones.","cs.LG,stat.CO,stat.ME,stat.ML"
"Non-stationary Reinforcement Learning without Prior Knowledge: An Optimal Black-box Approach. We propose a black-box reduction that turns a certain reinforcement learning
algorithm with optimal regret in a (near-)stationary environment into another
algorithm with optimal dynamic regret in a non-stationary environment,
importantly without any prior knowledge on the degree of non-stationarity. By
plugging different algorithms into our black-box, we provide a list of examples
showing that our approach not only recovers recent results for (contextual)
multi-armed bandits achieved by very specialized algorithms, but also
significantly improves the state of the art for (generalized) linear bandits,
episodic MDPs, and infinite-horizon MDPs in various ways. Specifically, in most
cases our algorithm achieves the optimal dynamic regret
$\widetilde{\mathcal{O}}(\min\{\sqrt{LT}, \Delta^{1/3}T^{2/3}\})$ where $T$ is
the number of rounds and $L$ and $\Delta$ are the number and amount of changes
of the world respectively, while previous works only obtain suboptimal bounds
and/or require the knowledge of $L$ and $\Delta$.","cs.AI,cs.LG,stat.ML"
"Style Transfer With Adaptation to the Central Objects of the Scene. Style transfer is a problem of rendering image with some content in the style
of another image, for example a family photo in the style of a painting of some
famous artist. The drawback of classical style transfer algorithm is that it
imposes style uniformly on all parts of the content image, which perturbs
central objects on the content image, such as faces or text, and makes them
unrecognizable. This work proposes a novel style transfer algorithm which
automatically detects central objects on the content image, generates spatial
importance mask and imposes style non-uniformly: central objects are stylized
less to preserve their recognizability and other parts of the image are
stylized as usual to preserve the style. Three methods of automatic central
object detection are proposed and evaluated qualitatively and via a user
evaluation study. Both comparisons demonstrate higher quality of stylization
compared to the classical style transfer method.","68T45,I.4.9; I.4.10,cs.CV"
"ST-GRAT: A Novel Spatio-temporal Graph Attention Network for Accurately Forecasting Dynamically Changing Road Speed. Predicting road traffic speed is a challenging task due to different types of
roads, abrupt speed change and spatial dependencies between roads; it requires
the modeling of dynamically changing spatial dependencies among roads and
temporal patterns over long input sequences. This paper proposes a novel
spatio-temporal graph attention (ST-GRAT) that effectively captures the
spatio-temporal dynamics in road networks. The novel aspects of our approach
mainly include spatial attention, temporal attention, and spatial sentinel
vectors. The spatial attention takes the graph structure information (e.g.,
distance between roads) and dynamically adjusts spatial correlation based on
road states. The temporal attention is responsible for capturing traffic speed
changes, and the sentinel vectors allow the model to retrieve new features from
spatially correlated nodes or preserve existing features. The experimental
results show that ST-GRAT outperforms existing models, especially in difficult
conditions where traffic speeds rapidly change (e.g., rush hours). We
additionally provide a qualitative study to analyze when and where ST-GRAT
tended to make accurate predictions during rush-hour times.","cs.LG,eess.SP"
"Communication Efficient Tensor Factorization for Decentralized Healthcare Networks. Tensor factorization has been proved as an efficient unsupervised learning
approach for health data analysis, especially for computational phenotyping,
where the high-dimensional Electronic Health Records (EHRs) with patients
history of medical procedures, medications, diagnosis, lab tests, etc., are
converted to meaningful and interpretable medical concepts. Federated tensor
factorization distributes the tensor computation to multiple workers under the
coordination of a central server, which enables jointly learning the phenotypes
across multiple hospitals while preserving the privacy of the patient
information. However, existing federated tensor factorization algorithms
encounter the single-point-failure issue with the involvement of the central
server, which is not only easily exposed to external attacks, but also limits
the number of clients sharing information with the server under restricted
uplink bandwidth. In this paper, we propose CiderTF, a communication-efficient
decentralized generalized tensor factorization, which reduces the uplink
communication cost by leveraging a four-level communication reduction strategy
designed for a generalized tensor factorization, which has the flexibility of
modeling different tensor distribution with multiple kinds of loss functions.
Experiments on two real-world EHR datasets demonstrate that CiderTF achieves
comparable convergence with the communication reduction up to 99.99%.","cs.DC,cs.LG"
"Auto-encoding brain networks with applications to analyzing large-scale brain imaging datasets. There has been huge interest in studying human brain connectomes inferred
from different imaging modalities and exploring their relationship with human
traits, such as cognition. Brain connectomes are usually represented as
networks, with nodes corresponding to different regions of interest (ROIs) and
edges to connection strengths between ROIs. Due to the high-dimensionality and
non-Euclidean nature of networks, it is challenging to depict their population
distribution and relate them to human traits. Current approaches focus on
summarizing the network using either pre-specified topological features or
principal components analysis (PCA). In this paper, building on recent advances
in deep learning, we develop a nonlinear latent factor model to characterize
the population distribution of brain graphs and infer the relationships between
brain structural connectomes and human traits. We refer to our method as Graph
AuTo-Encoding (GATE). We applied GATE to two large-scale brain imaging
datasets, the Adolescent Brain Cognitive Development (ABCD) study and the Human
Connectome Project (HCP) for adults, to understand the structural brain
connectome and its relationship with cognition. Numerical results demonstrate
huge advantages of GATE over competitors in terms of prediction accuracy,
statistical inference and computing efficiency. We found that structural
connectomes have a stronger association with a wide range of human cognitive
traits than was apparent using previous approaches.","cs.LG,q-bio.NC,stat.ML"
"Geometric graphs from data to aid classification tasks with graph convolutional networks. Traditional classification tasks learn to assign samples to given classes
based solely on sample features. This paradigm is evolving to include other
sources of information, such as known relations between samples. Here we show
that, even if additional relational information is not available in the data
set, one can improve classification by constructing geometric graphs from the
features themselves, and using them within a Graph Convolutional Network. The
improvement in classification accuracy is maximized by graphs that capture
sample similarity with relatively low edge density. We show that such
feature-derived graphs increase the alignment of the data to the ground truth
while improving class separation. We also demonstrate that the graphs can be
made more efficient using spectral sparsification, which reduces the number of
edges while still improving classification performance. We illustrate our
findings using synthetic and real-world data sets from various scientific
domains.","cs.LG,cs.SI,physics.soc-ph,stat.ML"
"Audiomer: A Convolutional Transformer for Keyword Spotting. Transformers have seen an unprecedented rise in Natural Language Processing
and Computer Vision tasks. However, in audio tasks, they are either infeasible
to train due to extremely large sequence length of audio waveforms or reach
competitive performance after feature extraction through Fourier-based methods,
incurring a loss-floor. In this work, we introduce an architecture, Audiomer,
where we combine 1D Residual Networks with Performer Attention to achieve
state-of-the-art performance in Keyword Spotting with raw audio waveforms,
out-performing all previous methods while also being computationally cheaper,
much more parameter and data-efficient. Audiomer allows for deployment in
compute-constrained devices and training on smaller datasets.","cs.CL,cs.LG,cs.SD,eess.AS"
"MCMC assisted by Belief Propagation. Markov Chain Monte Carlo (MCMC) and Belief Propagation (BP) are the most
popular algorithms for computational inference in Graphical Models (GM). In
principle, MCMC is an exact probabilistic method which, however, often suffers
from exponentially slow mixing. In contrast, BP is a deterministic method,
which is typically fast, empirically very successful, however in general
lacking control of accuracy over loopy graphs. In this paper, we introduce MCMC
algorithms correcting the approximation error of BP, i.e., we provide a way to
compensate for BP errors via a consecutive BP-aware MCMC. Our framework is
based on the Loop Calculus (LC) approach which allows expressing the BP error
as a sum of weighted generalized loops. Although the full series is
computationally intractable, it is known that a truncated series, summing up
all 2-regular loops, is computable in polynomial-time for planar pair-wise
binary GMs and it also provides a highly accurate approximation empirically.
Motivated by this, we first propose a polynomial-time approximation MCMC scheme
for the truncated series of general (non-planar) pair-wise binary models. Our
main idea here is to use the Worm algorithm, known to provide fast mixing in
other (related) problems, and then design an appropriate rejection scheme to
sample 2-regular loops. Furthermore, we also design an efficient rejection-free
MCMC scheme for approximating the full series. The main novelty underlying our
design is in utilizing the concept of cycle basis, which provides an efficient
decomposition of the generalized loops. In essence, the proposed MCMC schemes
run on transformed GM built upon the non-trivial BP solution, and our
experiments show that this synthesis of BP and MCMC outperforms both direct
MCMC and bare BP schemes.","cs.AI,cs.DS,stat.ML"
"S&P 500 Stock Price Prediction Using Technical, Fundamental and Text Data. We summarized both common and novel predictive models used for stock price
prediction and combined them with technical indices, fundamental
characteristics and text-based sentiment data to predict S&P stock prices. A
66.18% accuracy in S&P 500 index directional prediction and 62.09% accuracy in
individual stock directional prediction was achieved by combining different
machine learning models such as Random Forest and LSTM together into
state-of-the-art ensemble models. The data we use contains weekly historical
prices, finance reports, and text information from news items associated with
518 different common stocks issued by current and former S&P 500 large-cap
companies, from January 1, 2000 to December 31, 2019. Our study's innovation
includes utilizing deep language models to categorize and infer financial news
item sentiment; fusing different models containing different combinations of
variables and stocks to jointly make predictions; and overcoming the
insufficient data problem for machine learning models in time series by using
data across different stocks.","cs.LG,stat.ML"
"Navigating the Dynamics of Financial Embeddings over Time. Financial transactions constitute connections between entities and through
these connections a large scale heterogeneous weighted graph is formulated. In
this labyrinth of interactions that are continuously updated, there exists a
variety of similarity-based patterns that can provide insights into the
dynamics of the financial system. With the current work, we propose the
application of Graph Representation Learning in a scalable dynamic setting as a
means of capturing these patterns in a meaningful and robust way. We proceed to
perform a rigorous qualitative analysis of the latent trajectories to extract
real world insights from the proposed representations and their evolution over
time that is to our knowledge the first of its kind in the financial sector.
Shifts in the latent space are associated with known economic events and in
particular the impact of the recent Covid-19 pandemic to consumer patterns.
Capturing such patterns indicates the value added to financial modeling through
the incorporation of latent graph representations.","cs.LG,cs.SI,stat.ML"
"RL-Scope: Cross-Stack Profiling for Deep Reinforcement Learning Workloads. Deep reinforcement learning (RL) has made groundbreaking advancements in
robotics, data center management and other applications. Unfortunately,
system-level bottlenecks in RL workloads are poorly understood; we observe
fundamental structural differences in RL workloads that make them inherently
less GPU-bound than supervised learning (SL). To explain where training time is
spent in RL workloads, we propose RL-Scope, a cross-stack profiler that scopes
low-level CPU/GPU resource usage to high-level algorithmic operations, and
provides accurate insights by correcting for profiling overhead. Using
RL-Scope, we survey RL workloads across its major dimensions including ML
backend, RL algorithm, and simulator. For ML backends, we explain a $2.3\times$
difference in runtime between equivalent PyTorch and TensorFlow algorithm
implementations, and identify a bottleneck rooted in overly abstracted
algorithm implementations. For RL algorithms and simulators, we show that
on-policy algorithms are at least $3.5\times$ more simulation-bound than
off-policy algorithms. Finally, we profile a scale-up workload and demonstrate
that GPU utilization metrics reported by commonly used tools dramatically
inflate GPU usage, whereas RL-Scope reports true GPU-bound time. RL-Scope is an
open-source tool available at https://github.com/UofT-EcoSystem/rlscope .","cs.LG,cs.SE"
"Achieving Efficiency in Black Box Simulation of Distribution Tails with Self-structuring Importance Samplers. Motivated by the increasing adoption of models which facilitate greater
automation in risk management and decision-making, this paper presents a novel
Importance Sampling (IS) scheme for measuring distribution tails of objectives
modelled with enabling tools such as feature-based decision rules, mixed
integer linear programs, deep neural networks, etc. Conventional efficient IS
approaches suffer from feasibility and scalability concerns due to the need to
intricately tailor the sampler to the underlying probability distribution and
the objective. This challenge is overcome in the proposed black-box scheme by
automating the selection of an effective IS distribution with a transformation
that implicitly learns and replicates the concentration properties observed in
less rare samples. This novel approach is guided by a large deviations
principle that brings out the phenomenon of self-similarity of optimal IS
distributions. The proposed sampler is the first to attain asymptotically
optimal variance reduction across a spectrum of multivariate distributions
despite being oblivious to the underlying structure. The large deviations
principle additionally results in new distribution tail asymptotics capable of
yielding operational insights. The applicability is illustrated by considering
product distribution networks and portfolio credit risk models informed by
neural networks as examples.","cs.LG,math.PR,stat.ME,stat.ML"
"Deep Reinforcement Learning for Control of Probabilistic Boolean Networks. Probabilistic Boolean Networks (PBNs) were introduced as a computational
model for the study of complex dynamical systems, such as Gene Regulatory
Networks (GRNs). Controllability in this context is the process of making
strategic interventions to the state of a network in order to drive it towards
some other state that exhibits favourable biological properties. In this paper
we study the ability of a Double Deep Q-Network with Prioritized Experience
Replay in learning control strategies within a finite number of time steps that
drive a PBN towards a target state, typically an attractor. The control method
is model-free and does not require knowledge of the network's underlying
dynamics, making it suitable for applications where inference of such dynamics
is intractable. We present extensive experiment results on two synthetic PBNs
and the PBN model constructed directly from gene-expression data of a study on
metastatic-melanoma.","cs.AI,cs.LG,cs.SY,eess.SY"
"A Comparative Study of Machine Learning Models for Predicting the State of Reactive Mixing. Accurate predictions of reactive mixing are critical for many Earth and
environmental science problems. To investigate mixing dynamics over time under
different scenarios, a high-fidelity, finite-element-based numerical model is
built to solve the fast, irreversible bimolecular reaction-diffusion equations
to simulate a range of reactive-mixing scenarios. A total of 2,315 simulations
are performed using different sets of model input parameters comprising various
spatial scales of vortex structures in the velocity field, time-scales
associated with velocity oscillations, the perturbation parameter for the
vortex-based velocity, anisotropic dispersion contrast, and molecular
diffusion. Outputs comprise concentration profiles of the reactants and
products. The inputs and outputs of these simulations are concatenated into
feature and label matrices, respectively, to train 20 different machine
learning (ML) emulators to approximate system behavior. The 20 ML emulators
based on linear methods, Bayesian methods, ensemble learning methods, and
multilayer perceptron (MLP), are compared to assess these models. The ML
emulators are specifically trained to classify the state of mixing and predict
three quantities of interest (QoIs) characterizing species production, decay,
and degree of mixing. Linear classifiers and regressors fail to reproduce the
QoIs; however, ensemble methods (classifiers and regressors) and the MLP
accurately classify the state of reactive mixing and the QoIs. Among ensemble
methods, random forest and decision-tree-based AdaBoost faithfully predict the
QoIs. At run time, trained ML emulators are $\approx10^5$ times faster than the
high-fidelity numerical simulations. Speed and accuracy of the ensemble and MLP
models facilitate uncertainty quantification, which usually requires 1,000s of
model run, to estimate the uncertainty bounds on the QoIs.","cs.DC,cs.LG,cs.NA,math.NA,stat.ML"
"Automatic Localization of Deep Stimulation Electrodes Using Trajectory-based Segmentation Approach. Parkinson's disease (PD) is a degenerative condition of the nervous system,
which manifests itself primarily as muscle stiffness, hypokinesia,
bradykinesia, and tremor. In patients suffering from advanced stages of PD,
Deep Brain Stimulation neurosurgery (DBS) is the best alternative to medical
treatment, especially when they become tolerant to the drugs. This surgery
produces a neuronal activity, a result from electrical stimulation, whose
quantification is known as Volume of Tissue Activated (VTA). To locate
correctly the VTA in the cerebral volume space, one should be aware exactly the
location of the tip of the DBS electrodes, as well as their spatial projection.
  In this paper, we automatically locate DBS electrodes using a threshold-based
medical imaging segmentation methodology, determining the optimal value of this
threshold adaptively. The proposed methodology allows the localization of DBS
electrodes in Computed Tomography (CT) images, with high noise tolerance, using
automatic threshold detection methods.","cs.CV,q-bio.NC"
"Unsupervised Learning of Disentangled and Interpretable Representations from Sequential Data. We present a factorized hierarchical variational autoencoder, which learns
disentangled and interpretable representations from sequential data without
supervision. Specifically, we exploit the multi-scale nature of information in
sequential data by formulating it explicitly within a factorized hierarchical
graphical model that imposes sequence-dependent priors and sequence-independent
priors to different sets of latent variables. The model is evaluated on two
speech corpora to demonstrate, qualitatively, its ability to transform speakers
or linguistic content by manipulating different sets of latent variables; and
quantitatively, its ability to outperform an i-vector baseline for speaker
verification and reduce the word error rate by as much as 35% in mismatched
train/test scenarios for automatic speech recognition tasks.","cs.CL,cs.LG,cs.SD,eess.AS,stat.ML"
"Algorithmic Recourse in Partially and Fully Confounded Settings Through Bounding Counterfactual Effects. Algorithmic recourse aims to provide actionable recommendations to
individuals to obtain a more favourable outcome from an automated
decision-making system. As it involves reasoning about interventions performed
in the physical world, recourse is fundamentally a causal problem. Existing
methods compute the effect of recourse actions using a causal model learnt from
data under the assumption of no hidden confounding and modelling assumptions
such as additive noise. Building on the seminal work of Balke and Pearl (1994),
we propose an alternative approach for discrete random variables which relaxes
these assumptions and allows for unobserved confounding and arbitrary
structural equations. The proposed approach only requires specification of the
causal graph and confounding structure and bounds the expected counterfactual
effect of recourse actions. If the lower bound is above a certain threshold,
i.e., on the other side of the decision boundary, recourse is guaranteed in
expectation.","cs.AI,cs.LG,stat.ML"
"Adapting to Non-stationarity with Growing Expert Ensembles. When dealing with time series with complex non-stationarities, low
retrospective regret on individual realizations is a more appropriate goal than
low prospective risk in expectation. Online learning algorithms provide
powerful guarantees of this form, and have often been proposed for use with
non-stationary processes because of their ability to switch between different
forecasters or ``experts''. However, existing methods assume that the set of
experts whose forecasts are to be combined are all given at the start, which is
not plausible when dealing with a genuinely historical or evolutionary system.
We show how to modify the ``fixed shares'' algorithm for tracking the best
expert to cope with a steadily growing set of experts, obtained by fitting new
models to new data as it becomes available, and obtain regret bounds for the
growing ensemble.","cs.LG,physics.data-an,stat.ME,stat.ML"
"Non-convex Global Minimization and False Discovery Rate Control for the TREX. The TREX is a recently introduced method for performing sparse
high-dimensional regression. Despite its statistical promise as an alternative
to the lasso, square-root lasso, and scaled lasso, the TREX is computationally
challenging in that it requires solving a non-convex optimization problem. This
paper shows a remarkable result: despite the non-convexity of the TREX problem,
there exists a polynomial-time algorithm that is guaranteed to find the global
minimum. This result adds the TREX to a very short list of non-convex
optimization problems that can be globally optimized (principal components
analysis being a famous example). After deriving and developing this new
approach, we demonstrate that (i) the ability of the preexisting TREX heuristic
to reach the global minimum is strongly dependent on the difficulty of the
underlying statistical problem, (ii) the new polynomial-time algorithm for TREX
permits a novel variable ranking and selection scheme, (iii) this scheme can be
incorporated into a rule that controls the false discovery rate (FDR) of
included features in the model. To achieve this last aim, we provide an
extension of the results of Barber & Candes (2015) to establish that the
knockoff filter framework can be applied to the TREX. This investigation thus
provides both a rare case study of a heuristic for non-convex optimization and
a novel way of exploiting non-convexity for statistical inference.","cs.OH,stat.CO,stat.ME,stat.ML"
"Dirichlet Mixture Model based VQ Performance Prediction for Line Spectral Frequency. In this paper, we continue our previous work on the Dirichlet mixture model
(DMM)-based VQ to derive the performance bound of the LSF VQ. The LSF
parameters are transformed into the $\Delta$LSF domain and the underlying
distribution of the $\Delta$LSF parameters are modelled by a DMM with finite
number of mixture components. The quantization distortion, in terms of the mean
squared error (MSE), is calculated with the high rate theory. The mapping
relation between the perceptually motivated log spectral distortion (LSD) and
the MSE is empirically approximated by a polynomial. With this mapping
function, the minimum required bit rate for transparent coding of the LSF is
estimated.","cs.LG,cs.SD,eess.AS,stat.ML"
"Do Large Scale Molecular Language Representations Capture Important Structural Information?. Predicting chemical properties from the structure of a molecule is of great
importance in many applications including drug discovery and material design.
Machine learning based molecular property prediction holds the promise of
enabling accurate predictions at much less complexity, when compared to, for
example Density Functional Theory (DFT) calculations. Features extracted from
molecular graphs, using graph neural nets in a supervised manner, have emerged
as strong baselines for such tasks. However, the vast chemical space together
with the limited availability of labels makes supervised learning challenging,
calling for learning a general-purpose molecular representation. Recently,
pre-trained transformer-based language models (PTLMs) on large unlabeled corpus
have produced state-of-the-art results in many downstream natural language
processing tasks. Inspired by this development, here we present molecular
embeddings obtained by training an efficient transformer encoder model,
referred to as MoLFormer. This model was employed with a linear attention
mechanism and highly paralleized training on 1D SMILES sequences of 1.1 billion
unlabeled molecules from the PubChem and ZINC datasets. Experiments show that
the learned molecular representation performs competitively, when compared to
existing graph-based and fingerprint-based supervised learning baselines, on
the challenging tasks of predicting properties of QM8 and QM9 molecules.
Further task-specific fine-tuning of the MoLFormerr representation improves
performance on several of those property prediction benchmarks. These results
provide encouraging evidence that large-scale molecular language models can
capture sufficient structural information to be able to accurately predict
quantum chemical properties and beyond.","cs.CL,cs.LG,q-bio.BM"
"Personalization for Web-based Services using Offline Reinforcement Learning. Large-scale Web-based services present opportunities for improving UI
policies based on observed user interactions. We address challenges of learning
such policies through model-free offline Reinforcement Learning (RL) with
off-policy training. Deployed in a production system for user authentication in
a major social network, it significantly improves long-term objectives. We
articulate practical challenges, compare several ML techniques, provide
insights on training and evaluation of RL models, and discuss generalizations.","cs.HC,cs.LG,cs.SE"
"A Tour of Convolutional Networks Guided by Linear Interpreters. Convolutional networks are large linear systems divided into layers and
connected by non-linear units. These units are the ""articulations"" that allow
the network to adapt to the input. To understand how a network manages to solve
a problem we must look at the articulated decisions in entirety. If we could
capture the actions of non-linear units for a particular input, we would be
able to replay the whole system back and forth as if it was always linear. It
would also reveal the actions of non-linearities because the resulting linear
system, a Linear Interpreter, depends on the input image. We introduce a
hooking layer, called a LinearScope, which allows us to run the network and the
linear interpreter in parallel. Its implementation is simple, flexible and
efficient. From here we can make many curious inquiries: how do these linear
systems look like? When the rows and columns of the transformation matrix are
images, how do they look like? What type of basis do these linear
transformations rely on? The answers depend on the problems presented, through
which we take a tour to some popular architectures used for classification,
super-resolution (SR) and image-to-image translation (I2I). For classification
we observe that popular networks use a pixel-wise vote per class strategy and
heavily rely on bias parameters. For SR and I2I we find that CNNs use
wavelet-type basis similar to the human visual system. For I2I we reveal
copy-move and template-creation strategies to generate outputs.","cs.CV,cs.LG,cs.NA,eess.IV,math.NA"
"Latent-Space Laplacian Pyramids for Adversarial Representation Learning with 3D Point Clouds. Constructing high-quality generative models for 3D shapes is a fundamental
task in computer vision with diverse applications in geometry processing,
engineering, and design. Despite the recent progress in deep generative
modelling, synthesis of finely detailed 3D surfaces, such as high-resolution
point clouds, from scratch has not been achieved with existing approaches. In
this work, we propose to employ the latent-space Laplacian pyramid
representation within a hierarchical generative model for 3D point clouds. We
combine the recently proposed latent-space GAN and Laplacian GAN architectures
to form a multi-scale model capable of generating 3D point clouds at increasing
levels of detail. Our evaluation demonstrates that our model outperforms the
existing generative models for 3D point clouds.","cs.CV,eess.IV"
"Receiver operating characteristic (ROC) movies, universal ROC (UROC) curves, and coefficient of predictive ability (CPA). Throughout science and technology, receiver operating characteristic (ROC)
curves and associated area under the curve (AUC) measures constitute powerful
tools for assessing the predictive abilities of features, markers and tests in
binary classification problems. Despite its immense popularity, ROC analysis
has been subject to a fundamental restriction, in that it applies to
dichotomous (yes or no) outcomes only. Here we introduce ROC movies and
universal ROC (UROC) curves that apply to just any linearly ordered outcome,
along with an associated coefficient of predictive ability (CPA) measure. CPA
equals the area under the UROC curve, and admits appealing interpretations in
terms of probabilities and rank based covariances. For binary outcomes CPA
equals AUC, and for pairwise distinct outcomes CPA relates linearly to
Spearman's coefficient, in the same way that the C index relates linearly to
Kendall's coefficient. ROC movies, UROC curves, and CPA nest and generalize the
tools of classical ROC analysis, and are bound to supersede them in a wealth of
applications. Their usage is illustrated in data examples from biomedicine and
meteorology, where rank based measures yield new insights in the WeatherBench
comparison of the predictive performance of convolutional neural networks and
physical-numerical models for weather prediction.","cs.LG,stat.ME,stat.ML"
"Guiding Graph Embeddings using Path-Ranking Methods for Error Detection innoisy Knowledge Graphs. Nowadays Knowledge Graphs constitute a mainstream approach for the
representation of relational information on big heterogeneous data, however,
they may contain a big amount of imputed noise when constructed automatically.
To address this problem, different error detection methodologies have been
proposed, mainly focusing on path ranking and representation learning. This
work presents various mainstream approaches and proposes a hybrid and modular
methodology for the task. We compare different methods on two benchmarks and
one real-world biomedical publications dataset, showcasing the potential of our
approach and providing insights on graph embeddings when dealing with noisy
Knowledge Graphs.","cs.AI,cs.IR,cs.LG,stat.ML"
"Fold2Seq: A Joint Sequence(1D)-Fold(3D) Embedding-based Generative Model for Protein Design. Designing novel protein sequences for a desired 3D topological fold is a
fundamental yet non-trivial task in protein engineering. Challenges exist due
to the complex sequence--fold relationship, as well as the difficulties to
capture the diversity of the sequences (therefore structures and functions)
within a fold. To overcome these challenges, we propose Fold2Seq, a novel
transformer-based generative framework for designing protein sequences
conditioned on a specific target fold. To model the complex sequence--structure
relationship, Fold2Seq jointly learns a sequence embedding using a transformer
and a fold embedding from the density of secondary structural elements in 3D
voxels. On test sets with single, high-resolution and complete structure inputs
for individual folds, our experiments demonstrate improved or comparable
performance of Fold2Seq in terms of speed, coverage, and reliability for
sequence design, when compared to existing state-of-the-art methods that
include data-driven deep generative models and physics-based RosettaDesign. The
unique advantages of fold-based Fold2Seq, in comparison to a structure-based
deep model and RosettaDesign, become more evident on three additional
real-world challenges originating from low-quality, incomplete, or ambiguous
input structures. Source code and data are available at
https://github.com/IBM/fold2seq.","cs.LG,q-bio.BM"
"A Worrying Analysis of Probabilistic Time-series Models for Sales Forecasting. Probabilistic time-series models become popular in the forecasting field as
they help to make optimal decisions under uncertainty. Despite the growing
interest, a lack of thorough analysis hinders choosing what is worth applying
for the desired task. In this paper, we analyze the performance of three
prominent probabilistic time-series models for sales forecasting. To remove the
role of random chance in architecture's performance, we make two experimental
principles; 1) Large-scale dataset with various cross-validation sets. 2) A
standardized training and hyperparameter selection. The experimental results
show that a simple Multi-layer Perceptron and Linear Regression outperform the
probabilistic models on RMSE without any feature engineering. Overall, the
probabilistic models fail to achieve better performance on point estimation,
such as RMSE and MAPE, than comparably simple baselines. We analyze and discuss
the performances of probabilistic time-series models.","cs.AI,cs.LG,stat.CO,stat.ME"
"Explain Me the Painting: Multi-Topic Knowledgeable Art Description Generation. Have you ever looked at a painting and wondered what is the story behind it?
This work presents a framework to bring art closer to people by generating
comprehensive descriptions of fine-art paintings. Generating informative
descriptions for artworks, however, is extremely challenging, as it requires to
1) describe multiple aspects of the image such as its style, content, or
composition, and 2) provide background and contextual knowledge about the
artist, their influences, or the historical period. To address these
challenges, we introduce a multi-topic and knowledgeable art description
framework, which modules the generated sentences according to three artistic
topics and, additionally, enhances each description with external knowledge.
The framework is validated through an exhaustive analysis, both quantitative
and qualitative, as well as a comparative human evaluation, demonstrating
outstanding results in terms of both topic diversity and information veracity.","cs.AI,cs.CL,cs.CV"
"Automatic Classification of Irregularly Sampled Time Series with Unequal Lengths: A Case Study on Estimated Glomerular Filtration Rate. A patient's estimated glomerular filtration rate (eGFR) can provide important
information about disease progression and kidney function. Traditionally, an
eGFR time series is interpreted by a human expert labelling it as stable or
unstable. While this approach works for individual patients, the time consuming
nature of it precludes the quick evaluation of risk in large numbers of
patients. However, automating this process poses significant challenges as eGFR
measurements are usually recorded at irregular intervals and the series of
measurements differs in length between patients. Here we present a two-tier
system to automatically classify an eGFR trend. First, we model the time series
using Gaussian process regression (GPR) to fill in `gaps' by resampling a fixed
size vector of fifty time-dependent observations. Second, we classify the
resampled eGFR time series using a K-NN/SVM classifier, and evaluate its
performance via 5-fold cross validation. Using this approach we achieved an
F-score of 0.90, compared to 0.96 for 5 human experts when scored amongst
themselves.","cs.CE,cs.LG"
"Chaining Mutual Information and Tightening Generalization Bounds. Bounding the generalization error of learning algorithms has a long history,
which yet falls short in explaining various generalization successes including
those of deep learning. Two important difficulties are (i) exploiting the
dependencies between the hypotheses, (ii) exploiting the dependence between the
algorithm's input and output. Progress on the first point was made with the
chaining method, originating from the work of Kolmogorov, and used in the
VC-dimension bound. More recently, progress on the second point was made with
the mutual information method by Russo and Zou '15. Yet, these two methods are
currently disjoint. In this paper, we introduce a technique to combine the
chaining and mutual information methods, to obtain a generalization bound that
is both algorithm-dependent and that exploits the dependencies between the
hypotheses. We provide an example in which our bound significantly outperforms
both the chaining and the mutual information bounds. As a corollary, we tighten
Dudley's inequality when the learning algorithm chooses its output from a small
subset of hypotheses with high probability.","cs.IT,cs.LG,math.IT,math.PR,stat.ML"
"Modelling Paralinguistic Properties in Conversational Speech to Detect Bipolar Disorder and Borderline Personality Disorder. Bipolar disorder (BD) and borderline personality disorder (BPD) are two
chronic mental health conditions that clinicians find challenging to
distinguish based on clinical interviews, due to their overlapping symptoms. In
this work, we investigate the automatic detection of these two conditions by
modelling both verbal and non-verbal cues in a set of interviews. We propose a
new approach of modelling short-term features with visibility-signature
transform, and compare it with widely used high-level statistical functions. We
demonstrate the superior performance of our proposed signature-based model.
Furthermore, we show the role of different sets of features in characterising
BD and BPD.","60L10,cs.LG,cs.SD,eess.AS"
"Deep Neural Networks for automatic extraction of features in time series satellite images. Many earth observation programs such as Landsat, Sentinel, SPOT, and Pleiades
produce huge volume of medium to high resolution multi-spectral images every
day that can be organized in time series. In this work, we exploit both
temporal and spatial information provided by these images to generate land
cover maps. For this purpose, we combine a fully convolutional neural network
with a convolutional long short-term memory. Implementation details of the
proposed spatio-temporal neural network architecture are provided. Experimental
results show that the temporal information provided by time series images
allows increasing the accuracy of land cover classification, thus producing
up-to-date maps that can help in identifying changes on earth.","cs.CV,cs.LG,eess.IV"
"The Bottleneck Simulator: A Model-based Deep Reinforcement Learning Approach. Deep reinforcement learning has recently shown many impressive successes.
However, one major obstacle towards applying such methods to real-world
problems is their lack of data-efficiency. To this end, we propose the
Bottleneck Simulator: a model-based reinforcement learning method which
combines a learned, factorized transition model of the environment with rollout
simulations to learn an effective policy from few examples. The learned
transition model employs an abstract, discrete (bottleneck) state, which
increases sample efficiency by reducing the number of model parameters and by
exploiting structural properties of the environment. We provide a mathematical
analysis of the Bottleneck Simulator in terms of fixed points of the learned
policy, which reveals how performance is affected by four distinct sources of
error: an error related to the abstract space structure, an error related to
the transition model estimation variance, an error related to the transition
model estimation bias, and an error related to the transition model class bias.
Finally, we evaluate the Bottleneck Simulator on two natural language
processing tasks: a text adventure game and a real-world, complex dialogue
response selection task. On both tasks, the Bottleneck Simulator yields
excellent performance beating competing approaches.","I.5.1; I.2.7,cs.AI,cs.CL,cs.LG,cs.NE,stat.ML"
"Transform Quantization for CNN Compression. In this paper, we compress convolutional neural network (CNN) weights
post-training via transform quantization. Previous CNN quantization techniques
tend to ignore the joint statistics of weights and activations, producing
sub-optimal CNN performance at a given quantization bit-rate, or consider their
joint statistics during training only and do not facilitate efficient
compression of already trained CNN models. We optimally transform (decorrelate)
and quantize the weights post-training using a rate-distortion framework to
improve compression at any given quantization bit-rate. Transform quantization
unifies quantization and dimensionality reduction (decorrelation) techniques in
a single framework to facilitate low bit-rate compression of CNNs and efficient
inference in the transform domain. We first introduce a theory of rate and
distortion for CNN quantization, and pose optimum quantization as a
rate-distortion optimization problem. We then show that this problem can be
solved using optimal bit-depth allocation following decorrelation by the
optimal End-to-end Learned Transform (ELT) we derive in this paper. Experiments
demonstrate that transform quantization advances the state of the art in CNN
compression in both retrained and non-retrained quantization scenarios. In
particular, we find that transform quantization with retraining is able to
compress CNN models such as AlexNet, ResNet and DenseNet to very low bit-rates
(1-2 bits).","cs.CV,cs.IT,cs.LG,eess.IV,math.IT"
"Predicting Driver Takeover Time in Conditionally Automated Driving. It is extremely important to ensure a safe takeover transition in
conditionally automated driving. One of the critical factors that quantifies
the safe takeover transition is takeover time. Previous studies identified the
effects of many factors on takeover time, such as takeover lead time,
non-driving tasks, modalities of the takeover requests (TORs), and scenario
urgency. However, there is a lack of research to predict takeover time by
considering these factors all at the same time. Toward this end, we used
eXtreme Gradient Boosting (XGBoost) to predict the takeover time using a
dataset from a meta-analysis study [1]. In addition, we used SHAP (SHapley
Additive exPlanation) to analyze and explain the effects of the predictors on
takeover time. We identified seven most critical predictors that resulted in
the best prediction performance. Their main effects and interaction effects on
takeover time were examined. The results showed that the proposed approach
provided both good performance and explainability. Our findings have
implications on the design of in-vehicle monitoring and alert systems to
facilitate the interaction between the drivers and the automated vehicle.","cs.HC,cs.LG,cs.RO"
"Hierarchically-coupled hidden Markov models for learning kinetic rates from single-molecule data. We address the problem of analyzing sets of noisy time-varying signals that
all report on the same process but confound straightforward analyses due to
complex inter-signal heterogeneities and measurement artifacts. In particular
we consider single-molecule experiments which indirectly measure the distinct
steps in a biomolecular process via observations of noisy time-dependent
signals such as a fluorescence intensity or bead position. Straightforward
hidden Markov model (HMM) analyses attempt to characterize such processes in
terms of a set of conformational states, the transitions that can occur between
these states, and the associated rates at which those transitions occur; but
require ad-hoc post-processing steps to combine multiple signals. Here we
develop a hierarchically coupled HMM that allows experimentalists to deal with
inter-signal variability in a principled and automatic way. Our approach is a
generalized expectation maximization hyperparameter point estimation procedure
with variational Bayes at the level of individual time series that learns an
single interpretable representation of the overall data generating process.","physics.bio-ph,q-bio.QM,stat.ML"
"Uncertainty-Aware Deep Ensembles for Reliable and Explainable Predictions of Clinical Time Series. Deep learning-based support systems have demonstrated encouraging results in
numerous clinical applications involving the processing of time series data.
While such systems often are very accurate, they have no inherent mechanism for
explaining what influenced the predictions, which is critical for clinical
tasks. However, existing explainability techniques lack an important component
for trustworthy and reliable decision support, namely a notion of uncertainty.
In this paper, we address this lack of uncertainty by proposing a deep ensemble
approach where a collection of DNNs are trained independently. A measure of
uncertainty in the relevance scores is computed by taking the standard
deviation across the relevance scores produced by each model in the ensemble,
which in turn is used to make the explanations more reliable. The class
activation mapping method is used to assign a relevance score for each time
step in the time series. Results demonstrate that the proposed ensemble is more
accurate in locating relevant time steps and is more consistent across random
initializations, thus making the model more trustworthy. The proposed
methodology paves the way for constructing trustworthy and dependable support
systems for processing clinical time series for healthcare related tasks.","cs.LG,eess.SP"
"Machine Learning for Security in Vehicular Networks: A Comprehensive Survey. Machine Learning (ML) has emerged as an attractive and viable technique to
provide effective solutions for a wide range of application domains. An
important application domain is vehicular networks wherein ML-based approaches
are found to be very useful to address various problems. The use of wireless
communication between vehicular nodes and/or infrastructure makes it vulnerable
to different types of attacks. In this regard, ML and its variants are gaining
popularity to detect attacks and deal with different kinds of security issues
in vehicular communication. In this paper, we present a comprehensive survey of
ML-based techniques for different security issues in vehicular networks. We
first briefly introduce the basics of vehicular networks and different types of
communications. Apart from the traditional vehicular networks, we also consider
modern vehicular network architectures. We propose a taxonomy of security
attacks in vehicular networks and discuss various security challenges and
requirements. We classify the ML techniques developed in the literature
according to their use in vehicular network applications. We explain the
solution approaches and working principles of these ML techniques in addressing
various security challenges and provide insightful discussion. The limitations
and challenges in using ML-based methods in vehicular networks are discussed.
Finally, we present observations and lessons learned before we conclude our
work.","cs.CR,cs.LG,cs.NI"
"Transfer-based adaptive tree for multimodal sentiment analysis based on user latent aspects. Multimodal sentiment analysis benefits various applications such as
human-computer interaction and recommendation systems. It aims to infer the
users' bipolar ideas using visual, textual, and acoustic signals. Although
researchers affirm the association between cognitive cues and emotional
manifestations, most of the current multimodal approaches in sentiment analysis
disregard user-specific aspects. To tackle this issue, we devise a novel method
to perform multimodal sentiment prediction using cognitive cues, such as
personality. Our framework constructs an adaptive tree by hierarchically
dividing users and trains the LSTM-based submodels, utilizing an
attention-based fusion to transfer cognitive-oriented knowledge within the
tree. Subsequently, the framework consumes the conclusive agglomerative
knowledge from the adaptive tree to predict final sentiments. We also devise a
dynamic dropout method to facilitate data sharing between neighboring nodes,
reducing data sparsity. The empirical results on real-world datasets determine
that our proposed model for sentiment prediction can surpass trending rivals.
Moreover, compared to other ensemble approaches, the proposed transfer-based
algorithm can better utilize the latent cognitive cues and foster the
prediction outcomes. Based on the given extrinsic and intrinsic analysis
results, we note that compared to other theoretical-based techniques, the
proposed hierarchical clustering approach can better group the users within the
adaptive tree.","cs.DB,cs.IR,cs.LG,cs.MM"
"Learning Structural Edits via Incremental Tree Transformations. While most neural generative models generate outputs in a single pass, the
human creative process is usually one of iterative building and refinement.
Recent work has proposed models of editing processes, but these mostly focus on
editing sequential data and/or only model a single editing pass. In this paper,
we present a generic model for incremental editing of structured data (i.e.,
""structural edits""). Particularly, we focus on tree-structured data, taking
abstract syntax trees of computer programs as our canonical example. Our editor
learns to iteratively generate tree edits (e.g., deleting or adding a subtree)
and applies them to the partially edited data, thereby the entire editing
process can be formulated as consecutive, incremental tree transformations. To
show the unique benefits of modeling tree edits directly, we further propose a
novel edit encoder for learning to represent edits, as well as an imitation
learning method that allows the editor to be more robust. We evaluate our
proposed editor on two source code edit datasets, where results show that, with
the proposed edit encoder, our editor significantly improves accuracy over
previous approaches that generate the edited program directly in one pass.
Finally, we demonstrate that training our editor to imitate experts and correct
its mistakes dynamically can further improve its performance.","cs.LG,cs.SE"
"Geometry-aware Transformer for molecular property prediction. Recently, graph neural networks (GNNs) have achieved remarkable performances
for quantum mechanical problems. However, a graph convolution can only cover a
localized region, and cannot capture long-range interactions of atoms. This
behavior is contrary to theoretical interatomic potentials, which is a
fundamental limitation of the spatial based GNNs. In this work, we propose a
novel attention-based framework for molecular property prediction tasks. We
represent a molecular conformation as a discrete atomic sequence combined by
atom-atom distance attributes, named Geometry-aware Transformer (GeoT). In
particular, we adopt a Transformer architecture, which has been widely used for
sequential data. Our proposed model trains sequential representations of
molecular graphs based on globally constructed attentions, maintaining all
spatial arrangements of atom pairs. Our method does not suffer from cost
intensive computations, such as angle calculations. The experimental results on
several public benchmarks and visualization maps verified that keeping the
long-range interatomic attributes can significantly improve the model
predictability.","cs.LG,physics.chem-ph"
"$QD$-Learning: A Collaborative Distributed Strategy for Multi-Agent Reinforcement Learning Through Consensus + Innovations. The paper considers a class of multi-agent Markov decision processes (MDPs),
in which the network agents respond differently (as manifested by the
instantaneous one-stage random costs) to a global controlled state and the
control actions of a remote controller. The paper investigates a distributed
reinforcement learning setup with no prior information on the global state
transition and local agent cost statistics. Specifically, with the agents'
objective consisting of minimizing a network-averaged infinite horizon
discounted cost, the paper proposes a distributed version of $Q$-learning,
$\mathcal{QD}$-learning, in which the network agents collaborate by means of
local processing and mutual information exchange over a sparse (possibly
stochastic) communication network to achieve the network goal. Under the
assumption that each agent is only aware of its local online cost data and the
inter-agent communication network is \emph{weakly} connected, the proposed
distributed scheme is almost surely (a.s.) shown to yield asymptotically the
desired value function and the optimal stationary control policy at each
network agent. The analytical techniques developed in the paper to address the
mixed time-scale stochastic dynamics of the \emph{consensus + innovations}
form, which arise as a result of the proposed interactive distributed scheme,
are of independent interest.","cs.LG,cs.MA,math.OC,math.PR,stat.ML"
"On Robustness of Lane Detection Models to Physical-World Adversarial Attacks in Autonomous Driving. After the 2017 TuSimple Lane Detection Challenge, its evaluation based on
accuracy and F1 score has become the de facto standard to measure the
performance of lane detection methods. In this work, we conduct the first
large-scale empirical study to evaluate the robustness of state-of-the-art lane
detection methods under physical-world adversarial attacks in autonomous
driving. We evaluate 4 major types of lane detection approaches with the
conventional evaluation and end-to-end evaluation in autonomous driving
scenarios and then discuss the security proprieties of each lane detection
model. We demonstrate that the conventional evaluation fails to reflect the
robustness in end-to-end autonomous driving scenarios. Our results show that
the most robust model on the conventional metrics is the least robust in the
end-to-end evaluation. Although the competition dataset and its metrics have
played a substantial role in developing performant lane detection methods along
with the rapid development of deep neural networks, the conventional evaluation
is becoming obsolete and the gap between the metrics and practicality is
critical. We hope that our study will help the community make further progress
in building a more comprehensive framework to evaluate lane detection models.","cs.CR,cs.CV"
"Bayesian Topological Learning for Classifying the Structure of Biological Networks. Actin cytoskeleton networks generate local topological signatures due to the
natural variations in the number, size, and shape of holes of the networks.
Persistent homology is a method that explores these topological properties of
data and summarizes them as persistence diagrams. In this work, we analyze and
classify these filament networks by transforming them into persistence diagrams
whose variability is quantified via a Bayesian framework on the space of
persistence diagrams. The proposed generalized Bayesian framework adopts an
independent and identically distributed cluster point process characterization
of persistence diagrams and relies on a substitution likelihood argument. This
framework provides the flexibility to estimate the posterior cardinality
distribution of points in a persistence diagram and the posterior spatial
distribution simultaneously. We present a closed form of the posteriors under
the assumption of Gaussian mixtures and binomials for prior intensity and
cardinality respectively. Using this posterior calculation, we implement a
Bayes factor algorithm to classify the actin filament networks and benchmark it
against several state-of-the-art classification methods.","60G55,62-07,62F15,and 62P10,cs.LG,q-bio.QM,stat.ML"
"Online Classification Using a Voted RDA Method. We propose a voted dual averaging method for online classification problems
with explicit regularization. This method employs the update rule of the
regularized dual averaging (RDA) method, but only on the subsequence of
training examples where a classification error is made. We derive a bound on
the number of mistakes made by this method on the training set, as well as its
generalization error rate. We also introduce the concept of relative strength
of regularization, and show how it affects the mistake bound and generalization
performance. We experimented with the method using $\ell_1$ regularization on a
large-scale natural language processing task, and obtained state-of-the-art
classification performance with fairly sparse models.","cs.LG,stat.ML"
"Variable-rate discrete representation learning. Semantically meaningful information content in perceptual signals is usually
unevenly distributed. In speech signals for example, there are often many
silences, and the speed of pronunciation can vary considerably. In this work,
we propose slow autoencoders (SlowAEs) for unsupervised learning of high-level
variable-rate discrete representations of sequences, and apply them to speech.
We show that the resulting event-based representations automatically grow or
shrink depending on the density of salient information in the input signals,
while still allowing for faithful signal reconstruction. We develop run-length
Transformers (RLTs) for event-based representation modelling and use them to
construct language models in the speech domain, which are able to generate
grammatical and semantically coherent utterances and continuations.","cs.CL,cs.LG,cs.SD,eess.AS"
"Imitation from Observation: Learning to Imitate Behaviors from Raw Video via Context Translation. Imitation learning is an effective approach for autonomous systems to acquire
control policies when an explicit reward function is unavailable, using
supervision provided as demonstrations from an expert, typically a human
operator. However, standard imitation learning methods assume that the agent
receives examples of observation-action tuples that could be provided, for
instance, to a supervised learning algorithm. This stands in contrast to how
humans and animals imitate: we observe another person performing some behavior
and then figure out which actions will realize that behavior, compensating for
changes in viewpoint, surroundings, object positions and types, and other
factors. We term this kind of imitation learning ""imitation-from-observation,""
and propose an imitation learning method based on video prediction with context
translation and deep reinforcement learning. This lifts the assumption in
imitation learning that the demonstration should consist of observations in the
same environment configuration, and enables a variety of interesting
applications, including learning robotic skills that involve tool use simply by
observing videos of human tool use. Our experimental results show the
effectiveness of our approach in learning a wide range of real-world robotic
tasks modeled after common household chores from videos of a human
demonstrator, including sweeping, ladling almonds, pushing objects as well as a
number of tasks in simulation.","cs.AI,cs.CV,cs.LG,cs.NE,cs.RO"
"Spectrally-Encoded Single-Pixel Machine Vision Using Diffractive Networks. 3D engineering of matter has opened up new avenues for designing systems that
can perform various computational tasks through light-matter interaction. Here,
we demonstrate the design of optical networks in the form of multiple
diffractive layers that are trained using deep learning to transform and encode
the spatial information of objects into the power spectrum of the diffracted
light, which are used to perform optical classification of objects with a
single-pixel spectroscopic detector. Using a time-domain spectroscopy setup
with a plasmonic nanoantenna-based detector, we experimentally validated this
machine vision framework at terahertz spectrum to optically classify the images
of handwritten digits by detecting the spectral power of the diffracted light
at ten distinct wavelengths, each representing one class/digit. We also report
the coupling of this spectral encoding achieved through a diffractive optical
network with a shallow electronic neural network, separately trained to
reconstruct the images of handwritten digits based on solely the spectral
information encoded in these ten distinct wavelengths within the diffracted
light. These reconstructed images demonstrate task-specific image decompression
and can also be cycled back as new inputs to the same diffractive network to
improve its optical object classification. This unique machine vision framework
merges the power of deep learning with the spatial and spectral processing
capabilities of diffractive networks, and can also be extended to other
spectral-domain measurement systems to enable new 3D imaging and sensing
modalities integrated with spectrally encoded classification tasks performed
through diffractive optical networks.","cs.CV,cs.LG,cs.NE,eess.IV,physics.optics"
"Relevance of Rotationally Equivariant Convolutions for Predicting Molecular Properties. Equivariant neural networks (ENNs) are graph neural networks embedded in
$\mathbb{R}^3$ and are well suited for predicting molecular properties. The ENN
library e3nn has customizable convolutions, which can be designed to depend
only on distances between points, or also on angular features, making them
rotationally invariant, or equivariant, respectively. This paper studies the
practical value of including angular dependencies for molecular property
prediction directly via an ablation study with \texttt{e3nn} and the QM9 data
set. We find that, for fixed network depth and parameter count, adding angular
features decreased test error by an average of 23%. Meanwhile, increasing
network depth decreased test error by only 4% on average, implying that
rotationally equivariant layers are comparatively parameter efficient. We
present an explanation of the accuracy improvement on the dipole moment, the
target which benefited most from the introduction of angular features.","cs.LG,physics.chem-ph,physics.comp-ph,stat.ML"
"Dynamic Knapsack Optimization Towards Efficient Multi-Channel Sequential Advertising. In E-commerce, advertising is essential for merchants to reach their target
users. The typical objective is to maximize the advertiser's cumulative revenue
over a period of time under a budget constraint. In real applications, an
advertisement (ad) usually needs to be exposed to the same user multiple times
until the user finally contributes revenue (e.g., places an order). However,
existing advertising systems mainly focus on the immediate revenue with single
ad exposures, ignoring the contribution of each exposure to the final
conversion, thus usually falls into suboptimal solutions. In this paper, we
formulate the sequential advertising strategy optimization as a dynamic
knapsack problem. We propose a theoretically guaranteed bilevel optimization
framework, which significantly reduces the solution space of the original
optimization space while ensuring the solution quality. To improve the
exploration efficiency of reinforcement learning, we also devise an effective
action space reduction approach. Extensive offline and online experiments show
the superior performance of our approaches over state-of-the-art baselines in
terms of cumulative revenue.","cs.DS,cs.IR,cs.LG,cs.SY,eess.SY,stat.ML"
"Analysis of One-Hidden-Layer Neural Networks via the Resolvent Method. We compute the asymptotic empirical spectral distribution of a non-linear
random matrix model by using the resolvent method. Motivated by random neural
networks, we consider the random matrix $M = Y Y^\ast$ with $Y = f(WX)$, where
$W$ and $X$ are random rectangular matrices with i.i.d. centred entries and $f$
is a non-linear smooth function which is applied entry-wise. We prove that the
Stieltjes transform of the limiting spectral distribution satisfies a quartic
self-consistent equation up to some error terms, which is exactly the equation
obtained by [Pennington, Worah] and [Benigni, P\'{e}ch\'{e}] with the moment
method approach. In addition, we extend the previous results to the case of
additive bias $Y=f(WX+B)$ with $B$ being an independent rank-one Gaussian
random matrix, closer modelling the neural network infrastructures encountering
in practice. Our approach following the \emph{resolvent method} is more robust
than the moment method and is expected to provide insights also for models
where the combinatorics of the latter become intractable.","60B20,68T07,cs.LG,math.PR,stat.ML"
"Examining Deep Learning Models with Multiple Data Sources for COVID-19 Forecasting. The COVID-19 pandemic represents the most significant public health disaster
since the 1918 influenza pandemic. During pandemics such as COVID-19, timely
and reliable spatio-temporal forecasting of epidemic dynamics is crucial. Deep
learning-based time series models for forecasting have recently gained
popularity and have been successfully used for epidemic forecasting. Here we
focus on the design and analysis of deep learning-based models for COVID-19
forecasting. We implement multiple recurrent neural network-based deep learning
models and combine them using the stacking ensemble technique. In order to
incorporate the effects of multiple factors in COVID-19 spread, we consider
multiple sources such as COVID-19 confirmed and death case count data and
testing data for better predictions. To overcome the sparsity of training data
and to address the dynamic correlation of the disease, we propose
clustering-based training for high-resolution forecasting. The methods help us
to identify the similar trends of certain groups of regions due to various
spatio-temporal effects. We examine the proposed method for forecasting weekly
COVID-19 new confirmed cases at county-, state-, and country-level. A
comprehensive comparison between different time series models in COVID-19
context is conducted and analyzed. The results show that simple deep learning
models can achieve comparable or better performance when compared with more
complicated models. We are currently integrating our methods as a part of our
weekly forecasts that we provide state and federal authorities.","cs.LG,stat.AP"
"ChemBERTa: Large-Scale Self-Supervised Pretraining for Molecular Property Prediction. GNNs and chemical fingerprints are the predominant approaches to representing
molecules for property prediction. However, in NLP, transformers have become
the de-facto standard for representation learning thanks to their strong
downstream task transfer. In parallel, the software ecosystem around
transformers is maturing rapidly, with libraries like HuggingFace and BertViz
enabling streamlined training and introspection. In this work, we make one of
the first attempts to systematically evaluate transformers on molecular
property prediction tasks via our ChemBERTa model. ChemBERTa scales well with
pretraining dataset size, offering competitive downstream performance on
MoleculeNet and useful attention-based visualization modalities. Our results
suggest that transformers offer a promising avenue of future work for molecular
representation learning and property prediction. To facilitate these efforts,
we release a curated dataset of 77M SMILES from PubChem suitable for
large-scale self-supervised pretraining.","I.2.7; I.2.1; J.2; J.3,cs.CL,cs.LG,physics.chem-ph,q-bio.BM"
"Supporting Analysis of Dimensionality Reduction Results with Contrastive Learning. Dimensionality reduction (DR) is frequently used for analyzing and
visualizing high-dimensional data as it provides a good first glance of the
data. However, to interpret the DR result for gaining useful insights from the
data, it would take additional analysis effort such as identifying clusters and
understanding their characteristics. While there are many automatic methods
(e.g., density-based clustering methods) to identify clusters, effective
methods for understanding a cluster's characteristics are still lacking. A
cluster can be mostly characterized by its distribution of feature values.
Reviewing the original feature values is not a straightforward task when the
number of features is large. To address this challenge, we present a visual
analytics method that effectively highlights the essential features of a
cluster in a DR result. To extract the essential features, we introduce an
enhanced usage of contrastive principal component analysis (cPCA). Our method,
called ccPCA (contrasting clusters in PCA), can calculate each feature's
relative contribution to the contrast between one cluster and other clusters.
With ccPCA, we have created an interactive system including a scalable
visualization of clusters' feature contributions. We demonstrate the
effectiveness of our method and system with case studies using several publicly
available datasets.","I.3.8,cs.HC,cs.LG,stat.ML"
"On the Limitations of First-Order Approximation in GAN Dynamics. While Generative Adversarial Networks (GANs) have demonstrated promising
performance on multiple vision tasks, their learning dynamics are not yet well
understood, both in theory and in practice. To address this issue, we study GAN
dynamics in a simple yet rich parametric model that exhibits several of the
common problematic convergence behaviors such as vanishing gradients, mode
collapse, and diverging or oscillatory behavior. In spite of the non-convex
nature of our model, we are able to perform a rigorous theoretical analysis of
its convergence behavior. Our analysis reveals an interesting dichotomy: a GAN
with an optimal discriminator provably converges, while first order
approximations of the discriminator steps lead to unstable GAN dynamics and
mode collapse. Our result suggests that using first order discriminator steps
(the de-facto standard in most existing GAN setups) might be one of the factors
that makes GAN training challenging in practice.","cs.DS,cs.LG"
"Algorithmic Fairness Verification with Graphical Models. In recent years, machine learning (ML) algorithms have been deployed in
safety-critical and high-stake decision-making, where the fairness of
algorithms is of paramount importance. Fairness in ML centers on detecting bias
towards certain demographic populations induced by an ML classifier and
proposes algorithmic solutions to mitigate the bias with respect to different
fairness definitions. To this end, several fairness verifiers have been
proposed that compute the bias in the prediction of an ML classifier --
essentially beyond a finite dataset -- given the probability distribution of
input features. In the context of verifying linear classifiers, existing
fairness verifiers are limited by accuracy due to imprecise modelling of
correlations among features and scalability due to restrictive formulations of
the classifiers as SSAT or SMT formulas or by sampling. In this paper, we
propose an efficient fairness verifier, called FVGM, that encodes the
correlations among features as a Bayesian network. In contrast to existing
verifiers, FVGM proposes a stochastic subset-sum based approach for verifying
linear classifiers. Experimentally, we show that FVGM leads to an accurate and
scalable assessment for more diverse families of fairness-enhancing algorithms,
fairness attacks, and group/causal fairness metrics than the state-of-the-art.
We also demonstrate that FVGM facilitates the computation of fairness influence
functions as a stepping stone to detect the source of bias induced by subsets
of features.","cs.AI,cs.CY,cs.LG,stat.AP"
"Bayesian image segmentations by Potts prior and loopy belief propagation. This paper presents a Bayesian image segmentation model based on Potts prior
and loopy belief propagation. The proposed Bayesian model involves several
terms, including the pairwise interactions of Potts models, and the average
vectors and covariant matrices of Gauss distributions in color image modeling.
These terms are often referred to as hyperparameters in statistical machine
learning theory. In order to determine these hyperparameters, we propose a new
scheme for hyperparameter estimation based on conditional maximization of
entropy in the Potts prior. The algorithm is given based on loopy belief
propagation. In addition, we compare our conditional maximum entropy framework
with the conventional maximum likelihood framework, and also clarify how the
first order phase transitions in LBP's for Potts models influence our
hyperparameter estimation procedures.","cond-mat.dis-nn,cond-mat.stat-mech,cs.CV,cs.LG,stat.ML"
"GraphMI: Extracting Private Graph Data from Graph Neural Networks. As machine learning becomes more widely used for critical applications, the
need to study its implications in privacy turns to be urgent. Given access to
the target model and auxiliary information, the model inversion attack aims to
infer sensitive features of the training dataset, which leads to great privacy
concerns. Despite its success in grid-like domains, directly applying model
inversion techniques on non-grid domains such as graph achieves poor attack
performance due to the difficulty to fully exploit the intrinsic properties of
graphs and attributes of nodes used in Graph Neural Networks (GNN). To bridge
this gap, we present \textbf{Graph} \textbf{M}odel \textbf{I}nversion attack
(GraphMI), which aims to extract private graph data of the training graph by
inverting GNN, one of the state-of-the-art graph analysis tools. Specifically,
we firstly propose a projected gradient module to tackle the discreteness of
graph edges while preserving the sparsity and smoothness of graph features.
Then we design a graph auto-encoder module to efficiently exploit graph
topology, node attributes, and target model parameters for edge inference. With
the proposed methods, we study the connection between model inversion risk and
edge influence and show that edges with greater influence are more likely to be
recovered. Extensive experiments over several public datasets demonstrate the
effectiveness of our method. We also show that differential privacy in its
canonical form can hardly defend our attack while preserving decent utility.","cs.AI,cs.LG"
"Resource Constrained Deep Reinforcement Learning. In urban environments, supply resources have to be constantly matched to the
""right"" locations (where customer demand is present) so as to improve quality
of life. For instance, ambulances have to be matched to base stations regularly
so as to reduce response time for emergency incidents in EMS (Emergency
Management Systems); vehicles (cars, bikes, scooters etc.) have to be matched
to docking stations so as to reduce lost demand in shared mobility systems.
Such problem domains are challenging owing to the demand uncertainty,
combinatorial action spaces (due to allocation) and constraints on allocation
of resources (e.g., total resources, minimum and maximum number of resources at
locations and regions).
  Existing systems typically employ myopic and greedy optimization approaches
to optimize allocation of supply resources to locations. Such approaches
typically are unable to handle surges or variances in demand patterns well.
Recent research has demonstrated the ability of Deep RL methods in adapting
well to highly uncertain environments. However, existing Deep RL methods are
unable to handle combinatorial action spaces and constraints on allocation of
resources. To that end, we have developed three approaches on top of the well
known actor critic approach, DDPG (Deep Deterministic Policy Gradient) that are
able to handle constraints on resource allocation. More importantly, we
demonstrate that they are able to outperform leading approaches on simulators
validated on semi-real and real data sets.","68T05,cs.LG,stat.ML"
"Learning Features by Watching Objects Move. This paper presents a novel yet intuitive approach to unsupervised feature
learning. Inspired by the human visual system, we explore whether low-level
motion-based grouping cues can be used to learn an effective visual
representation. Specifically, we use unsupervised motion-based segmentation on
videos to obtain segments, which we use as 'pseudo ground truth' to train a
convolutional network to segment objects from a single frame. Given the
extensive evidence that motion plays a key role in the development of the human
visual system, we hope that this straightforward approach to unsupervised
learning will be more effective than cleverly designed 'pretext' tasks studied
in the literature. Indeed, our extensive experiments show that this is the
case. When used for transfer learning on object detection, our representation
significantly outperforms previous unsupervised approaches across multiple
settings, especially when training data for the target task is scarce.","cs.AI,cs.CV,cs.LG,cs.NE,stat.ML"
"Fold2Seq: A Joint Sequence(1D)-Fold(3D) Embedding-based Generative Model for Protein Design. Designing novel protein sequences for a desired 3D topological fold is a
fundamental yet non-trivial task in protein engineering. Challenges exist due
to the complex sequence--fold relationship, as well as the difficulties to
capture the diversity of the sequences (therefore structures and functions)
within a fold. To overcome these challenges, we propose Fold2Seq, a novel
transformer-based generative framework for designing protein sequences
conditioned on a specific target fold. To model the complex sequence--structure
relationship, Fold2Seq jointly learns a sequence embedding using a transformer
and a fold embedding from the density of secondary structural elements in 3D
voxels. On test sets with single, high-resolution and complete structure inputs
for individual folds, our experiments demonstrate improved or comparable
performance of Fold2Seq in terms of speed, coverage, and reliability for
sequence design, when compared to existing state-of-the-art methods that
include data-driven deep generative models and physics-based RosettaDesign. The
unique advantages of fold-based Fold2Seq, in comparison to a structure-based
deep model and RosettaDesign, become more evident on three additional
real-world challenges originating from low-quality, incomplete, or ambiguous
input structures. Source code and data are available at
https://github.com/IBM/fold2seq.","cs.LG,q-bio.BM"
"A Survey on Vision Transformer. Transformer, first applied to the field of natural language processing, is a
type of deep neural network mainly based on the self-attention mechanism.
Thanks to its strong representation capabilities, researchers are looking at
ways to apply transformer to computer vision tasks. In a variety of visual
benchmarks, transformer-based models perform similar to or better than other
types of networks such as convolutional and recurrent networks. Given its high
performance and less need for vision-specific inductive bias, transformer is
receiving more and more attention from the computer vision community. In this
paper, we review these vision transformer models by categorizing them in
different tasks and analyzing their advantages and disadvantages. The main
categories we explore include the backbone network, high/mid-level vision,
low-level vision, and video processing. We also include efficient transformer
methods for pushing transformer into real device-based applications.
Furthermore, we also take a brief look at the self-attention mechanism in
computer vision, as it is the base component in transformer. Toward the end of
this paper, we discuss the challenges and provide several further research
directions for vision transformers.","cs.AI,cs.CV"
"Path Integral Based Convolution and Pooling for Graph Neural Networks. Graph neural networks (GNNs) extends the functionality of traditional neural
networks to graph-structured data. Similar to CNNs, an optimized design of
graph convolution and pooling is key to success. Borrowing ideas from physics,
we propose a path integral based graph neural networks (PAN) for classification
and regression tasks on graphs. Specifically, we consider a convolution
operation that involves every path linking the message sender and receiver with
learnable weights depending on the path length, which corresponds to the
maximal entropy random walk. It generalizes the graph Laplacian to a new
transition matrix we call maximal entropy transition (MET) matrix derived from
a path integral formalism. Importantly, the diagonal entries of the MET matrix
are directly related to the subgraph centrality, thus providing a natural and
adaptive pooling mechanism. PAN provides a versatile framework that can be
tailored for different graph data with varying sizes and structures. We can
view most existing GNN architectures as special cases of PAN. Experimental
results show that PAN achieves state-of-the-art performance on various graph
classification/regression tasks, including a new benchmark dataset from
statistical mechanics we propose to boost applications of GNN in physical
sciences.","cond-mat.dis-nn,cs.LG,cs.NI,physics.data-an,stat.ML"
"Single-Photon Image Classification. Quantum computing-based machine learning mainly focuses on quantum computing
hardware that is experimentally challenging to realize due to requiring quantum
gates that operate at very low temperature. Instead, we demonstrate the
existence of a lower performance and much lower effort island on the
accuracy-vs-qubits graph that may well be experimentally accessible with room
temperature optics. This high temperature ""quantum computing toy model"" is
nevertheless interesting to study as it allows rather accessible explanations
of key concepts in quantum computing, in particular interference, entanglement,
and the measurement process.
  We specifically study the problem of classifying an example from the MNIST
and Fashion-MNIST datasets, subject to the constraint that we have to make a
prediction after the detection of the very first photon that passed a
coherently illuminated filter showing the example. Whereas a classical set-up
in which a photon is detected after falling on one of the $28\times 28$ image
pixels is limited to a (maximum likelihood estimation) accuracy of $21.27\%$
for MNIST, respectively $18.27\%$ for Fashion-MNIST, we show that the
theoretically achievable accuracy when exploiting inference by optically
transforming the quantum state of the photon is at least $41.27\%$ for MNIST,
respectively $36.14\%$ for Fashion-MNIST.
  We show in detail how to train the corresponding transformation with
TensorFlow and also explain how this example can serve as a teaching tool for
the measurement process in quantum mechanics.","cs.LG,quant-ph,stat.ML"
"Improving Movement Predictions of Traffic Actors in Bird's-Eye View Models using GANs and Differentiable Trajectory Rasterization. One of the most critical pieces of the self-driving puzzle is the task of
predicting future movement of surrounding traffic actors, which allows the
autonomous vehicle to safely and effectively plan its future route in a complex
world. Recently, a number of algorithms have been proposed to address this
important problem, spurred by a growing interest of researchers from both
industry and academia. Methods based on top-down scene rasterization on one
side and Generative Adversarial Networks (GANs) on the other have shown to be
particularly successful, obtaining state-of-the-art accuracies on the task of
traffic movement prediction. In this paper we build upon these two directions
and propose a raster-based conditional GAN architecture, powered by a novel
differentiable rasterizer module at the input of the conditional discriminator
that maps generated trajectories into the raster space in a differentiable
manner. This simplifies the task for the discriminator as trajectories that are
not scene-compliant are easier to discern, and allows the gradients to flow
back forcing the generator to output better, more realistic trajectories. We
evaluated the proposed method on a large-scale, real-world data set, showing
that it outperforms state-of-the-art GAN-based baselines.","cs.LG,cs.RO,eess.IV,stat.ML"
"PointManifoldCut: Point-wise Augmentation in the Manifold for Point Clouds. Augmentation can benefit point cloud learning due to the limited availability
of large-scale public datasets. This paper proposes a mix-up augmentation
approach, PointManifoldCut, which replaces the neural network embedded points,
rather than the Euclidean space coordinates. This approach takes the advantage
that points at the higher levels of the neural network are already trained to
embed its neighbors relations and mixing these representation will not mingle
the relation between itself and its label. This allows to regularize the
parameter space as the other augmentation methods but without worrying about
the proper label of the replaced points. The experiments show that our proposed
approach provides a competitive performance on point cloud classification and
segmentation when it is combined with the cutting-edge vanilla point cloud
networks. The result shows a consistent performance boosting compared to other
state-of-the-art point cloud augmentation method, such as PointMixup and
PointCutMix. The code of this paper is available at:
https://github.com/fun0515/PointManifoldCut.","68T45,I.4.3,cs.CV"
"TilinGNN: Learning to Tile with Self-Supervised Graph Neural Network. We introduce the first neural optimization framework to solve a classical
instance of the tiling problem. Namely, we seek a non-periodic tiling of an
arbitrary 2D shape using one or more types of tiles: the tiles maximally fill
the shape's interior without overlaps or holes. To start, we reformulate tiling
as a graph problem by modeling candidate tile locations in the target shape as
graph nodes and connectivity between tile locations as edges. Further, we build
a graph convolutional neural network, coined TilinGNN, to progressively
propagate and aggregate features over graph edges and predict tile placements.
TilinGNN is trained by maximizing the tiling coverage on target shapes, while
avoiding overlaps and holes between the tiles. Importantly, our network is
self-supervised, as we articulate these criteria as loss terms defined on the
network outputs, without the need of ground-truth tiling solutions. After
training, the runtime of TilinGNN is roughly linear to the number of candidate
tile locations, significantly outperforming traditional combinatorial search.
We conducted various experiments on a variety of shapes to showcase the speed
and versatility of TilinGNN. We also present comparisons to alternative methods
and manual solutions, robustness analysis, and ablation studies to demonstrate
the quality of our approach.","cs.CG,cs.CV,cs.GR,cs.LG"
"Simplicial Complex Representation Learning. Simplicial complexes form an important class of topological spaces that are
frequently used to in many applications areas such as computer-aided design,
computer graphics, and simulation. The representation learning on graphs, which
are just 1-d simplicial complexes, has witnessed a great attention and success
in the past few years. Due to the additional complexity higher dimensional
simplicial hold, there has not been enough effort to extend representation
learning to these objects especially when it comes to learn entire-simplicial
complex representation. In this work, we propose a method for simplicial
complex-level representation learning that embeds a simplicial complex to a
universal embedding space in a way that complex-to-complex proximity is
preserved. Our method utilizes a simplex-level embedding induced by a
pre-trained simplicial autoencoder to learn an entire simplicial complex
representation. To the best of our knowledge, this work presents the first
method for learning simplicial complex-level representation.","cs.CG,cs.CV,cs.LG,math.AT,stat.ML"
"Quantifying Attention Flow in Transformers. In the Transformer model, ""self-attention"" combines information from attended
embeddings into the representation of the focal embedding in the next layer.
Thus, across layers of the Transformer, information originating from different
tokens gets increasingly mixed. This makes attention weights unreliable as
explanations probes. In this paper, we consider the problem of quantifying this
flow of information through self-attention. We propose two methods for
approximating the attention to input tokens given attention weights, attention
rollout and attention flow, as post hoc methods when we use attention weights
as the relative relevance of the input tokens. We show that these methods give
complementary views on the flow of information, and compared to raw attention,
both yield higher correlations with importance scores of input tokens obtained
using an ablation method and input gradients.","cs.AI,cs.CL,cs.LG"
"DeepPCO: End-to-End Point Cloud Odometry through Deep Parallel Neural Network. Odometry is of key importance for localization in the absence of a map. There
is considerable work in the area of visual odometry (VO), and recent advances
in deep learning have brought novel approaches to VO, which directly learn
salient features from raw images. These learning-based approaches have led to
more accurate and robust VO systems. However, they have not been well applied
to point cloud data yet. In this work, we investigate how to exploit deep
learning to estimate point cloud odometry (PCO), which may serve as a critical
component in point cloud-based downstream tasks or learning-based systems.
Specifically, we propose a novel end-to-end deep parallel neural network called
DeepPCO, which can estimate the 6-DOF poses using consecutive point clouds. It
consists of two parallel sub-networks to estimate 3-D translation and
orientation respectively rather than a single neural network. We validate our
approach on KITTI Visual Odometry/SLAM benchmark dataset with different
baselines. Experiments demonstrate that the proposed approach achieves good
performance in terms of pose accuracy.","cs.CV,cs.RO"
"Universal time-series forecasting with mixture predictors. This book is devoted to the problem of sequential probability forecasting,
that is, predicting the probabilities of the next outcome of a growing sequence
of observations given the past. This problem is considered in a very general
setting that unifies commonly used probabilistic and non-probabilistic
settings, trying to make as few as possible assumptions on the mechanism
generating the observations. A common form that arises in various formulations
of this problem is that of mixture predictors, which are formed as a
combination of a finite or infinite set of other predictors attempting to
combine their predictive powers. The main subject of this book are such mixture
predictors, and the main results demonstrate the universality of this method in
a very general probabilistic setting, but also show some of its limitations.
While the problems considered are motivated by practical applications,
involving, for example, financial, biological or behavioural data, this
motivation is left implicit and all the results exposed are theoretical.
  The book targets graduate students and researchers interested in the problem
of sequential prediction, and, more generally, in theoretical analysis of
problems in machine learning and non-parametric statistics, as well as
mathematical and philosophical foundations of these fields.
  The material in this volume is presented in a way that presumes familiarity
with basic concepts of probability and statistics, up to and including
probability distributions over spaces of infinite sequences. Familiarity with
the literature on learning or stochastic processes is not required.","cs.AI,cs.IT,cs.LG,math.IT,math.ST,stat.ML,stat.TH"
"Sampling for Deep Learning Model Diagnosis (Technical Report). Deep learning (DL) models have achieved paradigm-changing performance in many
fields with high dimensional data, such as images, audio, and text. However,
the black-box nature of deep neural networks is a barrier not just to adoption
in applications such as medical diagnosis, where interpretability is essential,
but also impedes diagnosis of under performing models. The task of diagnosing
or explaining DL models requires the computation of additional artifacts, such
as activation values and gradients. These artifacts are large in volume, and
their computation, storage, and querying raise significant data management
challenges.
  In this paper, we articulate DL diagnosis as a data management problem, and
we propose a general, yet representative, set of queries to evaluate systems
that strive to support this new workload. We further develop a novel data
sampling technique that produce approximate but accurate results for these
model debugging queries. Our sampling technique utilizes the lower dimension
representation learned by the DL model and focuses on model decision boundaries
for the data in this lower dimensional space. We evaluate our techniques on one
standard computer vision and one scientific data set and demonstrate that our
sampling technique outperforms a variety of state-of-the-art alternatives in
terms of query accuracy.","cs.DB,cs.LG"
"Back-Projection based Fidelity Term for Ill-Posed Linear Inverse Problems. Ill-posed linear inverse problems appear in many image processing
applications, such as deblurring, super-resolution and compressed sensing. Many
restoration strategies involve minimizing a cost function, which is composed of
fidelity and prior terms, balanced by a regularization parameter. While a vast
amount of research has been focused on different prior models, the fidelity
term is almost always chosen to be the least squares (LS) objective, that
encourages fitting the linearly transformed optimization variable to the
observations. In this paper, we examine a different fidelity term, which has
been implicitly used by the recently proposed iterative denoising and backward
projections (IDBP) framework. This term encourages agreement between the
projection of the optimization variable onto the row space of the linear
operator and the pseudo-inverse of the linear operator (""back-projection"")
applied on the observations. We analytically examine the difference between the
two fidelity terms for Tikhonov regularization and identify cases (such as a
badly conditioned linear operator) where the new term has an advantage over the
standard LS one. Moreover, we demonstrate empirically that the behavior of the
two induced cost functions for sophisticated convex and non-convex priors, such
as total-variation, BM3D, and deep generative models, correlates with the
obtained theoretical analysis.","cs.CV,cs.NA,math.NA"
"Cross Pixel Optical Flow Similarity for Self-Supervised Learning. We propose a novel method for learning convolutional neural image
representations without manual supervision. We use motion cues in the form of
optical flow, to supervise representations of static images. The obvious
approach of training a network to predict flow from a single image can be
needlessly difficult due to intrinsic ambiguities in this prediction task. We
instead propose a much simpler learning goal: embed pixels such that the
similarity between their embeddings matches that between their optical flow
vectors. At test time, the learned deep network can be used without access to
video or flow information and transferred to tasks such as image
classification, detection, and segmentation. Our method, which significantly
simplifies previous attempts at using motion for self-supervision, achieves
state-of-the-art results in self-supervision using motion cues, competitive
results for self-supervision in general, and is overall state of the art in
self-supervised pretraining for semantic image segmentation, as demonstrated on
standard benchmarks.","68T45,cs.CV,cs.LG,cs.NE"
"Is Pessimism Provably Efficient for Offline RL?. We study offline reinforcement learning (RL), which aims to learn an optimal
policy based on a dataset collected a priori. Due to the lack of further
interactions with the environment, offline RL suffers from the insufficient
coverage of the dataset, which eludes most existing theoretical analysis. In
this paper, we propose a pessimistic variant of the value iteration algorithm
(PEVI), which incorporates an uncertainty quantifier as the penalty function.
Such a penalty function simply flips the sign of the bonus function for
promoting exploration in online RL, which makes it easily implementable and
compatible with general function approximators.
  Without assuming the sufficient coverage of the dataset, we establish a
data-dependent upper bound on the suboptimality of PEVI for general Markov
decision processes (MDPs). When specialized to linear MDPs, it matches the
information-theoretic lower bound up to multiplicative factors of the dimension
and horizon. In other words, pessimism is not only provably efficient but also
minimax optimal. In particular, given the dataset, the learned policy serves as
the ""best effort"" among all policies, as no other policies can do better. Our
theoretical analysis identifies the critical role of pessimism in eliminating a
notion of spurious correlation, which emerges from the ""irrelevant""
trajectories that are less covered by the dataset and not informative for the
optimal policy.","cs.AI,cs.LG,math.OC,math.ST,stat.ML,stat.TH"
"Graph Context Encoder: Graph Feature Inpainting for Graph Generation and Self-supervised Pretraining. We propose the Graph Context Encoder (GCE), a simple but efficient approach
for graph representation learning based on graph feature masking and
reconstruction.
  GCE models are trained to efficiently reconstruct input graphs similarly to a
graph autoencoder where node and edge labels are masked. In particular, our
model is also allowed to change graph structures by masking and reconstructing
graphs augmented by random pseudo-edges.
  We show that GCE can be used for novel graph generation, with applications
for molecule generation. Used as a pretraining method, we also show that GCE
improves baseline performances in supervised classification tasks tested on
multiple standard benchmark graph datasets.","68T07,cs.LG"
"I-GCN: Robust Graph Convolutional Network via Influence Mechanism. Deep learning models for graphs, especially Graph Convolutional Networks
(GCNs), have achieved remarkable performance in the task of semi-supervised
node classification. However, recent studies show that GCNs suffer from
adversarial perturbations. Such vulnerability to adversarial attacks
significantly decreases the stability of GCNs when being applied to
security-critical applications. Defense methods such as preprocessing,
attention mechanism and adversarial training have been discussed by various
studies. While being able to achieve desirable performance when the
perturbation rates are low, such methods are still vulnerable to high
perturbation rates. Meanwhile, some defending algorithms perform poorly when
the node features are not visible. Therefore, in this paper, we propose a novel
mechanism called influence mechanism, which is able to enhance the robustness
of the GCNs significantly. The influence mechanism divides the effect of each
node into two parts: introverted influence which tries to maintain its own
features and extroverted influence which exerts influences on other nodes.
Utilizing the influence mechanism, we propose the Influence GCN (I-GCN) model.
Extensive experiments show that our proposed model is able to achieve higher
accuracy rates than state-of-the-art methods when defending against
non-targeted attacks.","cs.LG,cs.SI,stat.ML"
"YASENN: Explaining Neural Networks via Partitioning Activation Sequences. We introduce a novel approach to feed-forward neural network interpretation
based on partitioning the space of sequences of neuron activations. In line
with this approach, we propose a model-specific interpretation method, called
YASENN. Our method inherits many advantages of model-agnostic distillation,
such as an ability to focus on the particular input region and to express an
explanation in terms of features different from those observed by a neural
network. Moreover, examination of distillation error makes the method
applicable to the problems with low tolerance to interpretation mistakes.
Technically, YASENN distills the network with an ensemble of layer-wise
gradient boosting decision trees and encodes the sequences of neuron
activations with leaf indices. The finite number of unique codes induces a
partitioning of the input space. Each partition may be described in a variety
of ways, including examination of an interpretable model (e.g. a logistic
regression or a decision tree) trained to discriminate between objects of those
partitions. Our experiments provide an intuition behind the method and
demonstrate revealed artifacts in neural network decision making.","cs.LG,stat.ML"
"A framework for reinforcement learning with autocorrelated actions. The subject of this paper is reinforcement learning. Policies are considered
here that produce actions based on states and random elements autocorrelated in
subsequent time instants. Consequently, an agent learns from experiments that
are distributed over time and potentially give better clues to policy
improvement. Also, physical implementation of such policies, e.g. in robotics,
is less problematic, as it avoids making robots shake. This is in opposition to
most RL algorithms which add white noise to control causing unwanted shaking of
the robots. An algorithm is introduced here that approximately optimizes the
aforementioned policy. Its efficiency is verified for four simulated learning
control problems (Ant, HalfCheetah, Hopper, and Walker2D) against three other
methods (PPO, SAC, ACER). The algorithm outperforms others in three of these
problems.","I.2.6,cs.LG,stat.ML"
"Field geology with a wearable computer: 1st results of the Cyborg Astrobiologist System. We present results from the first geological field tests of the `Cyborg
Astrobiologist', which is a wearable computer and video camcorder system that
we are using to test and train a computer-vision system towards having some of
the autonomous decision-making capabilities of a field-geologist. The Cyborg
Astrobiologist platform has thus far been used for testing and development of
these algorithms and systems: robotic acquisition of quasi-mosaics of images,
real-time image segmentation, and real-time determination of interesting points
in the image mosaics. This work is more of a test of the whole system, rather
than of any one part of the system. However, beyond the concept of the system
itself, the uncommon map (despite its simplicity) is the main innovative part
of the system. The uncommon map helps to determine interest-points in a
context-free manner. Overall, the hardware and software systems function
reliably, and the computer-vision algorithms are adequate for the first field
tests. In addition to the proof-of-concept aspect of these field tests, the
main result of these field tests is the enumeration of those issues that we can
improve in the future, including: dealing with structural shadow and
microtexture, and also, controlling the camera's zoom lens in an intelligent
manner. Nonetheless, despite these and other technical inadequacies, this
Cyborg Astrobiologist system, consisting of a camera-equipped wearable-computer
and its computer-vision algorithms, has demonstrated its ability of finding
genuinely interesting points in real-time in the geological scenery, and then
gathering more information about these interest points in an automated manner.
We use these capabilities for autonomous guidance towards geological
points-of-interest.","astro-ph,cs.AI,cs.CE,cs.CV,cs.HC,cs.RO"
"Multi-view Integration Learning for Irregularly-sampled Clinical Time Series. Electronic health record (EHR) data is sparse and irregular as it is recorded
at irregular time intervals, and different clinical variables are measured at
each observation point. In this work, we propose a multi-view features
integration learning from irregular multivariate time series data by
self-attention mechanism in an imputation-free manner. Specifically, we devise
a novel multi-integration attention module (MIAM) to extract complex
information inherent in irregular time series data. In particular, we
explicitly learn the relationships among the observed values, missing
indicators, and time interval between the consecutive observations,
simultaneously. The rationale behind our approach is the use of human knowledge
such as what to measure and when to measure in different situations, which are
indirectly represented in the data. In addition, we build an attention-based
decoder as a missing value imputer that helps empower the representation
learning of the inter-relations among multi-view observations for the
prediction task, which operates at the training phase only. We validated the
effectiveness of our method over the public MIMIC-III and PhysioNet challenge
2012 datasets by comparing with and outperforming the state-of-the-art methods
for in-hospital mortality prediction.","cs.AI,cs.LG"
"Explainable AI: Deep Reinforcement Learning Agents for Residential Demand Side Cost Savings in Smart Grids. Motivated by recent advancements in Deep Reinforcement Learning (RL), we have
developed an RL agent to manage the operation of storage devices in a household
and is designed to maximize demand-side cost savings. The proposed technique is
data-driven, and the RL agent learns from scratch how to efficiently use the
energy storage device given variable tariff structures. In most of the studies,
the RL agent is considered as a black box, and how the agent has learned is
often ignored. We explain the learning progression of the RL agent, and the
strategies it follows based on the capacity of the storage device.","cs.AI,cs.GT,cs.LG,stat.ML"
"Interpretable Machine Learning: Fundamental Principles and 10 Grand Challenges. Interpretability in machine learning (ML) is crucial for high stakes
decisions and troubleshooting. In this work, we provide fundamental principles
for interpretable ML, and dispel common misunderstandings that dilute the
importance of this crucial topic. We also identify 10 technical challenge areas
in interpretable machine learning and provide history and background on each
problem. Some of these problems are classically important, and some are recent
problems that have arisen in the last few years. These problems are: (1)
Optimizing sparse logical models such as decision trees; (2) Optimization of
scoring systems; (3) Placing constraints into generalized additive models to
encourage sparsity and better interpretability; (4) Modern case-based
reasoning, including neural networks and matching for causal inference; (5)
Complete supervised disentanglement of neural networks; (6) Complete or even
partial unsupervised disentanglement of neural networks; (7) Dimensionality
reduction for data visualization; (8) Machine learning models that can
incorporate physics and other generative or causal constraints; (9)
Characterization of the ""Rashomon set"" of good models; and (10) Interpretable
reinforcement learning. This survey is suitable as a starting point for
statisticians and computer scientists interested in working in interpretable
machine learning.","68T01,I.2.6,cs.LG,stat.ML"
"Tailoring Artificial Neural Networks for Optimal Learning. As one of the most important paradigms of recurrent neural networks, the echo
state network (ESN) has been applied to a wide range of fields, from robotics
to medicine, finance, and language processing. A key feature of the ESN
paradigm is its reservoir --- a directed and weighted network of neurons that
projects the input time series into a high dimensional space where linear
regression or classification can be applied. Despite extensive studies, the
impact of the reservoir network on the ESN performance remains unclear.
Combining tools from physics, dynamical systems and network science, we attempt
to open the black box of ESN and offer insights to understand the behavior of
general artificial neural networks. Through spectral analysis of the reservoir
network we reveal a key factor that largely determines the ESN memory capacity
and hence affects its performance. Moreover, we find that adding short loops to
the reservoir network can tailor ESN for specific tasks and optimize learning.
We validate our findings by applying ESN to forecast both synthetic and real
benchmark time series. Our results provide a new way to design task-specific
ESN. More importantly, it demonstrates the power of combining tools from
physics, dynamical systems and network science to offer new insights in
understanding the mechanisms of general artificial neural networks.","cs.LG,cs.NE"
"A Deep Actor-Critic Reinforcement Learning Framework for Dynamic Multichannel Access. To make efficient use of limited spectral resources, we in this work propose
a deep actor-critic reinforcement learning based framework for dynamic
multichannel access. We consider both a single-user case and a scenario in
which multiple users attempt to access channels simultaneously. We employ the
proposed framework as a single agent in the single-user case, and extend it to
a decentralized multi-agent framework in the multi-user scenario. In both
cases, we develop algorithms for the actor-critic deep reinforcement learning
and evaluate the proposed learning policies via experiments and numerical
results. In the single-user model, in order to evaluate the performance of the
proposed channel access policy and the framework's tolerance against
uncertainty, we explore different channel switching patterns and different
switching probabilities. In the case of multiple users, we analyze the
probabilities of each user accessing channels with favorable channel conditions
and the probability of collision. We also address a time-varying environment to
identify the adaptive ability of the proposed framework. Additionally, we
provide comparisons (in terms of both the average reward and time efficiency)
between the proposed actor-critic deep reinforcement learning framework, Deep-Q
network (DQN) based approach, random access, and the optimal policy when the
channel dynamics are known.","cs.IT,cs.LG,math.IT,stat.ML"
"Seeing eye-to-eye? A comparison of object recognition performance in humans and deep convolutional neural networks under image manipulation. For a considerable time, deep convolutional neural networks (DCNNs) have
reached human benchmark performance in object recognition. On that account,
computational neuroscience and the field of machine learning have started to
attribute numerous similarities and differences to artificial and biological
vision. This study aims towards a behavioral comparison of visual core object
recognition performance between humans and feedforward neural networks in a
classification learning paradigm on an ImageNet data set. For this purpose,
human participants (n = 65) competed in an online experiment against different
feedforward DCNNs. The designed approach based on a typical learning process of
seven different monkey categories included a training and validation phase with
natural examples, as well as a testing phase with novel, unexperienced shape
and color manipulations. Analyses of accuracy revealed that humans not only
outperform DCNNs on all conditions, but also display significantly greater
robustness towards shape and most notably color alterations. Furthermore, a
precise examination of behavioral patterns highlights these findings by
revealing independent classification errors between the groups. The obtained
results show that humans contrast strongly with artificial feedforward
architectures when it comes to visual core object recognition of manipulated
images. In general, these findings are in line with a growing body of
literature, that hints towards recurrence as a crucial factor for adequate
generalization abilities.","cs.CV,cs.LG,eess.IV,q-bio.NC"
"A Bayesian Approach to Learning Bayesian Networks with Local Structure. Recently several researchers have investigated techniques for using data to
learn Bayesian networks containing compact representations for the conditional
probability distributions (CPDs) stored at each node. The majority of this work
has concentrated on using decision-tree representations for the CPDs. In
addition, researchers typically apply non-Bayesian (or asymptotically Bayesian)
scoring functions such as MDL to evaluate the goodness-of-fit of networks to
the data. In this paper we investigate a Bayesian approach to learning Bayesian
networks that contain the more general decision-graph representations of the
CPDs. First, we describe how to evaluate the posterior probability that is, the
Bayesian score of such a network, given a database of observed cases. Second,
we describe various search spaces that can be used, in conjunction with a
scoring function and a search procedure, to identify one or more high-scoring
networks. Finally, we present an experimental evaluation of the search spaces,
using a greedy algorithm and a Bayesian scoring function.","cs.AI,cs.LG,stat.ML"
"Procams-Based Cybernetics. Procams-based cybernetics is a unique, emerging research field, which aims at
enhancing and supporting our activities by naturally connecting human and
computers/machines as a cooperative integrated system via projector-camera
systems (procams). It rests on various research domains such as
virtual/augmented reality, computer vision, computer graphics, projection
display, human computer interface, human robot interaction and so on. This
laboratory presentation provides a brief history including recent achievements
of our procams-based cybernetics project.","cs.CV,cs.GR,cs.HC"
"SIRUS: Stable and Interpretable RUle Set for Classification. State-of-the-art learning algorithms, such as random forests or neural
networks, are often qualified as ""black-boxes"" because of the high number and
complexity of operations involved in their prediction mechanism. This lack of
interpretability is a strong limitation for applications involving critical
decisions, typically the analysis of production processes in the manufacturing
industry. In such critical contexts, models have to be interpretable, i.e.,
simple, stable, and predictive. To address this issue, we design SIRUS (Stable
and Interpretable RUle Set), a new classification algorithm based on random
forests, which takes the form of a short list of rules. While simple models are
usually unstable with respect to data perturbation, SIRUS achieves a remarkable
stability improvement over cutting-edge methods. Furthermore, SIRUS inherits a
predictive accuracy close to random forests, combined with the simplicity of
decision trees. These properties are assessed both from a theoretical and
empirical point of view, through extensive numerical experiments based on our
R/C++ software implementation sirus available from CRAN.","cs.LG,math.ST,stat.ML,stat.TH"
"Graph Structure of Neural Networks. Neural networks are often represented as graphs of connections between
neurons. However, despite their wide use, there is currently little
understanding of the relationship between the graph structure of the neural
network and its predictive performance. Here we systematically investigate how
does the graph structure of neural networks affect their predictive
performance. To this end, we develop a novel graph-based representation of
neural networks called relational graph, where layers of neural network
computation correspond to rounds of message exchange along the graph structure.
Using this representation we show that: (1) a ""sweet spot"" of relational graphs
leads to neural networks with significantly improved predictive performance;
(2) neural network's performance is approximately a smooth function of the
clustering coefficient and average path length of its relational graph; (3) our
findings are consistent across many different tasks and datasets; (4) the sweet
spot can be identified efficiently; (5) top-performing neural networks have
graph structure surprisingly similar to those of real biological neural
networks. Our work opens new directions for the design of neural architectures
and the understanding on neural networks in general.","cs.CV,cs.LG,cs.SI,stat.ML"
"Transfer Learning in Multi-Agent Reinforcement Learning with Double Q-Networks for Distributed Resource Sharing in V2X Communication. This paper addresses the problem of decentralized spectrum sharing in
vehicle-to-everything (V2X) communication networks. The aim is to provide
resource-efficient coexistence of vehicle-to-infrastructure(V2I) and
vehicle-to-vehicle(V2V) links. A recent work on the topic proposes a
multi-agent reinforcement learning (MARL) approach based on deep Q-learning,
which leverages a fingerprint-based deep Q-network (DQN) architecture. This
work considers an extension of this framework by combining Double Q-learning
(via Double DQN) and transfer learning. The motivation behind is that Double
Q-learning can alleviate the problem of overestimation of the action values
present in conventional Q-learning, while transfer learning can leverage
knowledge acquired by an expert model to accelerate learning in the MARL
setting. The proposed algorithm is evaluated in a realistic V2X setting, with
synthetic data generated based on a geometry-based propagation model that
incorporates location-specific geographical descriptors of the simulated
environment(outlines of buildings, foliage, and vehicles). The advantages of
the proposed approach are demonstrated via numerical simulations.","cs.IT,cs.LG,eess.SP,math.IT"
"Neural Dynamic Mode Decomposition for End-to-End Modeling of Nonlinear Dynamics. Koopman spectral analysis has attracted attention for understanding nonlinear
dynamical systems by which we can analyze nonlinear dynamics with a linear
regime by lifting observations using a nonlinear function. For analysis, we
need to find an appropriate lift function. Although several methods have been
proposed for estimating a lift function based on neural networks, the existing
methods train neural networks without spectral analysis. In this paper, we
propose neural dynamic mode decomposition, in which neural networks are trained
such that the forecast error is minimized when the dynamics is modeled based on
spectral decomposition in the lifted space. With our proposed method, the
forecast error is backpropagated through the neural networks and the spectral
decomposition, enabling end-to-end learning of Koopman spectral analysis. When
information is available on the frequencies or the growth rates of the
dynamics, the proposed method can exploit it as regularizers for training. We
also propose an extension of our approach when observations are influenced by
exogenous control time-series. Our experiments demonstrate the effectiveness of
our proposed method in terms of eigenvalue estimation and forecast performance.","cs.LG,math.DS,stat.ML"
"Optimizing Large-Scale Fleet Management on a Road Network using Multi-Agent Deep Reinforcement Learning with Graph Neural Network. We propose a novel approach to optimize fleet management by combining
multi-agent reinforcement learning with graph neural network. To provide
ride-hailing service, one needs to optimize dynamic resources and demands over
spatial domain. While the spatial structure was previously approximated with a
regular grid, our approach represents the road network with a graph, which
better reflects the underlying geometric structure. Dynamic resource allocation
is formulated as multi-agent reinforcement learning, whose action-value
function (Q function) is approximated with graph neural networks. We use
stochastic policy update rule over the graph with deep Q-networks (DQN), and
achieve superior results over the greedy policy update. We design a realistic
simulator that emulates the empirical taxi call data, and confirm the
effectiveness of the proposed model under various conditions.","cs.LG,cs.SY,eess.SY"
"Existence, uniqueness, and convergence rates for gradient flows in the training of artificial neural networks with ReLU activation. The training of artificial neural networks (ANNs) with rectified linear unit
(ReLU) activation via gradient descent (GD) type optimization schemes is
nowadays a common industrially relevant procedure. Till this day in the
scientific literature there is in general no mathematical convergence analysis
which explains the numerical success of GD type optimization schemes in the
training of ANNs with ReLU activation. GD type optimization schemes can be
regarded as temporal discretization methods for the gradient flow (GF)
differential equations associated to the considered optimization problem and,
in view of this, it seems to be a natural direction of research to first aim to
develop a mathematical convergence theory for time-continuous GF differential
equations and, thereafter, to aim to extend such a time-continuous convergence
theory to implementable time-discrete GD type optimization methods. In this
article we establish two basic results for GF differential equations in the
training of fully-connected feedforward ANNs with one hidden layer and ReLU
activation. In the first main result of this article we establish in the
training of such ANNs under the assumption that the probability distribution of
the input data of the considered supervised learning problem is absolutely
continuous with a bounded density function that every GF differential equation
admits for every initial value a solution which is also unique among a suitable
class of solutions. In the second main result of this article we prove in the
training of such ANNs under the assumption that the target function and the
density function of the probability distribution of the input data are
piecewise polynomial that every non-divergent GF trajectory converges with an
appropriate rate of convergence to a critical point and that the risk of the
non-divergent GF trajectory converges with rate 1 to the risk of the critical
point.","cs.LG,cs.NA,math.DS,math.NA"
"CBinfer: Exploiting Frame-to-Frame Locality for Faster Convolutional Network Inference on Video Streams. The last few years have brought advances in computer vision at an amazing
pace, grounded on new findings in deep neural network construction and training
as well as the availability of large labeled datasets. Applying these networks
to images demands a high computational effort and pushes the use of
state-of-the-art networks on real-time video data out of reach of embedded
platforms. Many recent works focus on reducing network complexity for real-time
inference on embedded computing platforms. We adopt an orthogonal viewpoint and
propose a novel algorithm exploiting the spatio-temporal sparsity of pixel
changes. This optimized inference procedure resulted in an average speed-up of
9.1x over cuDNN on the Tegra X2 platform at a negligible accuracy loss of <0.1%
and no retraining of the network for a semantic segmentation application.
Similarly, an average speed-up of 7.0x has been achieved for a pose detection
DNN and a reduction of 5x of the number of arithmetic operations to be
performed for object detection on static camera video surveillance data. These
throughput gains combined with a lower power consumption result in an energy
efficiency of 511 GOp/s/W compared to 70 GOp/s/W for the baseline.","cs.AI,cs.CV,cs.NE,eess.IV"
"A Manifold Proximal Linear Method for Sparse Spectral Clustering with Application to Single-Cell RNA Sequencing Data Analysis. Spectral clustering is one of the fundamental unsupervised learning methods
widely used in data analysis. Sparse spectral clustering (SSC) imposes sparsity
to the spectral clustering and it improves the interpretability of the model.
This paper considers a widely adopted model for SSC, which can be formulated as
an optimization problem over the Stiefel manifold with nonsmooth and nonconvex
objective. Such an optimization problem is very challenging to solve. Existing
methods usually solve its convex relaxation or need to smooth its nonsmooth
part using certain smoothing techniques. In this paper, we propose a manifold
proximal linear method (ManPL) that solves the original SSC formulation. We
also extend the algorithm to solve the multiple-kernel SSC problems, for which
an alternating ManPL algorithm is proposed. Convergence and iteration
complexity results of the proposed methods are established. We demonstrate the
advantage of our proposed methods over existing methods via the single-cell RNA
sequencing data analysis.","cs.LG,math.OC,q-bio.GN,stat.ML"
"Massively Parallel Benders Decomposition for Correlation Clustering. We tackle the problem of graph partitioning for image segmentation using
correlation clustering (CC), which we treat as an integer linear program (ILP).
We reformulate optimization in the ILP so as to admit efficient optimization
via Benders decomposition, a classic technique from operations research. Our
Benders decomposition formulation has many subproblems, each associated with a
node in the CC instance's graph, which are solved in parallel. Each Benders
subproblem enforces the cycle inequalities corresponding to the negative weight
edges attached to its corresponding node in the CC instance. We generate
Magnanti-Wong Benders rows in addition to standard Benders rows, to accelerate
optimization. Our Benders decomposition approach provides a promising new
avenue to accelerate optimization for CC, and allows for massive
parallelization.","cs.CV,cs.DS"
"Comparative Analysis of Automatic Skin Lesion Segmentation with Two Different Implementations. Lesion segmentation from the surrounding skin is the first task for
developing automatic Computer-Aided Diagnosis of skin cancer. Variant features
of lesion like uneven distribution of color, irregular shape, border and
texture make this task challenging. The contribution of this paper is to
present and compare two different approaches to skin lesion segmentation. The
first approach uses watershed, while the second approach uses mean-shift.
Pre-processing steps were performed in both approaches for removing hair and
dark borders of microscopic images. The Evaluation of the proposed approaches
was performed using Jaccard Index (Intersection over Union or IoU). An
additional contribution of this paper is to present pipelines for performing
pre-processing and segmentation applying existing segmentation and
morphological algorithms which led to promising results. On average, the first
approach showed better performance than the second one with average Jaccard
Index over 200 ISIC-2017 challenge images are 89.16% and 76.94% respectively.","68U10,I.4.6; I.5.3,cs.CV"
"Distributional Gradient Matching for Learning Uncertain Neural Dynamics Models. Differential equations in general and neural ODEs in particular are an
essential technique in continuous-time system identification. While many
deterministic learning algorithms have been designed based on numerical
integration via the adjoint method, many downstream tasks such as active
learning, exploration in reinforcement learning, robust control, or filtering
require accurate estimates of predictive uncertainties. In this work, we
propose a novel approach towards estimating epistemically uncertain neural
ODEs, avoiding the numerical integration bottleneck. Instead of modeling
uncertainty in the ODE parameters, we directly model uncertainties in the state
space. Our algorithm - distributional gradient matching (DGM) - jointly trains
a smoother and a dynamics model and matches their gradients via minimizing a
Wasserstein loss. Our experiments show that, compared to traditional
approximate inference methods based on numerical integration, our approach is
faster to train, faster at predicting previously unseen trajectories, and in
the context of neural ODEs, significantly more accurate.","cs.LG,math.DS,stat.ML"
"A Comparison of Few-Shot Learning Methods for Underwater Optical and Sonar Image Classification. Deep convolutional neural networks generally perform well in underwater
object recognition tasks on both optical and sonar images. Many such methods
require hundreds, if not thousands, of images per class to generalize well to
unseen examples. However, obtaining and labeling sufficiently large volumes of
data can be relatively costly and time-consuming, especially when observing
rare objects or performing real-time operations. Few-Shot Learning (FSL)
efforts have produced many promising methods to deal with low data
availability. However, little attention has been given in the underwater
domain, where the style of images poses additional challenges for object
recognition algorithms. To the best of our knowledge, this is the first paper
to evaluate and compare several supervised and semi-supervised Few-Shot
Learning (FSL) methods using underwater optical and side-scan sonar imagery.
Our results show that FSL methods offer a significant advantage over the
traditional transfer learning methods that fine-tune pre-trained models. We
hope that our work will help apply FSL to autonomous underwater systems and
expand their learning capabilities.",cs.CV
"Supervised Feature Selection for Diagnosis of Coronary Artery Disease Based on Genetic Algorithm. Feature Selection (FS) has become the focus of much research on decision
support systems areas for which data sets with tremendous number of variables
are analyzed. In this paper we present a new method for the diagnosis of
Coronary Artery Diseases (CAD) founded on Genetic Algorithm (GA) wrapped Bayes
Naive (BN) based FS. Basically, CAD dataset contains two classes defined with
13 features. In GA BN algorithm, GA generates in each iteration a subset of
attributes that will be evaluated using the BN in the second step of the
selection procedure. The final set of attribute contains the most relevant
feature model that increases the accuracy. The algorithm in this case produces
85.50% classification accuracy in the diagnosis of CAD. Thus, the asset of the
Algorithm is then compared with the use of Support Vector Machine (SVM),
MultiLayer Perceptron (MLP) and C4.5 decision tree Algorithm. The result of
classification accuracy for those algorithms are respectively 83.5%, 83.16% and
80.85%. Consequently, the GA wrapped BN Algorithm is correspondingly compared
with other FS algorithms. The Obtained results have shown very promising
outcomes for the diagnosis of CAD.","cs.CE,cs.LG"
"Spatial Graph Attention and Curiosity-driven Policy for Antiviral Drug Discovery. We developed Distilled Graph Attention Policy Networks (DGAPNs), a
curiosity-driven reinforcement learning model to generate novel
graph-structured chemical representations that optimize user-defined objectives
by efficiently navigating a physically constrained domain. The framework is
examined on the task of generating molecules that are designed to bind,
noncovalently, to functional sites of SARS-CoV-2 proteins. We present a spatial
Graph Attention Network (sGAT) that leverages self-attention over both node and
edge attributes as well as encoding spatial structure -- this capability is of
considerable interest in areas such as molecular and synthetic biology and drug
discovery. An attentional policy network is then introduced to learn decision
rules for a dynamic, fragment-based chemical environment, and state-of-the-art
policy gradient techniques are employed to train the network with enhanced
stability. Exploration is efficiently encouraged by incorporating innovation
reward bonuses learned and proposed by random network distillation. In
experiments, our framework achieved outstanding results compared to
state-of-the-art algorithms, while increasing the diversity of proposed
molecules and reducing the complexity of paths to chemical synthesis.","cs.AI,cs.LG,q-bio.BM"
"Improving Generalization in Meta Reinforcement Learning using Learned Objectives. Biological evolution has distilled the experiences of many learners into the
general learning algorithms of humans. Our novel meta reinforcement learning
algorithm MetaGenRL is inspired by this process. MetaGenRL distills the
experiences of many complex agents to meta-learn a low-complexity neural
objective function that decides how future individuals will learn. Unlike
recent meta-RL algorithms, MetaGenRL can generalize to new environments that
are entirely different from those used for meta-training. In some cases, it
even outperforms human-engineered RL algorithms. MetaGenRL uses off-policy
second-order gradients during meta-training that greatly increase its sample
efficiency.","I.2.6,cs.AI,cs.LG,cs.NE,stat.ML"
"Short-term traffic prediction using physics-aware neural networks. In this work, we propose an algorithm performing short-term predictions of
the flux of vehicles on a stretch of road, using past measurements of the flux.
This algorithm is based on a physics-aware recurrent neural network. A
discretization of a macroscopic traffic flow model (using the so-called Traffic
Reaction Model) is embedded in the architecture of the network and yields flux
predictions based on estimated and predicted space-time dependent traffic
parameters. These parameters are themselves obtained using a succession of LSTM
ans simple recurrent neural networks. Besides, on top of the predictions, the
algorithm yields a smoothing of its inputs which is also physically-constrained
by the macroscopic traffic flow model. The algorithm is tested on raw flux
measurements obtained from loop detectors.","cs.LG,eess.SP"
"Logic and the $2$-Simplicial Transformer. We introduce the $2$-simplicial Transformer, an extension of the Transformer
which includes a form of higher-dimensional attention generalising the
dot-product attention, and uses this attention to update entity representations
with tensor products of value vectors. We show that this architecture is a
useful inductive bias for logical reasoning in the context of deep
reinforcement learning.","cs.LG,cs.LO,stat.ML"
"Fast-Slow Streamflow Model Using Mass-Conserving LSTM. Streamflow forecasting is key to effectively managing water resources and
preparing for the occurrence of natural calamities being exacerbated by climate
change. Here we use the concept of fast and slow flow components to create a
new mass-conserving Long Short-Term Memory (LSTM) neural network model. It uses
hydrometeorological time series and catchment attributes to predict daily river
discharges. Preliminary results evidence improvement in skills for different
scores compared to the recent literature.","68T07,J.2; I.2.m,cs.LG,physics.ao-ph"
"GATCluster: Self-Supervised Gaussian-Attention Network for Image Clustering. We propose a self-supervised Gaussian ATtention network for image Clustering
(GATCluster). Rather than extracting intermediate features first and then
performing the traditional clustering algorithm, GATCluster directly outputs
semantic cluster labels without further post-processing. Theoretically, we give
a Label Feature Theorem to guarantee the learned features are one-hot encoded
vectors, and the trivial solutions are avoided. To train the GATCluster in a
completely unsupervised manner, we design four self-learning tasks with the
constraints of transformation invariance, separability maximization, entropy
analysis, and attention mapping. Specifically, the transformation invariance
and separability maximization tasks learn the relationships between sample
pairs. The entropy analysis task aims to avoid trivial solutions. To capture
the object-oriented semantics, we design a self-supervised attention mechanism
that includes a parameterized attention module and a soft-attention loss. All
the guiding signals for clustering are self-generated during the training
process. Moreover, we develop a two-step learning algorithm that is
memory-efficient for clustering large-size images. Extensive experiments
demonstrate the superiority of our proposed method in comparison with the
state-of-the-art image clustering benchmarks. Our code has been made publicly
available at https://github.com/niuchuangnn/GATCluster.","cs.CV,cs.LG,eess.IV"
"Capsule GAN Using Capsule Network for Generator Architecture. This paper presents Capsule GAN, a Generative adversarial network using
Capsule Network not only in the discriminator but also in the generator.
Recently, Generative adversarial networks (GANs) has been intensively studied.
However, generating images by GANs is difficult. Therefore, GANs sometimes
generate poor quality images. These GANs use convolutional neural networks
(CNNs). However, CNNs have the defect that the relational information between
features of the image may be lost. Capsule Network, proposed by Hinton in 2017,
overcomes the defect of CNNs. Capsule GAN reported previously uses Capsule
Network in the discriminator. However, instead of using Capsule Network,
Capsule GAN reported in previous studies uses CNNs in generator architecture
like DCGAN. This paper introduces two approaches to use Capsule Network in the
generator. One is to use DigitCaps layer from the discriminator as the input to
the generator. DigitCaps layer is the output layer of Capsule Network. It has
the features of the input images of the discriminator. The other is to use the
reverse operation of recognition process in Capsule Network in the generator.
We compare Capsule GAN proposed in this paper with conventional GAN using CNN
and Capsule GAN which uses Capsule Network in the discriminator only. The
datasets are MNIST, Fashion-MNIST and color images. We show that Capsule GAN
outperforms the GAN using CNN and the GAN using Capsule Network in the
discriminator only. The architecture of Capsule GAN proposed in this paper is a
basic architecture using Capsule Network. Therefore, we can apply the existing
improvement techniques for GANs to Capsule GAN.","68T05,cs.CV,cs.LG,eess.IV"
"Subverting Privacy-Preserving GANs: Hiding Secrets in Sanitized Images. Unprecedented data collection and sharing have exacerbated privacy concerns
and led to increasing interest in privacy-preserving tools that remove
sensitive attributes from images while maintaining useful information for other
tasks. Currently, state-of-the-art approaches use privacy-preserving generative
adversarial networks (PP-GANs) for this purpose, for instance, to enable
reliable facial expression recognition without leaking users' identity.
However, PP-GANs do not offer formal proofs of privacy and instead rely on
experimentally measuring information leakage using classification accuracy on
the sensitive attributes of deep learning (DL)-based discriminators. In this
work, we question the rigor of such checks by subverting existing
privacy-preserving GANs for facial expression recognition. We show that it is
possible to hide the sensitive identification data in the sanitized output
images of such PP-GANs for later extraction, which can even allow for
reconstruction of the entire input images, while satisfying privacy checks. We
demonstrate our approach via a PP-GAN-based architecture and provide
qualitative and quantitative evaluations using two public datasets. Our
experimental results raise fundamental questions about the need for more
rigorous privacy checks of PP-GANs, and we provide insights into the social
impact of these.","cs.AI,cs.CR,cs.CV,cs.LG,stat.ML"
"Enforcing constraints for time series prediction in supervised, unsupervised and reinforcement learning. We assume that we are given a time series of data from a dynamical system and
our task is to learn the flow map of the dynamical system. We present a
collection of results on how to enforce constraints coming from the dynamical
system in order to accelerate the training of deep neural networks to represent
the flow map of the system as well as increase their predictive ability. In
particular, we provide ways to enforce constraints during training for all
three major modes of learning, namely supervised, unsupervised and
reinforcement learning. In general, the dynamic constraints need to include
terms which are analogous to memory terms in model reduction formalisms. Such
memory terms act as a restoring force which corrects the errors committed by
the learned flow map during prediction.
  For supervised learning, the constraints are added to the objective function.
For the case of unsupervised learning, in particular generative adversarial
networks, the constraints are introduced by augmenting the input of the
discriminator. Finally, for the case of reinforcement learning and in
particular actor-critic methods, the constraints are added to the reward
function. In addition, for the reinforcement learning case, we present a novel
approach based on homotopy of the action-value function in order to stabilize
and accelerate training. We use numerical results for the Lorenz system to
illustrate the various constructions.","37M05,37M10,62M45,68Q32,68T05,cs.LG,math.NA,stat.ML"
"Parametric Adversarial Divergences are Good Task Losses for Generative Modeling. Generative modeling of high dimensional data like images is a notoriously
difficult and ill-defined problem. In particular, how to evaluate a learned
generative model is unclear. In this position paper, we argue that adversarial
learning, pioneered with generative adversarial networks (GANs), provides an
interesting framework to implicitly define more meaningful task losses for
generative modeling tasks, such as for generating ""visually realistic"" images.
We refer to those task losses as parametric adversarial divergences and we give
two main reasons why we think parametric divergences are good learning
objectives for generative modeling. Additionally, we unify the processes of
choosing a good structured loss (in structured prediction) and choosing a
discriminator architecture (in generative modeling) using statistical decision
theory; we are then able to formalize and quantify the intuition that ""weaker""
losses are easier to learn from, in a specific setting. Finally, we propose two
new challenging tasks to evaluate parametric and nonparametric divergences: a
qualitative task of generating very high-resolution digits, and a quantitative
task of learning data that satisfies high-level algebraic constraints. We use
two common divergences to train a generator and show that the parametric
divergence outperforms the nonparametric divergence on both the qualitative and
the quantitative task.","cs.LG,stat.ML"
"Weakly Supervised Representation Learning for Unsynchronized Audio-Visual Events. Audio-visual representation learning is an important task from the
perspective of designing machines with the ability to understand complex
events. To this end, we propose a novel multimodal framework that instantiates
multiple instance learning. We show that the learnt representations are useful
for classifying events and localizing their characteristic audio-visual
elements. The system is trained using only video-level event labels without any
timing information. An important feature of our method is its capacity to learn
from unsynchronized audio-visual events. We achieve state-of-the-art results on
a large-scale dataset of weakly-labeled audio event videos. Visualizations of
localized visual regions and audio segments substantiate our system's efficacy,
especially when dealing with noisy situations where modality-specific cues
appear asynchronously.","cs.CV,cs.SD,eess.AS"
"Breaking the Spatio-Angular Trade-off for Light Field Super-Resolution via LSTM Modelling on Epipolar Plane Images. Light-field cameras (LFC) have received increasing attention due to their
wide-spread applications. However, current LFCs suffer from the well-known
spatio-angular trade-off, which is considered as an inherent and fundamental
limit for LFC designs. In this paper, by doing a detailed geometrical optical
analysis of the sampling process in an LFC, we show that the effective sampling
resolution is generally higher than the number of micro-lenses. This
contribution makes it theoretically possible to break the resolution trade-off.
Our second contribution is an epipolar plane image (EPI) based super-resolution
method, which can super-resolve the spatial and angular dimensions
simultaneously. We prove that the light field is a 2D series, thus, a
specifically designed CNN-LSTM network is proposed to capture the continuity
property of the EPI. Rather than leveraging semantic information, our network
focuses on extracting geometric continuity in the EPI. This gives our method an
improved generalization ability and makes it applicable to a wide range of
previously unseen scenes. Experiments on both synthetic and real light fields
demonstrate the improvements over state-of-the-art, especially in large
disparity areas.","cs.CV,cs.GR"
"LadderNet: Multi-path networks based on U-Net for medical image segmentation. U-Net has been providing state-of-the-art performance in many medical image
segmentation problems. Many modifications have been proposed for U-Net, such as
attention U-Net, recurrent residual convolutional U-Net (R2-UNet), and U-Net
with residual blocks or blocks with dense connections. However, all these
modifications have an encoder-decoder structure with skip connections, and the
number of paths for information flow is limited. We propose LadderNet in this
paper, which can be viewed as a chain of multiple U-Nets. Instead of only one
pair of encoder branch and decoder branch in U-Net, a LadderNet has multiple
pairs of encoder-decoder branches, and has skip connections between every pair
of adjacent decoder and decoder branches in each level. Inspired by the success
of ResNet and R2-UNet, we use modified residual blocks where two convolutional
layers in one block share the same weights. A LadderNet has more paths for
information flow because of skip connections and residual blocks, and can be
viewed as an ensemble of Fully Convolutional Networks (FCN). The equivalence to
an ensemble of FCNs improves segmentation accuracy, while the shared weights
within each residual block reduce parameter number. Semantic segmentation is
essential for retinal disease detection. We tested LadderNet on two benchmark
datasets for blood vessel segmentation in retinal images, and achieved superior
performance over methods in the literature. The implementation is provided
\url{https://github.com/juntang-zhuang/LadderNet}","cs.CV,eess.IV"
"Learning with Molecules beyond Graph Neural Networks. We demonstrate a deep learning framework which is inherently based in the
highly expressive language of relational logic, enabling to, among other
things, capture arbitrarily complex graph structures. We show how Graph Neural
Networks and similar models can be easily covered in the framework by
specifying the underlying propagation rules in the relational logic. The
declarative nature of the used language then allows to easily modify and extend
the propagation schemes into complex structures, such as the molecular rings
which we choose for a short demonstration in this paper.","cs.AI,cs.LG,cs.LO,cs.NE"
"Efficient Real-Time Image Recognition Using Collaborative Swarm of UAVs and Convolutional Networks. Unmanned Aerial Vehicles (UAVs) have recently attracted significant attention
due to their outstanding ability to be used in different sectors and serve in
difficult and dangerous areas. Moreover, the advancements in computer vision
and artificial intelligence have increased the use of UAVs in various
applications and solutions, such as forest fires detection and borders
monitoring. However, using deep neural networks (DNNs) with UAVs introduces
several challenges of processing deeper networks and complex models, which
restricts their on-board computation. In this work, we present a strategy
aiming at distributing inference requests to a swarm of resource-constrained
UAVs that classifies captured images on-board and finds the minimum
decision-making latency. We formulate the model as an optimization problem that
minimizes the latency between acquiring images and making the final decisions.
The formulated optimization solution is an NP-hard problem. Hence it is not
adequate for online resource allocation. Therefore, we introduce an online
heuristic solution, namely DistInference, to find the layers placement strategy
that gives the best latency among the available UAVs. The proposed approach is
general enough to be used for different low decision-latency applications as
well as for all CNN types organized into the pipeline of layers (e.g., VGG) or
based on residual blocks (e.g., ResNet).","cs.CV,cs.DC"
"Zero-Shot Adaptation for mmWave Beam-Tracking on Overhead Messenger Wires through Robust Adversarial Reinforcement Learning. Millimeter wave (mmWave) beam-tracking based on machine learning enables the
development of accurate tracking policies while obviating the need to
periodically solve beam-optimization problems. However, its applicability is
still arguable when training-test gaps exist in terms of environmental
parameters that affect the node dynamics. From this skeptical point of view,
the contribution of this study is twofold. First, by considering an example
scenario, we confirm that the training-test gap adversely affects the
beam-tracking performance. More specifically, we consider nodes placed on
overhead messenger wires, where the node dynamics are affected by several
environmental parameters, e.g, the wire mass and tension. Although these are
particular scenarios, they yield insight into the validation of the
training-test gap problems. Second, we demonstrate the feasibility of
\textit{zero-shot adaptation} as a solution, where a learning agent adapts to
environmental parameters unseen during training. This is achieved by leveraging
a robust adversarial reinforcement learning (RARL) technique, where such
training-and-test gaps are regarded as disturbances by adversaries that are
jointly trained with a legitimate beam-tracking agent. Numerical evaluations
demonstrate that the beam-tracking policy learned via RARL can be applied to a
wide range of environmental parameters without severely degrading the received
power.","cs.LG,cs.NI"
"PySS3: A Python package implementing a novel text classifier with visualization tools for Explainable AI. A recently introduced text classifier, called SS3, has obtained
state-of-the-art performance on the CLEF's eRisk tasks. SS3 was created to deal
with risk detection over text streams and, therefore, not only supports
incremental training and classification but also can visually explain its
rationale. However, little attention has been paid to the potential use of SS3
as a general classifier. We believe this could be due to the unavailability of
an open-source implementation of SS3. In this work, we introduce PySS3, a
package that implements SS3 and also comes with visualization tools that allow
researchers to deploy robust, explainable, and trusty machine learning models
for text classification.","cs.AI,cs.IR,cs.LG,cs.SE,stat.ML"
"Text Classification and Clustering with Annealing Soft Nearest Neighbor Loss. We define disentanglement as how far class-different data points from each
other are, relative to the distances among class-similar data points. When
maximizing disentanglement during representation learning, we obtain a
transformed feature representation where the class memberships of the data
points are preserved. If the class memberships of the data points are
preserved, we would have a feature representation space in which a nearest
neighbour classifier or a clustering algorithm would perform well. We take
advantage of this method to learn better natural language representation, and
employ it on text classification and text clustering tasks. Through
disentanglement, we obtain text representations with better-defined clusters
and improve text classification performance. Our approach had a test
classification accuracy of as high as 90.11% and test clustering accuracy of
88% on the AG News dataset, outperforming our baseline models -- without any
other training tricks or regularization.","cs.CL,cs.LG,cs.NE"
"Precise Statistical Analysis of Classification Accuracies for Adversarial Training. Despite the wide empirical success of modern machine learning algorithms and
models in a multitude of applications, they are known to be highly susceptible
to seemingly small indiscernible perturbations to the input data known as
adversarial attacks. A variety of recent adversarial training procedures have
been proposed to remedy this issue. Despite the success of such procedures at
increasing accuracy on adversarially perturbed inputs or robust accuracy, these
techniques often reduce accuracy on natural unperturbed inputs or standard
accuracy. Complicating matters further the effect and trend of adversarial
training procedures on standard and robust accuracy is rather counter intuitive
and radically dependent on a variety of factors including the perceived form of
the perturbation during training, size/quality of data, model
overparameterization, etc. In this paper we focus on binary classification
problems where the data is generated according to the mixture of two Gaussians
with general anisotropic covariance matrices and derive a precise
characterization of the standard and robust accuracy for a class of minimax
adversarially trained models. We consider a general norm-based adversarial
model, where the adversary can add perturbations of bounded $\ell_p$ norm to
each input data, for an arbitrary $p\ge 1$. Our comprehensive analysis allows
us to theoretically explain several intriguing empirical phenomena and provide
a precise understanding of the role of different problem parameters on standard
and robust accuracies.","cs.LG,math.ST,stat.ML,stat.TH"
"Putting a bug in ML: The moth olfactory network learns to read MNIST. We seek to (i) characterize the learning architectures exploited in
biological neural networks for training on very few samples, and (ii) port
these algorithmic structures to a machine learning context. The Moth Olfactory
Network is among the simplest biological neural systems that can learn, and its
architecture includes key structural elements and mechanisms widespread in
biological neural nets, such as cascaded networks, competitive inhibition, high
intrinsic noise, sparsity, reward mechanisms, and Hebbian plasticity. These
structural biological elements, in combination, enable rapid learning.
  MothNet is a computational model of the Moth Olfactory Network, closely
aligned with the moth's known biophysics and with in vivo electrode data
collected from moths learning new odors. We assign this model the task of
learning to read the MNIST digits. We show that MothNet successfully learns to
read given very few training samples (1 to 10 samples per class). In this
few-samples regime, it outperforms standard machine learning methods such as
nearest-neighbors, support-vector machines, and neural networks (NNs), and
matches specialized one-shot transfer-learning methods but without the need for
pre-training. The MothNet architecture illustrates how algorithmic structures
derived from biological brains can be used to build alternative NNs that may
avoid some of the learning rate limitations of current engineered NNs.","I.2.6; I.5.0,cs.LG,cs.NE,q-bio.NC"
"2D SEM images turn into 3D object models. The scanning electron microscopy (SEM) is probably one the most fascinating
examination approach that has been used since more than two decades to detailed
inspection of micro scale objects. Most of the scanning electron microscopes
could only produce 2D images that could not assist operational analysis of
microscopic surface properties. Computer vision algorithms combined with very
advanced geometry and mathematical approaches turn any SEM into a full 3D
measurement device. This work focuses on a methodical literature review for
automatic 3D surface reconstruction of scanning electron microscope images.","cs.CV,cs.GR"
"Profile Prediction: An Alignment-Based Pre-Training Task for Protein Sequence Models. For protein sequence datasets, unlabeled data has greatly outpaced labeled
data due to the high cost of wet-lab characterization. Recent deep-learning
approaches to protein prediction have shown that pre-training on unlabeled data
can yield useful representations for downstream tasks. However, the optimal
pre-training strategy remains an open question. Instead of strictly borrowing
from natural language processing (NLP) in the form of masked or autoregressive
language modeling, we introduce a new pre-training task: directly predicting
protein profiles derived from multiple sequence alignments. Using a set of
five, standardized downstream tasks for protein models, we demonstrate that our
pre-training task along with a multi-task objective outperforms masked language
modeling alone on all five tasks. Our results suggest that protein sequence
models may benefit from leveraging biologically-inspired inductive biases that
go beyond existing language modeling techniques in NLP.","cs.LG,q-bio.BM"
"Monte Carlo Inference via Greedy Importance Sampling. We present a new method for conducting Monte Carlo inference in graphical
models which combines explicit search with generalized importance sampling. The
idea is to reduce the variance of importance sampling by searching for
significant points in the target distribution. We prove that it is possible to
introduce search and still maintain unbiasedness. We then demonstrate our
procedure on a few simple inference tasks and show that it can improve the
inference quality of standard MCMC methods, including Gibbs sampling,
Metropolis sampling, and Hybrid Monte Carlo. This paper extends previous work
which showed how greedy importance sampling could be correctly realized in the
one-dimensional case.","cs.LG,stat.CO,stat.ML"
"Learning Complexity of Simulated Annealing. Simulated annealing is an effective and general means of optimization. It is
in fact inspired by metallurgy, where the temperature of a material determines
its behavior in thermodynamics. Likewise, in simulated annealing, the actions
that the algorithm takes depend entirely on the value of a variable which
captures the notion of temperature. Typically, simulated annealing starts with
a high temperature, which makes the algorithm pretty unpredictable, and
gradually cools the temperature down to become more stable.
  A key component that plays a crucial role in the performance of simulated
annealing is the criteria under which the temperature changes namely, the
cooling schedule. Motivated by this, we study the following question in this
work: ""Given enough samples to the instances of a specific class of
optimization problems, can we design optimal (or approximately optimal) cooling
schedules that minimize the runtime or maximize the success rate of the
algorithm on average when the underlying problem is drawn uniformly at random
from the same class?""
  We provide positive results both in terms of sample complexity and simulation
complexity. For sample complexity, we show that $\tilde O(\sqrt{m})$ samples
suffice to find an approximately optimal cooling schedule of length $m$. We
complement this result by giving a lower bound of $\tilde \Omega(m^{1/3})$ on
the sample complexity of any learning algorithm that provides an almost optimal
cooling schedule. These results are general and rely on no assumption. For
simulation complexity, however, we make additional assumptions to measure the
success rate of an algorithm. To this end, we introduce the monotone stationary
graph that models the performance of simulated annealing. Based on this model,
we present polynomial time algorithms with provable guarantees for the learning
problem.","cs.LG,stat.ML"
"Image sequence interpolation using optimal control. The problem of the generation of an intermediate image between two given
images in an image sequence is considered. The problem is formulated as an
optimal control problem governed by a transport equation. This approach bears
similarities with the Horn \& Schunck method for optical flow calculation but
in fact the model is quite different. The images are modelled in $BV$ and an
analysis of solutions of transport equations with values in $BV$ is included.
Moreover, the existence of optimal controls is proven and necessary conditions
are derived. Finally, two algorithms are given and numerical results are
compared with existing methods. The new method is competitive with
state-of-the-art methods and even outperforms several existing methods.","49J20,65D18,68U10,cs.CV,math.AP,math.OC"
"Robustness of on-device Models: Adversarial Attack to Deep Learning Models on Android Apps. Deep learning has shown its power in many applications, including object
detection in images, natural-language understanding, and speech recognition. To
make it more accessible to end users, many deep learning models are now
embedded in mobile apps. Compared to offloading deep learning from smartphones
to the cloud, performing machine learning on-device can help improve latency,
connectivity, and power consumption. However, most deep learning models within
Android apps can easily be obtained via mature reverse engineering, while the
models' exposure may invite adversarial attacks. In this study, we propose a
simple but effective approach to hacking deep learning models using adversarial
attacks by identifying highly similar pre-trained models from TensorFlow Hub.
All 10 real-world Android apps in the experiment are successfully attacked by
our approach. Apart from the feasibility of the model attack, we also carry out
an empirical study that investigates the characteristics of deep learning
models used by hundreds of Android apps on Google Play. The results show that
many of them are similar to each other and widely use fine-tuning techniques to
pre-trained models on the Internet.","cs.CR,cs.LG,cs.SE"
"Deep learning for determining a near-optimal topological design without any iteration. In this study, we propose a novel deep learning-based method to predict an
optimized structure for a given boundary condition and optimization setting
without using any iterative scheme. For this purpose, first, using open-source
topology optimization code, datasets of the optimized structures paired with
the corresponding information on boundary conditions and optimization settings
are generated at low (32 x 32) and high (128 x 128) resolutions. To construct
the artificial neural network for the proposed method, a convolutional neural
network (CNN)-based encoder and decoder network is trained using the training
dataset generated at low resolution. Then, as a two-stage refinement, the
conditional generative adversarial network (cGAN) is trained with the optimized
structures paired at both low and high resolutions, and is connected to the
trained CNN-based encoder and decoder network. The performance evaluation
results of the integrated network demonstrate that the proposed method can
determine a near-optimal structure in terms of pixel values and compliance with
negligible computational time.","cs.LG,physics.comp-ph"
"Regularized spectral methods for clustering signed networks. We study the problem of $k$-way clustering in signed graphs. Considerable
attention in recent years has been devoted to analyzing and modeling signed
graphs, where the affinity measure between nodes takes either positive or
negative values. Recently, Cucuringu et al. [CDGT 2019] proposed a spectral
method, namely SPONGE (Signed Positive over Negative Generalized Eigenproblem),
which casts the clustering task as a generalized eigenvalue problem optimizing
a suitably defined objective function. This approach is motivated by social
balance theory, where the clustering task aims to decompose a given network
into disjoint groups, such that individuals within the same group are connected
by as many positive edges as possible, while individuals from different groups
are mainly connected by negative edges. Through extensive numerical
simulations, SPONGE was shown to achieve state-of-the-art empirical
performance. On the theoretical front, [CDGT 2019] analyzed SPONGE and the
popular Signed Laplacian method under the setting of a Signed Stochastic Block
Model (SSBM), for $k=2$ equal-sized clusters, in the regime where the graph is
moderately dense.
  In this work, we build on the results in [CDGT 2019] on two fronts for the
normalized versions of SPONGE and the Signed Laplacian. Firstly, for both
algorithms, we extend the theoretical analysis in [CDGT 2019] to the general
setting of $k \geq 2$ unequal-sized clusters in the moderately dense regime.
Secondly, we introduce regularized versions of both methods to handle sparse
graphs -- a regime where standard spectral methods underperform -- and provide
theoretical guarantees under the same SSBM model. To the best of our knowledge,
regularized spectral methods have so far not been considered in the setting of
clustering signed graphs. We complement our theoretical results with an
extensive set of numerical experiments on synthetic data.","cs.LG,math.ST,stat.ML,stat.TH"
"A Type II Fuzzy Entropy Based Multi-Level Image Thresholding Using Adaptive Plant Propagation Algorithm. One of the most straightforward, direct and efficient approaches to Image
Segmentation is Image Thresholding. Multi-level Image Thresholding is an
essential viewpoint in many image processing and Pattern Recognition based
real-time applications which can effectively and efficiently classify the
pixels into various groups denoting multiple regions in an Image. Thresholding
based Image Segmentation using fuzzy entropy combined with intelligent
optimization approaches are commonly used direct methods to properly identify
the thresholds so that they can be used to segment an Image accurately. In this
paper a novel approach for multi-level image thresholding is proposed using
Type II Fuzzy sets combined with Adaptive Plant Propagation Algorithm (APPA).
Obtaining the optimal thresholds for an image by maximizing the entropy is
extremely tedious and time consuming with increase in the number of thresholds.
Hence, Adaptive Plant Propagation Algorithm (APPA), a memetic algorithm based
on plant intelligence, is used for fast and efficient selection of optimal
thresholds. This fact is reasonably justified by comparing the accuracy of the
outcomes and computational time consumed by other modern state-of-the-art
algorithms such as Particle Swarm Optimization (PSO), Gravitational Search
Algorithm (GSA) and Genetic Algorithm (GA).","cs.CV,math.OC,physics.data-an,stat.CO"
"On the consistency of supervised learning with missing values. In many application settings, the data have missing entries which make
analysis challenging. An abundant literature addresses missing values in an
inferential framework: estimating parameters and their variance from incomplete
tables. Here, we consider supervised-learning settings: predicting a target
when missing values appear in both training and testing data. We show the
consistency of two approaches in prediction. A striking result is that the
widely-used method of imputing with a constant, such as the mean prior to
learning is consistent when missing values are not informative. This contrasts
with inferential settings where mean imputation is pointed at for distorting
the distribution of the data. That such a simple approach can be consistent is
important in practice. We also show that a predictor suited for complete
observations can predict optimally on incomplete data,through multiple
imputation.Finally, to compare imputation with learning directly with a model
that accounts for missing values, we analyze further decision trees. These can
naturally tackle empirical risk minimization with missing values, due to their
ability to handle the half-discrete nature of incomplete variables. After
comparing theoretically and empirically different missing values strategies in
trees, we recommend using the ""missing incorporated in attribute"" method as it
can handle both non-informative and informative missing values.","cs.LG,math.ST,stat.ML,stat.TH"
"Learn to Generate Time Series Conditioned Graphs with Generative Adversarial Nets. Deep learning based approaches have been utilized to model and generate
graphs subjected to different distributions recently. However, they are
typically unsupervised learning based and unconditioned generative models or
simply conditioned on the graph-level contexts, which are not associated with
rich semantic node-level contexts. Differently, in this paper, we are
interested in a novel problem named Time Series Conditioned Graph Generation:
given an input multivariate time series, we aim to infer a target relation
graph modeling the underlying interrelationships between time series with each
node corresponding to each time series. For example, we can study the
interrelationships between genes in a gene regulatory network of a certain
disease conditioned on their gene expression data recorded as time series. To
achieve this, we propose a novel Time Series conditioned Graph
Generation-Generative Adversarial Networks (TSGG-GAN) to handle challenges of
rich node-level context structures conditioning and measuring similarities
directly between graphs and time series. Extensive experiments on synthetic and
real-word gene regulatory networks datasets demonstrate the effectiveness and
generalizability of the proposed TSGG-GAN.","cs.LG,cs.SI,stat.ML"
"Understanding GANs: the LQG Setting. Generative Adversarial Networks (GANs) have become a popular method to learn
a probability model from data. In this paper, we aim to provide an
understanding of some of the basic issues surrounding GANs including their
formulation, generalization and stability on a simple benchmark where the data
has a high-dimensional Gaussian distribution. Even in this simple benchmark,
the GAN problem has not been well-understood as we observe that existing
state-of-the-art GAN architectures may fail to learn a proper generative
distribution owing to (1) stability issues (i.e., convergence to bad local
solutions or not converging at all), (2) approximation issues (i.e., having
improper global GAN optimizers caused by inappropriate GAN's loss functions),
and (3) generalizability issues (i.e., requiring large number of samples for
training). In this setup, we propose a GAN architecture which recovers the
maximum-likelihood solution and demonstrates fast generalization. Moreover, we
analyze global stability of different computational approaches for the proposed
GAN optimization and highlight their pros and cons. Finally, we outline an
extension of our model-based approach to design GANs in more complex setups
than the considered Gaussian benchmark.","cs.IT,cs.LG,math.IT,stat.ML"
"Dissipative SymODEN: Encoding Hamiltonian Dynamics with Dissipation and Control into Deep Learning. In this work, we introduce Dissipative SymODEN, a deep learning architecture
which can infer the dynamics of a physical system with dissipation from
observed state trajectories. To improve prediction accuracy while reducing
network size, Dissipative SymODEN encodes the port-Hamiltonian dynamics with
energy dissipation and external input into the design of its computation graph
and learns the dynamics in a structured way. The learned model, by revealing
key aspects of the system, such as the inertia, dissipation, and potential
energy, paves the way for energy-based controllers.","cs.LG,cs.SY,eess.SY,stat.ML"
"Sparse models for Computer Vision. The representation of images in the brain is known to be sparse. That is, as
neural activity is recorded in a visual area ---for instance the primary visual
cortex of primates--- only a few neurons are active at a given time with
respect to the whole population. It is believed that such a property reflects
the efficient match of the representation with the statistics of natural
scenes. Applying such a paradigm to computer vision therefore seems a promising
approach towards more biomimetic algorithms. Herein, we will describe a
biologically-inspired approach to this problem. First, we will describe an
unsupervised learning paradigm which is particularly adapted to the efficient
coding of image patches. Then, we will outline a complete multi-scale framework
---SparseLets--- implementing a biologically inspired sparse representation of
natural images. Finally, we will propose novel methods for integrating prior
information into these algorithms and provide some preliminary experimental
results. We will conclude by giving some perspective on applying such
algorithms to computer vision. More specifically, we will propose that
bio-inspired approaches may be applied to computer vision using predictive
coding schemes, sparse models being one simple and efficient instance of such
schemes.","cs.CV,q-bio.NC"
"Tactics of Adversarial Attack on Deep Reinforcement Learning Agents. We introduce two tactics to attack agents trained by deep reinforcement
learning algorithms using adversarial examples, namely the strategically-timed
attack and the enchanting attack. In the strategically-timed attack, the
adversary aims at minimizing the agent's reward by only attacking the agent at
a small subset of time steps in an episode. Limiting the attack activity to
this subset helps prevent detection of the attack by the agent. We propose a
novel method to determine when an adversarial example should be crafted and
applied. In the enchanting attack, the adversary aims at luring the agent to a
designated target state. This is achieved by combining a generative model and a
planning algorithm: while the generative model predicts the future states, the
planning algorithm generates a preferred sequence of actions for luring the
agent. A sequence of adversarial examples is then crafted to lure the agent to
take the preferred sequence of actions. We apply the two tactics to the agents
trained by the state-of-the-art deep reinforcement learning algorithm including
DQN and A3C. In 5 Atari games, our strategically timed attack reduces as much
reward as the uniform attack (i.e., attacking at every time step) does by
attacking the agent 4 times less often. Our enchanting attack lures the agent
toward designated target states with a more than 70% success rate. Videos are
available at http://yenchenlin.me/adversarial_attack_RL/","cs.CR,cs.LG,stat.ML"
"Deep Hashing for Secure Multimodal Biometrics. When compared to unimodal systems, multimodal biometric systems have several
advantages, including lower error rate, higher accuracy, and larger population
coverage. However, multimodal systems have an increased demand for integrity
and privacy because they must store multiple biometric traits associated with
each user. In this paper, we present a deep learning framework for
feature-level fusion that generates a secure multimodal template from each
user's face and iris biometrics. We integrate a deep hashing (binarization)
technique into the fusion architecture to generate a robust binary multimodal
shared latent representation. Further, we employ a hybrid secure architecture
by combining cancelable biometrics with secure sketch techniques and integrate
it with a deep hashing framework, which makes it computationally prohibitive to
forge a combination of multiple biometrics that pass the authentication. The
efficacy of the proposed approach is shown using a multimodal database of face
and iris and it is observed that the matching performance is improved due to
the fusion of multiple biometrics. Furthermore, the proposed approach also
provides cancelability and unlinkability of the templates along with improved
privacy of the biometric data. Additionally, we also test the proposed hashing
function for an image retrieval application using a benchmark dataset. The main
goal of this paper is to develop a method for integrating multimodal fusion,
deep hashing, and biometric security, with an emphasis on structural data from
modalities like face and iris. The proposed approach is in no way a general
biometric security framework that can be applied to all biometric modalities,
as further research is needed to extend the proposed framework to other
unconstrained biometric modalities.","cs.AI,cs.CV,cs.IT,math.IT"
"A Compendium on Network and Host based Intrusion Detection Systems. The techniques of deep learning have become the state of the art methodology
for executing complicated tasks from various domains of computer vision,
natural language processing, and several other areas. Due to its rapid
development and promising benchmarks in those fields, researchers started
experimenting with this technique to perform in the area of, especially in
intrusion detection related tasks. Deep learning is a subset and a natural
extension of classical Machine learning and an evolved model of neural
networks. This paper contemplates and discusses all the methodologies related
to the leading edge Deep learning and Neural network models purposing to the
arena of Intrusion Detection Systems.","cs.CR,cs.LG,cs.NE,cs.NI,stat.ML"
"Intrinsic dimension estimation for locally undersampled data. High-dimensional data are ubiquitous in contemporary science and finding
methods to compress them is one of the primary goals of machine learning. Given
a dataset lying in a high-dimensional space (in principle hundreds to several
thousands of dimensions), it is often useful to project it onto a
lower-dimensional manifold, without loss of information. Identifying the
minimal dimension of such manifold is a challenging problem known in the
literature as intrinsic dimension estimation (IDE). Traditionally, most IDE
algorithms are either based on multiscale principal component analysis (PCA) or
on the notion of correlation dimension (and more in general on
k-nearest-neighbors distances). These methods are affected, in different ways,
by a severe curse of dimensionality. In particular, none of the existing
algorithms can provide accurate ID estimates in the extreme locally
undersampled regime, i.e. in the limit where the number of samples in any local
patch of the manifold is less than (or of the same order of) the ID of the
dataset. Here we introduce a new ID estimator that leverages on simple
properties of the tangent space of a manifold to overcome these shortcomings.
The method is based on the full correlation integral, going beyond the limit of
small radius used for the estimation of the correlation dimension. Our
estimator alleviates the extreme undersampling problem, intractable with other
methods. Based on this insight, we explore a multiscale generalization of the
algorithm. We show that it is capable of (i) identifying multiple
dimensionalities in a dataset, and (ii) providing accurate estimates of the ID
of extremely curved manifolds. In particular, we test the method on manifolds
generated from global transformations of high-contrast images, relevant for
invariant object recognition and considered a challenge for state-of-the-art ID
estimators.","cond-mat.dis-nn,cs.LG,stat.ML"
"CardioGAN: Attentive Generative Adversarial Network with Dual Discriminators for Synthesis of ECG from PPG. Electrocardiogram (ECG) is the electrical measurement of cardiac activity,
whereas Photoplethysmogram (PPG) is the optical measurement of volumetric
changes in blood circulation. While both signals are used for heart rate
monitoring, from a medical perspective, ECG is more useful as it carries
additional cardiac information. Despite many attempts toward incorporating ECG
sensing in smartwatches or similar wearable devices for continuous and reliable
cardiac monitoring, PPG sensors are the main feasible sensing solution
available. In order to tackle this problem, we propose CardioGAN, an
adversarial model which takes PPG as input and generates ECG as output. The
proposed network utilizes an attention-based generator to learn local salient
features, as well as dual discriminators to preserve the integrity of generated
data in both time and frequency domains. Our experiments show that the ECG
generated by CardioGAN provides more reliable heart rate measurements compared
to the original input PPG, reducing the error from 9.74 beats per minute
(measured from the PPG) to 2.89 (measured from the generated ECG).","cs.LG,eess.SP"
"E2E-VLP: End-to-End Vision-Language Pre-training Enhanced by Visual Learning. Vision-language pre-training (VLP) on large-scale image-text pairs has
achieved huge success for the cross-modal downstream tasks. The most existing
pre-training methods mainly adopt a two-step training procedure, which firstly
employs a pre-trained object detector to extract region-based visual features,
then concatenates the image representation and text embedding as the input of
Transformer to train. However, these methods face problems of using
task-specific visual representation of the specific object detector for generic
cross-modal understanding, and the computation inefficiency of two-stage
pipeline. In this paper, we propose the first end-to-end vision-language
pre-trained model for both V+L understanding and generation, namely E2E-VLP,
where we build a unified Transformer framework to jointly learn visual
representation, and semantic alignments between image and text. We incorporate
the tasks of object detection and image captioning into pre-training with a
unified Transformer encoder-decoder architecture for enhancing visual learning.
An extensive set of experiments have been conducted on well-established
vision-language downstream tasks to demonstrate the effectiveness of this novel
VLP paradigm.","cs.AI,cs.CL,cs.CV"
"Learning to Optimize Variational Quantum Circuits to Solve Combinatorial Problems. Quantum computing is a computational paradigm with the potential to
outperform classical methods for a variety of problems. Proposed recently, the
Quantum Approximate Optimization Algorithm (QAOA) is considered as one of the
leading candidates for demonstrating quantum advantage in the near term. QAOA
is a variational hybrid quantum-classical algorithm for approximately solving
combinatorial optimization problems. The quality of the solution obtained by
QAOA for a given problem instance depends on the performance of the classical
optimizer used to optimize the variational parameters. In this paper, we
formulate the problem of finding optimal QAOA parameters as a learning task in
which the knowledge gained from solving training instances can be leveraged to
find high-quality solutions for unseen test instances. To this end, we develop
two machine-learning-based approaches. Our first approach adopts a
reinforcement learning (RL) framework to learn a policy network to optimize
QAOA circuits. Our second approach adopts a kernel density estimation (KDE)
technique to learn a generative model of optimal QAOA parameters. In both
approaches, the training procedure is performed on small-sized problem
instances that can be simulated on a classical computer; yet the learned RL
policy and the generative model can be used to efficiently solve larger
problems. Extensive simulations using the IBM Qiskit Aer quantum circuit
simulator demonstrate that our proposed RL- and KDE-based approaches reduce the
optimality gap by factors up to 30.15 when compared with other commonly used
off-the-shelf optimizers.","cs.LG,quant-ph,stat.ML"
"Molecular graph generation with Graph Neural Networks. Drug Discovery is a fundamental and ever-evolving field of research. The
design of new candidate molecules requires large amounts of time and money, and
computational methods are being increasingly employed to cut these costs.
Machine learning methods are ideal for the design of large amounts of potential
new candidate molecules, which are naturally represented as graphs. Graph
generation is being revolutionized by deep learning methods, and molecular
generation is one of its most promising applications. In this paper, we
introduce a sequential molecular graph generator based on a set of graph neural
network modules, which we call MG^2N^2. At each step, a node or a group of
nodes is added to the graph, along with its connections. The modular
architecture simplifies the training procedure, also allowing an independent
retraining of a single module. Sequentiality and modularity make the generation
process interpretable. The use of graph neural networks maximizes the
information in input at each generative step, which consists of the subgraph
produced during the previous steps. Experiments of unconditional generation on
the QM9 and Zinc datasets show that our model is capable of generalizing
molecular patterns seen during the training phase, without overfitting. The
results indicate that our method is competitive, and outperforms challenging
baselines for unconditional generation.","cs.LG,q-bio.BM,stat.ML"
"Sparse Partial Least Squares for Coarse Noisy Graph Alignment. Graph signal processing (GSP) provides a powerful framework for analyzing
signals arising in a variety of domains. In many applications of GSP, multiple
network structures are available, each of which captures different aspects of
the same underlying phenomenon. To integrate these different data sources,
graph alignment techniques attempt to find the best correspondence between
vertices of two graphs. We consider a generalization of this problem, where
there is no natural one-to-one mapping between vertices, but where there is
correspondence between the community structures of each graph. Because we seek
to learn structure at this higher community level, we refer to this problem as
""coarse"" graph alignment. To this end, we propose a novel regularized partial
least squares method which both incorporates the observed graph structures and
imposes sparsity in order to reflect the underlying block community structure.
We provide efficient algorithms for our method and demonstrate its
effectiveness in simulations.","cs.LG,cs.SI,eess.SP,stat.ME,stat.ML"
"PyMT5: multi-mode translation of natural language and Python code with transformers. Simultaneously modeling source code and natural language has many exciting
applications in automated software development and understanding. Pursuant to
achieving such technology, we introduce PyMT5, the Python method text-to-text
transfer transformer, which is trained to translate between all pairs of Python
method feature combinations: a single model that can both predict whole methods
from natural language documentation strings (docstrings) and summarize code
into docstrings of any common style. We present an analysis and modeling effort
of a large-scale parallel corpus of 26 million Python methods and 7.7 million
method-docstring pairs, demonstrating that for docstring and method generation,
PyMT5 outperforms similarly-sized auto-regressive language models (GPT2) which
were English pre-trained or randomly initialized. On the CodeSearchNet test
set, our best model predicts 92.1% syntactically correct method bodies,
achieved a BLEU score of 8.59 for method generation and 16.3 for docstring
generation (summarization), and achieved a ROUGE-L F-score of 24.8 for method
generation and 36.7 for docstring generation.","cs.LG,cs.SE"
"Imitation Learning of Neural Spatio-Temporal Point Processes. We present a novel Neural Embedding Spatio-Temporal (NEST) point process
model for spatio-temporal discrete event data and develop an efficient
imitation learning (a type of reinforcement learning) based approach for model
fitting. Despite the rapid development of one-dimensional temporal point
processes for discrete event data, the study of spatial-temporal aspects of
such data is relatively scarce. Our model captures complex spatio-temporal
dependence between discrete events by carefully design a mixture of
heterogeneous Gaussian diffusion kernels, whose parameters are parameterized by
neural networks. This new kernel is the key that our model can capture
intricate spatial dependence patterns and yet still lead to interpretable
results as we examine maps of Gaussian diffusion kernel parameters. The
imitation learning model fitting for the NEST is more robust than the maximum
likelihood estimate. It directly measures the divergence between the empirical
distributions between the training data and the model-generated data. Moreover,
our imitation learning-based approach enjoys computational efficiency due to
the explicit characterization of the reward function related to the likelihood
function; furthermore, the likelihood function under our model enjoys tractable
expression due to Gaussian kernel parameterization. Experiments based on real
data show our method's good performance relative to the state-of-the-art and
the good interpretability of NEST's result.","cs.LG,stat.AP,stat.ML"
"Individual Explanations in Machine Learning Models: A Survey for Practitioners. In recent years, the use of sophisticated statistical models that influence
decisions in domains of high societal relevance is on the rise. Although these
models can often bring substantial improvements in the accuracy and efficiency
of organizations, many governments, institutions, and companies are reluctant
to their adoption as their output is often difficult to explain in
human-interpretable ways. Hence, these models are often regarded as
black-boxes, in the sense that their internal mechanisms can be opaque to human
audit. In real-world applications, particularly in domains where decisions can
have a sensitive impact--e.g., criminal justice, estimating credit scores,
insurance risk, health risks, etc.--model interpretability is desired.
Recently, the academic literature has proposed a substantial amount of methods
for providing interpretable explanations to machine learning models. This
survey reviews the most relevant and novel methods that form the
state-of-the-art for addressing the particular problem of explaining individual
instances in machine learning. It seeks to provide a succinct review that can
guide data science and machine learning practitioners in the search for
appropriate methods to their problem domain.","cs.AI,cs.LG,stat.AP"
"Spectral estimation from simulations via sketching. Sketching is a stochastic dimension reduction method that preserves geometric
structures of data and has applications in high-dimensional regression, low
rank approximation and graph sparsification. In this work, we show that
sketching can be used to compress simulation data and still accurately estimate
time autocorrelation and power spectral density. For a given compression ratio,
the accuracy is much higher than using previously known methods. In addition to
providing theoretical guarantees, we apply sketching to a molecular dynamics
simulation of methanol and find that the estimate of spectral density is 90%
accurate using only 10% of the data.","65Z05,cs.LG,eess.SP,physics.comp-ph,stat.ML"
"Posterior Mean Super-resolution with a Causal Gaussian Markov Random Field Prior. We propose a Bayesian image super-resolution (SR) method with a causal
Gaussian Markov random field (MRF) prior. SR is a technique to estimate a
spatially high-resolution image from given multiple low-resolution images. An
MRF model with the line process supplies a preferable prior for natural images
with edges. We improve the existing image transformation model, the compound
MRF model, and its hyperparameter prior model. We also derive the optimal
estimator -- not the joint maximum a posteriori (MAP) or marginalized maximum
likelihood (ML), but the posterior mean (PM) -- from the objective function of
the L2-norm (mean square error) -based peak signal-to-noise ratio (PSNR). Point
estimates such as MAP and ML are generally not stable in ill-posed
high-dimensional problems because of overfitting, while PM is a stable
estimator because all the parameters in the model are evaluated as
distributions. The estimator is numerically determined by using variational
Bayes. Variational Bayes is a widely used method that approximately determines
a complicated posterior distribution, but it is generally hard to use because
it needs the conjugate prior. We solve this problem with simple Taylor
approximations. Experimental results have shown that the proposed method is
more accurate or comparable to existing methods.","62F15,68U10,I.4.5; I.4.4; G.3,cs.CV"
"Health Status Prediction with Local-Global Heterogeneous Behavior Graph. Health management is getting increasing attention all over the world.
However, existing health management mainly relies on hospital examination and
treatment, which are complicated and untimely. The emerging of mobile devices
provides the possibility to manage people's health status in a convenient and
instant way. Estimation of health status can be achieved with various kinds of
data streams continuously collected from wearable sensors. However, these data
streams are multi-source and heterogeneous, containing complex temporal
structures with local contextual and global temporal aspects, which makes the
feature learning and data joint utilization challenging. We propose to model
the behavior-related multi-source data streams with a local-global graph, which
contains multiple local context sub-graphs to learn short term local context
information with heterogeneous graph neural networks and a global temporal
sub-graph to learn long term dependency with self-attention networks. Then
health status is predicted based on the structure-aware representation learned
from the local-global behavior graph. We take experiments on StudentLife
dataset, and extensive results demonstrate the effectiveness of our proposed
model.","cs.LG,cs.MM"
"Automatic Test Suite Generation for Key-Points Detection DNNs using Many-Objective Search (Experience Paper). Automatically detecting the positions of key-points (e.g., facial key-points
or finger key-points) in an image is an essential problem in many applications,
such as driver's gaze detection and drowsiness detection in automated driving
systems. With the recent advances of Deep Neural Networks (DNNs), Key-Points
detection DNNs (KP-DNNs) have been increasingly employed for that purpose.
Nevertheless, KP-DNN testing and validation have remained a challenging problem
because KP-DNNs predict many independent key-points at the same time -- where
each individual key-point may be critical in the targeted application -- and
images can vary a great deal according to many factors.
  In this paper, we present an approach to automatically generate test data for
KP-DNNs using many-objective search. In our experiments, focused on facial
key-points detection DNNs developed for an industrial automotive application,
we show that our approach can generate test suites to severely mispredict, on
average, more than 93% of all key-points. In comparison, random search-based
test data generation can only severely mispredict 41% of them. Many of these
mispredictions, however, are not avoidable and should not therefore be
considered failures. We also empirically compare state-of-the-art,
many-objective search algorithms and their variants, tailored for test suite
generation. Furthermore, we investigate and demonstrate how to learn specific
conditions, based on image characteristics (e.g., head posture and skin color),
that lead to severe mispredictions. Such conditions serve as a basis for risk
analysis or DNN retraining.","cs.CV,cs.SE"
"Protein-Ligand Scoring with Convolutional Neural Networks. Computational approaches to drug discovery can reduce the time and cost
associated with experimental assays and enable the screening of novel
chemotypes. Structure-based drug design methods rely on scoring functions to
rank and predict binding affinities and poses. The ever-expanding amount of
protein-ligand binding and structural data enables the use of deep machine
learning techniques for protein-ligand scoring.
  We describe convolutional neural network (CNN) scoring functions that take as
input a comprehensive 3D representation of a protein-ligand interaction. A CNN
scoring function automatically learns the key features of protein-ligand
interactions that correlate with binding. We train and optimize our CNN scoring
functions to discriminate between correct and incorrect binding poses and known
binders and non-binders. We find that our CNN scoring function outperforms the
AutoDock Vina scoring function when ranking poses both for pose prediction and
virtual screening.","cs.LG,q-bio.BM,stat.ML"
"Dynamic Character Graph via Online Face Clustering for Movie Analysis. An effective approach to automated movie content analysis involves building a
network (graph) of its characters. Existing work usually builds a static
character graph to summarize the content using metadata, scripts or manual
annotations. We propose an unsupervised approach to building a dynamic
character graph that captures the temporal evolution of character interaction.
We refer to this as the character interaction graph(CIG). Our approach has two
components:(i) an online face clustering algorithm that discovers the
characters in the video stream as they appear, and (ii) simultaneous creation
of a CIG using the temporal dynamics of the resulting clusters. We demonstrate
the usefulness of the CIG for two movie analysis tasks: narrative structure
(acts) segmentation, and major character retrieval. Our evaluation on
full-length movies containing more than 5000 face tracks shows that the
proposed approach achieves superior performance for both the tasks.","cs.CV,cs.MM"
"Uncertainty quantification for Markov Random Fields. We present an information-based uncertainty quantification method for general
Markov Random Fields. Markov Random Fields (MRF) are structured, probabilistic
graphical models over undirected graphs, and provide a fundamental unifying
modeling tool for statistical mechanics, probabilistic machine learning, and
artificial intelligence. Typically MRFs are complex and high-dimensional with
nodes and edges (connections) built in a modular fashion from simpler,
low-dimensional probabilistic models and their local connections; in turn, this
modularity allows to incorporate available data to MRFs and efficiently
simulate them by leveraging their graph-theoretic structure. Learning graphical
models from data and/or constructing them from physical modeling and
constraints necessarily involves uncertainties inherited from data, modeling
choices, or numerical approximations. These uncertainties in the MRF can be
manifested either in the graph structure or the probability distribution
functions, and necessarily will propagate in predictions for quantities of
interest. Here we quantify such uncertainties using tight, information based
bounds on the predictions of quantities of interest; these bounds take
advantage of the graphical structure of MRFs and are capable of handling the
inherent high-dimensionality of such graphical models. We demonstrate our
methods in MRFs for medical diagnostics and statistical mechanics models. In
the latter, we develop uncertainty quantification bounds for finite size
effects and phase diagrams, which constitute two of the typical predictions
goals of statistical mechanics modeling.","62H22,82B20,94A17,cs.IT,cs.LG,math.IT,math.PR,stat.ML"
"Causality based Feature Fusion for Brain Neuro-Developmental Analysis. Human brain development is a complex and dynamic process that is affected by
several factors such as genetics, sex hormones, and environmental changes. A
number of recent studies on brain development have examined functional
connectivity (FC) defined by the temporal correlation between time series of
different brain regions. We propose to add the directional flow of information
during brain maturation. To do so, we extract effective connectivity (EC)
through Granger causality (GC) for two different groups of subjects, i.e.,
children and young adults. The motivation is that the inclusion of causal
interaction may further discriminate brain connections between two age groups
and help to discover new connections between brain regions. The contributions
of this study are threefold. First, there has been a lack of attention to
EC-based feature extraction in the context of brain development. To this end,
we propose a new kernel-based GC (KGC) method to learn nonlinearity of complex
brain network, where a reduced Sine hyperbolic polynomial (RSP) neural network
was used as our proposed learner. Second, we used causality values as the
weight for the directional connectivity between brain regions. Our findings
indicated that the strength of connections was significantly higher in young
adults relative to children. In addition, our new EC-based feature outperformed
FC-based analysis from Philadelphia neurocohort (PNC) study with better
discrimination of the different age groups. Moreover, the fusion of these two
sets of features (FC + EC) improved brain age prediction accuracy by more than
4%, indicating that they should be used together for brain development studies.","cs.AI,cs.CV,q-bio.NC"
"Multi-Stage Influence Function. Multi-stage training and knowledge transfer, from a large-scale pretraining
task to various finetuning tasks, have revolutionized natural language
processing and computer vision resulting in state-of-the-art performance
improvements. In this paper, we develop a multi-stage influence function score
to track predictions from a finetuned model all the way back to the pretraining
data. With this score, we can identify the pretraining examples in the
pretraining task that contribute most to a prediction in the finetuning task.
The proposed multi-stage influence function generalizes the original influence
function for a single model in (Koh & Liang, 2017), thereby enabling influence
computation through both pretrained and finetuned models. We study two
different scenarios with the pretrained embeddings fixed or updated in the
finetuning tasks. We test our proposed method in various experiments to show
its effectiveness and potential applications.","cs.CV,cs.LG,stat.ML"
"Probabilistic Graphical Models and Tensor Networks: A Hybrid Framework. We investigate a correspondence between two formalisms for discrete
probabilistic modeling: probabilistic graphical models (PGMs) and tensor
networks (TNs), a powerful modeling framework for simulating complex quantum
systems. The graphical calculus of PGMs and TNs exhibits many similarities,
with discrete undirected graphical models (UGMs) being a special case of TNs.
However, more general probabilistic TN models such as Born machines (BMs)
employ complex-valued hidden states to produce novel forms of correlation among
the probabilities. While representing a new modeling resource for capturing
structure in discrete probability distributions, this behavior also renders the
direct application of standard PGM tools impossible. We aim to bridge this gap
by introducing a hybrid PGM-TN formalism that integrates quantum-like
correlations into PGM models in a principled manner, using the
physically-motivated concept of decoherence. We first prove that applying
decoherence to the entirety of a BM model converts it into a discrete UGM, and
conversely, that any subgraph of a discrete UGM can be represented as a
decohered BM. This method allows a broad family of probabilistic TN models to
be encoded as partially decohered BMs, a fact we leverage to combine the
representational strengths of both model families. We experimentally verify the
performance of such hybrid models in a sequential modeling task, and identify
promising uses of our method within the context of existing applications of
graphical models.","cs.LG,quant-ph,stat.ML"
"Compositional generalization in a deep seq2seq model by separating syntax and semantics. Standard methods in deep learning for natural language processing fail to
capture the compositional structure of human language that allows for
systematic generalization outside of the training distribution. However, human
learners readily generalize in this way, e.g. by applying known grammatical
rules to novel words. Inspired by work in neuroscience suggesting separate
brain systems for syntactic and semantic processing, we implement a
modification to standard approaches in neural machine translation, imposing an
analogous separation. The novel model, which we call Syntactic Attention,
substantially outperforms standard methods in deep learning on the SCAN
dataset, a compositional generalization task, without any hand-engineered
features or additional supervision. Our work suggests that separating syntactic
from semantic learning may be a useful heuristic for capturing compositional
structure.","cs.CL,cs.LG,stat.ML"
"A Deep Bag-of-Features Model for Music Auto-Tagging. Feature learning and deep learning have drawn great attention in recent years
as a way of transforming input data into more effective representations using
learning algorithms. Such interest has grown in the area of music information
retrieval (MIR) as well, particularly in music audio classification tasks such
as auto-tagging. In this paper, we present a two-stage learning model to
effectively predict multiple labels from music audio. The first stage learns to
project local spectral patterns of an audio track onto a high-dimensional
sparse space in an unsupervised manner and summarizes the audio track as a
bag-of-features. The second stage successively performs the unsupervised
learning on the bag-of-features in a layer-by-layer manner to initialize a deep
neural network and finally fine-tunes it with the tag labels. Through the
experiment, we rigorously examine training choices and tuning parameters, and
show that the model achieves high performance on Magnatagatune, a popularly
used dataset in music auto-tagging.","cs.LG,cs.SD,stat.ML"
"Get Rid of Suspended Animation Problem: Deep Diffusive Neural Network on Graph Semi-Supervised Classification. Existing graph neural networks may suffer from the ""suspended animation
problem"" when the model architecture goes deep. Meanwhile, for some graph
learning scenarios, e.g., nodes with text/image attributes or graphs with
long-distance node correlations, deep graph neural networks will be necessary
for effective graph representation learning. In this paper, we propose a new
graph neural network, namely DIFNET (Graph Diffusive Neural Network), for graph
representation learning and node classification. DIFNET utilizes both neural
gates and graph residual learning for node hidden state modeling, and includes
an attention mechanism for node neighborhood information diffusion. Extensive
experiments will be done in this paper to compare DIFNET against several
state-of-the-art graph neural network models. The experimental results can
illustrate both the learning performance advantages and effectiveness of
DIFNET, especially in addressing the ""suspended animation problem"".","cs.LG,cs.NE,stat.ML"
"Hybrid modeling of the human cardiovascular system using NeuralFMUs. Hybrid modeling, the combination of first principle and machine learning
models, is an emerging research field that gathers more and more attention.
Even if hybrid models produce formidable results for academic examples, there
are still different technical challenges that hinder the use of hybrid modeling
in real-world applications. By presenting NeuralFMUs, the fusion of a FMU, a
numerical ODE solver and an ANN, we are paving the way for the use of a variety
of first principle models from different modeling tools as parts of hybrid
models. This contribution handles the hybrid modeling of a complex, real-world
example: Starting with a simplified 1D-fluid model of the human cardiovascular
system (arterial side), the aim is to learn neglected physical effects like
arterial elasticity from data. We will show that the hybrid modeling process is
more comfortable, needs less system knowledge and is therefore less error-prone
compared to modeling solely based on first principle. Further, the resulting
hybrid model has improved in computation performance, compared to a pure first
principle white-box model, while still fulfilling the requirements regarding
accuracy of the considered hemodynamic quantities. The use of the presented
techniques is explained in a general manner and the considered use-case can
serve as example for other modeling and simulation applications in and beyond
the medical domain.","cs.CE,cs.LG"
"Uncertainty quantification for Markov Random Fields. We present an information-based uncertainty quantification method for general
Markov Random Fields. Markov Random Fields (MRF) are structured, probabilistic
graphical models over undirected graphs, and provide a fundamental unifying
modeling tool for statistical mechanics, probabilistic machine learning, and
artificial intelligence. Typically MRFs are complex and high-dimensional with
nodes and edges (connections) built in a modular fashion from simpler,
low-dimensional probabilistic models and their local connections; in turn, this
modularity allows to incorporate available data to MRFs and efficiently
simulate them by leveraging their graph-theoretic structure. Learning graphical
models from data and/or constructing them from physical modeling and
constraints necessarily involves uncertainties inherited from data, modeling
choices, or numerical approximations. These uncertainties in the MRF can be
manifested either in the graph structure or the probability distribution
functions, and necessarily will propagate in predictions for quantities of
interest. Here we quantify such uncertainties using tight, information based
bounds on the predictions of quantities of interest; these bounds take
advantage of the graphical structure of MRFs and are capable of handling the
inherent high-dimensionality of such graphical models. We demonstrate our
methods in MRFs for medical diagnostics and statistical mechanics models. In
the latter, we develop uncertainty quantification bounds for finite size
effects and phase diagrams, which constitute two of the typical predictions
goals of statistical mechanics modeling.","62H22,82B20,94A17,cs.IT,cs.LG,math.IT,math.PR,stat.ML"
"Latent Time-Adaptive Drift-Diffusion Model. Animals can quickly learn the timing of events with fixed intervals and their
rate of acquisition does not depend on the length of the interval. In contrast,
recurrent neural networks that use gradient based learning have difficulty
predicting the timing of events that depend on stimulus that occurred long ago.
We present the latent time-adaptive drift-diffusion model (LTDDM), an extension
to the time-adaptive drift-diffusion model (TDDM), a model for animal learning
of timing that exhibits behavioural properties consistent with experimental
data from animals. The performance of LTDDM is compared to that of a state of
the art long short-term memory (LSTM) recurrent neural network across three
timing tasks. Differences in the relative performance of these two models is
discussed and it is shown how LTDDM can learn these events time series orders
of magnitude faster than recurrent neural networks.","I.2.6,cs.LG"
"A first step towards automated species recognition from camera trap images of mammals using AI in a European temperate forest. Camera traps are used worldwide to monitor wildlife. Despite the increasing
availability of Deep Learning (DL) models, the effective usage of this
technology to support wildlife monitoring is limited. This is mainly due to the
complexity of DL technology and high computing requirements. This paper
presents the implementation of the light-weight and state-of-the-art YOLOv5
architecture for automated labeling of camera trap images of mammals in the
Bialowieza Forest (BF), Poland. The camera trapping data were organized and
harmonized using TRAPPER software, an open source application for managing
large-scale wildlife monitoring projects. The proposed image recognition
pipeline achieved an average accuracy of 85% F1-score in the identification of
the 12 most commonly occurring medium-size and large mammal species in BF using
a limited set of training and testing data (a total 2659 images with animals).
  Based on the preliminary results, we concluded that the YOLOv5 object
detection and classification model is a promising light-weight DL solution
after the adoption of transfer learning technique. It can be efficiently
plugged in via an API into existing web-based camera trapping data processing
platforms such as e.g. TRAPPER system. Since TRAPPER is already used to manage
and classify (manually) camera trapping datasets by many research groups in
Europe, the implementation of AI-based automated species classification may
significantly speed up the data processing workflow and thus better support
data-driven wildlife monitoring and conservation. Moreover, YOLOv5 developers
perform better performance on edge devices which may open a new chapter in
animal population monitoring in real time directly from camera trap devices.","68T07,I.2; I.4,cs.CV,cs.LG,eess.IV"
"On Approximate Nonlinear Gaussian Message Passing On Factor Graphs. Factor graphs have recently gained increasing attention as a unified
framework for representing and constructing algorithms for signal processing,
estimation, and control. One capability that does not seem to be well explored
within the factor graph tool kit is the ability to handle deterministic
nonlinear transformations, such as those occurring in nonlinear filtering and
smoothing problems, using tabulated message passing rules. In this
contribution, we provide general forward (filtering) and backward (smoothing)
approximate Gaussian message passing rules for deterministic nonlinear
transformation nodes in arbitrary factor graphs fulfilling a Markov property,
based on numerical quadrature procedures for the forward pass and a
Rauch-Tung-Striebel-type approximation of the backward pass. These message
passing rules can be employed for deriving many algorithms for solving
nonlinear problems using factor graphs, as is illustrated by the proposition of
a nonlinear modified Bryson-Frazier (MBF) smoother based on the presented
message passing rules.","cs.LG,cs.SY,eess.SP,stat.ML"
"Machine Learning and Data Science approach towards trend and predictors analysis of CDC Mortality Data for the USA. The research on mortality is an active area of research for any country where
the conclusions are driven from the provided data and conditions. The domain
knowledge is an essential but not a mandatory skill (though some knowledge is
still required) in order to derive conclusions based on data intuition using
machine learning and data science practices. The purpose of conducting this
project was to derive conclusions based on the statistics from the provided
dataset and predict label(s) of the dataset using supervised or unsupervised
learning algorithms. The study concluded (based on a sample) life expectancy
regardless of gender, and their central tendencies; Marital status of the
people also affected how frequent deaths were for each of them. The study also
helped in finding out that due to more categorical and numerical data, anomaly
detection or under-sampling could be a viable solution since there are
possibilities of more class labels than the other(s). The study shows that
machine learning predictions aren't as viable for the data as it might be
apparent.","cs.CY,cs.LG"
"Supervised Learning with Projected Entangled Pair States. Tensor networks, a model that originated from quantum physics, has been
gradually generalized as efficient models in machine learning in recent years.
However, in order to achieve exact contraction, only tree-like tensor networks
such as the matrix product states and tree tensor networks have been
considered, even for modeling two-dimensional data such as images. In this
work, we construct supervised learning models for images using the projected
entangled pair states (PEPS), a two-dimensional tensor network having a similar
structure prior to natural images. Our approach first performs a feature map,
which transforms the image data to a product state on a grid, then contracts
the product state to a PEPS with trainable parameters to predict image labels.
The tensor elements of PEPS are trained by minimizing differences between
training labels and predicted labels. The proposed model is evaluated on image
classifications using the MNIST and the Fashion-MNIST datasets. We show that
our model is significantly superior to existing models using tree-like tensor
networks. Moreover, using the same input features, our method performs as well
as the multilayer perceptron classifier, but with much fewer parameters and is
more stable. Our results shed light on potential applications of
two-dimensional tensor network models in machine learning.","cond-mat.str-el,cs.CV,cs.LG,quant-ph,stat.ML"
"Graph Attention Networks with LSTM-based Path Reweighting. Graph Neural Networks (GNNs) have been extensively used for mining
graph-structured data with impressive performance. However, traditional GNNs
suffer from over-smoothing, non-robustness and over-fitting problems. To solve
these weaknesses, we design a novel GNN solution, namely Graph Attention
Network with LSTM-based Path Reweighting (PR-GAT). PR-GAT can automatically
aggregate multi-hop information, highlight important paths and filter out
noises. In addition, we utilize random path sampling in PR-GAT for data
augmentation. The augmented data is used for predicting the distribution of
corresponding labels. Finally, we demonstrate that PR-GAT can mitigate the
issues of over-smoothing, non-robustness and overfitting. We achieve
state-of-the-art accuracy on 5 out of 7 datasets and competitive accuracy for
other 2 datasets. The average accuracy of 7 datasets have been improved by
0.5\% than the best SOTA from literature.","68R99,68T07,68T30,I.2.0; I.2.4,cs.LG"
"Large Motion Video Super-Resolution with Dual Subnet and Multi-Stage Communicated Upsampling. Video super-resolution (VSR) aims at restoring a video in low-resolution (LR)
and improving it to higher-resolution (HR). Due to the characteristics of video
tasks, it is very important that motion information among frames should be well
concerned, summarized and utilized for guidance in a VSR algorithm. Especially,
when a video contains large motion, conventional methods easily bring
incoherent results or artifacts. In this paper, we propose a novel deep neural
network with Dual Subnet and Multi-stage Communicated Upsampling (DSMC) for
super-resolution of videos with large motion. We design a new module named
U-shaped residual dense network with 3D convolution (U3D-RDN) for fine implicit
motion estimation and motion compensation (MEMC) as well as coarse spatial
feature extraction. And we present a new Multi-Stage Communicated Upsampling
(MSCU) module to make full use of the intermediate results of upsampling for
guiding the VSR. Moreover, a novel dual subnet is devised to aid the training
of our DSMC, whose dual loss helps to reduce the solution space as well as
enhance the generalization ability. Our experimental results confirm that our
method achieves superior performance on videos with large motion compared to
state-of-the-art methods.","cs.AI,cs.CV"
"Audeo: Audio Generation for a Silent Performance Video. We present a novel system that gets as an input video frames of a musician
playing the piano and generates the music for that video. Generation of music
from visual cues is a challenging problem and it is not clear whether it is an
attainable goal at all. Our main aim in this work is to explore the
plausibility of such a transformation and to identify cues and components able
to carry the association of sounds with visual events. To achieve the
transformation we built a full pipeline named `\textit{Audeo}' containing three
components. We first translate the video frames of the keyboard and the
musician hand movements into raw mechanical musical symbolic representation
Piano-Roll (Roll) for each video frame which represents the keys pressed at
each time step. We then adapt the Roll to be amenable for audio synthesis by
including temporal correlations. This step turns out to be critical for
meaningful audio generation. As a last step, we implement Midi synthesizers to
generate realistic music. \textit{Audeo} converts video to audio smoothly and
clearly with only a few setup constraints. We evaluate \textit{Audeo} on `in
the wild' piano performance videos and obtain that their generated music is of
reasonable audio quality and can be successfully recognized with high precision
by popular music identification software.","cs.CV,cs.LG,cs.MM,cs.SD,eess.AS,eess.IV"
"Fusion of Embeddings Networks for Robust Combination of Text Dependent and Independent Speaker Recognition. By implicitly recognizing a user based on his/her speech input, speaker
identification enables many downstream applications, such as personalized
system behavior and expedited shopping checkouts. Based on whether the speech
content is constrained or not, both text-dependent (TD) and text-independent
(TI) speaker recognition models may be used. We wish to combine the advantages
of both types of models through an ensemble system to make more reliable
predictions. However, any such combined approach has to be robust to incomplete
inputs, i.e., when either TD or TI input is missing. As a solution we propose a
fusion of embeddings network foenet architecture, combining joint learning with
neural attention. We compare foenet with four competitive baseline methods on a
dataset of voice assistant inputs, and show that it achieves higher accuracy
than the baseline and score fusion methods, especially in the presence of
incomplete inputs.","cs.CL,cs.LG,cs.SD,eess.AS"
"Challenges for cognitive decoding using deep learning methods. In cognitive decoding, researchers aim to characterize a brain region's
representations by identifying the cognitive states (e.g., accepting/rejecting
a gamble) that can be identified from the region's activity. Deep learning (DL)
methods are highly promising for cognitive decoding, with their unmatched
ability to learn versatile representations of complex data. Yet, their
widespread application in cognitive decoding is hindered by their general lack
of interpretability as well as difficulties in applying them to small datasets
and in ensuring their reproducibility and robustness. We propose to approach
these challenges by leveraging recent advances in explainable artificial
intelligence and transfer learning, while also providing specific
recommendations on how to improve the reproducibility and robustness of DL
modeling results.","cs.LG,stat.ME"
"Detecting and Understanding Real-World Differential Performance Bugs in Machine Learning Libraries. Programming errors that degrade the performance of systems are widespread,
yet there is little tool support for analyzing these bugs. We present a method
based on differential performance analysis---we find inputs for which the
performance varies widely, despite having the same size. To ensure that the
differences in the performance are robust (i.e. hold also for large inputs), we
compare the performance of not only single inputs, but of classes of inputs,
where each class has similar inputs parameterized by their size. Thus, each
class is represented by a performance function from the input size to
performance. Importantly, we also provide an explanation for why the
performance differs in a form that can be readily used to fix a performance
bug.
  The two main phases in our method are discovery with fuzzing and explanation
with decision tree classifiers, each of which is supported by clustering.
First, we propose an evolutionary fuzzing algorithm to generate inputs. For
this fuzzing task, the unique challenge is that we not only need the input
class with the worst performance, but rather a set of classes exhibiting
differential performance. We use clustering to merge similar input classes
which significantly improves the efficiency of our fuzzer. Second, we explain
the differential performance in terms of program inputs and internals. We adapt
discriminant learning approaches with clustering and decision trees to localize
suspicious code regions.
  We applied our techniques to a set of applications. On a set of
micro-benchmarks, we show that our approach outperforms state-of-the-art
fuzzers in finding inputs to characterize the differential performance. On a
set of case-studies, we discover and explain multiple performance bugs in
popular machine learning frameworks. Four of these bugs, reported first in this
paper, have since been fixed by the developers.","D.2.5,cs.LG,cs.PF,cs.SE"
"Introduction to Multi-Armed Bandits. Multi-armed bandits a simple but very powerful framework for algorithms that
make decisions over time under uncertainty. An enormous body of work has
accumulated over the years, covered in several books and surveys. This book
provides a more introductory, textbook-like treatment of the subject. Each
chapter tackles a particular line of work, providing a self-contained,
teachable technical introduction and a brief review of the further
developments; many of the chapters conclude with exercises.
  The book is structured as follows. The first four chapters are on IID
rewards, from the basic model to impossibility results to Bayesian priors to
Lipschitz rewards. The next three chapters cover adversarial rewards, from the
full-feedback version to adversarial bandits to extensions with linear rewards
and combinatorially structured actions. Chapter 8 is on contextual bandits, a
middle ground between IID and adversarial bandits in which the change in reward
distributions is completely explained by observable contexts. The last three
chapters cover connections to economics, from learning in repeated games to
bandits with supply/budget constraints to exploration in the presence of
incentives. The appendix provides sufficient background on concentration and
KL-divergence.
  The chapters on ""bandits with similarity information"", ""bandits with
knapsacks"" and ""bandits and agents"" can also be consumed as standalone surveys
on the respective topics.","cs.AI,cs.DS,cs.LG,stat.ML"
"GANs May Have No Nash Equilibria. Generative adversarial networks (GANs) represent a zero-sum game between two
machine players, a generator and a discriminator, designed to learn the
distribution of data. While GANs have achieved state-of-the-art performance in
several benchmark learning tasks, GAN minimax optimization still poses great
theoretical and empirical challenges. GANs trained using first-order
optimization methods commonly fail to converge to a stable solution where the
players cannot improve their objective, i.e., the Nash equilibrium of the
underlying game. Such issues raise the question of the existence of Nash
equilibrium solutions in the GAN zero-sum game. In this work, we show through
several theoretical and numerical results that indeed GAN zero-sum games may
not have any local Nash equilibria. To characterize an equilibrium notion
applicable to GANs, we consider the equilibrium of a new zero-sum game with an
objective function given by a proximal operator applied to the original
objective, a solution we call the proximal equilibrium. Unlike the Nash
equilibrium, the proximal equilibrium captures the sequential nature of GANs,
in which the generator moves first followed by the discriminator. We prove that
the optimal generative model in Wasserstein GAN problems provides a proximal
equilibrium. Inspired by these results, we propose a new approach, which we
call proximal training, for solving GAN problems. We discuss several numerical
experiments demonstrating the existence of proximal equilibrium solutions in
GAN minimax problems.","cs.GT,cs.LG,stat.ML"
"PointIso: Point Cloud Based Deep Learning Model for Detecting Arbitrary-Precision Peptide Features in LC-MS Map through Attention Based Segmentation. A promising technique of discovering disease biomarkers is to measure the
relative protein abundance in multiple biofluid samples through liquid
chromatography with tandem mass spectrometry (LC-MS/MS) based quantitative
proteomics. The key step involves peptide feature detection in LC-MS map, along
with its charge and intensity. Existing heuristic algorithms suffer from
inaccurate parameters since different settings of the parameters result in
significantly different outcomes. Therefore, we propose PointIso, to serve the
necessity of an automated system for peptide feature detection that is able to
find out the proper parameters itself, and is easily adaptable to different
types of datasets. It consists of an attention based scanning step for
segmenting the multi-isotopic pattern of peptide features along with charge and
a sequence classification step for grouping those isotopes into potential
peptide features. PointIso is the first point cloud based, arbitrary-precision
deep learning network to address the problem and achieves 98% detection of high
quality MS/MS identifications in a benchmark dataset, which is higher than
several other widely used algorithms. Besides contributing to the proteomics
study, we believe our novel segmentation technique should serve the general
image processing domain as well.","cs.CV,cs.LG,q-bio.QM"
"Delving Into Deep Walkers: A Convergence Analysis of Random-Walk-Based Vertex Embeddings. Graph vertex embeddings based on random walks have become increasingly
influential in recent years, showing good performance in several tasks as they
efficiently transform a graph into a more computationally digestible format
while preserving relevant information. However, the theoretical properties of
such algorithms, in particular the influence of hyperparameters and of the
graph structure on their convergence behaviour, have so far not been
well-understood. In this work, we provide a theoretical analysis for
random-walks based embeddings techniques. Firstly, we prove that, under some
weak assumptions, vertex embeddings derived from random walks do indeed
converge both in the single limit of the number of random walks $N \to \infty$
and in the double limit of both $N$ and the length of each random walk
$L\to\infty$. Secondly, we derive concentration bounds quantifying the converge
rate of the corpora for the single and double limits. Thirdly, we use these
results to derive a heuristic for choosing the hyperparameters $N$ and $L$. We
validate and illustrate the practical importance of our findings with a range
of numerical and visual experiments on several graphs drawn from real-world
applications.","cs.LG,math.PR,stat.ML"
"Physics-aware, probabilistic model order reduction with guaranteed stability. Given (small amounts of) time-series' data from a high-dimensional,
fine-grained, multiscale dynamical system, we propose a generative framework
for learning an effective, lower-dimensional, coarse-grained dynamical model
that is predictive of the fine-grained system's long-term evolution but also of
its behavior under different initial conditions. We target fine-grained models
as they arise in physical applications (e.g. molecular dynamics, agent-based
models), the dynamics of which are strongly non-stationary but their transition
to equilibrium is governed by unknown slow processes which are largely
inaccessible by brute-force simulations. Approaches based on domain knowledge
heavily rely on physical insight in identifying temporally slow features and
fail to enforce the long-term stability of the learned dynamics. On the other
hand, purely statistical frameworks lack interpretability and rely on large
amounts of expensive simulation data (long and multiple trajectories) as they
cannot infuse domain knowledge. The generative framework proposed achieves the
aforementioned desiderata by employing a flexible prior on the complex plane
for the latent, slow processes, and an intermediate layer of physics-motivated
latent variables that reduces reliance on data and imbues inductive bias. In
contrast to existing schemes, it does not require the a priori definition of
projection operators from the fine-grained description and addresses
simultaneously the tasks of dimensionality reduction and model estimation. We
demonstrate its efficacy and accuracy in multiscale physical systems of
particle dynamics where probabilistic, long-term predictions of phenomena not
contained in the training data are produced.","cs.LG,physics.comp-ph,stat.ML"
"Graph coarsening: From scientific computing to machine learning. The general method of graph coarsening or graph reduction has been a
remarkably useful and ubiquitous tool in scientific computing and it is now
just starting to have a similar impact in machine learning. The goal of this
paper is to take a broad look into coarsening techniques that have been
successfully deployed in scientific computing and see how similar principles
are finding their way in more recent applications related to machine learning.
In scientific computing, coarsening plays a central role in algebraic multigrid
methods as well as the related class of multilevel incomplete LU
factorizations. In machine learning, graph coarsening goes under various names,
e.g., graph downsampling or graph reduction. Its goal in most cases is to
replace some original graph by one which has fewer nodes, but whose structure
and characteristics are similar to those of the original graph. As will be
seen, a common strategy in these methods is to rely on spectral properties to
define the coarse graph.","cs.DS,cs.LG,cs.NA,math.NA"
"TimeTraveler: Reinforcement Learning for Temporal Knowledge Graph Forecasting. Temporal knowledge graph (TKG) reasoning is a crucial task that has gained
increasing research interest in recent years. Most existing methods focus on
reasoning at past timestamps to complete the missing facts, and there are only
a few works of reasoning on known TKGs to forecast future facts. Compared with
the completion task, the forecasting task is more difficult that faces two main
challenges: (1) how to effectively model the time information to handle future
timestamps? (2) how to make inductive inference to handle previously unseen
entities that emerge over time? To address these challenges, we propose the
first reinforcement learning method for forecasting. Specifically, the agent
travels on historical knowledge graph snapshots to search for the answer. Our
method defines a relative time encoding function to capture the timespan
information, and we design a novel time-shaped reward based on Dirichlet
distribution to guide the model learning. Furthermore, we propose a novel
representation method for unseen entities to improve the inductive inference
ability of the model. We evaluate our method for this link prediction task at
future timestamps. Extensive experiments on four benchmark datasets demonstrate
substantial performance improvement meanwhile with higher explainability, less
calculation, and fewer parameters when compared with existing state-of-the-art
methods.","cs.AI,cs.CL,cs.LG"
"The Surprising Simplicity of the Early-Time Learning Dynamics of Neural Networks. Modern neural networks are often regarded as complex black-box functions
whose behavior is difficult to understand owing to their nonlinear dependence
on the data and the nonconvexity in their loss landscapes. In this work, we
show that these common perceptions can be completely false in the early phase
of learning. In particular, we formally prove that, for a class of well-behaved
input distributions, the early-time learning dynamics of a two-layer
fully-connected neural network can be mimicked by training a simple linear
model on the inputs. We additionally argue that this surprising simplicity can
persist in networks with more layers and with convolutional architecture, which
we verify empirically. Key to our analysis is to bound the spectral norm of the
difference between the Neural Tangent Kernel (NTK) at initialization and an
affine transform of the data kernel; however, unlike many previous results
utilizing the NTK, we do not require the network to have disproportionately
large width, and the network is allowed to escape the kernel regime later in
training.","cs.LG,cs.NE,stat.ML"
"Integer Programming-based Error-Correcting Output Code Design for Robust Classification. Error-Correcting Output Codes (ECOCs) offer a principled approach for
combining simple binary classifiers into multiclass classifiers. In this paper,
we investigate the problem of designing optimal ECOCs to achieve both nominal
and adversarial accuracy using Support Vector Machines (SVMs) and binary deep
learning models. In contrast to previous literature, we present an Integer
Programming (IP) formulation to design minimal codebooks with desirable error
correcting properties. Our work leverages the advances in IP solvers to
generate codebooks with optimality guarantees. To achieve tractability, we
exploit the underlying graph-theoretic structure of the constraint set in our
IP formulation. This enables us to use edge clique covers to substantially
reduce the constraint set. Our codebooks achieve a high nominal accuracy
relative to standard codebooks (e.g., one-vs-all, one-vs-one, and dense/sparse
codes). We also estimate the adversarial accuracy of our ECOC-based classifiers
in a white-box setting. Our IP-generated codebooks provide non-trivial
robustness to adversarial perturbations even without any adversarial training.","cs.CV,cs.IT,cs.LG,math.IT,stat.CO,stat.ML"
"Partially Conditioned Generative Adversarial Networks. Generative models are undoubtedly a hot topic in Artificial Intelligence,
among which the most common type is Generative Adversarial Networks (GANs).
These architectures let one synthesise artificial datasets by implicitly
modelling the underlying probability distribution of a real-world training
dataset. With the introduction of Conditional GANs and their variants, these
methods were extended to generating samples conditioned on ancillary
information available for each sample within the dataset. From a practical
standpoint, however, one might desire to generate data conditioned on partial
information. That is, only a subset of the ancillary conditioning variables
might be of interest when synthesising data. In this work, we argue that
standard Conditional GANs are not suitable for such a task and propose a new
Adversarial Network architecture and training strategy to deal with the ensuing
problems. Experiments illustrating the value of the proposed approach in digit
and face image synthesis under partial conditioning information are presented,
showing that the proposed method can effectively outperform the standard
approach under these circumstances.","68T07,cs.LG,stat.ML"
"Granular Computing: An Augmented Scheme of Degranulation Through a Modified Partition Matrix. As an important technology in artificial intelligence Granular Computing
(GrC) has emerged as a new multi-disciplinary paradigm and received much
attention in recent years. Information granules forming an abstract and
efficient characterization of large volumes of numeric data have been
considered as the fundamental constructs of GrC. By generating prototypes and
partition matrix, fuzzy clustering is a commonly encountered way of information
granulation. Degranulation involves data reconstruction completed on a basis of
the granular representatives. Previous studies have shown that there is a
relationship between the reconstruction error and the performance of the
granulation process. Typically, the lower the degranulation error is, the
better performance of granulation is. However, the existing methods of
degranulation usually cannot restore the original numeric data, which is one of
the important reasons behind the occurrence of the reconstruction error. To
enhance the quality of degranulation, in this study, we develop an augmented
scheme through modifying the partition matrix. By proposing the augmented
scheme, we dwell on a novel collection of granulation-degranulation mechanisms.
In the constructed approach, the prototypes can be expressed as the product of
the dataset matrix and the partition matrix. Then, in the degranulation
process, the reconstructed numeric data can be decomposed into the product of
the partition matrix and the matrix of prototypes. Both the granulation and
degranulation are regarded as generalized rotation between the data subspace
and the prototype subspace with the partition matrix and the fuzzification
factor. By modifying the partition matrix, the new partition matrix is
constructed through a series of matrix operations. We offer a thorough analysis
of the developed scheme. The experimental results are in agreement with the
underlying conceptual framework","cs.LG,cs.SY,eess.SP,eess.SY"
"Complex Query Answering with Neural Link Predictors. Neural link predictors are immensely useful for identifying missing edges in
large scale Knowledge Graphs. However, it is still not clear how to use these
models for answering more complex queries that arise in a number of domains,
such as queries using logical conjunctions ($\land$), disjunctions ($\lor$) and
existential quantifiers ($\exists$), while accounting for missing edges. In
this work, we propose a framework for efficiently answering complex queries on
incomplete Knowledge Graphs. We translate each query into an end-to-end
differentiable objective, where the truth value of each atom is computed by a
pre-trained neural link predictor. We then analyse two solutions to the
optimisation problem, including gradient-based and combinatorial search. In our
experiments, the proposed approach produces more accurate results than
state-of-the-art methods -- black-box neural models trained on millions of
generated queries -- without the need of training on a large and diverse set of
complex queries. Using orders of magnitude less training data, we obtain
relative improvements ranging from 8% up to 40% in Hits@3 across different
knowledge graphs containing factual information. Finally, we demonstrate that
it is possible to explain the outcome of our model in terms of the intermediate
solutions identified for each of the complex query atoms. All our source code
and datasets are available online, at https://github.com/uclnlp/cqd.","cs.AI,cs.LG,cs.LO,cs.NE"
"Extra Proximal-Gradient Inspired Non-local Network. Variational method and deep learning method are two mainstream powerful
approaches to solve inverse problems in computer vision. To take advantages of
advanced optimization algorithms and powerful representation ability of deep
neural networks, we propose a novel deep network for image reconstruction. The
architecture of this network is inspired by our proposed accelerated extra
proximal gradient algorithm. It is able to incorporate non-local operation to
exploit the non-local self-similarity of the images and to learn the nonlinear
transform, under which the solution is sparse. All the parameters in our
network are learned from minimizing a loss function. Our experimental results
show that our network outperforms several state-of-the-art deep networks with
almost the same number of learnable parameter.","cs.CV,eess.IV,math.OC"
"Towards Symbolic Reinforcement Learning with Common Sense. Deep Reinforcement Learning (deep RL) has made several breakthroughs in
recent years in applications ranging from complex control tasks in unmanned
vehicles to game playing. Despite their success, deep RL still lacks several
important capacities of human intelligence, such as transfer learning,
abstraction and interpretability. Deep Symbolic Reinforcement Learning (DSRL)
seeks to incorporate such capacities to deep Q-networks (DQN) by learning a
relevant symbolic representation prior to using Q-learning. In this paper, we
propose a novel extension of DSRL, which we call Symbolic Reinforcement
Learning with Common Sense (SRL+CS), offering a better balance between
generalization and specialization, inspired by principles of common sense when
assigning rewards and aggregating Q-values. Experiments reported in this paper
show that SRL+CS learns consistently faster than Q-learning and DSRL, achieving
also a higher accuracy. In the hardest case, where agents were trained in a
deterministic environment and tested in a random environment, SRL+CS achieves
nearly 100% average accuracy compared to DSRL's 70% and DQN's 50% accuracy. To
the best of our knowledge, this is the first case of near perfect zero-shot
transfer learning using Reinforcement Learning.","I.2.6,cs.AI,cs.LG,stat.ML"
"Towards Accountability for Machine Learning Datasets: Practices from Software Engineering and Infrastructure. Rising concern for the societal implications of artificial intelligence
systems has inspired demands for greater transparency and accountability.
However the datasets which empower machine learning are often used, shared and
re-used with little visibility into the processes of deliberation which led to
their creation. Which stakeholder groups had their perspectives included when
the dataset was conceived? Which domain experts were consulted regarding how to
model subgroups and other phenomena? How were questions of representational
biases measured and addressed? Who labeled the data? In this paper, we
introduce a rigorous framework for dataset development transparency which
supports decision-making and accountability. The framework uses the cyclical,
infrastructural and engineering nature of dataset development to draw on best
practices from the software development lifecycle. Each stage of the data
development lifecycle yields a set of documents that facilitate improved
communication and decision-making, as well as drawing attention the value and
necessity of careful data work. The proposed framework is intended to
contribute to closing the accountability gap in artificial intelligence
systems, by making visible the often overlooked work that goes into dataset
creation.","cs.CY,cs.DB,cs.LG,cs.SE"
"Topology-Preserving Deep Image Segmentation. Segmentation algorithms are prone to make topological errors on fine-scale
structures, e.g., broken connections. We propose a novel method that learns to
segment with correct topology. In particular, we design a continuous-valued
loss function that enforces a segmentation to have the same topology as the
ground truth, i.e., having the same Betti number. The proposed
topology-preserving loss function is differentiable and we incorporate it into
end-to-end training of a deep neural network. Our method achieves much better
performance on the Betti number error, which directly accounts for the
topological correctness. It also performs superiorly on other topology-relevant
metrics, e.g., the Adjusted Rand Index and the Variation of Information. We
illustrate the effectiveness of the proposed method on a broad spectrum of
natural and biomedical datasets.","cs.CG,cs.CV"
"Gesture-to-Gesture Translation in the Wild via Category-Independent Conditional Maps. Recent works have shown Generative Adversarial Networks (GANs) to be
particularly effective in image-to-image translations. However, in tasks such
as body pose and hand gesture translation, existing methods usually require
precise annotations, e.g. key-points or skeletons, which are time-consuming to
draw. In this work, we propose a novel GAN architecture that decouples the
required annotations into a category label - that specifies the gesture type -
and a simple-to-draw category-independent conditional map - that expresses the
location, rotation and size of the hand gesture. Our architecture synthesizes
the target gesture while preserving the background context, thus effectively
dealing with gesture translation in the wild. To this aim, we use an attention
module and a rolling guidance approach, which loops the generated images back
into the network and produces higher quality images compared to competing
works. Thus, our GAN learns to generate new images from simple annotations
without requiring key-points or skeleton labels. Results on two public datasets
show that our method outperforms state of the art approaches both
quantitatively and qualitatively. To the best of our knowledge, no work so far
has addressed the gesture-to-gesture translation in the wild by requiring
user-friendly annotations.","cs.CV,cs.MM"
"ScheduleNet: Learn to solve multi-agent scheduling problems with reinforcement learning. We propose ScheduleNet, a RL-based real-time scheduler, that can solve
various types of multi-agent scheduling problems. We formulate these problems
as a semi-MDP with episodic reward (makespan) and learn ScheduleNet, a
decentralized decision-making policy that can effectively coordinate multiple
agents to complete tasks. The decision making procedure of ScheduleNet
includes: (1) representing the state of a scheduling problem with the
agent-task graph, (2) extracting node embeddings for agent and tasks nodes, the
important relational information among agents and tasks, by employing the
type-aware graph attention (TGA), and (3) computing the assignment probability
with the computed node embeddings. We validate the effectiveness of ScheduleNet
as a general learning-based scheduler for solving various types of multi-agent
scheduling tasks, including multiple salesman traveling problem (mTSP) and job
shop scheduling problem (JSP).","cs.AI,cs.LG,cs.MA,cs.SY,eess.SY"
"Multimodal Deep Learning for Mental Disorders Prediction from Audio Speech Samples. Key features of mental illnesses are reflected in speech. Our research
focuses on designing a multimodal deep learning structure that automatically
extracts salient features from recorded speech samples for predicting various
mental disorders including depression, bipolar, and schizophrenia. We adopt a
variety of pre-trained models to extract embeddings from both audio and text
segments. We use several state-of-the-art embedding techniques including BERT,
FastText, and Doc2VecC for the text representation learning and WaveNet and
VGG-ish models for audio encoding. We also leverage huge auxiliary
emotion-labeled text and audio corpora to train emotion-specific embeddings and
use transfer learning in order to address the problem of insufficient annotated
multimodal data available. All these embeddings are then combined into a joint
representation in a multimodal fusion layer and finally a recurrent neural
network is used to predict the mental disorder. Our results show that mental
disorders can be predicted with acceptable accuracy through multimodal analysis
of clinical interviews.","cs.LG,cs.SD,eess.AS,stat.ML"
"Reinforcement Learning for Traffic Signal Control: Comparison with Commercial Systems. Recently, Intelligent Transportation Systems are leveraging the power of
increased sensory coverage and computing power to deliver data-intensive
solutions achieving higher levels of performance than traditional systems.
Within Traffic Signal Control (TSC), this has allowed the emergence of Machine
Learning (ML) based systems. Among this group, Reinforcement Learning (RL)
approaches have performed particularly well. Given the lack of industry
standards in ML for TSC, literature exploring RL often lacks comparison against
commercially available systems and straightforward formulations of how the
agents operate. Here we attempt to bridge that gap. We propose three different
architectures for TSC RL agents and compare them against the currently used
commercial systems MOVA, SurTrac and Cyclic controllers and provide pseudo-code
for them. The agents use variations of Deep Q-Learning and Actor Critic, using
states and rewards based on queue lengths. Their performance is compared in
across different map scenarios with variable demand, assessing them in terms of
the global delay and average queue length. We find that the RL-based systems
can significantly and consistently achieve lower delays when compared with
existing commercial systems.","90-05,90-06,90-08,90-10,I.2.6; I.2.8; J.7,cs.AI,cs.LG,cs.SY,eess.SY"
"When Do Extended Physics-Informed Neural Networks (XPINNs) Improve Generalization?. Physics-informed neural networks (PINNs) have become a popular choice for
solving high-dimensional partial differential equations (PDEs) due to their
excellent approximation power and generalization ability. Recently, Extended
PINNs (XPINNs) based on domain decomposition methods have attracted
considerable attention due to their effectiveness in modeling multiscale and
multiphysics problems and their parallelization. However, theoretical
understanding on their convergence and generalization properties remains
unexplored. In this study, we take an initial step towards understanding how
and when XPINNs outperform PINNs. Specifically, for general multi-layer PINNs
and XPINNs, we first provide a prior generalization bound via the complexity of
the target functions in the PDE problem, and a posterior generalization bound
via the posterior matrix norms of the networks after optimization. Moreover,
based on our bounds, we analyze the conditions under which XPINNs improve
generalization. Concretely, our theory shows that the key building block of
XPINN, namely the domain decomposition, introduces a tradeoff for
generalization. On the one hand, XPINNs decompose the complex PDE solution into
several simple parts, which decreases the complexity needed to learn each part
and boosts generalization. On the other hand, decomposition leads to less
training data being available in each subdomain, and hence such model is
typically prone to overfitting and may become less generalizable. Empirically,
we choose five PDEs to show when XPINNs perform better than, similar to, or
worse than PINNs, hence demonstrating and justifying our new theory.","cs.LG,cs.NA,math.DS,math.NA,stat.ML"
"Inverse Reinforcement Learning for Strategy Identification. In adversarial environments, one side could gain an advantage by identifying
the opponent's strategy. For example, in combat games, if an opponents strategy
is identified as overly aggressive, one could lay a trap that exploits the
opponent's aggressive nature. However, an opponent's strategy is not always
apparent and may need to be estimated from observations of their actions. This
paper proposes to use inverse reinforcement learning (IRL) to identify
strategies in adversarial environments. Specifically, the contributions of this
work are 1) the demonstration of this concept on gaming combat data generated
from three pre-defined strategies and 2) the framework for using IRL to achieve
strategy identification. The numerical experiments demonstrate that the
recovered rewards can be identified using a variety of techniques. In this
paper, the recovered reward are visually displayed, clustered using
unsupervised learning, and classified using a supervised learner.","cs.AI,cs.GT,cs.LG"
"Steering a Historical Disease Forecasting Model Under a Pandemic: Case of Flu and COVID-19. Forecasting influenza in a timely manner aids health organizations and
policymakers in adequate preparation and decision making. However, effective
influenza forecasting still remains a challenge despite increasing research
interest. It is even more challenging amidst the COVID pandemic, when the
influenza-like illness (ILI) counts are affected by various factors such as
symptomatic similarities with COVID-19 and shift in healthcare seeking patterns
of the general population. Under the current pandemic, historical influenza
models carry valuable expertise about the disease dynamics but face
difficulties adapting. Therefore, we propose CALI-Net, a neural transfer
learning architecture which allows us to 'steer' a historical disease
forecasting model to new scenarios where flu and COVID co-exist. Our framework
enables this adaptation by automatically learning when it should emphasize
learning from COVID-related signals and when it should learn from the
historical model. Thus, we exploit representations learned from historical ILI
data as well as the limited COVID-related signals. Our experiments demonstrate
that our approach is successful in adapting a historical forecasting model to
the current pandemic. In addition, we show that success in our primary goal,
adaptation, does not sacrifice overall performance as compared with
state-of-the-art influenza forecasting approaches.","cs.LG,stat.AP"
"Butterfly-Net2: Simplified Butterfly-Net and Fourier Transform Initialization. Structured CNN designed using the prior information of problems potentially
improves efficiency over conventional CNNs in various tasks in solving PDEs and
inverse problems in signal processing. This paper introduces BNet2, a
simplified Butterfly-Net and inline with the conventional CNN. Moreover, a
Fourier transform initialization is proposed for both BNet2 and CNN with
guaranteed approximation power to represent the Fourier transform operator.
Experimentally, BNet2 and the Fourier transform initialization strategy are
tested on various tasks, including approximating Fourier transform operator,
end-to-end solvers of linear and nonlinear PDEs, and denoising and deblurring
of 1D signals. On all tasks, under the same initialization, BNet2 achieves
similar accuracy as CNN but has fewer parameters. And Fourier transform
initialized BNet2 and CNN consistently improve the training and testing
accuracy over the randomly initialized CNN.","cs.LG,cs.NA,math.NA,stat.ML"
"Neural Networks as Geometric Chaotic Maps. The use of artificial neural networks as models of chaotic dynamics has been
rapidly expanding. Still, a theoretical understanding of how neural networks
learn chaos is lacking. Here, we employ a geometric perspective to show that
neural networks can efficiently model chaotic dynamics by becoming structurally
chaotic themselves. We first confirm neural network's efficiency in emulating
chaos by showing that a parsimonious neural network trained only on few data
points can reconstruct strange attractors, extrapolate outside training data
boundaries, and accurately predict local divergence rates. We then posit that
the trained network's map comprises sequential geometric stretching, rotation,
and compression operations. These geometric operations indicate topological
mixing and chaos, explaining why neural networks are naturally suitable to
emulate chaotic dynamics.","I.2.6,cs.LG,math.DS,nlin.CD,stat.ML"
"Music Transformer. Music relies heavily on repetition to build structure and meaning.
Self-reference occurs on multiple timescales, from motifs to phrases to reusing
of entire sections of music, such as in pieces with ABA structure. The
Transformer (Vaswani et al., 2017), a sequence model based on self-attention,
has achieved compelling results in many generation tasks that require
maintaining long-range coherence. This suggests that self-attention might also
be well-suited to modeling music. In musical composition and performance,
however, relative timing is critically important. Existing approaches for
representing relative positional information in the Transformer modulate
attention based on pairwise distance (Shaw et al., 2018). This is impractical
for long sequences such as musical compositions since their memory complexity
for intermediate relative information is quadratic in the sequence length. We
propose an algorithm that reduces their intermediate memory requirement to
linear in the sequence length. This enables us to demonstrate that a
Transformer with our modified relative attention mechanism can generate
minute-long compositions (thousands of steps, four times the length modeled in
Oore et al., 2018) with compelling structure, generate continuations that
coherently elaborate on a given motif, and in a seq2seq setup generate
accompaniments conditioned on melodies. We evaluate the Transformer with our
relative attention mechanism on two datasets, JSB Chorales and
Piano-e-Competition, and obtain state-of-the-art results on the latter.","cs.LG,cs.SD,eess.AS,stat.ML"
"High throughput screening with machine learning. This study assesses the efficiency of several popular machine learning
approaches in the prediction of molecular binding affinity: CatBoost, Graph
Attention Neural Network, and Bidirectional Encoder Representations from
Transformers. The models were trained to predict binding affinities in terms of
inhibition constants $K_i$ for pairs of proteins and small organic molecules.
First two approaches use thoroughly selected physico-chemical features, while
the third one is based on textual molecular representations - it is one of the
first attempts to apply Transformer-based predictors for the binding affinity.
We also discuss the visualization of attention layers within the Transformer
approach in order to highlight the molecular sites responsible for
interactions. All approaches are free from atomic spatial coordinates thus
avoiding bias from known structures and being able to generalize for compounds
with unknown conformations. The achieved accuracy for all suggested approaches
prove their potential in high throughput screening.","cs.LG,physics.chem-ph"
"Asymptotics of representation learning in finite Bayesian neural networks. Recent works have suggested that finite Bayesian neural networks may
outperform their infinite cousins because finite networks can flexibly adapt
their internal representations. However, our theoretical understanding of how
the learned hidden layer representations of finite networks differ from the
fixed representations of infinite networks remains incomplete. Perturbative
finite-width corrections to the network prior and posterior have been studied,
but the asymptotics of learned features have not been fully characterized.
Here, we argue that the leading finite-width corrections to the average feature
kernels for any Bayesian network with linear readout and quadratic cost have a
largely universal form. We illustrate this explicitly for two classes of fully
connected networks: deep linear networks and networks with a single nonlinear
hidden layer. Our results begin to elucidate which features of data wide
Bayesian neural networks learn to represent.","cond-mat.dis-nn,cs.LG,stat.ML"
"Improving Prediction Accuracy in Building Performance Models Using Generative Adversarial Networks (GANs). Building performance discrepancies between building design and operation are
one of the causes that lead many new designs fail to achieve their goals and
objectives. One of main factors contributing to the discrepancy is occupant
behaviors. Occupants responding to a new design are influenced by several
factors. Existing building performance models (BPMs) ignore or partially
address those factors (called contextual factors) while developing BPMs. To
potentially reduce the discrepancies and improve the prediction accuracy of
BPMs, this paper proposes a computational framework for learning mixture models
by using Generative Adversarial Networks (GANs) that appropriately combining
existing BPMs with knowledge on occupant behaviors to contextual factors in new
designs. Immersive virtual environments (IVEs) experiments are used to acquire
data on such behaviors. Performance targets are used to guide appropriate
combination of existing BPMs with knowledge on occupant behaviors. The
resulting model obtained is called an augmented BPM. Two different experiments
related to occupant lighting behaviors are shown as case study. The results
reveal that augmented BPMs significantly outperformed existing BPMs with
respect to achieving specified performance targets. The case study confirms the
potential of the computational framework for improving prediction accuracy of
BPMs during design.","68T05,cs.LG,stat.ML"
"Informing Computer Vision with Optical Illusions. Illusions are fascinating and immediately catch people's attention and
interest, but they are also valuable in terms of giving us insights into human
cognition and perception. A good theory of human perception should be able to
explain the illusion, and a correct theory will actually give quantifiable
results. We investigate here the efficiency of a computational filtering model
utilised for modelling the lateral inhibition of retinal ganglion cells and
their responses to a range of Geometric Illusions using isotropic Differences
of Gaussian filters. This study explores the way in which illusions have been
explained and shows how a simple standard model of vision based on classical
receptive fields can predict the existence of these illusions as well as the
degree of effect. A fundamental contribution of this work is to link bottom-up
processes to higher level perception and cognition consistent with Marr's
theory of vision and edge map representation.","cs.CV,q-bio.NC"
"GridToPix: Training Embodied Agents with Minimal Supervision. While deep reinforcement learning (RL) promises freedom from hand-labeled
data, great successes, especially for Embodied AI, require significant work to
create supervision via carefully shaped rewards. Indeed, without shaped
rewards, i.e., with only terminal rewards, present-day Embodied AI results
degrade significantly across Embodied AI problems from single-agent
Habitat-based PointGoal Navigation (SPL drops from 55 to 0) and two-agent
AI2-THOR-based Furniture Moving (success drops from 58% to 1%) to three-agent
Google Football-based 3 vs. 1 with Keeper (game score drops from 0.6 to 0.1).
As training from shaped rewards doesn't scale to more realistic tasks, the
community needs to improve the success of training with terminal rewards. For
this we propose GridToPix: 1) train agents with terminal rewards in gridworlds
that generically mirror Embodied AI environments, i.e., they are independent of
the task; 2) distill the learned policy into agents that reside in complex
visual worlds. Despite learning from only terminal rewards with identical
models and RL algorithms, GridToPix significantly improves results across
tasks: from PointGoal Navigation (SPL improves from 0 to 64) and Furniture
Moving (success improves from 1% to 25%) to football gameplay (game score
improves from 0.1 to 0.6). GridToPix even helps to improve the results of
shaped reward training.","cs.AI,cs.CV,cs.LG,cs.MA"
"Restricted Boltzmann Machine and Deep Belief Network: Tutorial and Survey. This is a tutorial and survey paper on Boltzmann Machine (BM), Restricted
Boltzmann Machine (RBM), and Deep Belief Network (DBN). We start with the
required background on probabilistic graphical models, Markov random field,
Gibbs sampling, statistical physics, Ising model, and the Hopfield network.
Then, we introduce the structures of BM and RBM. The conditional distributions
of visible and hidden variables, Gibbs sampling in RBM for generating
variables, training BM and RBM by maximum likelihood estimation, and
contrastive divergence are explained. Then, we discuss different possible
discrete and continuous distributions for the variables. We introduce
conditional RBM and how it is trained. Finally, we explain deep belief network
as a stack of RBM models. This paper on Boltzmann machines can be useful in
various fields including data science, statistics, neural computation, and
statistical physics.","cs.LG,cs.NE,physics.data-an,stat.ML"
"Exathlon: A Benchmark for Explainable Anomaly Detection over Time Series. Access to high-quality data repositories and benchmarks have been
instrumental in advancing the state of the art in many experimental research
domains. While advanced analytics tasks over time series data have been gaining
lots of attention, lack of such community resources severely limits scientific
progress. In this paper, we present Exathlon, the first comprehensive public
benchmark for explainable anomaly detection over high-dimensional time series
data. Exathlon has been systematically constructed based on real data traces
from repeated executions of large-scale stream processing jobs on an Apache
Spark cluster. Some of these executions were intentionally disturbed by
introducing instances of six different types of anomalous events (e.g.,
misbehaving inputs, resource contention, process failures). For each of the
anomaly instances, ground truth labels for the root cause interval as well as
those for the extended effect interval are provided, supporting the development
and evaluation of a wide range of anomaly detection (AD) and explanation
discovery (ED) tasks. We demonstrate the practical utility of Exathlon's
dataset, evaluation methodology, and end-to-end data science pipeline design
through an experimental study with three state-of-the-art AD and ED techniques.","cs.DB,cs.LG"
"G5: A Universal GRAPH-BERT for Graph-to-Graph Transfer and Apocalypse Learning. The recent GRAPH-BERT model introduces a new approach to learning graph
representations merely based on the attention mechanism. GRAPH-BERT provides an
opportunity for transferring pre-trained models and learned graph
representations across different tasks within the same graph dataset. In this
paper, we will further investigate the graph-to-graph transfer of a universal
GRAPH-BERT for graph representation learning across different graph datasets,
and our proposed model is also referred to as the G5 for simplicity. Many
challenges exist in learning G5 to adapt the distinct input and output
configurations for each graph data source, as well as the information
distributions differences. G5 introduces a pluggable model architecture: (a)
each data source will be pre-processed with a unique input representation
learning component; (b) each output application task will also have a specific
functional component; and (c) all such diverse input and output components will
all be conjuncted with a universal GRAPH-BERT core component via an input size
unification layer and an output representation fusion layer, respectively.
  The G5 model removes the last obstacle for cross-graph representation
learning and transfer. For the graph sources with very sparse training data,
the G5 model pre-trained on other graphs can still be utilized for
representation learning with necessary fine-tuning. What's more, the
architecture of G5 also allows us to learn a supervised functional classifier
for data sources without any training data at all. Such a problem is also named
as the Apocalypse Learning task in this paper. Two different label reasoning
strategies, i.e., Cross-Source Classification Consistency Maximization (CCCM)
and Cross-Source Dynamic Routing (CDR), are introduced in this paper to address
the problem.","cs.LG,cs.NE,cs.SI,stat.ML"
"Pair-view Unsupervised Graph Representation Learning. Low-dimension graph embeddings have proved extremely useful in various
downstream tasks in large graphs, e.g., link-related content recommendation and
node classification tasks, etc. Most existing embedding approaches take nodes
as the basic unit for information aggregation, e.g., node perception fields in
GNN or con-textual nodes in random walks. The main drawback raised by such
node-view is its lack of support for expressing the compound relationships
between nodes, which results in the loss of a certain degree of graph
information during embedding. To this end, this paper pro-poses PairE(Pair
Embedding), a solution to use ""pair"", a higher level unit than a ""node"" as the
core for graph embeddings. Accordingly, a multi-self-supervised auto-encoder is
designed to fulfill two pretext tasks, to reconstruct the feature distribution
for respective pairs and their surrounding context. PairE has three major
advantages: 1) Informative, embedding beyond node-view are capable to preserve
richer information of the graph; 2) Simple, the solutions provided by PairE are
time-saving, storage-efficient, and require the fewer hyper-parameters; 3) High
adaptability, with the introduced translator operator to map pair embeddings to
the node embeddings, PairE can be effectively used in both the link-based and
the node-based graph analysis. Experiment results show that PairE consistently
outperforms the state of baselines in all four downstream tasks, especially
with significant edges in the link-prediction and multi-label node
classification tasks.","cs.LG,cs.SI"
"Visual Sentiment Prediction with Deep Convolutional Neural Networks. Images have become one of the most popular types of media through which users
convey their emotions within online social networks. Although vast amount of
research is devoted to sentiment analysis of textual data, there has been very
limited work that focuses on analyzing sentiment of image data. In this work,
we propose a novel visual sentiment prediction framework that performs image
understanding with Deep Convolutional Neural Networks (CNN). Specifically, the
proposed sentiment prediction framework performs transfer learning from a CNN
with millions of parameters, which is pre-trained on large-scale data for
object recognition. Experiments conducted on two real-world datasets from
Twitter and Tumblr demonstrate the effectiveness of the proposed visual
sentiment analysis framework.","cs.CV,cs.NE,stat.ML"
"An Automatic System to Monitor the Physical Distance and Face Mask Wearing of Construction Workers in COVID-19 Pandemic. The COVID-19 pandemic has caused many shutdowns in different industries
around the world. Sectors such as infrastructure construction and maintenance
projects have not been suspended due to their significant effect on people's
routine life. In such projects, workers work close together that makes a high
risk of infection. The World Health Organization recommends wearing a face mask
and practicing physical distancing to mitigate the virus's spread. This paper
developed a computer vision system to automatically detect the violation of
face mask wearing and physical distancing among construction workers to assure
their safety on infrastructure projects during the pandemic. For the face mask
detection, the paper collected and annotated 1,000 images, including different
types of face mask wearing, and added them to a pre-existing face mask dataset
to develop a dataset of 1,853 images. Then trained and tested multiple
Tensorflow state-of-the-art object detection models on the face mask dataset
and chose the Faster R-CNN Inception ResNet V2 network that yielded the
accuracy of 99.8%. For physical distance detection, the paper employed the
Faster R-CNN Inception V2 to detect people. A transformation matrix was used to
eliminate the camera angle's effect on the object distances on the image. The
Euclidian distance used the pixels of the transformed image to compute the
actual distance between people. A threshold of six feet was considered to
capture physical distance violation. The paper also used transfer learning for
training the model. The final model was applied on four videos of road
maintenance projects in Houston, TX, that effectively detected the face mask
and physical distance. We recommend that construction owners use the proposed
system to enhance construction workers' safety in the pandemic situation.","cs.CV,cs.CY"
"Long Range Arena: A Benchmark for Efficient Transformers. Transformers do not scale very well to long sequence lengths largely because
of quadratic self-attention complexity. In the recent months, a wide spectrum
of efficient, fast Transformers have been proposed to tackle this problem, more
often than not claiming superior or comparable model quality to vanilla
Transformer models. To this date, there is no well-established consensus on how
to evaluate this class of models. Moreover, inconsistent benchmarking on a wide
spectrum of tasks and datasets makes it difficult to assess relative model
quality amongst many models. This paper proposes a systematic and unified
benchmark, LRA, specifically focused on evaluating model quality under
long-context scenarios. Our benchmark is a suite of tasks consisting of
sequences ranging from $1K$ to $16K$ tokens, encompassing a wide range of data
types and modalities such as text, natural, synthetic images, and mathematical
expressions requiring similarity, structural, and visual-spatial reasoning. We
systematically evaluate ten well-established long-range Transformer models
(Reformers, Linformers, Linear Transformers, Sinkhorn Transformers, Performers,
Synthesizers, Sparse Transformers, and Longformers) on our newly proposed
benchmark suite. LRA paves the way towards better understanding this class of
efficient Transformer models, facilitates more research in this direction, and
presents new challenging tasks to tackle. Our benchmark code will be released
at https://github.com/google-research/long-range-arena.","cs.AI,cs.CL,cs.CV,cs.IR,cs.LG"
