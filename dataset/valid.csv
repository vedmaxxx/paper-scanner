ID,Computer Science,Physics,Mathematics,Statistics,Quantitative Biology,Quantitative Finance
18821,Observing the Atmospheres of Known Temperate Earth-sized Planets with JWST,"  Nine transiting Earth-sized planets have recently been discovered around
nearby late M dwarfs, including the TRAPPIST-1 planets and two planets
discovered by the MEarth survey, GJ 1132b and LHS 1140b. These planets are the
smallest known planets that may have atmospheres amenable to detection with
JWST. We present model thermal emission and transmission spectra for each
planet, varying composition and surface pressure of the atmosphere. We base
elemental compositions on those of Earth, Titan, and Venus and calculate the
molecular compositions assuming chemical equilibrium, which can strongly depend
on temperature. Both thermal emission and transmission spectra are sensitive to
the atmospheric composition; thermal emission spectra are sensitive to surface
pressure and temperature. We predict the observability of each planet's
atmosphere with JWST. GJ 1132b and TRAPPIST-1b are excellent targets for
emission spectroscopy with JWST/MIRI, requiring fewer than 10 eclipse
observations. Emission photometry for TRAPPIST-1c requires 5-15 eclipses; LHS
1140b and TRAPPIST-1d, TRAPPIST-1e, and TRAPPIST-1f, which could possibly have
surface liquid water, may be accessible with photometry. Seven of the nine
planets are strong candidates for transmission spectroscopy measurements with
JWST, though the number of transits required depends strongly on the planets'
actual masses. Using the measured masses, fewer than 20 transits are required
for a 5 sigma detection of spectral features for GJ 1132b and six of the
TRAPPIST-1 planets. Dedicated campaigns to measure the atmospheres of these
nine planets will allow us, for the first time, to probe formation and
evolution processes of terrestrial planetary atmospheres beyond our solar
system.
",0,1,0,0,0,0
18822,Front Propagation for Nonlocal KPP Reaction-Diffusion Equations in Periodic Media,"  We study front propagation phenomena for a large class of nonlocal KPP-type
reaction-diffusion equations in oscillatory environments, which model various
forms of population growth with periodic dependence. The nonlocal diffusion is
an anisotropic integro-differential operator of order $\alpha \in (0,2)$.
",0,0,1,0,0,0
18823,Distributed Functional Observers for LTI Systems,"  We study the problem of designing distributed functional observers for LTI
systems. Specifically, we consider a setting consisting of a state vector that
evolves over time according to a dynamical process. A set of nodes distributed
over a communication network wish to collaboratively estimate certain functions
of the state. We first show that classical existence conditions for the design
of centralized functional observers do not directly translate to the
distributed setting, due to the coupling that exists between the dynamics of
the functions of interest and the diverse measurements at the various nodes.
Accordingly, we design transformations that reveal such couplings and identify
portions of the corresponding dynamics that are locally detectable at each
sensor node. We provide sufficient conditions on the network, along with state
estimate update and exchange rules for each node, that guarantee asymptotic
reconstruction of the functions at each sensor node.
",0,0,1,0,0,0
18824,Wavelet eigenvalue regression for $n$-variate operator fractional Brownian motion,"  In this contribution, we extend the methodology proposed in Abry and Didier
(2017) to obtain the first joint estimator of the real parts of the Hurst
eigenvalues of $n$-variate OFBM. The procedure consists of a wavelet regression
on the log-eigenvalues of the sample wavelet spectrum. The estimator is shown
to be consistent for any time reversible OFBM and, under stronger assumptions,
also asymptotically normal starting from either continuous or discrete time
measurements. Simulation studies establish the finite sample effectiveness of
the methodology and illustrate its benefits compared to univariate-like
(entrywise) analysis. As an application, we revisit the well-known self-similar
character of Internet traffic by applying the proposed methodology to 4-variate
time series of modern, high quality Internet traffic data. The analysis reveals
the presence of a rich multivariate self-similarity structure.
",0,0,1,1,0,0
18825,Deep Neural Network for Analysis of DNA Methylation Data,"  Many researches demonstrated that the DNA methylation, which occurs in the
context of a CpG, has strong correlation with diseases, including cancer. There
is a strong interest in analyzing the DNA methylation data to find how to
distinguish different subtypes of the tumor. However, the conventional
statistical methods are not suitable for analyzing the highly dimensional DNA
methylation data with bounded support. In order to explicitly capture the
properties of the data, we design a deep neural network, which composes of
several stacked binary restricted Boltzmann machines, to learn the low
dimensional deep features of the DNA methylation data. Experiments show these
features perform best in breast cancer DNA methylation data cluster analysis,
comparing with some state-of-the-art methods.
",0,0,0,1,1,0
18826,One look at the rating of scientific publications and corresponding toy-model,"  A toy-model of publications and citations processes is proposed. The model
shows that the role of randomness in the processes is essential and cannot be
ignored. Some other aspects of scientific publications rating are discussed.
",1,0,0,1,0,0
18827,"Alignment, Orientation, and Coulomb Explosion of Difluoroiodobenzene Studied with the Pixel Imaging Mass Spectrometry (PImMS) Camera","  Laser-induced adiabatic alignment and mixed-field orientation of
2,6-difluoroiodobenzene (C6H3F2I) molecules are probed by Coulomb explosion
imaging following either near-infrared strong-field ionization or
extreme-ultraviolet multi-photon inner-shell ionization using free-electron
laser pulses. The resulting photoelectrons and fragment ions are captured by a
double-sided velocity map imaging spectrometer and projected onto two
position-sensitive detectors. The ion side of the spectrometer is equipped with
the Pixel Imaging Mass Spectrometry (PImMS) camera, a time-stamping pixelated
detector that can record the hit positions and arrival times of up to four ions
per pixel per acquisition cycle. Thus, the time-of-flight trace and ion
momentum distributions for all fragments can be recorded simultaneously. We
show that we can obtain a high degree of one- and three-dimensional alignment
and mixed- field orientation, and compare the Coulomb explosion process induced
at both wavelengths.
",0,1,0,0,0,0
18828,Unsupervised Machine Learning of Open Source Russian Twitter Data Reveals Global Scope and Operational Characteristics,"  We developed and used a collection of statistical methods (unsupervised
machine learning) to extract relevant information from a Twitter supplied data
set consisting of alleged Russian trolls who (allegedly) attempted to influence
the 2016 US Presidential election. These unsupervised statistical methods allow
fast identification of (i) emergent language communities within the troll
population, (ii) the transnational scope of the operation and (iii) operational
characteristics of trolls that can be used for future identification. Using
natural language processing, manifold learning and Fourier analysis, we
identify an operation that includes not only the 2016 US election, but also the
French National and both local and national German elections. We show the
resulting troll population is composed of users with common, but clearly
customized, behavioral characteristics.
",1,0,0,0,0,0
18829,"Distributed, scalable and gossip-free consensus optimization with application to data analysis","  Distributed algorithms for solving additive or consensus optimization
problems commonly rely on first-order or proximal splitting methods. These
algorithms generally come with restrictive assumptions and at best enjoy a
linear convergence rate. Hence, they can require many iterations or
communications among agents to converge. In many cases, however, we do not seek
a highly accurate solution for consensus problems. Based on this we propose a
controlled relaxation of the coupling in the problem which allows us to compute
an approximate solution, where the accuracy of the approximation can be
controlled by the level of relaxation. The relaxed problem can be efficiently
solved in a distributed way using a combination of primal-dual interior-point
methods (PDIPMs) and message-passing. This algorithm purely relies on
second-order methods and thus requires far fewer iterations and communications
to converge. This is illustrated in numerical experiments, showing its superior
performance compared to existing methods.
",0,0,1,0,0,0
18830,Consistency of Maximum Likelihood for Continuous-Space Network Models,"  Network analysis needs tools to infer distributions over graphs of arbitrary
size from a single graph. Assuming the distribution is generated by a
continuous latent space model which obeys certain natural symmetry and
smoothness properties, we establish three levels of consistency for
non-parametric maximum likelihood inference as the number of nodes grows: (i)
the estimated locations of all nodes converge in probability on their true
locations; (ii) the distribution over locations in the latent space converges
on the true distribution; and (iii) the distribution over graphs of arbitrary
size converges.
",0,0,1,1,0,0
18831,Consistency Results for Stationary Autoregressive Processes with Constrained Coefficients,"  We consider stationary autoregressive processes with coefficients restricted
to an ellipsoid, which includes autoregressive processes with absolutely
summable coefficients. We provide consistency results under different norms for
the estimation of such processes using constrained and penalized estimators. As
an application we show some weak form of universal consistency. Simulations
show that directly including the constraint in the estimation can lead to more
robust results.
",0,0,0,1,0,0
18832,Higher order mobile coverage control with application to localization,"  Most current results on coverage control using mobile sensors require that
one partitioned cell is associated with precisely one sensor. In this paper, we
consider a class of coverage control problems involving higher order Voronoi
partitions, motivated by applications where more than one sensor is required to
monitor and cover one cell. Such applications are frequent in scenarios
requiring the sensors to localize targets. We introduce a framework depending
on a coverage performance function incorporating higher order Voronoi cells and
then design a gradient-based controller which allows the multi-sensor system to
achieve a local equilibrium in a distributed manner. The convergence properties
are studied and related to Lloyd algorithm. We study also the extension to
coverage of a discrete set of points. In addition, we provide a number of real
world scenarios where our framework can be applied. Simulation results are also
provided to show the controller performance.
",1,0,0,0,0,0
18833,Borg's Periodicity Theorems for first order self-adjoint systems with complex potentials,"  A self-adjoint first order system with Hermitian $\pi$-periodic potential
$Q(z)$, integrable on compact sets, is considered. It is shown that all zeros
of $\Delta + 2e^{-i\int_0^\pi \Im q dt}$ are double zeros if and only if this
self-adjoint system is unitarily equivalent to one in which $Q(z)$ is
$\frac{\pi}{2}$-periodic. Furthermore, the zeros of $\Delta - 2e^{-i\int_0^\pi
\Im q dt}$ are all double zeros if and only if the associated self-adjoint
system is unitarily equivalent to one in which $Q(z) = \sigma_2 Q(z) \sigma_2$.
Here $\Delta$ denotes the discriminant of the system and $\sigma_0$, $\sigma_2$
are Pauli matrices. Finally, it is shown that all instability intervals vanish
if and only if $Q = r\sigma_0 + q\sigma_2$, for some real valued $\pi$-periodic
functions $r$ and $q$ integrable on compact sets.
",0,0,1,0,0,0
18834,On the Ubiquity of Information Inconsistency for Conjugate Priors,"  Informally, ""Information Inconsistency"" is the property that has been
observed in many Bayesian hypothesis testing and model selection procedures
whereby the Bayesian conclusion does not become definitive when the data seems
to become definitive. An example is that, when performing a t-test using
standard conjugate priors, the Bayes factor of the alternative hypothesis to
the null hypothesis remains bounded as the t statistic grows to infinity. This
paper shows that information inconsistency is ubiquitous in Bayesian hypothesis
testing under conjugate priors. Yet the title does not fully describe the
paper, since we also show that theoretically recommended priors, including
scale mixtures of conjugate priors and adaptive priors, are information
consistent. Hence the paper is simply a forceful warning that use of conjugate
priors in testing and model selection is highly problematical, and should be
replaced by the information consistent alternatives.
",0,0,1,1,0,0
18835,Competition evolution of Rayleigh-Taylor bubbles,"  Material mixing induced by a Rayleigh-Taylor instability occurs ubiquitously
in either nature or engineering when a light fluid pushes against a heavy
fluid, accompanying with the formation and evolution of chaotic bubbles. Its
general evolution involves two mechanisms: bubble-merge and bubble-competition.
The former obeys a universa1 evolution law and has been well-studied, while the
latter depends on many factors and has not been well-recognized. In this paper,
we establish a theory for the latter to clarify and quantify the longstanding
open question: the dependence of bubbles evolution on the dominant factors of
arbitrary density ratio, broadband initial perturbations and various material
properties (e.g., viscosity, miscibility, surface tensor). Evolution of the
most important characteristic quantities, i.e., the diameter of dominant bubble
$D$ and the height of bubble zone $h$, is derived: (i) the $D$ expands
self-similarly with steady aspect ratio $\beta \equiv D/h \thickapprox (1{\rm{
+ }}A)/4$, depending only on dimensionless density ratio $A$, and (ii) the $h$
grows quadratically with constant growth coefficient $\alpha \equiv h/(Ag{t^2})
\thickapprox [2\phi/{\ln}(2{\eta _{\rm{0}}})]^2$, depending on both
dimensionless initial perturbation amplitude ${\eta _{\rm{0}}}$ and
material-property-associated linear growth rate ratio
$\phi\equiv\Gamma_{actual}/\Gamma_{ideal}\leqslant1$. The theory successfully
explains the continued puzzle about the widely varying $\alpha\in (0.02,0.12)$
in experiments and simulations, conducted at all value of $A \in (0,1)$ and
widely varying value of ${\eta _{\rm{0}}} \in [{10^{ - 7}},{10^{ - 2}}]$ with
different materials. The good agreement between theory and experiments implies
that majority of actual mixing depends on initial perturbations and material
properties, to which more attention should be paid in either natural or
engineering problems.
",0,1,0,0,0,0
18836,Nonlinear oblique projections,"  We construct nonlinear oblique projections along subalgebras of nilpotent Lie
algebras in terms of the Baker-Campbell-Hausdorff multiplication. We prove that
these nonlinear projections are real analytic on every Schubert cell of the
Grassmann manifold whose points are the subalgebras of the nilpotent Lie
algebra under consideration.
",0,0,1,0,0,0
18837,Granger Mediation Analysis of Multiple Time Series with an Application to fMRI,"  It becomes increasingly popular to perform mediation analysis for complex
data from sophisticated experimental studies. In this paper, we present Granger
Mediation Analysis (GMA), a new framework for causal mediation analysis of
multiple time series. This framework is motivated by a functional magnetic
resonance imaging (fMRI) experiment where we are interested in estimating the
mediation effects between a randomized stimulus time series and brain activity
time series from two brain regions. The stable unit treatment assumption for
causal mediation analysis is thus unrealistic for this type of time series
data. To address this challenge, our framework integrates two types of models:
causal mediation analysis across the variables and vector autoregressive models
across the temporal observations. We further extend this framework to handle
multilevel data to address individual variability and correlated errors between
the mediator and the outcome variables. These models not only provide valid
causal mediation for time series data but also model the causal dynamics across
time. We show that the modeling parameters in our models are identifiable, and
we develop computationally efficient methods to maximize the likelihood-based
optimization criteria. Simulation studies show that our method reduces the
estimation bias and improve statistical power, compared to existing approaches.
On a real fMRI data set, our approach not only infers the causal effects of
brain pathways but accurately captures the feedback effect of the outcome
region on the mediator region.
",0,0,0,1,0,0
18838,Parameterization of Sequence of MFCCs for DNN-based voice disorder detection,"  In this article a DNN-based system for detection of three common voice
disorders (vocal nodules, polyps and cysts; laryngeal neoplasm; unilateral
vocal paralysis) is presented. The input to the algorithm is (at least 3-second
long) audio recording of sustained vowel sound /a:/. The algorithm was
developed as part of the ""2018 FEMH Voice Data Challenge"" organized by Far
Eastern Memorial Hospital and obtained score value (defined in the challenge
specification) of 77.44. This was the second best result before final
submission. Final challenge results are not yet known during writing of this
document. The document also reports changes that were made for the final
submission which improved the score value in cross-validation by 0.6% points.
",1,0,0,0,0,0
18839,Near-Optimal Adversarial Policy Switching for Decentralized Asynchronous Multi-Agent Systems,"  A key challenge in multi-robot and multi-agent systems is generating
solutions that are robust to other self-interested or even adversarial parties
who actively try to prevent the agents from achieving their goals. The
practicality of existing works addressing this challenge is limited to only
small-scale synchronous decision-making scenarios or a single agent planning
its best response against a single adversary with fixed, procedurally
characterized strategies. In contrast this paper considers a more realistic
class of problems where a team of asynchronous agents with limited observation
and communication capabilities need to compete against multiple strategic
adversaries with changing strategies. This problem necessitates agents that can
coordinate to detect changes in adversary strategies and plan the best response
accordingly. Our approach first optimizes a set of stratagems that represent
these best responses. These optimized stratagems are then integrated into a
unified policy that can detect and respond when the adversaries change their
strategies. The near-optimality of the proposed framework is established
theoretically as well as demonstrated empirically in simulation and hardware.
",1,0,0,0,0,0
18840,Structural Connectome Validation Using Pairwise Classification,"  In this work, we study the extent to which structural connectomes and
topological derivative measures are unique to individual changes within human
brains. To do so, we classify structural connectome pairs from two large
longitudinal datasets as either belonging to the same individual or not. Our
data is comprised of 227 individuals from the Alzheimer's Disease Neuroimaging
Initiative (ADNI) and 226 from the Parkinson's Progression Markers Initiative
(PPMI). We achieve 0.99 area under the ROC curve score for features which
represent either weights or network structure of the connectomes (node degrees,
PageRank and local efficiency). Our approach may be useful for eliminating
noisy features as a preprocessing step in brain aging studies and early
diagnosis classification problems.
",1,0,0,0,0,0
18841,Deep Robust Kalman Filter,"  A Robust Markov Decision Process (RMDP) is a sequential decision making model
that accounts for uncertainty in the parameters of dynamic systems. This
uncertainty introduces difficulties in learning an optimal policy, especially
for environments with large state spaces. We propose two algorithms, RTD-DQN
and Deep-RoK, for solving large-scale RMDPs using nonlinear approximation
schemes such as deep neural networks. The RTD-DQN algorithm incorporates the
robust Bellman temporal difference error into a robust loss function, yielding
robust policies for the agent. The Deep-RoK algorithm is a robust Bayesian
method, based on the Extended Kalman Filter (EKF), that accounts for both the
uncertainty in the weights of the approximated value function and the
uncertainty in the transition probabilities, improving the robustness of the
agent. We provide theoretical results for our approach and test the proposed
algorithms on a continuous state domain.
",1,0,0,1,0,0
18842,Universal Statistics of Fisher Information in Deep Neural Networks: Mean Field Approach,"  The Fisher information matrix (FIM) is a fundamental quantity to represent
the characteristics of a stochastic model, including deep neural networks
(DNNs). The present study reveals novel statistics of FIM that are universal
among a wide class of DNNs. To this end, we use random weights and large width
limits, which enables us to utilize mean field theories. We investigate the
asymptotic statistics of the FIM's eigenvalues and reveal that most of them are
close to zero while the maximum takes a huge value. This implies that the
eigenvalue distribution has a long tail. Because the landscape of the parameter
space is defined by the FIM, it is locally flat in most dimensions, but
strongly distorted in others. We also demonstrate the potential usage of the
derived statistics through two exercises. First, small eigenvalues that induce
flatness can be connected to a norm-based capacity measure of generalization
ability. Second, the maximum eigenvalue that induces the distortion enables us
to quantitatively estimate an appropriately sized learning rate for gradient
methods to converge.
",0,0,0,1,0,0
18843,An extension problem and trace Hardy inequality for the sublaplacian on $H$-type groups,"  In this paper we study the extension problem for the sublaplacian on a
$H$-type group and use the solutions to prove trace Hardy and Hardy
inequalities for fractional powers of the sublaplacian.
",0,0,1,0,0,0
18844,Centered Isotonic Regression: Point and Interval Estimation for Dose-Response Studies,"  Univariate isotonic regression (IR) has been used for nonparametric
estimation in dose-response and dose-finding studies. One undesirable property
of IR is the prevalence of piecewise-constant stretches in its estimates,
whereas the dose-response function is usually assumed to be strictly
increasing. We propose a simple modification to IR, called centered isotonic
regression (CIR). CIR's estimates are strictly increasing in the interior of
the dose range. In the absence of monotonicity violations, CIR and IR both
return the original observations. Numerical examination indicates that for
sample sizes typical of dose-response studies and with realistic dose-response
curves, CIR provides a substantial reduction in estimation error compared with
IR when monotonicity violations occur. We also develop analytical interval
estimates for IR and CIR, with good coverage behavior. An R package implements
these point and interval estimates.
",0,0,0,1,0,0
18845,Denoising Neural Machine Translation Training with Trusted Data and Online Data Selection,"  Measuring domain relevance of data and identifying or selecting well-fit
domain data for machine translation (MT) is a well-studied topic, but denoising
is not yet. Denoising is concerned with a different type of data quality and
tries to reduce the negative impact of data noise on MT training, in
particular, neural MT (NMT) training. This paper generalizes methods for
measuring and selecting data for domain MT and applies them to denoising NMT
training. The proposed approach uses trusted data and a denoising curriculum
realized by online data selection. Intrinsic and extrinsic evaluations of the
approach show its significant effectiveness for NMT to train on data with
severe noise.
",0,0,0,1,0,0
18846,Discovering Signals from Web Sources to Predict Cyber Attacks,"  Cyber attacks are growing in frequency and severity. Over the past year alone
we have witnessed massive data breaches that stole personal information of
millions of people and wide-scale ransomware attacks that paralyzed critical
infrastructure of several countries. Combating the rising cyber threat calls
for a multi-pronged strategy, which includes predicting when these attacks will
occur. The intuition driving our approach is this: during the planning and
preparation stages, hackers leave digital traces of their activities on both
the surface web and dark web in the form of discussions on platforms like
hacker forums, social media, blogs and the like. These data provide predictive
signals that allow anticipating cyber attacks. In this paper, we describe
machine learning techniques based on deep neural networks and autoregressive
time series models that leverage external signals from publicly available Web
sources to forecast cyber attacks. Performance of our framework across ground
truth data over real-world forecasting tasks shows that our methods yield a
significant lift or increase of F1 for the top signals on predicted cyber
attacks. Our results suggest that, when deployed, our system will be able to
provide an effective line of defense against various types of targeted cyber
attacks.
",1,0,0,1,0,0
18847,Higher-degree Smoothness of Perturbations I,"  In this paper and its sequels, we give an unified treatment of the
higher-degree smoothness of admissible perturbations and related results used
in the global perturbation method for GW and Floer theories.
",0,0,1,0,0,0
18848,Space Telescope and Optical Reverberation Mapping Project. V. Optical Spectroscopic Campaign and Emission-Line Analysis for NGC 5548,"  We present the results of an optical spectroscopic monitoring program
targeting NGC 5548 as part of a larger multi-wavelength reverberation mapping
campaign. The campaign spanned six months and achieved an almost daily cadence
with observations from five ground-based telescopes. The H$\beta$ and He II
$\lambda$4686 broad emission-line light curves lag that of the 5100 $\AA$
optical continuum by $4.17^{+0.36}_{-0.36}$ days and $0.79^{+0.35}_{-0.34}$
days, respectively. The H$\beta$ lag relative to the 1158 $\AA$ ultraviolet
continuum light curve measured by the Hubble Space Telescope is roughly
$\sim$50% longer than that measured against the optical continuum, and the lag
difference is consistent with the observed lag between the optical and
ultraviolet continua. This suggests that the characteristic radius of the
broad-line region is $\sim$50% larger than the value inferred from optical data
alone. We also measured velocity-resolved emission-line lags for H$\beta$ and
found a complex velocity-lag structure with shorter lags in the line wings,
indicative of a broad-line region dominated by Keplerian motion. The responses
of both the H$\beta$ and He II $\lambda$4686 emission lines to the driving
continuum changed significantly halfway through the campaign, a phenomenon also
observed for C IV, Ly $\alpha$, He II(+O III]), and Si IV(+O IV]) during the
same monitoring period. Finally, given the optical luminosity of NGC 5548
during our campaign, the measured H$\beta$ lag is a factor of five shorter than
the expected value implied by the $R_\mathrm{BLR} - L_\mathrm{AGN}$ relation
based on the past behavior of NGC 5548.
",0,1,0,0,0,0
18849,An Inexact Regularized Newton Framework with a Worst-Case Iteration Complexity of $\mathcal{O}(ε^{-3/2})$ for Nonconvex Optimization,"  An algorithm for solving smooth nonconvex optimization problems is proposed
that, in the worst-case, takes $\mathcal{O}(\epsilon^{-3/2})$ iterations to
drive the norm of the gradient of the objective function below a prescribed
positive real number $\epsilon$ and can take $\mathcal{O}(\epsilon^{-3})$
iterations to drive the leftmost eigenvalue of the Hessian of the objective
above $-\epsilon$. The proposed algorithm is a general framework that covers a
wide range of techniques including quadratically and cubically regularized
Newton methods, such as the Adaptive Regularisation using Cubics (ARC) method
and the recently proposed Trust-Region Algorithm with Contractions and
Expansions (TRACE). The generality of our method is achieved through the
introduction of generic conditions that each trial step is required to satisfy,
which in particular allow for inexact regularized Newton steps to be used.
These conditions center around a new subproblem that can be approximately
solved to obtain trial steps that satisfy the conditions. A new instance of the
framework, distinct from ARC and TRACE, is described that may be viewed as a
hybrid between quadratically and cubically regularized Newton methods.
Numerical results demonstrate that our hybrid algorithm outperforms a cublicly
regularized Newton method.
",0,0,1,0,0,0
18850,An inverse problem for Maxwell's equations with Lipschitz parameters,"  We consider an inverse boundary value problem for Maxwell's equations, which
aims to recover the electromagnetic material properties of a body from
measurements on the boundary. We show that a Lipschitz continuous conductivity,
electric permittivity, and magnetic permeability are uniquely determined by
knowledge of all tangential electric and magnetic fields on the boundary of the
body at a fixed frequency.
",0,0,1,0,0,0
18851,Context Aware Robot Navigation using Interactively Built Semantic Maps,"  We discuss the process of building semantic maps, how to interactively label
entities in them, and how to use them to enable context-aware navigation
behaviors in human environments. We utilize planar surfaces, such as walls and
tables, and static objects, such as door signs, as features for our semantic
mapping approach. Users can interactively annotate these features by having the
robot follow him/her, entering the label through a mobile app, and performing a
pointing gesture toward the landmark of interest. Our gesture based approach
can reliably estimate which object is being pointed at and detect ambiguous
gestures with probabilistic modeling. Our person following method attempts to
maximize future utility by a search for future actions assuming constant
velocity model for the human. We describe a method to extract metric goals from
a semantic map landmark and to plan a human aware path that takes into account
the personal spaces of people. Finally, we demonstrate context-awareness for
person following in two scenarios: interactive labeling and door passing. We
believe that future navigation approaches and service robotics applications can
be made more effective by further exploiting the structure of human
environments.
",1,0,0,0,0,0
18852,Disentangling by Partitioning: A Representation Learning Framework for Multimodal Sensory Data,"  Multimodal sensory data resembles the form of information perceived by humans
for learning, and are easy to obtain in large quantities. Compared to unimodal
data, synchronization of concepts between modalities in such data provides
supervision for disentangling the underlying explanatory factors of each
modality. Previous work leveraging multimodal data has mainly focused on
retaining only the modality-invariant factors while discarding the rest. In
this paper, we present a partitioned variational autoencoder (PVAE) and several
training objectives to learn disentangled representations, which encode not
only the shared factors, but also modality-dependent ones, into separate latent
variables. Specifically, PVAE integrates a variational inference framework and
a multimodal generative model that partitions the explanatory factors and
conditions only on the relevant subset of them for generation. We evaluate our
model on two parallel speech/image datasets, and demonstrate its ability to
learn disentangled representations by qualitatively exploring within-modality
and cross-modality conditional generation with semantics and styles specified
by examples. For quantitative analysis, we evaluate the classification accuracy
of automatically discovered semantic units. Our PVAE can achieve over 99%
accuracy on both modalities.
",0,0,0,1,0,0
18853,Blind Gain and Phase Calibration via Sparse Spectral Methods,"  Blind gain and phase calibration (BGPC) is a bilinear inverse problem
involving the determination of unknown gains and phases of the sensing system,
and the unknown signal, jointly. BGPC arises in numerous applications, e.g.,
blind albedo estimation in inverse rendering, synthetic aperture radar
autofocus, and sensor array auto-calibration. In some cases, sparse structure
in the unknown signal alleviates the ill-posedness of BGPC. Recently there has
been renewed interest in solutions to BGPC with careful analysis of error
bounds. In this paper, we formulate BGPC as an eigenvalue/eigenvector problem,
and propose to solve it via power iteration, or in the sparsity or joint
sparsity case, via truncated power iteration. Under certain assumptions, the
unknown gains, phases, and the unknown signal can be recovered simultaneously.
Numerical experiments show that power iteration algorithms work not only in the
regime predicted by our main results, but also in regimes where theoretical
analysis is limited. We also show that our power iteration algorithms for BGPC
compare favorably with competing algorithms in adversarial conditions, e.g.,
with noisy measurement or with a bad initial estimate.
",1,0,0,0,0,0
18854,"Reconciling cooperation, biodiversity and stability in complex ecological communities","  Empirical observations show that ecological communities can have a huge
number of coexisting species, also with few or limited number of resources.
These ecosystems are characterized by multiple type of interactions, in
particular displaying cooperative behaviors. However, standard modeling of
population dynamics based on Lotka-Volterra type of equations predicts that
ecosystem stability should decrease as the number of species in the community
increases and that cooperative systems are less stable than communities with
only competitive and/or exploitative interactions. Here we propose a stochastic
model of population dynamics, which includes exploitative interactions as well
as cooperative interactions induced by cross-feeding. The model is exactly
solved and we obtain results for relevant macro-ecological patterns, such as
species abundance distributions and correlation functions. In the large system
size limit, any number of species can coexist for a very general class of
interaction networks and stability increases as the number of species grows.
For pure mutualistic/commensalistic interactions we determine the topological
properties of the network that guarantee species coexistence. We also show that
the stationary state is globally stable and that inferring species interactions
through species abundance correlation analysis may be misleading. Our
theoretical approach thus show that appropriate models of cooperation naturally
leads to a solution of the long-standing question about complexity-stability
paradox and on how highly biodiverse communities can coexist.
",0,0,0,0,1,0
18855,The Young L Dwarf 2MASS J11193254-1137466 is a Planetary-Mass Binary,"  We have discovered that the extremely red, low-gravity L7 dwarf 2MASS
J11193254-1137466 is a 0.14"" (3.6 AU) binary using Keck laser guide star
adaptive optics imaging. 2MASS J11193254-1137466 has previously been identified
as a likely member of the TW Hydrae Association (TWA). Using our updated
photometric distance and proper motion, a kinematic analysis based on the
BANYAN II model gives an 82% probability of TWA membership. At TWA's 10$\pm$3
Myr age and using hot-start evolutionary models, 2MASS J11193254-1137466AB is a
pair of $3.7^{+1.2}_{-0.9}$ $M_{\rm Jup}$ brown dwarfs, making it the
lowest-mass binary discovered to date. We estimate an orbital period of
$90^{+80}_{-50}$ years. One component is marginally brighter in $K$ band but
fainter in $J$ band, making this a probable flux-reversal binary, the first
discovered with such a young age. We also imaged the spectrally similar TWA L7
dwarf WISEA J114724.10-204021.3 with Keck and found no sign of binarity. Our
evolutionary model-derived $T_{\rm eff}$ estimate for WISEA J114724.10-204021.3
is $\approx$230 K higher than for 2MASS J11193254-1137466AB, at odds with their
spectral similarity. This discrepancy suggests that WISEA J114724.10-204021.3
may actually be a tight binary with masses and temperatures very similar to
2MASS J11193254-1137466AB, or further supporting the idea that near-infrared
spectra of young ultracool dwarfs are shaped by factors other than temperature
and gravity. 2MASS J11193254-1137466AB will be an essential benchmark for
testing evolutionary and atmospheric models in the young planetary-mass regime.
",0,1,0,0,0,0
18856,Diversity of Abundance Patterns of Light Neutron-capture Elements in Very-metal-poor Stars,"  We determine the abundances of neutron-capture elements from Sr to Eu for
five very-metal-poor stars (-3<[Fe/H]<-2) in the Milky Way halo to reveal the
origin of light neutron-capture elements. Previous spectroscopic studies have
shown evidence of at least two components in the r-process; one referred to as
the ""main r-process"" and the other as the ""weak r-process,"" which is mainly
responsible for producing heavy and light neutron-capture elements,
respectively. Observational studies of metal-poor stars suggest that there is a
universal pattern in the main r-process, similar to the abundance pattern of
the r-process component of solar-system material. Still, it is uncertain
whether the abundance pattern of the weak r-process shows universality or
diversity, due to the sparseness of measured light neutron-capture elements. We
have detected the key elements, Mo, Ru, and Pd, in five target stars to give an
answer to this question. The abundance patterns of light neutron-capture
elements from Sr to Pd suggest a diversity in the weak r-process. In
particular, scatter in the abundance ratio between Ru and Pd is significant
when the abundance patterns are normalized at Zr. Our results are compared with
the elemental abundances predicted by nucleosynthesis models of supernovae with
parameters such as electron fraction or proto-neutron-star mass, to investigate
sources of such diversity in the abundance patterns of light neutron-capture
elements. This paper presents that the variation in the abundances of observed
stars can be explained with a small range of parameters, which can serve as
constraints on future modeling of supernova models.
",0,1,0,0,0,0
18857,Hypergames and Cyber-Physical Security for Control Systems,"  The identification of the Stuxnet worm in 2010 provided a highly publicized
example of a cyber attack used to damage an industrial control system
physically. This raised public awareness about the possibility of similar
attacks against other industrial targets -- including critical infrastructure.
In this paper, we use hypergames to analyze how adversarial perturbations can
be used to manipulate a system using optimal control. Hypergames form an
extension of game theory that enables us to model strategic interactions where
the players may have significantly different perceptions of the game(s) they
are playing. Past work with hypergames has been limited to relatively simple
interactions consisting of a small set of discrete choices for each player, but
here, we apply hypergames to larger systems with continuous variables. We find
that manipulating constraints can be a more effective attacker strategy than
directly manipulating objective function parameters. Moreover, the attacker
need not change the underlying system to carry out a successful attack -- it
may be sufficient to deceive the defender controlling the system. It is
possible to scale our approach up to even larger systems, but the ability to do
so will depend on the characteristics of the system in question, and we
identify several characteristics that will make those systems amenable to
hypergame analysis.
",1,0,0,0,0,0
18858,Measuring the academic reputation through citation networks via PageRank,"  The objective assessment of the prestige of an academic institution is a
difficult and hotly debated task. In the last few years, different types of
University Rankings have been proposed to quantify the excellence of different
research institutions in the world. Albeit met with criticism in some cases,
the relevance of university rankings is being increasingly acknowledged:
indeed, rankings are having a major impact on the design of research policies,
both at the institutional and governmental level. Yet, the debate on what
rankings are {\em exactly} measuring is enduring. Here, we address the issue by
measuring a quantitive and reliable proxy of the academic reputation of a given
institution and by evaluating its correlation with different university
rankings. Specifically, we study citation patterns among universities in five
different Web of Science Subject Categories and use the \pr~algorithm on the
five resulting citation networks. The rationale behind our work is that
scientific citations are driven by the reputation of the reference so that the
PageRank algorithm is expected to yield a rank which reflects the reputation of
an academic institution in a specific field. Our results allow to quantifying
the prestige of a set of institutions in a certain research field based only on
hard bibliometric data. Given the volume of the data analysed, our findings are
statistically robust and less prone to bias, at odds with ad--hoc surveys often
employed by ranking bodies in order to attain similar results. Because our
findings are found to correlate extremely well with the ARWU Subject rankings,
the approach we propose in our paper may open the door to new, Academic Ranking
methodologies that go beyond current methods by reconciling the qualitative
evaluation of Academic Prestige with its quantitative measurements via
publication impact.
",1,0,0,0,0,0
18859,A Model Order Reduction Algorithm for Estimating the Absorption Spectrum,"  The ab initio description of the spectral interior of the absorption spectrum
poses both a theoretical and computational challenge for modern electronic
structure theory. Due to the often spectrally dense character of this domain in
the quantum propagator's eigenspectrum for medium-to-large sized systems,
traditional approaches based on the partial diagonalization of the propagator
often encounter oscillatory and stagnating convergence. Electronic structure
methods which solve the molecular response problem through the solution of
spectrally shifted linear systems, such as the complex polarization propagator,
offer an alternative approach which is agnostic to the underlying spectral
density or domain location. This generality comes at a seemingly high
computational cost associated with solving a large linear system for each
spectral shift in some discretization of the spectral domain of interest. We
present a novel, adaptive solution based on model order reduction techniques
via interpolation. Model order reduction reduces the computational complexity
of mathematical models and is ubiquitous in the simulation of dynamical
systems. The efficiency and effectiveness of the proposed algorithm in the ab
initio prediction of X-Ray absorption spectra is demonstrated using a test set
of challenging water clusters which are spectrally dense in the neighborhood of
the oxygen K-edge. Based on a single, user defined tolerance we automatically
determine the order of the reduced models and approximate the absorption
spectrum up to the given tolerance. We also illustrate that the automatically
determined model order increases logarithmically with the problem dimension,
compared to a linear increase of the number of eigenvalues within the energy
window. Furthermore, we observed that the computational cost of the proposed
algorithm only scales quadratically with respect to the problem dimension.
",1,1,0,0,0,0
18860,"Uniform rank gradient, cost and local-global convergence","  We analyze the rank gradient of finitely generated groups with respect to
sequences of subgroups of finite index that do not necessarily form a chain, by
connecting it to the cost of p.m.p. actions. We generalize several results that
were only known for chains before. The connection is made by the notion of
local-global convergence.
In particular, we show that for a finitely generated group $\Gamma$ with
fixed price $c$, every Farber sequence has rank gradient $c-1$. By adapting
Lackenby's trichotomy theorem to this setting, we also show that in a finitely
presented amenable group, every sequence of subgroups with index tending to
infinity has vanishing rank gradient.
",0,0,1,0,0,0
18861,Optimal design of a model energy conversion device,"  Fuel cells, batteries, thermochemical and other energy conversion devices
involve the transport of a number of (electro-)chemical species through
distinct materials so that they can meet and react at specified multi-material
interfaces. Therefore, morphology or arrangement of these different materials
can be critical in the performance of an energy conversion device. In this
paper, we study a model problem motivated by a solar-driven thermochemical
conversion device that splits water into hydrogen and oxygen. We formulate the
problem as a system of coupled multi-material reaction-diffusion equations
where each species diffuses selectively through a given material and where the
reaction occurs at multi-material interfaces. We express the problem of optimal
design of the material arrangement as a saddle point problem and obtain an
effective functional which shows that regions with very fine phase mixtures of
the material arise naturally. To explore this further, we introduce a
phase-field formulation of the optimal design problem, and numerically study
selected examples.
",0,1,1,0,0,0
18862,Mining Communication Data in a Music Community: A Preliminary Analysis,"  Comments play an important role within online creative communities because
they make it possible to foster the production and improvement of authors'
artifacts. We investigate how comment-based communication help shape members'
behavior within online creative communities. In this paper, we report the
results of a preliminary study aimed at mining the communication network of a
music community for collaborative songwriting, where users collaborate online
by first uploading new songs and then by adding new tracks and providing
feedback in forms of comments.
",1,0,0,0,0,0
18863,Multidimensional VlasovPoisson Simulations with High-order Monotonicity- and Positivity-preserving Schemes,"  We develop new numerical schemes for Vlasov--Poisson equations with
high-order accuracy. Our methods are based on a spatially
monotonicity-preserving (MP) scheme and are modified suitably so that
positivity of the distribution function is also preserved. We adopt an
efficient semi-Lagrangian time integration scheme that is more accurate and
computationally less expensive than the three-stage TVD Runge-Kutta
integration. We apply our spatially fifth- and seventh-order schemes to a suite
of simulations of collisionless self-gravitating systems and electrostatic
plasma simulations, including linear and nonlinear Landau damping in one
dimension and Vlasov--Poisson simulations in a six-dimensional phase space. The
high-order schemes achieve a significantly improved accuracy in comparison with
the third-order positive-flux-conserved scheme adopted in our previous study.
With the semi-Lagrangian time integration, the computational cost of our
high-order schemes does not significantly increase, but remains roughly the
same as that of the third-order scheme. Vlasov--Poisson simulations on $128^3
\times 128^3$ mesh grids have been successfully performed on a massively
parallel computer.
",0,1,0,0,0,0
18864,Markov Chain Monte Carlo Methods for Bayesian Data Analysis in Astronomy,"  Markov Chain Monte Carlo based Bayesian data analysis has now become the
method of choice for analyzing and interpreting data in almost all disciplines
of science. In astronomy, over the last decade, we have also seen a steady
increase in the number of papers that employ Monte Carlo based Bayesian
analysis. New, efficient Monte Carlo based methods are continuously being
developed and explored. In this review, we first explain the basics of Bayesian
theory and discuss how to set up data analysis problems within this framework.
Next, we provide an overview of various Monte Carlo based methods for
performing Bayesian data analysis. Finally, we discuss advanced ideas that
enable us to tackle complex problems and thus hold great promise for the
future. We also distribute downloadable computer software (available at
this https URL ) that implements some of the algorithms and
examples discussed here.
",0,1,0,1,0,0
18865,Stable splitting of mapping spaces via nonabelian Poincaré duality,"  We use nonabelian Poincaré duality to recover the stable splitting of
compactly supported mapping spaces, $\rm{Map_c}$$(M,\Sigma^nX)$, where $M$ is a
parallelizable $n$-manifold. Our method for deriving this splitting is new, and
naturally extends to give a more general stable splitting of the space of
compactly supported sections of a certain bundle on $M$ with fibers
$\Sigma^nX$, twisted by the tangent bundle of $M$. This generalization
incorporates possible $O(n)$-actions on $X$ as well as accommodating
non-parallelizable manifolds.
",0,0,1,0,0,0
18866,Static Gesture Recognition using Leap Motion,"  In this report, an automated bartender system was developed for making orders
in a bar using hand gestures. The gesture recognition of the system was
developed using Machine Learning techniques, where the model was trained to
classify gestures using collected data. The final model used in the system
reached an average accuracy of 95%. The system raised ethical concerns both in
terms of user interaction and having such a system in a real world scenario,
but it could initially work as a complement to a real bartender.
",1,0,0,1,0,0
18867,B-spline-like bases for $C^2$ cubics on the Powell-Sabin 12-split,"  For spaces of constant, linear, and quadratic splines of maximal smoothness
on the Powell-Sabin 12-split of a triangle, the so-called S-bases were recently
introduced. These are simplex spline bases with B-spline-like properties on the
12-split of a single triangle, which are tied together across triangles in a
Bézier-like manner.
In this paper we give a formal definition of an S-basis in terms of certain
basic properties. We proceed to investigate the existence of S-bases for the
aforementioned spaces and additionally the cubic case, resulting in an
exhaustive list. From their nature as simplex splines, we derive simple
differentiation and recurrence formulas to other S-bases. We establish a
Marsden identity that gives rise to various quasi-interpolants and domain
points forming an intuitive control net, in terms of which conditions for
$C^0$-, $C^1$-, and $C^2$-smoothness are derived.
",1,0,0,0,0,0
18868,Synthesizing Correlations with Computational Likelihood Approach: Vitamin C Data,"  It is known that the primary source of dietary vitamin C is fruit and
vegetables and the plasma level of vitamin C has been considered a good
surrogate biomarker of vitamin C intake by fruit and vegetable consumption. To
combine the information about association between vitamin C intake and the
plasma level of vitamin C, numerical approximation methods for likelihood
function of correlation coefficient are studied. The least squares approach is
used to estimate a log-likelihood function by a function from a space of
B-splines having desirable mathematical properties. The likelihood interval
from the Highest Likelihood Regions (HLR) is used for further inference. This
approach can be easily extended to the realm of meta-analysis involving sample
correlations from different studies by use of an approximated combined
likelihood function. The sample correlations between vitamin C intake and serum
level of vitamin C from many studies are used to illustrate application of this
approach.
",0,0,0,1,0,0
18869,Axion detection via Topological Casimir Effect,"  We propose a new table-top experimental configuration for the direct
detection of dark matter axions with mass in the $(10^{-6} \rm eV - 10^{-2} \rm
eV)$ range using non-perturbative effects in a system with non-trivial spatial
topology. Different from most experimental setups found in literature on direct
dark matter axion detection, which relies on $\dot{\theta}$ or
$\vec{\nabla}\theta$, we found that our system is in principle sensitive to a
static $\theta\geq 10^{-14}$ and can also be used to set limit on the
fundamental constant $\theta_{\rm QED}$ which becomes the fundamental
observable parameter of the Maxwell system if some conditions are met.
Connection with Witten effect when the induced electric charge $e'$ is
proportional to $\theta$ and the magnetic monopole becomes the dyon with
non-vanishing $e'=-e \frac{\theta}{2\pi}$ is also discussed.
",0,1,0,0,0,0
18870,Novel Feature-Based Clustering of Micro-Panel Data (CluMP),"  Micro-panel data are collected and analysed in many research and industry
areas. Cluster analysis of micro-panel data is an unsupervised learning
exploratory method identifying subgroup clusters in a data set which include
homogeneous objects in terms of the development dynamics of monitored
variables. The supply of clustering methods tailored to micro-panel data is
limited. The present paper focuses on a feature-based clustering method,
introducing a novel two-step characteristic-based approach designed for this
type of data. The proposed CluMP method aims to identify clusters that are at
least as internally homogeneous and externally heterogeneous as those obtained
by alternative methods already implemented in the statistical system R. We
compare the clustering performance of the devised algorithm with two extant
methods using simulated micro-panel data sets. Our approach has yielded similar
or better outcomes than the other methods, the advantage of the proposed
algorithm being time efficiency which makes it applicable for large data sets.
",0,0,0,1,0,0
18871,Ab initio effective Hamiltonians for cuprate superconductors,"  Ab initio low-energy effective Hamiltonians of two typical high-temperature
copper-oxide superconductors, whose mother compounds are La$_2$CuO$_4$ and
HgBa$_2$CuO$_4$, are derived by utilizing the multi-scale ab initio scheme for
correlated electrons (MACE). The effective Hamiltonians obtained in the present
study serve as platforms of future studies to accurately solve the low-energy
effective Hamiltonians beyond the density functional theory. It allows further
study on the superconducting mechanism from the first principles and
quantitative basis without adjustable parameters not only for the available
cuprates but also for future design of higher Tc in general. More concretely,
we derive effective Hamiltonians for three variations, 1)one-band Hamiltonian
for the antibonding orbital generated from strongly hybridized Cu
$3d_{x^2-y^2}$ and O $2p_\sigma$ orbitals 2)two-band Hamiltonian constructed
from the antibonding orbital and Cu $3d_{3z^2-r^2}$ orbital hybridized mainly
with the apex oxygen $p_z$ orbital 3)three-band Hamiltonian consisting mainly
of Cu $3d_{x^2-y^2}$ orbitals and two O $2p_\sigma$ orbitals. Differences
between the Hamiltonians for La$_2$CuO$_4$ and HgBa$_2$CuO$_4$, which have
relatively low and high critical temperatures, respectively, at optimally doped
compounds, are elucidated. The main differences are summarized as i) the oxygen
$2p_\sigma$ orbitals are farther(~3.7eV) below from the Cu $d_{x^2-y^2}$
orbital for the La compound than the Hg compound(~2.4eV) in the three-band
Hamiltonian. This causes a substantial difference in the character of the
$d_{x^2-y^2}-2p_\sigma$ antibonding band at the Fermi level and makes the
effective onsite Coulomb interaction U larger for the La compound than the Hg
compound for the two- and one-band Hamiltonians. ii)The ratio of the
second-neighbor to the nearest transfer t'/t is also substantially different
(~0.26) for the Hg and ~0.15 for the La compound in the one-band Hamiltonian.
",0,1,0,0,0,0
18872,Longitudinal data analysis using matrix completion,"  In clinical practice and biomedical research, measurements are often
collected sparsely and irregularly in time while the data acquisition is
expensive and inconvenient. Examples include measurements of spine bone mineral
density, cancer growth through mammography or biopsy, a progression of defect
of vision, or assessment of gait in patients with neurological disorders. Since
the data collection is often costly and inconvenient, estimation of progression
from sparse observations is of great interest for practitioners.
From the statistical standpoint, such data is often analyzed in the context
of a mixed-effect model where time is treated as both random and fixed effect.
Alternatively, researchers analyze Gaussian processes or functional data where
observations are assumed to be drawn from a certain distribution of processes.
These models are flexible but rely on probabilistic assumptions and require
very careful implementation.
In this study, we propose an alternative elementary framework for analyzing
longitudinal data, relying on matrix completion. Our method yields point
estimates of progression curves by iterative application of the SVD. Our
framework covers multivariate longitudinal data, regression and can be easily
extended to other settings.
We apply our methods to understand trends of progression of motor impairment
in children with Cerebral Palsy. Our model approximates individual progression
curves and explains 30% of the variability. Low-rank representation of
progression trends enables discovering that subtypes of Cerebral Palsy exhibit
different progression trends.
",0,0,0,1,0,0
18873,Emission-line Diagnostics of Nearby HII Regions Including Supernova Hosts,"  We present a new model of the optical nebular emission from HII regions by
combin- ing the results of the Binary Population and Spectral Synthesis (bpass)
code with the photoion- ization code cloudy (Ferland et al. 1998). We explore a
variety of emission-line diagnostics of these star-forming HII regions and
examine the effects of metallicity and interacting binary evo- lution on the
nebula emission-line production. We compare the line emission properties of HII
regions with model stellar populations, and provide new constraints on their
stellar populations and supernova progenitors. We find that models including
massive binary stars can successfully match all the observational constraints
and provide reasonable age and mass estimation of the HII regions and supernova
progenitors.
",0,1,0,0,0,0
18874,On monomial linearisation and supercharacters of pattern subgroups,"  Column closed pattern subgroups $U$ of the finite upper unitriangular groups
$U_n(q)$ are defined as sets of matrices in $U_n(q)$ having zeros in a
prescribed set of columns besides the diagonal ones. We explain Jedlitschky's
construction of monomial linearisation and apply this to $C U$ yielding a
generalisation of Yan's coadjoint cluster representations. Then we give a
complete classification of the resulting supercharacters, by describing the
resulting orbits and determining the Hom-spaces between orbit modules.
",0,0,1,0,0,0
18875,The Structural Fate of Individual Multicomponent Metal-Oxide Nanoparticles in Polymer Nanoreactors,"  Multicomponent nanoparticles can be synthesized with either homogeneous or
phase-segregated architectures depending on the synthesis conditions and
elements incorporated. To understand the parameters that determine their
structural fate, multicomponent metal-oxide nanoparticles consisting of
combinations of Co, Ni, and Cu were synthesized via scanning probe block
copolymer lithography and characterized using correlated electron microscopy.
These studies revealed that the miscibility, ratio of the metallic components,
and the synthesis temperature determine the crystal structure and architecture
of the nanoparticles. A Co-Ni-O system forms a rock salt structure largely due
to the miscibility of CoO and NiO, while Cu-Ni-O, which has large miscibility
gaps, forms either homogeneous oxides, heterojunctions, or alloys depending on
the annealing temperature and composition. Moreover, a higher ordered
structure, Co-Ni-Cu-O, was found to follow the behavior of lower ordered
systems.
",0,1,0,0,0,0
18876,On Compression of Unsupervised Neural Nets by Pruning Weak Connections,"  Unsupervised neural nets such as Restricted Boltzmann Machines(RBMs) and Deep
Belif Networks(DBNs), are powerful in automatic feature extraction,unsupervised
weight initialization and density estimation. In this paper,we demonstrate that
the parameters of these neural nets can be dramatically reduced without
affecting their performance. We describe a method to reduce the parameters
required by RBM which is the basic building block for deep architectures.
Further we propose an unsupervised sparse deep architectures selection
algorithm to form sparse deep neural networks.Experimental results show that
there is virtually no loss in either generative or discriminative performance.
",1,0,0,0,0,0
18877,A matrix generalization of a theorem of Fine,"  In 1947 Nathan Fine gave a beautiful product for the number of binomial
coefficients $\binom{n}{m}$, for $m$ in the range $0 \leq m \leq n$, that are
not divisible by $p$. We give a matrix product that generalizes Fine's formula,
simultaneously counting binomial coefficients with $p$-adic valuation $\alpha$
for each $\alpha \geq 0$. For each $n$ this information is naturally encoded in
a polynomial generating function, and the sequence of these polynomials is
$p$-regular in the sense of Allouche and Shallit. We also give a further
generalization to multinomial coefficients.
",1,0,1,0,0,0
18878,Training Multi-Task Adversarial Network For Extracting Noise-Robust Speaker Embedding,"  Under noisy environments, to achieve the robust performance of speaker
recognition is still a challenging task. Motivated by the promising performance
of multi-task training in a variety of image processing tasks, we explore the
potential of multi-task adversarial training for learning a noise-robust
speaker embedding. In this paper we present a novel framework which consists of
three components: an encoder that extracts noise-robust speaker embedding; a
classifier that classifies the speakers; a discriminator that discriminates the
noise type of the speaker embedding. Besides, we propose a training strategy
using the training accuracy as an indicator to stabilize the multi-class
adversarial optimization process. We conduct our experiments on the English and
Mandarin corpus and the experimental results demonstrate that our proposed
multi-task adversarial training method could greatly outperform the other
methods without adversarial training in noisy environments. Furthermore,
experiments indicate that our method is also able to improve the speaker
verification performance the clean condition.
",1,0,0,0,0,0
18879,Playing Games with Bounded Entropy,"  In this paper, we consider zero-sum repeated games in which the maximizer is
restricted to strategies requiring no more than a limited amount of randomness.
Particularly, we analyze the maxmin payoff of the maximizer in two models: the
first model forces the maximizer to randomize her action in each stage just by
conditioning her decision to outcomes of a given sequence of random source,
whereas, in the second model, the maximizer is a team of players who are free
to privately randomize their corresponding actions but do not have access to
any explicit source of shared randomness needed for cooperation. The works of
Gossner and Vieille, and Gossner and Tomala adopted the method of types to
establish their results; however, we utilize the idea of random hashing which
is the core of randomness extractors in the information theory literature. In
addition, we adopt the well-studied tool of simulation of a source from another
source. By utilizing these tools, we are able to simplify the prior results and
generalize them as well. We characterize the maxmin payoff of the maximizer in
the repeated games under study. Particularly, the maxmin payoff of the first
model is fully described by the function $J(h)$ which is the maximum payoff
that the maximizer can secure in a one-shot game by choosing mixed strategies
of entropy at most $h$. In the second part of the paper, we study the
computational aspects of $J(h)$. We offer three explicit lower bounds on the
entropy-payoff trade-off curve. To do this, we provide and utilize new results
for the set of distributions that guarantee a certain payoff for Alice. In
particular, we study how this set of distributions shrinks as we increase the
security level. While the use of total variation distance is common in game
theory, our derivation indicates the suitability of utilizing the
Renyi-divergence of order two.
",1,0,0,0,0,0
18880,Exact Hausdorff and packing measures for random self-similar code-trees with necks,"  Random code-trees with necks were introduced recently to generalise the
notion of $V$-variable and random homogeneous sets. While it is known that the
Hausdorff and packing dimensions coincide irrespective of overlaps, their exact
Hausdorff and packing measure has so far been largely ignored. In this article
we consider the general question of an appropriate gauge function for positive
and finite Hausdorff and packing measure. We first survey the current state of
knowledge and establish some bounds on these gauge functions. We then show that
self-similar code-trees do not admit a gauge functions that simultaneously give
positive and finite Hausdorff measure almost surely. This surprising result is
in stark contrast to the random recursive model and sheds some light on the
question of whether $V$-variable sets interpolate between random homogeneous
and random recursive sets. We conclude by discussing implications of our
results.
",0,0,1,0,0,0
18881,On Modules over a G-set,"  Let R be a commutative ring with unity, M a module over R and let S be a
G-set for a finite group G. We define a set MS to be the set of elements
expressed as the formal finite sum of the form similar to the elements of group
ring RG. The set MS is a module over the group ring RG under the addition and
the scalar multiplication similar to the RG-module MG. With this notion, we not
only generalize but also unify the theories of both of the group algebra and
the group module, and we also establish some significant properties of MS. In
particular, we describe a method for decomposing a given RG-module MS as a
direct sum of RG-submodules. Furthermore, we prove the semisimplicity problem
of MS with regard to the properties of M, S and G.
",0,0,1,0,0,0
18882,Polynomial-time algorithms for the Longest Induced Path and Induced Disjoint Paths problems on graphs of bounded mim-width,"  We give the first polynomial-time algorithms on graphs of bounded maximum
induced matching width (mim-width) for problems that are not locally checkable.
In particular, we give $n^{\mathcal{O}(w)}$-time algorithms on graphs of
mim-width at most $w$, when given a decomposition, for the following problems:
Longest Induced Path, Induced Disjoint Paths and $H$-Induced Topological Minor
for fixed $H$. Our results imply that the following graph classes have
polynomial-time algorithms for these three problems: Interval and Bi-Interval
graphs, Circular Arc, Permutation and Circular Permutation graphs, Convex
graphs, $k$-Trapezoid, Circular $k$-Trapezoid, $k$-Polygon, Dilworth-$k$ and
Co-$k$-Degenerate graphs for fixed $k$.
",1,0,0,0,0,0
18883,Bug or Not? Bug Report Classification Using N-Gram IDF,"  Previous studies have found that a significant number of bug reports are
misclassified between bugs and non-bugs, and that manually classifying bug
reports is a time-consuming task. To address this problem, we propose a bug
reports classification model with N-gram IDF, a theoretical extension of
Inverse Document Frequency (IDF) for handling words and phrases of any length.
N-gram IDF enables us to extract key terms of any length from texts, these key
terms can be used as the features to classify bug reports. We build
classification models with logistic regression and random forest using features
from N-gram IDF and topic modeling, which is widely used in various software
engineering tasks. With a publicly available dataset, our results show that our
N-gram IDF-based models have a superior performance than the topic-based models
on all of the evaluated cases. Our models show promising results and have a
potential to be extended to other software engineering tasks.
",1,0,0,0,0,0
18884,Prior Information Guided Regularized Deep Learning for Cell Nucleus Detection,"  Cell nuclei detection is a challenging research topic because of limitations
in cellular image quality and diversity of nuclear morphology, i.e. varying
nuclei shapes, sizes, and overlaps between multiple cell nuclei. This has been
a topic of enduring interest with promising recent success shown by deep
learning methods. These methods train Convolutional Neural Networks (CNNs) with
a training set of input images and known, labeled nuclei locations. Many such
methods are supplemented by spatial or morphological processing. Using a set of
canonical cell nuclei shapes, prepared with the help of a domain expert, we
develop a new approach that we call Shape Priors with Convolutional Neural
Networks (SP-CNN). We further extend the network to introduce a shape prior
(SP) layer and then allowing it to become trainable (i.e. optimizable). We call
this network tunable SP-CNN (TSP-CNN). In summary, we present new network
structures that can incorporate 'expected behavior' of nucleus shapes via two
components: learnable layers that perform the nucleus detection and a fixed
processing part that guides the learning with prior information. Analytically,
we formulate two new regularization terms that are targeted at: 1) learning the
shapes, 2) reducing false positives while simultaneously encouraging detection
inside the cell nucleus boundary. Experimental results on two challenging
datasets reveal that the proposed SP-CNN and TSP-CNN can outperform
state-of-the-art alternatives.
",1,0,0,1,0,0
18885,Anisotropic hydrodynamic turbulence in accretion disks,"  Recently, the vertical shear instability (VSI) has become an attractive
purely hydrodynamic candidate for the anomalous angular momentum transport
required for weakly ionized accretion disks. In direct three-dimensional
numerical simulations of VSI turbulence in disks, a meridional circulation
pattern was observed that is opposite to the usual viscous flow behavior. Here,
we investigate whether this feature can possibly be explained by an anisotropy
of the VSI turbulence. Using three-dimensional hydrodynamical simulations, we
calculate the turbulent Reynolds stresses relevant for angular momentum
transport for a representative section of a disk.
We find that the vertical stress is significantly stronger than the radial
stress. Using our results in viscous disk simulations with different viscosity
coefficients for the radial and vertical direction, we find good agreement with
the VSI turbulence for the stresses and meridional flow; this provides
additional evidence for the anisotropy. The results are important with respect
to the transport of small embedded particles in disks.
",0,1,0,0,0,0
18886,Optimal Algorithms for Distributed Optimization,"  In this paper, we study the optimal convergence rate for distributed convex
optimization problems in networks. We model the communication restrictions
imposed by the network as a set of affine constraints and provide optimal
complexity bounds for four different setups, namely: the function $F(\xb)
\triangleq \sum_{i=1}^{m}f_i(\xb)$ is strongly convex and smooth, either
strongly convex or smooth or just convex. Our results show that Nesterov's
accelerated gradient descent on the dual problem can be executed in a
distributed manner and obtains the same optimal rates as in the centralized
version of the problem (up to constant or logarithmic factors) with an
additional cost related to the spectral gap of the interaction matrix. Finally,
we discuss some extensions to the proposed setup such as proximal friendly
functions, time-varying graphs, improvement of the condition numbers.
",1,0,0,1,0,0
18887,Evaluation of Classical Features and Classifiers in Brain-Computer Interface Tasks,"  Brain-Computer Interface (BCI) uses brain signals in order to provide a new
method for communication between human and outside world. Feature extraction,
selection and classification are among the main matters of concerns in signal
processing stage of BCI. In this article, we present our findings about the
most effective features and classifiers in some brain tasks. Six different
groups of classical features and twelve classifiers have been examined in nine
datasets of brain signal. The results indicate that energy of brain signals in
{\alpha} and \b{eta} frequency bands, together with some statistical parameters
are more effective, comparing to the other types of extracted features. In
addition, Bayesian classifier with Gaussian distribution assumption and also
Support Vector Machine (SVM) show to classify different BCI datasets more
accurately than the other classifiers. We believe that the results can give an
insight about a strategy for blind classification of brain signals in
brain-computer interface.
",1,0,0,1,0,0
18888,On exceptional compact homogeneous geometries of type C3,"  We provide a uniform framework to study the exceptional homogeneous compact
geometries of type C3. This framework is then used to show that these are
simply connected, answering a question by Kramer and Lytchak, and to calculate
the full automorphism groups.
",0,0,1,0,0,0
18889,Intensity estimation of transaction arrivals on the intraday electricity market,"  In the following paper we present a simple intensity estimation method of
transaction arrivals on the intraday electricity market. Assuming the
interarrival times distribution, we utilize a maximum likelihood estimation.
The method's performance is briefly tested using German Intraday Continuous
data. Despite the simplicity of the method, the results are encouraging. The
supplementary materials containing the R-codes and the data are attached to
this paper.
",0,0,0,1,0,1
18890,Capacitive Mechanism of Oxygen Functional Groups on Carbon Surface in Supercapacitors,"  Oxygen functional groups are one of the most important subjects in the study
of electrochemical properties of carbon materials which can change the
wettability, conductivity and pore size distributions of carbon materials, and
can occur redox reactions. In the electrode materials of carbon-based
supercapacitors, the oxygen functional groups have widely been used to improve
the capacitive performance. In this paper, we not only analyzed the reasons for
the increase of the capacity that promoted by oxygen functional groups in the
charge-discharge cycling tests, but also analyzed the mechanism how the
pseudocapacitance was provided by the oxygen functional groups in the
acid/alkaline aqueous electrolyte. Moreover, we also discussed the effect of
the oxygen functional groups in electrochemical impedance spectroscopy.
",0,1,0,0,0,0
18891,On conditional least squares estimation for affine diffusions based on continuous time observations,"  We study asymptotic properties of conditional least squares estimators for
the drift parameters of two-factor affine diffusions based on continuous time
observations. We distinguish three cases: subcritical, critical and
supercritical. For all the drift parameters, in the subcritical and
supercritical cases, asymptotic normality and asymptotic mixed normality is
proved, while in the critical case, non-standard asymptotic behavior is
described.
",0,0,1,1,0,0
18892,Discretization error estimates for penalty formulations of a linearized Canham-Helfrich type energy,"  This paper is concerned with minimization of a fourth-order linearized
Canham-Helfrich energy subject to Dirichlet boundary conditions on curves
inside the domain. Such problems arise in the modeling of the mechanical
interaction of biomembranes with embedded particles. There, the curve
conditions result from the imposed particle--membrane coupling. We prove
almost-$H^{\frac{5}{2}}$ regularity of the solution and then consider two
possible penalty formulations. For the combination of these penalty
formulations with a Bogner-Fox-Schmit finite element discretization we prove
discretization error estimates which are optimal in view of the solution's
reduced regularity. The error estimates are based on a general estimate for
linear penalty problems in Hilbert spaces. Finally, we illustrate the
theoretical results by numerical computations. An important feature of the
presented discretization is that it does not require to resolve the particle
boundary. This is crucial in order to avoid re-meshing if the presented problem
arises as subproblem in a model where particles are allowed to move or rotate.
",0,0,1,0,0,0
18893,On the Humphreys conjecture on support varieties of tilting modules,"  Let $G$ be a simply-connected semisimple algebraic group over an
algebraically closed field of characteristic $p$, assumed to be larger than the
Coxeter number. The ""support variety"" of a $G$-module $M$ is a certain closed
subvariety of the nilpotent cone of $G$, defined in terms of cohomology for the
first Frobenius kernel $G_1$. In the 1990s, Humphreys proposed a conjectural
description of the support varieties of tilting modules; this conjecture has
been proved for $G = \mathrm{SL}_n$ in earlier work of the second author.
In this paper, we show that for any $G$, the support variety of a tilting
module always contains the variety predicted by Humphreys, and that they
coincide (i.e., the Humphreys conjecture is true) when $p$ is sufficiently
large. We also prove variants of these statements involving ""relative support
varieties.""
",0,0,1,0,0,0
18894,Using Randomness to Improve Robustness of Machine-Learning Models Against Evasion Attacks,"  Machine learning models have been widely used in security applications such
as intrusion detection, spam filtering, and virus or malware detection.
However, it is well-known that adversaries are always trying to adapt their
attacks to evade detection. For example, an email spammer may guess what
features spam detection models use and modify or remove those features to avoid
detection. There has been some work on making machine learning models more
robust to such attacks. However, one simple but promising approach called {\em
randomization} is underexplored. This paper proposes a novel
randomization-based approach to improve robustness of machine learning models
against evasion attacks. The proposed approach incorporates randomization into
both model training time and model application time (meaning when the model is
used to detect attacks). We also apply this approach to random forest, an
existing ML method which already has some degree of randomness. Experiments on
intrusion detection and spam filtering data show that our approach further
improves robustness of random-forest method. We also discuss how this approach
can be applied to other ML models.
",0,0,0,1,0,0
18895,Further extension of the generalized Hurwitz-Lerch Zeta function of two variables,"  The main aim of this paper is to give a new generalization of Hurwitz-Lerch
Zeta function of two variables.Also, we investigate several interesting
properties such as integral representations, summation formula and a connection
with generalized hypergeometric function. To strengthen the main results we
also consider many important special cases.
",0,0,1,0,0,0
18896,A Physarum-inspired model for the probit-based stochastic user equilibrium problem,"  Stochastic user equilibrium is an important issue in the traffic assignment
problems, tradition models for the stochastic user equilibrium problem are
designed as mathematical programming problems. In this article, a
Physarum-inspired model for the probit-based stochastic user equilibrium
problem is proposed. There are two main contributions of our work. On the one
hand, the origin Physarum model is modified to find the shortest path in
traffic direction networks with the properties of two-way traffic
characteristic. On the other hand, the modified Physarum-inspired model could
get the equilibrium flows when traveller's perceived transportation cost
complies with normal distribution. The proposed method is constituted with a
two-step procedure. First, the modified Physarum model is applied to get the
auxiliary flows. Second, the auxiliary flows are averaged to obtain the
equilibrium flows. Numerical examples are conducted to illustrate the
performance of the proposed method, which is compared with the Method of
Successive Average method.
",1,0,0,0,0,0
18897,Going Higher in First-Order Quantifier Alternation Hierarchies on Words,"  We investigate quantifier alternation hierarchies in first-order logic on
finite words. Levels in these hierarchies are defined by counting the number of
quantifier alternations in formulas. We prove that one can decide membership of
a regular language in the levels $\mathcal{B}{\Sigma}_2$ (finite boolean
combinations of formulas having only one alternation) and ${\Sigma}_3$
(formulas having only two alternations and beginning with an existential
block). Our proofs work by considering a deeper problem, called separation,
which, once solved for lower levels, allows us to solve membership for higher
levels.
",1,0,0,0,0,0
18898,Invariance of Ideal Limit Points,"  Let $\mathcal{I}$ be an analytic P-ideal [respectively, a summable ideal] on
the positive integers and let $(x_n)$ be a sequence taking values in a metric
space $X$. First, it is shown that the set of ideal limit points of $(x_n)$ is
an $F_\sigma$-set [resp., a closet set]. Let us assume that $X$ is also
separable and the ideal $\mathcal{I}$ satisfies certain additional assumptions,
which however includes several well-known examples, e.g., the collection of
sets with zero asymptotic density, sets with zero logarithmic density, and some
summable ideals. Then, it is shown that the set of ideal limit points of
$(x_n)$ is equal to the set of ideal limit points of almost all its
subsequences.
",0,0,1,0,0,0
18899,"Optimizing the Wisdom of the Crowd: Inference, Learning, and Teaching","  The unprecedented demand for large amount of data has catalyzed the trend of
combining human insights with machine learning techniques, which facilitate the
use of crowdsourcing to enlist label information both effectively and
efficiently. The classic work on crowdsourcing mainly focuses on the label
inference problem under the categorization setting. However, inferring the true
label requires sophisticated aggregation models that usually can only perform
well under certain assumptions. Meanwhile, no matter how complicated the
aggregation model is, the true model that generated the crowd labels remains
unknown. Therefore, the label inference problem can never infer the ground
truth perfectly. Based on the fact that the crowdsourcing labels are abundant
and utilizing aggregation will lose such kind of rich annotation information
(e.g., which worker provided which labels), we believe that it is critical to
take the diverse labeling abilities of the crowdsourcing workers as well as
their correlations into consideration. To address the above challenge, we
propose to tackle three research problems, namely inference, learning, and
teaching.
",0,0,0,1,0,0
18900,Learning With Errors and Extrapolated Dihedral Cosets,"  The hardness of the learning with errors (LWE) problem is one of the most
fruitful resources of modern cryptography. In particular, it is one of the most
prominent candidates for secure post-quantum cryptography. Understanding its
quantum complexity is therefore an important goal. We show that under quantum
polynomial time reductions, LWE is equivalent to a relaxed version of the
dihedral coset problem (DCP), which we call extrapolated DCP (eDCP). The extent
of extrapolation varies with the LWE noise rate. By considering different
extents of extrapolation, our result generalizes Regev's famous proof that if
DCP is in BQP (quantum poly-time) then so is LWE (FOCS'02). We also discuss a
connection between eDCP and Childs and Van Dam's algorithm for generalized
hidden shift problems (SODA'07). Our result implies that a BQP solution for LWE
might not require the full power of solving DCP, but rather only a solution for
its relaxed version, eDCP, which could be easier.
",1,0,0,0,0,0
18901,Fermi bubbles: high latitude X-ray supersonic shell,"  The nature of the bipolar, $\gamma$-ray Fermi bubbles (FB) is still unclear,
in part because their faint, high-latitude X-ray counterpart has until now
eluded a clear detection. We stack ROSAT data at varying distances from the FB
edges, thus boosting the signal and identifying an expanding shell behind the
southwest, southeast, and northwest edges, albeit not in the dusty northeast
sector near Loop I. A Primakoff-like model for the underlying flow is invoked
to show that the signals are consistent with halo gas heated by a strong,
forward shock to $\sim$keV temperatures. Assuming ion--electron thermal
equilibrium then implies a $\sim10^{56}$ erg event near the Galactic centre
$\sim7$ Myr ago. However, the reported high absorption-line velocities suggest
a preferential shock-heating of ions, and thus more energetic ($\sim 10^{57}$
erg), younger ($\lesssim 3$ Myr) FBs.
",0,1,0,0,0,0
18902,Passivity-Based Generalization of Primal-Dual Dynamics for Non-Strictly Convex Cost Functions,"  In this paper, we revisit primal-dual dynamics for convex optimization and
present a generalization of the dynamics based on the concept of passivity. It
is then proved that supplying a stable zero to one of the integrators in the
dynamics allows one to eliminate the assumption of strict convexity on the cost
function based on the passivity paradigm together with the invariance principle
for Caratheodory systems. We then show that the present algorithm is also a
generalization of existing augmented Lagrangian-based primal-dual dynamics, and
discuss the benefit of the present generalization in terms of noise reduction
and convergence speed.
",1,0,0,0,0,0
18903,"An integral formula for the powered sum of the independent, identically and normally distributed random variables","  The distribution of the sum of r-th power of standard normal random variables
is a generalization of the chi-squared distribution. In this paper, we
represent the probability density function of the random variable by an
one-dimensional absolutely convergent integral with the characteristic
function. Our integral formula is expected to be applied for evaluation of the
density function. Our integral formula is based on the inversion formula, and
we utilize a summation method. We also discuss on our formula in the view point
of hyperfunctions.
",0,0,1,0,0,0
18904,A Dynamic Boosted Ensemble Learning Method Based on Random Forest,"  We propose a dynamic boosted ensemble learning method based on random forest
(DBRF), a novel ensemble algorithm that incorporates the notion of hard example
mining into Random Forest (RF) and thus combines the high accuracy of Boosting
algorithm with the strong generalization of Bagging algorithm. Specifically, we
propose to measure the quality of each leaf node of every decision tree in the
random forest to determine hard examples. By iteratively training and then
removing easy examples from training data, we evolve the random forest to focus
on hard examples dynamically so as to learn decision boundaries better. Data
can be cascaded through these random forests learned in each iteration in
sequence to generate predictions, thus making RF deep. We also propose to use
evolution mechanism and smart iteration mechanism to improve the performance of
the model. DBRF outperforms RF on three UCI datasets and achieved
state-of-the-art results compared to other deep models. Moreover, we show that
DBRF is also a new way of sampling and can be very useful when learning from
imbalanced data.
",0,0,0,1,0,0
18905,"Inter-Operator Resource Management for Millimeter Wave, Multi-Hop Backhaul Networks","  In this paper, a novel framework is proposed for optimizing the operation and
performance of a large-scale, multi-hop millimeter wave (mmW) backhaul within a
wireless small cell network (SCN) that encompasses multiple mobile network
operators (MNOs). The proposed framework enables the small base stations (SBSs)
to jointly decide on forming the multi-hop, mmW links over backhaul
infrastructure that belongs to multiple, independent MNOs, while properly
allocating resources across those links. In this regard, the problem is
addressed using a novel framework based on matching theory that is composed to
two, highly inter-related stages: a multi-hop network formation stage and a
resource management stage. One unique feature of this framework is that it
jointly accounts for both wireless channel characteristics and economic factors
during both network formation and resource management. The multi-hop network
formation stage is formulated as a one-to-many matching game which is solved
using a novel algorithm, that builds on the so-called deferred acceptance
algorithm and is shown to yield a stable and Pareto optimal multi-hop mmW
backhaul network. Then, a one-to-many matching game is formulated to enable
proper resource allocation across the formed multi-hop network. This game is
then shown to exhibit peer effects and, as such, a novel algorithm is developed
to find a stable and optimal resource management solution that can properly
cope with these peer effects. Simulation results show that the proposed
framework yields substantial gains, in terms of the average sum rate, reaching
up to 27% and 54%, respectively, compared to a non-cooperative scheme in which
inter-operator sharing is not allowed and a random allocation approach. The
results also show that our framework provides insights on how to manage pricing
and the cost of the cooperative mmW backhaul network for the MNOs.
",1,0,0,0,0,0
18906,An Online Learning Approach to Generative Adversarial Networks,"  We consider the problem of training generative models with a Generative
Adversarial Network (GAN). Although GANs can accurately model complex
distributions, they are known to be difficult to train due to instabilities
caused by a difficult minimax optimization problem. In this paper, we view the
problem of training GANs as finding a mixed strategy in a zero-sum game.
Building on ideas from online learning we propose a novel training method named
Chekhov GAN 1 . On the theory side, we show that our method provably converges
to an equilibrium for semi-shallow GAN architectures, i.e. architectures where
the discriminator is a one layer network and the generator is arbitrary. On the
practical side, we develop an efficient heuristic guided by our theoretical
results, which we apply to commonly used deep GAN architectures. On several
real world tasks our approach exhibits improved stability and performance
compared to standard GAN training.
",1,0,0,1,0,0
18907,Weakly supervised CRNN system for sound event detection with large-scale unlabeled in-domain data,"  Sound event detection (SED) is typically posed as a supervised learning
problem requiring training data with strong temporal labels of sound events.
However, the production of datasets with strong labels normally requires
unaffordable labor cost. It limits the practical application of supervised SED
methods. The recent advances in SED approaches focuses on detecting sound
events by taking advantages of weakly labeled or unlabeled training data. In
this paper, we propose a joint framework to solve the SED task using
large-scale unlabeled in-domain data. In particular, a state-of-the-art general
audio tagging model is first employed to predict weak labels for unlabeled
data. On the other hand, a weakly supervised architecture based on the
convolutional recurrent neural network (CRNN) is developed to solve the strong
annotations of sound events with the aid of the unlabeled data with predicted
labels. It is found that the SED performance generally increases as more
unlabeled data is added into the training. To address the noisy label problem
of unlabeled data, an ensemble strategy is applied to increase the system
robustness. The proposed system is evaluated on the SED dataset of DCASE 2018
challenge. It reaches a F1-score of 21.0%, resulting in an improvement of 10%
over the baseline system.
",1,0,0,0,0,0
18908,Revealing the Coulomb interaction strength in a cuprate superconductor,"  We study optimally doped
Bi$_{2}$Sr$_{2}$Ca$_{0.92}$Y$_{0.08}$Cu$_{2}$O$_{8+\delta}$ (Bi2212) using
angle-resolved two-photon photoemission spectroscopy. Three spectral features
are resolved near 1.5, 2.7, and 3.6 eV above the Fermi level. By tuning the
photon energy, we determine that the 2.7-eV feature arises predominantly from
unoccupied states. The 1.5- and 3.6-eV features reflect unoccupied states whose
spectral intensities are strongly modulated by the corresponding occupied
states. These unoccupied states are consistent with the prediction from a
cluster perturbation theory based on the single-band Hubbard model. Through
this comparison, a Coulomb interaction strength U of 2.7 eV is extracted. Our
study complements equilibrium photoemission spectroscopy and provides a direct
spectroscopic measurement of the unoccupied states in cuprates. The determined
Coulomb U indicates that the charge-transfer gap of optimally doped Bi2212 is
1.1 eV.
",0,1,0,0,0,0
18909,Ordering dynamics of self-propelled particles in an inhomogeneous medium,"  Ordering dynamics of self-propelled particles in an inhomogeneous medium in
two-dimensions is studied. We write coarse-grained hydrodynamic equations of
motion for coarse-grained density and velocity fields in the presence of an
external random disorder field, which is quenched in time. The strength of
inhomogeneity is tuned from zero disorder (clean system) to large disorder. In
the clean system, the velocity field grows algebraically as $L_{\rm V} \sim
t^{0.5}$. The density field does not show clean power-law growth; however, it
follows $L_{\rm \rho} \sim t^{0.8}$ approximately. In the inhomogeneous system,
we find a disorder dependent growth. For both the density and the velocity,
growth slow down with increasing strength of disorder. The velocity shows a
disorder dependent power-law growth $L_{\rm V}(t,\Delta) \sim t^{1/\bar z_{\rm
V}(\Delta)}$ for intermediate times. At late times, there is a crossover to
logarithmic growth $L_{\rm V}(t,\Delta) \sim (\ln t)^{1/\varphi}$, where
$\varphi$ is a disorder independent exponent. Two-point correlation functions
for the velocity shows dynamical scaling, but the density does not.
",0,1,0,0,0,0
18910,Inflationary preheating dynamics with ultracold atoms,"  We discuss the amplification of loop corrections in quantum many-body systems
through dynamical instabilities. As an example, we investigate both
analytically and numerically a two-component ultracold atom system in one
spatial dimension. The model features a tachyonic instability, which
incorporates characteristic aspects of the mechanisms for particle production
in early-universe inflaton models. We establish a direct correspondence between
measureable macroscopic growth rates for occupation numbers of the ultracold
Bose gas and the underlying microscopic processes in terms of Feynman loop
diagrams. We analyze several existing ultracold atom setups featuring dynamical
instabilities and propose optimized protocols for their experimental
realization. We demonstrate that relevant dynamical processes can be enhanced
using a seeding procedure for unstable modes and clarify the role of initial
quantum fluctuations and the generation of a non-linear secondary stage for the
amplification of modes.
",0,1,0,0,0,0
18911,Autonomous Extracting a Hierarchical Structure of Tasks in Reinforcement Learning and Multi-task Reinforcement Learning,"  Reinforcement learning (RL), while often powerful, can suffer from slow
learning speeds, particularly in high dimensional spaces. The autonomous
decomposition of tasks and use of hierarchical methods hold the potential to
significantly speed up learning in such domains. This paper proposes a novel
practical method that can autonomously decompose tasks, by leveraging
association rule mining, which discovers hidden relationship among entities in
data mining. We introduce a novel method called ARM-HSTRL (Association Rule
Mining to extract Hierarchical Structure of Tasks in Reinforcement Learning).
It extracts temporal and structural relationships of sub-goals in RL, and
multi-task RL. In particular,it finds sub-goals and relationship among them. It
is shown the significant efficiency and performance of the proposed method in
two main topics of RL.
",1,0,0,0,0,0
18912,Quantification of tumour evolution and heterogeneity via Bayesian epiallele detection,"  Motivation: Epigenetic heterogeneity within a tumour can play an important
role in tumour evolution and the emergence of resistance to treatment. It is
increasingly recognised that the study of DNA methylation (DNAm) patterns along
the genome -- so-called `epialleles' -- offers greater insight into epigenetic
dynamics than conventional analyses which examine DNAm marks individually.
Results: We have developed a Bayesian model to infer which epialleles are
present in multiple regions of the same tumour. We apply our method to reduced
representation bisulfite sequencing (RRBS) data from multiple regions of one
lung cancer tumour and a matched normal sample. The model borrows information
from all tumour regions to leverage greater statistical power. The total number
of epialleles, the epiallele DNAm patterns, and a noise hyperparameter are all
automatically inferred from the data. Uncertainty as to which epiallele an
observed sequencing read originated from is explicitly incorporated by
marginalising over the appropriate posterior densities. The degree to which
tumour samples are contaminated with normal tissue can be estimated and
corrected for. By tracing the distribution of epialleles throughout the tumour
we can infer the phylogenetic history of the tumour, identify epialleles that
differ between normal and cancer tissue, and define a measure of global
epigenetic disorder.
",0,0,1,1,0,0
18913,A bootstrap test to detect prominent Granger-causalities across frequencies,"  Granger-causality in the frequency domain is an emerging tool to analyze the
causal relationship between two time series. We propose a bootstrap test on
unconditional and conditional Granger-causality spectra, as well as on their
difference, to catch particularly prominent causality cycles in relative terms.
In particular, we consider a stochastic process derived applying independently
the stationary bootstrap to the original series. Our null hypothesis is that
each causality or causality difference is equal to the median across
frequencies computed on that process. In this way, we are able to disambiguate
causalities which depart significantly from the median one obtained ignoring
the causality structure. Our test shows power one as the process tends to
non-stationarity, thus being more conservative than parametric alternatives. As
an example, we infer about the relationship between money stock and GDP in the
Euro Area via our approach, considering inflation, unemployment and interest
rates as conditioning variables. We point out that during the period 1999-2017
the money stock aggregate M1 had a significant impact on economic output at all
frequencies, while the opposite relationship is significant only at high
frequencies.
",0,0,0,1,0,1
18914,Optimal segregation of proteins: phase transitions and symmetry breaking,"  Asymmetric segregation of key proteins at cell division -- be it a beneficial
or deleterious protein -- is ubiquitous in unicellular organisms and often
considered as an evolved trait to increase fitness in a stressed environment.
Here, we provide a general framework to describe the evolutionary origin of
this asymmetric segregation. We compute the population fitness as a function of
the protein segregation asymmetry $a$, and show that the value of $a$ which
optimizes the population growth manifests a phase transition between symmetric
and asymmetric partitioning phases. Surprisingly, the nature of phase
transition is different for the case of beneficial proteins as opposed to
proteins which decrease the single-cell growth rate. Our study elucidates the
optimization problem faced by evolution in the context of protein segregation,
and motivates further investigation of asymmetric protein segregation in
biological systems.
",0,0,0,0,1,0
18915,Optimal make-take fees for market making regulation,"  We consider an exchange who wishes to set suitable make-take fees to attract
liquidity on its platform. Using a principal-agent approach, we are able to
describe in quasi-explicit form the optimal contract to propose to a market
maker. This contract depends essentially on the market maker inventory
trajectory and on the volatility of the asset. We also provide the optimal
quotes that should be displayed by the market maker. The simplicity of our
formulas allows us to analyze in details the effects of optimal contracting
with an exchange, compared to a situation without contract. We show in
particular that it leads to higher quality liquidity and lower trading costs
for investors.
",0,0,0,0,0,1
18916,Spontaneous symmetry breaking due to the trade-off between attractive and repulsive couplings,"  Spontaneous symmetry breaking (SSB) is an important phenomenon observed in
various fields including physics and biology. In this connection, we here show
that the trade-off between attractive and repulsive couplings can induce
spontaneous symmetry breaking in a homogeneous system of coupled oscillators.
With a simple model of a system of two coupled Stuart-Landau oscillators, we
demonstrate how the tendency of attractive coupling in inducing in-phase
synchronized (IPS) oscillations and the tendency of repulsive coupling in
inducing out-of-phase synchronized (OPS) oscillations compete with each other
and give rise to symmetry breaking oscillatory (SBO) states and interesting
multistabilities. Further, we provide explicit expressions for synchronized and
anti-synchronized oscillatory states as well as the so called oscillation death
(OD) state and study their stability. If the Hopf bifurcation parameter
(${\lambda}$) is greater than the natural frequency ($\omega$) of the system,
the attractive coupling favours the emergence of an anti-symmetric OD state via
a Hopf bifurcation whereas the repulsive coupling favours the emergence of a
similar state through a saddle-node bifurcation. We show that an increase in
the repulsive coupling not only destabilizes the IPS state but also facilitates
the re-entrance of the IPS state.
",0,1,0,0,0,0
18917,Evolutionary Image Composition Using Feature Covariance Matrices,"  Evolutionary algorithms have recently been used to create a wide range of
artistic work. In this paper, we propose a new approach for the composition of
new images from existing ones, that retain some salient features of the
original images. We introduce evolutionary algorithms that create new images
based on a fitness function that incorporates feature covariance matrices
associated with different parts of the images. This approach is very flexible
in that it can work with a wide range of features and enables targeting
specific regions in the images. For the creation of the new images, we propose
a population-based evolutionary algorithm with mutation and crossover operators
based on random walks. Our experimental results reveal a spectrum of
aesthetically pleasing images that can be obtained with the aid of our
evolutionary process.
",1,0,0,0,0,0
18918,Community Detection with Colored Edges,"  In this paper, we prove a sharp limit on the community detection problem with
colored edges. We assume two equal-sized communities and there are $m$
different types of edges. If two vertices are in the same community, the
distribution of edges follows $p_i=\alpha_i\log{n}/n$ for $1\leq i \leq m$,
otherwise the distribution of edges is $q_i=\beta_i\log{n}/n$ for $1\leq i \leq
m$, where $\alpha_i$ and $\beta_i$ are positive constants and $n$ is the total
number of vertices. Under these assumptions, a fundamental limit on community
detection is characterized using the Hellinger distance between the two
distributions. If $\sum_{i=1}^{m} {(\sqrt{\alpha_i} - \sqrt{\beta_i})}^2 >2$,
then the community detection via maximum likelihood (ML) estimator is possible
with high probability. If $\sum_{i=1}^m {(\sqrt{\alpha_i} - \sqrt{\beta_i})}^2
< 2$, the probability that the ML estimator fails to detect the communities
does not go to zero.
",1,1,0,0,0,0
18919,"Nonlocal Neural Networks, Nonlocal Diffusion and Nonlocal Modeling","  Nonlocal neural networks have been proposed and shown to be effective in
several computer vision tasks, where the nonlocal operations can directly
capture long-range dependencies in the feature space. In this paper, we study
the nature of diffusion and damping effect of nonlocal networks by doing
spectrum analysis on the weight matrices of the well-trained networks, and then
propose a new formulation of the nonlocal block. The new block not only learns
the nonlocal interactions but also has stable dynamics, thus allowing deeper
nonlocal structures. Moreover, we interpret our formulation from the general
nonlocal modeling perspective, where we make connections between the proposed
nonlocal network and other nonlocal models, such as nonlocal diffusion process
and Markov jump process.
",0,0,0,1,0,0
18920,Proof of FLT by Algebra Identities and Linear Algebra,"  The main aim of the present paper is to represent an exact and simple proof
for FLT by using properties of the algebra identities and linear algebra.
",0,0,1,0,0,0
18921,An Observational Diagnostic for Distinguishing Between Clouds and Haze in Hot Exoplanet Atmospheres,"  The nature of aerosols in hot exoplanet atmospheres is one of the primary
vexing questions facing the exoplanet field. The complex chemistry, multiple
formation pathways, and lack of easily identifiable spectral features
associated with aerosols make it especially challenging to constrain their key
properties. We propose a transmission spectroscopy technique to identify the
primary aerosol formation mechanism for the most highly irradiated hot Jupiters
(HIHJs). The technique is based on the expectation that the two key types of
aerosols -- photochemically generated hazes and equilibrium condensate clouds
-- are expected to form and persist in different regions of a highly irradiated
planet's atmosphere. Haze can only be produced on the permanent daysides of
tidally-locked hot Jupiters, and will be carried downwind by atmospheric
dynamics to the evening terminator (seen as the trailing limb during transit).
Clouds can only form in cooler regions on the night side and morning terminator
of HIHJs (seen as the leading limb during transit). Because opposite limbs are
expected to be impacted by different types of aerosols, ingress and egress
spectra, which primarily probe opposing sides of the planet, will reveal the
dominant aerosol formation mechanism. We show that the benchmark HIHJ,
WASP-121b, has a transmission spectrum consistent with partial aerosol coverage
and that ingress-egress spectroscopy would constrain the location and formation
mechanism of those aerosols. In general, using this diagnostic we find that
observations with JWST and potentially with HST should be able to distinguish
between clouds and haze for currently known HIHJs.
",0,1,0,0,0,0
18922,$\mathcal{P}$-schemes and Deterministic Polynomial Factoring over Finite Fields,"  We introduce a family of mathematical objects called $\mathcal{P}$-schemes,
where $\mathcal{P}$ is a poset of subgroups of a finite group $G$. A
$\mathcal{P}$-scheme is a collection of partitions of the right coset spaces
$H\backslash G$, indexed by $H\in\mathcal{P}$, that satisfies a list of axioms.
These objects generalize the classical notion of association schemes as well as
the notion of $m$-schemes (Ivanyos et al. 2009).
Based on $\mathcal{P}$-schemes, we develop a unifying framework for the
problem of deterministic factoring of univariate polynomials over finite fields
under the generalized Riemann hypothesis (GRH).
",1,0,1,0,0,0
18923,Similarity Preserving Representation Learning for Time Series Analysis,"  A considerable amount of machine learning algorithms take instance-feature
matrices as their inputs. As such, they cannot directly analyze time series
data due to its temporal nature, usually unequal lengths, and complex
properties. This is a great pity since many of these algorithms are effective,
robust, efficient, and easy to use. In this paper, we bridge this gap by
proposing an efficient representation learning framework that is able to
convert a set of time series with equal or unequal lengths to a matrix format.
In particular, we guarantee that the pairwise similarities between time series
are well preserved after the transformation. The learned feature representation
is particularly suitable to the class of learning problems that are sensitive
to data similarities. Given a set of $n$ time series, we first construct an
$n\times n$ partially observed similarity matrix by randomly sampling $O(n \log
n)$ pairs of time series and computing their pairwise similarities. We then
propose an extremely efficient algorithm that solves a highly non-convex and
NP-hard problem to learn new features based on the partially observed
similarity matrix. We use the learned features to conduct experiments on both
data classification and clustering tasks. Our extensive experimental results
demonstrate that the proposed framework is both effective and efficient.
",1,0,0,0,0,0
18924,Modeling Social Organizations as Communication Networks,"  We identify the ""organization"" of a human social group as the communication
network(s) within that group. We then introduce three theoretical approaches to
analyzing what determines the structures of human organizations. All three
approaches adopt a group-selection perspective, so that the group's network
structure is (approximately) optimal, given the information-processing
limitations of agents within the social group, and the exogenous welfare
function of the overall group. In the first approach we use a new sub-field of
telecommunications theory called network coding, and focus on a welfare
function that involves the ability of the organization to convey information
among the agents. In the second approach we focus on a scenario where agents
within the organization must allocate their future communication resources when
the state of the future environment is uncertain. We show how this formulation
can be solved with a linear program. In the third approach, we introduce an
information synthesis problem in which agents within an organization receive
information from various sources and must decide how to transform such
information and transmit the results to other agents in the organization. We
propose leveraging the computational power of neural networks to solve such
problems. These three approaches formalize and synthesize work in fields
including anthropology, archeology, economics and psychology that deal with
organization structure, theory of the firm, span of control and cognitive
limits on communication.
",1,1,0,0,0,0
18925,The Ricci flow on solvmanifolds of real type,"  We show that for any solvable Lie group of real type, any homogeneous Ricci
flow solution converges in Cheeger-Gromov topology to a unique non-flat
solvsoliton, which is independent of the initial left-invariant metric. As an
application, we obtain results on the isometry groups of non-flat solvsoliton
metrics and Einstein solvmanifolds.
",0,0,1,0,0,0
18926,Model-based clustering of multi-tissue gene expression data,"  Recently, it has become feasible to generate large-scale, multi-tissue gene
expression data, where expression profiles are obtained from multiple tissues
or organs sampled from dozens to hundreds of individuals. When traditional
clustering methods are applied to this type of data, important information is
lost, because they either require all tissues to be analyzed independently,
ignoring dependencies and similarities between tissues, or to merge tissues in
a single, monolithic dataset, ignoring individual characteristics of tissues.
We developed a Bayesian model-based multi-tissue clustering algorithm, revamp,
which can incorporate prior information on physiological tissue similarity, and
which results in a set of clusters, each consisting of a core set of genes
conserved across tissues as well as differential sets of genes specific to one
or more subsets of tissues. Using data from seven vascular and metabolic
tissues from over 100 individuals in the STockholm Atherosclerosis Gene
Expression (STAGE) study, we demonstrate that multi-tissue clusters inferred by
revamp are more enriched for tissue-dependent protein-protein interactions
compared to alternative approaches. We further demonstrate that revamp results
in easily interpretable multi-tissue gene expression associations to key
coronary artery disease processes and clinical phenotypes in the STAGE
individuals. Revamp is implemented in the Lemon-Tree software, available at
this https URL
",0,0,0,0,1,0
18927,Fastest Convergence for Q-learning,"  The Zap Q-learning algorithm introduced in this paper is an improvement of
Watkins' original algorithm and recent competitors in several respects. It is a
matrix-gain algorithm designed so that its asymptotic variance is optimal.
Moreover, an ODE analysis suggests that the transient behavior is a close match
to a deterministic Newton-Raphson implementation. This is made possible by a
two time-scale update equation for the matrix gain sequence.
The analysis suggests that the approach will lead to stable and efficient
computation even for non-ideal parameterized settings. Numerical experiments
confirm the quick convergence, even in such non-ideal cases.
A secondary goal of this paper is tutorial. The first half of the paper
contains a survey on reinforcement learning algorithms, with a focus on minimum
variance algorithms.
",1,0,1,0,0,0
18928,New Bounds on the Field Size for Maximally Recoverable Codes Instantiating Grid-like Topologies,"  In recent years, the rapidly increasing amounts of data created and processed
through the internet resulted in distributed storage systems employing erasure
coding based schemes. Aiming to balance the tradeoff between data recovery for
correlated failures and efficient encoding and decoding, distributed storage
systems employing maximally recoverable codes came up. Unifying a number of
topologies considered both in theory and practice, Gopalan \cite{Gopalan2017}
initiated the study of maximally recoverable codes for grid-like topologies.
In this paper, we focus on the maximally recoverable codes that instantiate
grid-like topologies $T_{m\times n}(1,b,0)$. To characterize the property of
codes for these topologies, we introduce the notion of \emph{pseudo-parity
check matrix}. Then, using the hypergraph independent set approach, we
establish the first polynomial upper bound on the field size needed for
achieving the maximal recoverability in topologies $T_{m\times n}(1,b,0)$, when
$n$ is large enough. And we further improve this general upper bound for
topologies $T_{4\times n}(1,2,0)$ and $T_{3\times n}(1,3,0)$. By relating the
problem to generalized \emph{Sidon sets} in $\mathbb{F}_q$, we also obtain
non-trivial lower bounds on the field size for maximally recoverable codes that
instantiate topologies $T_{4\times n}(1,2,0)$ and $T_{3\times n}(1,3,0)$.
",1,0,0,0,0,0
18929,Two-Step Disentanglement for Financial Data,"  In this work, we address the problem of disentanglement of factors that
generate a given data into those that are correlated with the labeling and
those that are not. Our solution is simpler than previous solutions and employs
adversarial training in a straightforward manner. We demonstrate the new method
on visual datasets as well as on financial data. In order to evaluate the
latter, we developed a hypothetical trading strategy whose performance is
affected by the performance of the disentanglement, namely, it trades better
when the factors are better separated.
",1,0,0,1,0,0
18930,Optimal control of a Vlasov-Poisson plasma by an external magnetic field - Analysis of a tracking type optimal control problem,"  In the paper ""Optimal control of a Vlasov-Poisson plasma by an external
magnetic field - The basics for variational calculus"" [arXiv:1708.02464] we
have already introduced a set of admissible magnetic fields and we have proved
that each of those fields induces a unique strong solution of the
Vlasov-Poisson system. We have also established that the field-state operator
that maps any admissible field onto its corresponding solution is continuous
and weakly compact. In this paper we will show that this operator is also
Fréchet differentiable and we will continue to analyze the optimal control
problem that was introduced in [arXiv:1708.02464]. More precisely, we will
establish necessary and sufficient conditions for local optimality and we will
show that an optimal solution is unique under certain conditions.
",0,0,1,0,0,0
18931,Towards an algebraic natural proofs barrier via polynomial identity testing,"  We observe that a certain kind of algebraic proof - which covers essentially
all known algebraic circuit lower bounds to date - cannot be used to prove
lower bounds against VP if and only if what we call succinct hitting sets exist
for VP. This is analogous to the Razborov-Rudich natural proofs barrier in
Boolean circuit complexity, in that we rule out a large class of lower bound
techniques under a derandomization assumption. We also discuss connections
between this algebraic natural proofs barrier, geometric complexity theory, and
(algebraic) proof complexity.
",1,0,1,0,0,0
18932,In Defense of the Indefensible: A Very Naive Approach to High-Dimensional Inference,"  In recent years, a great deal of interest has focused on conducting inference
on the parameters in a linear model in the high-dimensional setting. In this
paper, we consider a simple and very naïve two-step procedure for this
task, in which we (i) fit a lasso model in order to obtain a subset of the
variables; and (ii) fit a least squares model on the lasso-selected set.
Conventional statistical wisdom tells us that we cannot make use of the
standard statistical inference tools for the resulting least squares model
(such as confidence intervals and $p$-values), since we peeked at the data
twice: once in running the lasso, and again in fitting the least squares model.
However, in this paper, we show that under a certain set of assumptions, with
high probability, the set of variables selected by the lasso is deterministic.
Consequently, the naïve two-step approach can yield confidence intervals
that have asymptotically correct coverage, as well as p-values with proper
Type-I error control. Furthermore, this two-step approach unifies two existing
camps of work on high-dimensional inference: one camp has focused on inference
based on a sub-model selected by the lasso, and the other has focused on
inference using a debiased version of the lasso estimator.
",0,0,1,1,0,0
18933,Asymptotic Analysis of Plausible Tree Hash Modes for SHA-3,"  Discussions about the choice of a tree hash mode of operation for a
standardization have recently been undertaken. It appears that a single tree
mode cannot address adequately all possible uses and specifications of a
system. In this paper, we review the tree modes which have been proposed, we
discuss their problems and propose remedies. We make the reasonable assumption
that communicating systems have different specifications and that software
applications are of different types (securing stored content or live-streamed
content). Finally, we propose new modes of operation that address the resource
usage problem for the three most representative categories of devices and we
analyse their asymptotic behavior.
",1,0,0,0,0,0
18934,Neurally Plausible Model of Robot Reaching Inspired by Infant Motor Babbling,"  In this paper we present a neurally plausible model of robot reaching
inspired by human infant reaching that is based on embodied artificial
intelligence, which emphasizes the importance of the sensory-motor interaction
of an agent and the world. This model encompasses both learning sensory-motor
correlations through motor babbling and also arm motion planning using
spreading activation. This model is organized in three layers of neural maps
with parallel structures representing the same sensory-motor space. The motor
babbling period shapes the structure of the three neural maps as well as the
connections within and between them. We describe an implementation of this
model and an investigation of this implementation using a simple reaching task
on a humanoid robot. The robot has learned successfully to plan reaching
motions from a test set with high accuracy and smoothness.
",1,0,0,0,0,0
18935,When Will AI Exceed Human Performance? Evidence from AI Experts,"  Advances in artificial intelligence (AI) will transform modern life by
reshaping transportation, health, science, finance, and the military. To adapt
public policy, we need to better anticipate these advances. Here we report the
results from a large survey of machine learning researchers on their beliefs
about progress in AI. Researchers predict AI will outperform humans in many
activities in the next ten years, such as translating languages (by 2024),
writing high-school essays (by 2026), driving a truck (by 2027), working in
retail (by 2031), writing a bestselling book (by 2049), and working as a
surgeon (by 2053). Researchers believe there is a 50% chance of AI
outperforming humans in all tasks in 45 years and of automating all human jobs
in 120 years, with Asian respondents expecting these dates much sooner than
North Americans. These results will inform discussion amongst researchers and
policymakers about anticipating and managing trends in AI.
",1,0,0,0,0,0
18936,E-PUR: An Energy-Efficient Processing Unit for Recurrent Neural Networks,"  Recurrent Neural Networks (RNNs) are a key technology for emerging
applications such as automatic speech recognition, machine translation or image
description. Long Short Term Memory (LSTM) networks are the most successful RNN
implementation, as they can learn long term dependencies to achieve high
accuracy. Unfortunately, the recurrent nature of LSTM networks significantly
constrains the amount of parallelism and, hence, multicore CPUs and many-core
GPUs exhibit poor efficiency for RNN inference. In this paper, we present
E-PUR, an energy-efficient processing unit tailored to the requirements of LSTM
computation. The main goal of E-PUR is to support large recurrent neural
networks for low-power mobile devices. E-PUR provides an efficient hardware
implementation of LSTM networks that is flexible to support diverse
applications. One of its main novelties is a technique that we call Maximizing
Weight Locality (MWL), which improves the temporal locality of the memory
accesses for fetching the synaptic weights, reducing the memory requirements by
a large extent. Our experimental results show that E-PUR achieves real-time
performance for different LSTM networks, while reducing energy consumption by
orders of magnitude with respect to general-purpose processors and GPUs, and it
requires a very small chip area. Compared to a modern mobile SoC, an NVIDIA
Tegra X1, E-PUR provides an average energy reduction of 92x.
",1,0,0,0,0,0
18937,Increasing Geminid meteor shower activity,"  Mathematical modelling has shown that activity of the Geminid meteor shower
should rise with time, and that was confirmed by analysis of visual
observations 1985--2016. We do not expect any outburst activity of the Geminid
shower in 2017, even though the asteroid (3200) Phaethon has close approach to
Earth in December of 2017. A small probability to observe dust ejected at
perihelia 2009--2016 still exists.
",0,1,0,0,0,0
18938,The Bennett-Orlicz norm,"  Lederer and van de Geer (2013) introduced a new Orlicz norm, the
Bernstein-Orlicz norm, which is connected to Bernstein type inequalities. Here
we introduce another Orlicz norm, the Bennett-Orlicz norm, which is connected
to Bennett type inequalities. The new Bennett-Orlicz norm yields inequalities
for expectations of maxima which are potentially somewhat tighter than those
resulting from the Bernstein-Orlicz norm when they are both applicable. We
discuss cross connections between these norms, exponential inequalities of the
Bernstein, Bennett, and Prokhorov types, and make comparisons with results of
Talagrand (1989, 1994), and Boucheron, Lugosi, and Massart (2013).
",0,0,1,1,0,0
18939,Temporal Justification Logic,"  Justification logics are modal-like logics with the additional capability of
recording the reason, or justification, for modalities in syntactic structures,
called justification terms. Justification logics can be seen as explicit
counterparts to modal logics. The behavior and interaction of agents in
distributed system is often modeled using logics of knowledge and time. In this
paper, we sketch some preliminary ideas on how the modal knowledge part of such
logics of knowledge and time could be replaced with an appropriate
justification logic.
",1,0,0,0,0,0
18940,Simultaneous Multiparty Communication Complexity of Composed Functions,"  In the Number On the Forehead (NOF) multiparty communication model, $k$
players want to evaluate a function $F : X_1 \times\cdots\times X_k\rightarrow
Y$ on some input $(x_1,\dots,x_k)$ by broadcasting bits according to a
predetermined protocol. The input is distributed in such a way that each player
$i$ sees all of it except $x_i$. In the simultaneous setting, the players
cannot speak to each other but instead send information to a referee. The
referee does not know the players' input, and cannot give any information back.
At the end, the referee must be able to recover $F(x_1,\dots,x_k)$ from what
she obtained.
A central open question, called the $\log n$ barrier, is to find a function
which is hard to compute for $polylog(n)$ or more players (where the $x_i$'s
have size $poly(n)$) in the simultaneous NOF model. This has important
applications in circuit complexity, as it could help to separate $ACC^0$ from
other complexity classes. One of the candidates belongs to the family of
composed functions. The input to these functions is represented by a $k\times
(t\cdot n)$ boolean matrix $M$, whose row $i$ is the input $x_i$ and $t$ is a
block-width parameter. A symmetric composed function acting on $M$ is specified
by two symmetric $n$- and $kt$-variate functions $f$ and $g$, that output
$f\circ g(M)=f(g(B_1),\dots,g(B_n))$ where $B_j$ is the $j$-th block of width
$t$ of $M$. As the majority function $MAJ$ is conjectured to be outside of
$ACC^0$, Babai et. al. suggested to study $MAJ\circ MAJ_t$, with $t$ large
enough.
So far, it was only known that $t=1$ is not enough for $MAJ\circ MAJ_t$ to
break the $\log n$ barrier in the simultaneous deterministic NOF model. In this
paper, we extend this result to any constant block-width $t>1$, by giving a
protocol of cost $2^{O(2^t)}\log^{2^{t+1}}(n)$ for any symmetric composed
function when there are $2^{\Omega(2^t)}\log n$ players.
",1,0,0,0,0,0
18941,Online Learning for Changing Environments using Coin Betting,"  A key challenge in online learning is that classical algorithms can be slow
to adapt to changing environments. Recent studies have proposed ""meta""
algorithms that convert any online learning algorithm to one that is adaptive
to changing environments, where the adaptivity is analyzed in a quantity called
the strongly-adaptive regret. This paper describes a new meta algorithm that
has a strongly-adaptive regret bound that is a factor of $\sqrt{\log(T)}$
better than other algorithms with the same time complexity, where $T$ is the
time horizon. We also extend our algorithm to achieve a first-order (i.e.,
dependent on the observed losses) strongly-adaptive regret bound for the first
time, to our knowledge. At its heart is a new parameter-free algorithm for the
learning with expert advice (LEA) problem in which experts sometimes do not
output advice for consecutive time steps (i.e., \emph{sleeping} experts). This
algorithm is derived by a reduction from optimal algorithms for the so-called
coin betting problem. Empirical results show that our algorithm outperforms
state-of-the-art methods in both learning with expert advice and metric
learning scenarios.
",1,0,0,1,0,0
18942,On the global sup-norm of GL(3) cusp forms,"  Let $\phi$ be a spherical Hecke-Maass cusp form on the non-compact space
$\mathrm{PGL}_3(\mathbb{Z})\backslash\mathrm{PGL}_3(\mathbb{R})$. We establish
various pointwise upper bounds for $\phi$ in terms of its Laplace eigenvalue
$\lambda_\phi$. These imply, for $\phi$ arithmetically normalized and tempered
at the archimedean place, the bound $\|\phi\|_\infty\ll_\epsilon
\lambda_{\phi}^{39/40+\epsilon}$ for the global sup-norm (without restriction
to a compact subset). On the way, we derive a new uniform upper bound for the
$\mathrm{GL}_3$ Jacquet-Whittaker function.
",0,0,1,0,0,0
18943,Four revolutions in physics and the second quantum revolution -- a unification of force and matter by quantum information,"  Newton's mechanical revolution unifies the motion of planets in the sky and
falling of apple on earth. Maxwell's electromagnetic revolution unifies
electricity, magnetism, and light. Einstein's relativity revolution unifies
space with time, and gravity with space-time distortion. The quantum revolution
unifies particle with waves, and energy with frequency. Each of those
revolution changes our world view. In this article, we will describe a
revolution that is happening now: the second quantum revolution which unifies
matter/space with information. In other words, the new world view suggests that
elementary particles (the bosonic force particles and fermionic matter
particles) all originated from quantum information (qubits): they are
collective excitations of an entangled qubit ocean that corresponds to our
space. The beautiful geometric Yang-Mills gauge theory and the strange Fermi
statistics of matter particles now have a common algebraic quantum
informational origin.
",0,1,0,0,0,0
18944,Concept Drift Learning with Alternating Learners,"  Data-driven predictive analytics are in use today across a number of
industrial applications, but further integration is hindered by the requirement
of similarity among model training and test data distributions. This paper
addresses the need of learning from possibly nonstationary data streams, or
under concept drift, a commonly seen phenomenon in practical applications. A
simple dual-learner ensemble strategy, alternating learners framework, is
proposed. A long-memory model learns stable concepts from a long relevant time
window, while a short-memory model learns transient concepts from a small
recent window. The difference in prediction performance of these two models is
monitored and induces an alternating policy to select, update and reset the two
models. The method features an online updating mechanism to maintain the
ensemble accuracy, and a concept-dependent trigger to focus on relevant data.
Through empirical studies the method demonstrates effective tracking and
prediction when the steaming data carry abrupt and/or gradual changes.
",1,0,0,1,0,0
18945,A new proof of the competitive exclusion principle in the chemostat,"  We give an new proof of the well-known competitive exclusion principle in the
chemostat model with $n$ species competing for a single resource, for any set
of increasing growth functions. The proof is constructed by induction on the
number of the species, after being ordered. It uses elementary analysis and
comparisons of solutions of ordinary differential equations.
",0,0,1,0,0,0
18946,From Random Differential Equations to Structural Causal Models: the stochastic case,"  Random Differential Equations provide a natural extension of Ordinary
Differential Equations to the stochastic setting. We show how, and under which
conditions, every equilibrium state of a Random Differential Equation (RDE) can
be described by a Structural Causal Model (SCM), while pertaining the causal
semantics. This provides an SCM that captures the stochastic and causal
behavior of the RDE, which can model both cycles and confounders. This enables
the study of the equilibrium states of the RDE by applying the theory and
statistical tools available for SCMs, for example, marginalizations and Markov
properties, as we illustrate by means of an example. Our work thus provides a
direct connection between two fields that so far have been developing in
isolation.
",0,0,0,1,0,0
18947,Ties That Bind - Characterizing Classes by Attributes and Social Ties,"  Given a set of attributed subgraphs known to be from different classes, how
can we discover their differences? There are many cases where collections of
subgraphs may be contrasted against each other. For example, they may be
assigned ground truth labels (spam/not-spam), or it may be desired to directly
compare the biological networks of different species or compound networks of
different chemicals.
In this work we introduce the problem of characterizing the differences
between attributed subgraphs that belong to different classes. We define this
characterization problem as one of partitioning the attributes into as many
groups as the number of classes, while maximizing the total attributed quality
score of all the given subgraphs.
We show that our attribute-to-class assignment problem is NP-hard and an
optimal $(1 - 1/e)$-approximation algorithm exists. We also propose two
different faster heuristics that are linear-time in the number of attributes
and subgraphs. Unlike previous work where only attributes were taken into
account for characterization, here we exploit both attributes and social ties
(i.e. graph structure).
Through extensive experiments, we compare our proposed algorithms, show
findings that agree with human intuition on datasets from Amazon co-purchases,
Congressional bill sponsorships, and DBLP co-authorships. We also show that our
approach of characterizing subgraphs is better suited for sense-making than
discriminating classification approaches.
",1,1,0,0,0,0
18948,Probing dark matter with star clusters: a dark matter core in the ultra-faint dwarf Eridanus II,"  We present a new technique to probe the central dark matter (DM) density
profile of galaxies that harnesses both the survival and observed properties of
star clusters. As a first application, we apply our method to the `ultra-faint'
dwarf Eridanus II (Eri II) that has a lone star cluster ~45 pc from its centre.
Using a grid of collisional $N$-body simulations, incorporating the effects of
stellar evolution, external tides and dynamical friction, we show that a DM
core for Eri II naturally reproduces the size and the projected position of its
star cluster. By contrast, a dense cusped galaxy requires the cluster to lie
implausibly far from the centre of Eri II (>1 kpc), with a high inclination
orbit that must be observed at a particular orbital phase. Our results,
therefore, favour a dark matter core. This implies that either a cold DM cusp
was `heated up' at the centre of Eri II by bursty star formation, or we are
seeing an evidence for physics beyond cold DM.
",0,1,0,0,0,0
18949,Predicting Foreground Object Ambiguity and Efficiently Crowdsourcing the Segmentation(s),"  We propose the ambiguity problem for the foreground object segmentation task
and motivate the importance of estimating and accounting for this ambiguity
when designing vision systems. Specifically, we distinguish between images
which lead multiple annotators to segment different foreground objects
(ambiguous) versus minor inter-annotator differences of the same object. Taking
images from eight widely used datasets, we crowdsource labeling the images as
""ambiguous"" or ""not ambiguous"" to segment in order to construct a new dataset
we call STATIC. Using STATIC, we develop a system that automatically predicts
which images are ambiguous. Experiments demonstrate the advantage of our
prediction system over existing saliency-based methods on images from vision
benchmarks and images taken by blind people who are trying to recognize objects
in their environment. Finally, we introduce a crowdsourcing system to achieve
cost savings for collecting the diversity of all valid ""ground truth""
foreground object segmentations by collecting extra segmentations only when
ambiguity is expected. Experiments show our system eliminates up to 47% of
human effort compared to existing crowdsourcing methods with no loss in
capturing the diversity of ground truths.
",1,0,0,0,0,0
18950,On Some Generalized Polyhedral Convex Constructions,"  Generalized polyhedral convex sets, generalized polyhedral convex functions
on locally convex Hausdorff topological vector spaces, and the related
constructions such as sum of sets, sum of functions, directional derivative,
infimal convolution, normal cone, conjugate function, subdifferential, are
studied thoroughly in this paper. Among other things, we show how a generalized
polyhedral convex set can be characterized via the finiteness of the number of
its faces. In addition, it is proved that the infimal convolution of a
generalized polyhedral convex function and a polyhedral convex function is a
polyhedral convex function. The obtained results can be applied to scalar
optimization problems described by generalized polyhedral convex sets and
generalized polyhedral convex functions.
",0,0,1,0,0,0
18951,mGPfusion: Predicting protein stability changes with Gaussian process kernel learning and data fusion,"  Proteins are commonly used by biochemical industry for numerous processes.
Refining these proteins' properties via mutations causes stability effects as
well. Accurate computational method to predict how mutations affect protein
stability are necessary to facilitate efficient protein design. However,
accuracy of predictive models is ultimately constrained by the limited
availability of experimental data. We have developed mGPfusion, a novel
Gaussian process (GP) method for predicting protein's stability changes upon
single and multiple mutations. This method complements the limited experimental
data with large amounts of molecular simulation data. We introduce a Bayesian
data fusion model that re-calibrates the experimental and in silico data
sources and then learns a predictive GP model from the combined data. Our
protein-specific model requires experimental data only regarding the protein of
interest and performs well even with few experimental measurements. The
mGPfusion models proteins by contact maps and infers the stability effects
caused by mutations with a mixture of graph kernels. Our results show that
mGPfusion outperforms state-of-the-art methods in predicting protein stability
on a dataset of 15 different proteins and that incorporating molecular
simulation data improves the model learning and prediction accuracy.
",0,0,0,1,1,0
18952,How Do Software Startups Pivot? Empirical Results from a Multiple Case Study,"  In order to handle intense time pressure and survive in dynamic market,
software startups have to make crucial decisions constantly on whether to
change directions or stay on chosen courses, or in the terms of Lean Startup,
to pivot or to persevere. The existing research and knowledge on software
startup pivots are very limited. In this study, we focused on understanding the
pivoting processes of software startups, and identified the triggering factors
and pivot types. To achieve this, we employed a multiple case study approach,
and analyzed the data obtained from four software startups. The initial
findings show that different software startups make different types of pivots
related to business and technology during their product development life cycle.
The pivots are triggered by various factors including negative customer
feedback.
",1,0,0,0,0,0
18953,Some criteria for Wind Riemannian completeness and existence of Cauchy hypersurfaces,"  Recently, a link between Lorentzian and Finslerian Geometries has been
carried out, leading to the notion of wind Riemannian structure (WRS), a
generalization of Finslerian Randers metrics. Here, we further develop this
notion and its applications to spacetimes, by introducing some
characterizations and criteria for the completeness of WRS's.
As an application, we consider a general class of spacetimes admitting a time
function $t$ generated by the flow of a complete Killing vector field
(generalized standard stationary spacetimes or, more precisely, SSTK ones) and
derive simple criteria ensuring that its slices $t=$ constant are Cauchy.
Moreover, a brief summary on the Finsler/Lorentz link for readers with some
acquaintance in Lorentzian Geometry, plus some simple examples in Mathematical
Relativity, are provided.
",0,0,1,0,0,0
18954,A generalization of Schönemann's theorem via a graph theoretic method,"  Recently, Grynkiewicz et al. [{\it Israel J. Math.} {\bf 193} (2013),
359--398], using tools from additive combinatorics and group theory, proved
necessary and sufficient conditions under which the linear congruence
$a_1x_1+\cdots +a_kx_k\equiv b \pmod{n}$, where $a_1,\ldots,a_k,b,n$ ($n\geq
1$) are arbitrary integers, has a solution $\langle x_1,\ldots,x_k \rangle \in
\Z_{n}^k$ with all $x_i$ distinct modulo $n$. So, it would be an interesting
problem to give an explicit formula for the number of such solutions. Quite
surprisingly, this problem was first considered, in a special case, by
Schönemann almost two centuries ago(!) but his result seems to have been
forgotten. Schönemann [{\it J. Reine Angew. Math.} {\bf 1839} (1839),
231--243] proved an explicit formula for the number of such solutions when
$b=0$, $n=p$ a prime, and $\sum_{i=1}^k a_i \equiv 0 \pmod{p}$ but $\sum_{i \in
I} a_i \not\equiv 0 \pmod{p}$ for all $I\varsubsetneq \lbrace 1, \ldots,
k\rbrace$. In this paper, we generalize Schönemann's theorem using a result
on the number of solutions of linear congruences due to D. N. Lehmer and also a
result on graph enumeration recently obtained by Ardila et al. [{\it Int. Math.
Res. Not.} {\bf 2015} (2015), 3830--3877]. This seems to be a rather uncommon
method in the area; besides, our proof technique or its modifications may be
useful for dealing with other cases of this problem (or even the general case)
or other relevant problems.
",1,0,0,0,0,0
18955,Map-based Multi-Policy Reinforcement Learning: Enhancing Adaptability of Robots by Deep Reinforcement Learning,"  In order for robots to perform mission-critical tasks, it is essential that
they are able to quickly adapt to changes in their environment as well as to
injuries and or other bodily changes. Deep reinforcement learning has been
shown to be successful in training robot control policies for operation in
complex environments. However, existing methods typically employ only a single
policy. This can limit the adaptability since a large environmental
modification might require a completely different behavior compared to the
learning environment. To solve this problem, we propose Map-based Multi-Policy
Reinforcement Learning (MMPRL), which aims to search and store multiple
policies that encode different behavioral features while maximizing the
expected reward in advance of the environment change. Thanks to these policies,
which are stored into a multi-dimensional discrete map according to its
behavioral feature, adaptation can be performed within reasonable time without
retraining the robot. An appropriate pre-trained policy from the map can be
recalled using Bayesian optimization. Our experiments show that MMPRL enables
robots to quickly adapt to large changes without requiring any prior knowledge
on the type of injuries that could occur. A highlight of the learned behaviors
can be found here: this https URL .
",1,0,0,0,0,0
18956,Testing convexity of a discrete distribution,"  Based on the convex least-squares estimator, we propose two different
procedures for testing convexity of a probability mass function supported on N
with an unknown finite support. The procedures are shown to be asymptotically
calibrated.
",0,0,1,1,0,0
18957,Small and Strong Formulations for Unions of Convex Sets from the Cayley Embedding,"  There is often a significant trade-off between formulation strength and size
in mixed integer programming (MIP). When modeling convex disjunctive
constraints (e.g. unions of convex sets), adding auxiliary continuous variables
can sometimes help resolve this trade-off. However, standard formulations that
use such auxiliary continuous variables can have a worse-than-expected
computational effectiveness, which is often attributed precisely to these
auxiliary continuous variables. For this reason, there has been considerable
interest in constructing strong formulations that do not use continuous
auxiliary variables. We introduce a technique to construct formulations without
these detrimental continuous auxiliary variables. To develop this technique we
introduce a natural non-polyhedral generalization of the Cayley embedding of a
family of polytopes and show it inherits many geometric properties of the
original embedding. We then show how the associated formulation technique can
be used to construct small and strong formulation for a wide range of
disjunctive constraints. In particular, we show it can recover and generalize
all known strong formulations without continuous auxiliary variables.
",0,0,1,0,0,0
18958,Safe Robotic Grasping: Minimum Impact-Force Grasp Selection,"  This paper addresses the problem of selecting from a choice of possible
grasps, so that impact forces will be minimised if a collision occurs while the
robot is moving the grasped object along a post-grasp trajectory. Such
considerations are important for safety in human-robot interaction, where even
a certified ""human-safe"" (e.g. compliant) arm may become hazardous once it
grasps and begins moving an object, which may have significant mass, sharp
edges or other dangers. Additionally, minimising collision forces is critical
to preserving the longevity of robots which operate in uncertain and hazardous
environments, e.g. robots deployed for nuclear decommissioning, where removing
a damaged robot from a contaminated zone for repairs may be extremely difficult
and costly. Also, unwanted collisions between a robot and critical
infrastructure (e.g. pipework) in such high-consequence environments can be
disastrous. In this paper, we investigate how the safety of the post-grasp
motion can be considered during the pre-grasp approach phase, so that the
selected grasp is optimal in terms applying minimum impact forces if a
collision occurs during a desired post-grasp manipulation. We build on the
methods of augmented robot-object dynamics models and ""effective mass"" and
propose a method for combining these concepts with modern grasp and trajectory
planners, to enable the robot to achieve a grasp which maximises the safety of
the post-grasp trajectory, by minimising potential collision forces. We
demonstrate the effectiveness of our approach through several experiments with
both simulated and real robots.
",1,0,0,0,0,0
18959,Nonparametric Kernel Density Estimation for Univariate Curent Status Data,"  We derive estimators of the density of the event times of current status
data. The estimators are derived for the situations where the distribution of
the observation times is known and where this distribution is unknown. The
density estimators are constructed from kernel estimators of the density of
transformed current status data, which have a distribution similar to uniform
deconvolution data. Expansions of the expectation and variance as well as
asymptotic normality are derived. A reference density based bandwidth selection
method is proposed. A simulated example is presented.
",0,0,1,1,0,0
18960,Non-Kähler Mirror Symmetry of the Iwasawa Manifold,"  We propose a new approach to the Mirror Symmetry Conjecture in a form
suitable to possibly non-Kähler compact complex manifolds whose canonical
bundle is trivial. We apply our methods by proving that the Iwasawa manifold
$X$, a well-known non-Kähler compact complex manifold of dimension $3$, is
its own mirror dual to the extent that its Gauduchon cone, replacing the
classical Kähler cone that is empty in this case, corresponds to what we call
the local universal family of essential deformations of $X$. These are obtained
by removing from the Kuranishi family the two ""superfluous"" dimensions of
complex parallelisable deformations that have a similar geometry to that of the
Iwasawa manifold. The remaining four dimensions are shown to have a clear
geometric meaning including in terms of the degeneration at $E_2$ of the
Frölicher spectral sequence. On the local moduli space of ""essential"" complex
structures, we obtain a canonical Hodge decomposition of weight $3$ and a
variation of Hodge structures, construct coordinates and Yukawa couplings while
implicitly proving a local Torelli theorem. On the metric side of the mirror,
we construct a variation of Hodge structures parametrised by a subset of the
complexified Gauduchon cone of the Iwasawa manifold using the sGG property of
all the small deformations of this manifold proved in earlier joint work of the
author with L. Ugarte. Finally, we define a mirror map linking the two
variations of Hodge structures and we highlight its properties.
",0,0,1,0,0,0
18961,Estimation and Inference for Moments of Ratios with Robustness against Large Trimming Bias,"  Empirical researchers often trim observations with small denominator A when
they estimate moments of the form E[B/A]. Large trimming is a common practice
to mitigate variance, but it incurs large trimming bias. This paper provides a
novel method of correcting large trimming bias. If a researcher is willing to
assume that the joint distribution between A and B is smooth, then a large
trimming bias may be estimated well. With the bias correction, we also develop
a valid and robust inference result for E[B/A].
",0,0,0,1,0,0
18962,FPGA-based real-time 105-channel data acquisition platform for imaging system,"  In this paper, a real-time 105-channel data acquisition platform based on
FPGA for imaging will be implemented for mm-wave imaging systems. PC platform
is also realized for imaging results monitoring purpose. Mm-wave imaging
expands our vision by letting us see things under poor visibility conditions.
With this extended vision ability, a wide range of military imaging missions
would benefit, such as surveillance, precision targeting, navigation, and
rescue. Based on the previously designed imager modules, this project would go
on finishing the PCB design (both schematic and layout) of the following signal
processing systems consisting of Programmable Gain Amplifier(PGA) (4 PGA for
each ADC) and 16-channel Analog to Digital Converter (ADC) (7 ADC in total).
Then the system verification would be performed on the Artix-7 35T Arty FPGA
with the developing of proper controlling code to configure the ADC and realize
the communication between the FPGA and the PC (through both UART and Ethernet).
For the verification part, a simple test on a breadboard with a simple analog
input (generated from a resistor divider) would first be performed. After the
PCB design is finished, the whole system would be tested again with a precise
reference and analog input.
",1,0,0,0,0,0
18963,Zn-induced in-gap electronic states in La214 probed by uniform magnetic susceptibility: relevance to the suppression of superconducting Tc,"  Substitution of isovalent non-magnetic defects, such as Zn, in CuO2 plane
strongly modifies the magnetic properties of strongly electron correlated hole
doped cuprate superconductors. The reason for enhanced uniform magnetic
susceptibility, \c{hi}, in Zn substituted cuprates is debatable. So far, the
observed magnetic behavior has been analyzed mainly in terms of two somewhat
contrasting scenarios, (a) that due to independent localized moments appearing
in the vicinity of Zn arising because of the strong electronic/magnetic
correlations present in the host compound and (b) that due to transfer of
quasiparticle spectral weight and creation of weakly localized low energy
electronic states associated with each Zn atom in place of an in-plane Cu. If
the second scenario is correct, one should expect a direct correspondence
between Zn induced suppression of superconducting transition temperature, Tc,
and the extent of the enhanced magnetic susceptibility at low temperature. In
this case, the low-T enhancement of \c{hi} would be due to weakly localized
quasiparticle states at low energy and these electronic states will be
precluded from taking part in Cooper pairing. We explore this second
possibility by analyzing the \c{hi}(T) data for La2-xSrxCu1-yZnyO4 with
different hole contents, p (= x), and Zn concentrations (y) in this paper.
Results of our analysis support this scenario.
",0,1,0,0,0,0
18964,Predicate Specialization for Definitional Higher-order Logic Programs,"  Higher-order logic programming is an interesting extension of traditional
logic programming that allows predicates to appear as arguments and variables
to be used where predicates typically occur. Higher-order characteristics are
indeed desirable but on the other hand they are also usually more expensive to
support. In this paper we propose a program specialization technique based on
partial evaluation that can be applied to a modest but useful class of
higher-order logic programs and can transform them into first-order programs
without introducing additional data structures. The resulting first-order
programs can be executed by conventional logic programming interpreters and
benefit from other optimizations that might be available. We provide an
implementation and experimental results that suggest the efficiency of the
transformation.
",1,0,0,0,0,0
18965,Metastability and bifurcation in superconducting nanorings,"  We describe an approach, based on direct numerical solution of the Usadel
equation, to finding stationary points of the free energy of superconducting
nanorings. We consider both uniform (equilibrium) solutions and the critical
droplets that mediate activated transitions between them. For the uniform
solutions, we compute the critical current as a function of the temperature,
thus obtaining a correction factor to Bardeen's 1962 interpolation formula. For
the droplets, we present a metastability chart that shows the activation energy
as a function of the temperature and current. A comparison of the activation
energy for a ring to experimental results for a wire connected to
superconducting leads reveals a discrepancy at large currents. We discuss
possible reasons for it. We also discuss the nature of the bifurcation point at
which the droplet merges with the uniform solution.
",0,1,0,0,0,0
18966,Origin of life in a digital microcosm,"  While all organisms on Earth descend from a common ancestor, there is no
consensus on whether the origin of this ancestral self-replicator was a one-off
event or whether it was only the final survivor of multiple origins. Here we
use the digital evolution system Avida to study the origin of self-replicating
computer programs. By using a computational system, we avoid many of the
uncertainties inherent in any biochemical system of self-replicators (while
running the risk of ignoring a fundamental aspect of biochemistry). We
generated the exhaustive set of minimal-genome self-replicators and analyzed
the network structure of this fitness landscape. We further examined the
evolvability of these self-replicators and found that the evolvability of a
self-replicator is dependent on its genomic architecture. We studied the
differential ability of replicators to take over the population when competed
against each other (akin to a primordial-soup model of biogenesis) and found
that the probability of a self-replicator out-competing the others is not
uniform. Instead, progenitor (most-recent common ancestor) genotypes are
clustered in a small region of the replicator space. Our results demonstrate
how computational systems can be used as test systems for hypotheses concerning
the origin of life.
",1,1,0,0,0,0
18967,The equivalence of two tax processes,"  We introduce two models of taxation, the latent and natural tax processes,
which have both been used to represent loss-carry-forward taxation on the
capital of an insurance company. In the natural tax process, the tax rate is a
function of the current level of capital, whereas in the latent tax process,
the tax rate is a function of the capital that would have resulted if no tax
had been paid. Whereas up to now these two types of tax processes have been
treated separately, we show that, in fact, they are essentially equivalent.
This allows a unified treatment, translating results from one model to the
other. Significantly, we solve the question of existence and uniqueness for the
natural tax process, which is defined via an integral equation. Our results
clarify the existing literature on processes with tax.
",0,0,0,0,0,1
18968,Space-Time Geostatistical Models with both Linear and Seasonal Structures in the Temporal Components,"  We provide a novel approach to model space-time random fields where the
temporal argument is decomposed into two parts. The former captures the linear
argument, which is related, for instance, to the annual evolution of the field.
The latter is instead a circular variable describing, for instance, monthly
observations. The basic intuition behind this construction is to consider a
random field defined over space (a compact set of the $d$-dimensional Euclidean
space) across time, which is considered as the product space $\mathbb{R} \times
\mathbb{S}^1$, with $\mathbb{S}^1$ being the unit circle. Under such framework,
we derive new parametric families of covariance functions. In particular, we
focus on two classes of parametric families. The former being parenthetical to
the Gneiting class of covariance functions. The latter is instead obtained by
proposing a new Lagrangian framework for the space-time domain considered in
the manuscript. Our findings are illustrated through a real dataset of surface
air temperatures. We show that the incorporation of both temporal variables can
produce significant improvements in the predictive performances of the model.
We also discuss the extension of this approach for fields defined spatially on
a sphere, which allows to model space-time phenomena over large portions of
planet Earth.
",0,0,1,1,0,0
18969,Stability and Robust Regulation of Passive Linear Systems,"  We study the stability of coupled impedance passive regular linear systems
under power-preserving interconnections. We present new conditions for strong,
exponential, and non-uniform stability of the closed-loop system. We apply the
stability results to the construction of passive error feedback controllers for
robust output tracking and disturbance rejection for strongly stabilizable
passive systems. In the case of nonsmooth reference and disturbance signals we
present conditions for non-uniform rational and logarithmic rates of
convergence of the output. The results are illustrated with examples on
designing controllers for linear wave and heat equations, and on studying the
stability of a system of coupled partial differential equations.
",0,0,1,0,0,0
18970,Bayes Minimax Competitors of Preliminary Test Estimators in k Sample Problems,"  In this paper, we consider the estimation of a mean vector of a multivariate
normal population where the mean vector is suspected to be nearly equal to mean
vectors of $k-1$ other populations. As an alternative to the preliminary test
estimator based on the test statistic for testing hypothesis of equal means, we
derive empirical and hierarchical Bayes estimators which shrink the sample mean
vector toward a pooled mean estimator given under the hypothesis. The
minimaxity of those Bayesian estimators are shown, and their performances are
investigated by simulation.
",0,0,1,1,0,0
18971,Non-orthogonal Multiple Access for High-reliable and Low-latency V2X Communications,"  In this paper, we consider a dense vehicular communication network where each
vehicle broadcasts its safety information to its neighborhood in each
transmission period. Such applications require low latency and high
reliability, and thus, we propose a non-orthogonal multiple access scheme to
reduce the latency and to improve the packet reception probability. In the
proposed scheme, the BS performs the semi-persistent scheduling to optimize the
time scheduling and allocate frequency resources in a non-orthogonal manner
while the vehicles autonomously perform distributed power control. We formulate
the centralized scheduling and resource allocation problem as equivalent to a
multi-dimensional stable roommate matching problem, in which the users and
time/frequency resources are considered as disjoint sets of players to be
matched with each other. We then develop a novel rotation matching algorithm,
which converges to a q-exchange stable matching after a limited number of
iterations. Simulation results show that the proposed scheme outperforms the
traditional orthogonal multiple access scheme in terms of the latency and
reliability.
",1,0,0,0,0,0
18972,Cohomology monoids of monoids with coefficients in semimodules II,"  We relate the old and new cohomology monoids of an arbitrary monoid $M$ with
coefficients in semimodules over $M$, introduced in the author's previous
papers, to monoid and group extensions. More precisely, the old and new second
cohomology monoids describe Schreier extensions of semimodules by monoids, and
the new third cohomology monoid is related to a certain group extension
problem.
",0,0,1,0,0,0
18973,Lower Bounds for Higher-Order Convex Optimization,"  State-of-the-art methods in convex and non-convex optimization employ
higher-order derivative information, either implicitly or explicitly. We
explore the limitations of higher-order optimization and prove that even for
convex optimization, a polynomial dependence on the approximation guarantee and
higher-order smoothness parameters is necessary. As a special case, we show
Nesterov's accelerated cubic regularization method to be nearly tight.
",1,0,0,1,0,0
18974,Superconductivity in La1-xCexOBiSSe: carrier doping by mixed valence of Ce ions,"  We report the effects of Ce substitution on structural, electronic, and
magnetic properties of layered bismuth-chalcogenide La1-xCexOBiSSe (x = 0-0.9),
which are newly obtained in this study. Metallic conductivity was observed for
x > 0.1 because of electron carriers induced by mixed valence of Ce ions, as
revealed by bond valence sum calculation and magnetization measurements. Zero
resistivity and clear diamagnetic susceptibility were obtained for x = 0.2-0.6,
indicating the emergence of bulk superconductivity in these compounds.
Dome-shaped superconductivity phase diagram with the highest transition
temperature (Tc) of 3.1 K, which is slightly lower than that of F-doped
LaOBiSSe (Tc = 3.7 K), was established. The present study clearly shows that
the mixed valence of Ce ions can be utilized as an alternative approach for
electron-doping in layered bismuth-chalcogenides to induce superconductivity.
",0,1,0,0,0,0
18975,Gaussian Processes Over Graphs,"  We propose Gaussian processes for signals over graphs (GPG) using the apriori
knowledge that the target vectors lie over a graph. We incorporate this
information using a graph- Laplacian based regularization which enforces the
target vectors to have a specific profile in terms of graph Fourier transform
coeffcients, for example lowpass or bandpass graph signals. We discuss how the
regularization affects the mean and the variance in the prediction output. In
particular, we prove that the predictive variance of the GPG is strictly
smaller than the conventional Gaussian process (GP) for any non-trivial graph.
We validate our concepts by application to various real-world graph signals.
Our experiments show that the performance of the GPG is superior to GP for
small training data sizes and under noisy training.
",0,0,0,1,0,0
18976,Differences of Type I error rates for ANOVA and Multilevel-Linear-Models using SAS and SPSS for repeated measures designs,"  To derive recommendations on how to analyze longitudinal data, we examined
Type I error rates of Multilevel Linear Models (MLM) and repeated measures
Analysis of Variance (rANOVA) using SAS and SPSS.We performed a simulation with
the following specifications: To explore the effects of high numbers of
measurement occasions and small sample sizes on Type I error, measurement
occasions of m = 9 and 12 were investigated as well as sample sizes of n = 15,
20, 25 and 30. Effects of non-sphericity in the population on Type I error were
also inspected: 5,000 random samples were drawn from two populations containing
neither a within-subject nor a between-group effect. They were analyzed
including the most common options to correct rANOVA and MLM-results: The
Huynh-Feldt-correction for rANOVA (rANOVA-HF) and the Kenward-Roger-correction
for MLM (MLM-KR), which could help to correct progressive bias of MLM with an
unstructured covariance matrix (MLM-UN). Moreover, uncorrected rANOVA and MLM
assuming a compound symmetry covariance structure (MLM-CS) were also taken into
account. The results showed a progressive bias for MLM-UN for small samples
which was stronger in SPSS than in SAS. Moreover, an appropriate bias
correction for Type I error via rANOVA-HF and an insufficient correction by
MLM-UN-KR for n < 30 were found. These findings suggest MLM-CS or rANOVA if
sphericity holds and a correction of a violation via rANOVA-HF. If an analysis
requires MLM, SPSS yields more accurate Type I error rates for MLM-CS and SAS
yields more accurate Type I error rates for MLM-UN.
",0,0,0,1,0,0
18977,Efficient algorithms for Bayesian Nearest Neighbor Gaussian Processes,"  We consider alternate formulations of recently proposed hierarchical Nearest
Neighbor Gaussian Process (NNGP) models (Datta et al., 2016a) for improved
convergence, faster computing time, and more robust and reproducible Bayesian
inference. Algorithms are defined that improve CPU memory management and
exploit existing high-performance numerical linear algebra libraries.
Computational and inferential benefits are assessed for alternate NNGP
specifications using simulated datasets and remotely sensed light detection and
ranging (LiDAR) data collected over the US Forest Service Tanana Inventory Unit
(TIU) in a remote portion of Interior Alaska. The resulting data product is the
first statistically robust map of forest canopy for the TIU.
",0,0,0,1,0,0
18978,Variegation and space weathering on asteroid 21 Lutetia,"  During the flyby in 2010, the OSIRIS camera on-board Rosetta acquired
hundreds of high-resolution images of asteroid Lutetia's surface through a
range of narrow-band filters. While Lutetia appears very bland in the visible
wavelength range, Magrin et al. (2012) tentatively identified UV color
variations in the Baetica cluster, a group of relatively young craters close to
the north pole. As Lutetia remains a poorly understood asteroid, such color
variations may provide clues to the nature of its surface. We take the color
analysis one step further. First we orthorectify the images using a shape model
and improved camera pointing, then apply a variety of techniques (photometric
correction, principal component analysis) to the resulting color cubes. We
characterize variegation in the Baetica crater cluster at high spatial
resolution, identifying crater rays and small, fresh impact craters. We argue
that at least some of the color variation is due to space weathering, which
makes Lutetia's regolith redder and brighter.
",0,1,0,0,0,0
18979,Rokhlin dimension for compact quantum group actions,"  We show that, for a given compact or discrete quantum group $G$, the class of
actions of $G$ on C*-algebras is first-order axiomatizable in the logic for
metric structures. As an application, we extend the notion of Rokhlin property
for $G$-C*-algebra, introduced by Barlak, Szabó, and Voigt in the case when
$G$ is second countable and coexact, to an arbitrary compact quantum group $G$.
All the the preservations and rigidity results for Rokhlin actions of second
countable coexact compact quantum groups obtained by Barlak, Szabó, and
Voigt are shown to hold in this general context. As a further application, we
extend the notion of equivariant order zero dimension for equivariant
*-homomorphisms, introduced in the classical setting by the first and third
authors, to actions of compact quantum groups. This allows us to define the
Rokhlin dimension of an action of a compact quantum group on a C*-algebra,
recovering the Rokhlin property as Rokhlin dimension zero. We conclude by
establishing a preservation result for finite nuclear dimension and finite
decomposition rank when passing to fixed point algebras and crossed products by
compact quantum group actions with finite Rokhlin dimension.
",0,0,1,0,0,0
18980,Parametric Identification Using Weighted Null-Space Fitting,"  In identification of dynamical systems, the prediction error method using a
quadratic cost function provides asymptotically efficient estimates under
Gaussian noise and additional mild assumptions, but in general it requires
solving a non-convex optimization problem. An alternative class of methods uses
a non-parametric model as intermediate step to obtain the model of interest.
Weighted null-space fitting (WNSF) belongs to this class. It is a weighted
least-squares method consisting of three steps. In the first step, a high-order
ARX model is estimated. In a second least-squares step, this high-order
estimate is reduced to a parametric estimate. In the third step, weighted least
squares is used to reduce the variance of the estimates. The method is flexible
in parametrization and suitable for both open- and closed-loop data. In this
paper, we show that WNSF provides estimates with the same asymptotic properties
as PEM with a quadratic cost function when the model orders are chosen
according to the true system. Also, simulation studies indicate that WNSF may
be competitive with state-of-the-art methods.
",1,0,0,0,0,0
18981,Learning Large-Scale Bayesian Networks with the sparsebn Package,"  Learning graphical models from data is an important problem with wide
applications, ranging from genomics to the social sciences. Nowadays datasets
often have upwards of thousands---sometimes tens or hundreds of thousands---of
variables and far fewer samples. To meet this challenge, we have developed a
new R package called sparsebn for learning the structure of large, sparse
graphical models with a focus on Bayesian networks. While there are many
existing software packages for this task, this package focuses on the unique
setting of learning large networks from high-dimensional data, possibly with
interventions. As such, the methods provided place a premium on scalability and
consistency in a high-dimensional setting. Furthermore, in the presence of
interventions, the methods implemented here achieve the goal of learning a
causal network from data. Additionally, the sparsebn package is fully
compatible with existing software packages for network analysis.
",1,0,0,1,0,0
18982,An Asymptotically Optimal Index Policy for Finite-Horizon Restless Bandits,"  We consider restless multi-armed bandit (RMAB) with a finite horizon and
multiple pulls per period. Leveraging the Lagrangian relaxation, we approximate
the problem with a collection of single arm problems. We then propose an
index-based policy that uses optimal solutions of the single arm problems to
index individual arms, and offer a proof that it is asymptotically optimal as
the number of arms tends to infinity. We also use simulation to show that this
index-based policy performs better than the state-of-art heuristics in various
problem settings.
",0,0,1,0,0,0
18983,Effective holographic theory of charge density waves,"  We use Gauge/Gravity duality to write down an effective low energy
holographic theory of charge density waves. We consider a simple gravity model
which breaks translations spontaneously in the dual field theory in a
homogeneous manner, capturing the low energy dynamics of phonons coupled to
conserved currents. We first focus on the leading two-derivative action, which
leads to excited states with non-zero strain. We show that including subleading
quartic derivative terms leads to dynamical instabilities of AdS$_2$
translation invariant states and to stable phases breaking translations
spontaneously. We compute analytically the real part of the electric
conductivity. The model allows to construct Lifshitz-like hyperscaling
violating quantum critical ground states breaking translations spontaneously.
At these critical points, the real part of the dc conductivity can be metallic
or insulating.
",0,1,0,0,0,0
18984,Interactive Certificates for Polynomial Matrices with Sub-Linear Communication,"  We develop and analyze new protocols to verify the correctness of various
computations on matrices over F[x], where F is a field. The properties we
verify concern an F[x]-module and therefore cannot simply rely on
previously-developed linear algebra certificates which work only for vector
spaces. Our protocols are interactive certificates, often randomized, and
featuring a constant number of rounds of communication between the prover and
verifier. We seek to minimize the communication cost so that the amount of data
sent during the protocol is significantly smaller than the size of the result
being verified, which can be useful when combining protocols or in some
multi-party settings. The main tools we use are reductions to existing linear
algebra certificates and a new protocol to verify that a given vector is in the
F[x]-linear span of a given matrix.
",1,0,0,0,0,0
18985,Cost Functions for Robot Motion Style,"  We focus on autonomously generating robot motion for day to day physical
tasks that is expressive of a certain style or emotion. Because we seek
generalization across task instances and task types, we propose to capture
style via cost functions that the robot can use to augment its nominal task
cost and task constraints in a trajectory optimization process. We compare two
approaches to representing such cost functions: a weighted linear combination
of hand-designed features, and a neural network parameterization operating on
raw trajectory input. For each cost type, we learn weights for each style from
user feedback. We contrast these approaches to a nominal motion across
different tasks and for different styles in a user study, and find that they
both perform on par with each other, and significantly outperform the baseline.
Each approach has its advantages: featurized costs require learning fewer
parameters and can perform better on some styles, but neural network
representations do not require expert knowledge to design features and could
even learn more complex, nuanced costs than an expert can easily design.
",1,0,0,0,0,0
18986,Revealing the basins of convergence in the planar equilateral restricted four-body problem,"  The planar equilateral restricted four-body problem where two of the
primaries have equal masses is used in order to determine the Newton-Raphson
basins of convergence associated with the equilibrium points. The parametric
variation of the position of the libration points is monitored when the value
of the mass parameter $m_3$ varies in predefined intervals. The regions on the
configuration $(x,y)$ plane occupied by the basins of attraction are revealed
using the multivariate version of the Newton-Raphson iterative scheme. The
correlations between the attracting domains of the equilibrium points and the
corresponding number of iterations needed for obtaining the desired accuracy
are also illustrated. We perform a thorough and systematic numerical
investigation by demonstrating how the dynamical parameter $m_3$ influences the
shape, the geometry and the degree of fractality of the converging regions. Our
numerical outcomes strongly indicate that the mass parameter is indeed one of
the most influential factors in this dynamical system.
",0,1,0,0,0,0
18987,The Eccentric Kozai-Lidov mechanism for Outer Test Particle,"  The secular approximation of the hierarchical three body systems has been
proven to be very useful in addressing many astrophysical systems, from
planets, stars to black holes. In such a system two objects are on a tight
orbit, and the tertiary is on a much wider orbit. Here we study the dynamics of
a system by taking the tertiary mass to zero and solve the hierarchical three
body system up to the octupole level of approximation. We find a rich dynamics
that the outer orbit undergoes due to gravitational perturbations from the
inner binary. The nominal result of the precession of the nodes is mostly
limited for the lowest order of approximation, however, when the octupole-level
of approximation is introduced the system becomes chaotic, as expected, and the
tertiary oscillates below and above 90deg, similarly to the non-test particle
flip behavior (e.g., Naoz 2016). We provide the Hamiltonian of the system and
investigate the dynamics of the system from the quadrupole to the octupole
level of approximations. We also analyze the chaotic and quasi-periodic orbital
evolution by studying the surfaces of sections. Furthermore, including general
relativity, we show case the long term evolution of individual debris disk
particles under the influence of a far away interior eccentric planet. We show
that this dynamics can naturally result in retrograde objects and a puffy disk
after a long timescale evolution (few Gyr) for initially aligned configuration.
",0,1,0,0,0,0
18988,A more symmetric picture for Kasparov's KK-bifunctor,"  For C*-algebras $A$ and $B$, we generalize the notion of a quasihomomorphism
from $A$ to $B$, due to Cuntz, by considering quasihomomorphisms from some
C*-algebra $C$ to $B$ such that $C$ surjects onto $A$, and the two maps forming
a quasihomomorphism agree on the kernel of this surjection. Under an additional
assumption, the group of homotopy classes of such generalized
quasihomomorphisms coincides with $KK(A,B)$. This makes the definition of
Kasparov's bifunctor slightly more symmetric and gives more flexibility for
constructing elements of $KK$-groups. These generalized quasihomomorphisms can
be viewed as pairs of maps directly from $A$ (instead of various $C$'s), but
these maps need not be $*$-homomorphisms.
",0,0,1,0,0,0
18989,Spot dynamics in a reaction-diffusion model of plant root hair initiation,"  We study pattern formation in a 2-D reaction-diffusion (RD) sub-cellular
model characterizing the effect of a spatial gradient of a plant hormone
distribution on a family of G-proteins associated with root-hair (RH)
initiation in the plant cell Arabidopsis thaliana. The activation of these
G-proteins, known as the Rho of Plants (ROPs), by the plant hormone auxin, is
known to promote certain protuberances on root hair cells, which are crucial
for both anchorage and the uptake of nutrients from the soil. Our mathematical
model for the activation of ROPs by the auxin gradient is an extension of the
model of Payne and Grierson [PLoS ONE, 12(4), (2009)], and consists of a
two-component Schnakenberg-type RD system with spatially heterogeneous
coefficients on a 2-D domain. The nonlinear kinetics in this RD system model
the nonlinear interactions between the active and inactive forms of ROPs. By
using a singular perturbation analysis to study 2-D localized spatial patterns
of active ROPs, it is shown that the spatial variations in the nonlinear
reaction kinetics, due to the auxin gradient, lead to a slow spatial alignment
of the localized regions of active ROPs along the longitudinal midline of the
plant cell. Numerical bifurcation analysis, together with time-dependent
numerical simulations of the RD system are used to illustrate both 2-D
localized patterns in the model, and the spatial alignment of localized
structures.
",0,1,0,0,0,0
18990,Incorporating Covariates into Integrated Factor Analysis of Multi-View Data,"  In modern biomedical research, it is ubiquitous to have multiple data sets
measured on the same set of samples from different views (i.e., multi-view
data). For example, in genetic studies, multiple genomic data sets at different
molecular levels or from different cell types are measured for a common set of
individuals to investigate genetic regulation. Integration and reduction of
multi-view data have the potential to leverage information in different data
sets, and to reduce the magnitude and complexity of data for further
statistical analysis and interpretation. In this paper, we develop a novel
statistical model, called supervised integrated factor analysis (SIFA), for
integrative dimension reduction of multi-view data while incorporating
auxiliary covariates. The model decomposes data into joint and individual
factors, capturing the joint variation across multiple data sets and the
individual variation specific to each set respectively. Moreover, both joint
and individual factors are partially informed by auxiliary covariates via
nonparametric models. We devise a computationally efficient
Expectation-Maximization (EM) algorithm to fit the model under some
identifiability conditions. We apply the method to the Genotype-Tissue
Expression (GTEx) data, and provide new insights into the variation
decomposition of gene expression in multiple tissues. Extensive simulation
studies and an additional application to a pediatric growth study demonstrate
the advantage of the proposed method over competing methods.
",0,0,0,1,0,0
18991,Batch-normalized joint training for DNN-based distant speech recognition,"  Improving distant speech recognition is a crucial step towards flexible
human-machine interfaces. Current technology, however, still exhibits a lack of
robustness, especially when adverse acoustic conditions are met. Despite the
significant progress made in the last years on both speech enhancement and
speech recognition, one potential limitation of state-of-the-art technology
lies in composing modules that are not well matched because they are not
trained jointly. To address this concern, a promising approach consists in
concatenating a speech enhancement and a speech recognition deep neural network
and to jointly update their parameters as if they were within a single bigger
network. Unfortunately, joint training can be difficult because the output
distribution of the speech enhancement system may change substantially during
the optimization procedure. The speech recognition module would have to deal
with an input distribution that is non-stationary and unnormalized. To mitigate
this issue, we propose a joint training approach based on a fully
batch-normalized architecture. Experiments, conducted using different datasets,
tasks and acoustic conditions, revealed that the proposed framework
significantly overtakes other competitive solutions, especially in challenging
environments.
",1,0,0,0,0,0
18992,Learning Word Embeddings from the Portuguese Twitter Stream: A Study of some Practical Aspects,"  This paper describes a preliminary study for producing and distributing a
large-scale database of embeddings from the Portuguese Twitter stream. We start
by experimenting with a relatively small sample and focusing on three
challenges: volume of training data, vocabulary size and intrinsic evaluation
metrics. Using a single GPU, we were able to scale up vocabulary size from 2048
words embedded and 500K training examples to 32768 words over 10M training
examples while keeping a stable validation loss and approximately linear trend
on training time per epoch. We also observed that using less than 50\% of the
available training examples for each vocabulary size might result in
overfitting. Results on intrinsic evaluation show promising performance for a
vocabulary size of 32768 words. Nevertheless, intrinsic evaluation metrics
suffer from over-sensitivity to their corresponding cosine similarity
thresholds, indicating that a wider range of metrics need to be developed to
track progress.
",1,0,0,0,0,0
18993,A temperate exo-Earth around a quiet M dwarf at 3.4 parsecs,"  The combination of high-contrast imaging and high-dispersion spectroscopy,
which has successfully been used to detect the atmosphere of a giant planet, is
one of the most promising potential probes of the atmosphere of Earth-size
worlds. The forthcoming generation of extremely large telescopes (ELTs) may
obtain sufficient contrast with this technique to detect O$_2$ in the
atmosphere of those worlds that orbit low-mass M dwarfs. This is strong
motivation to carry out a census of planets around cool stars for which
habitable zones can be resolved by ELTs, i.e. for M dwarfs within $\sim$5
parsecs. Our HARPS survey has been a major contributor to that sample of nearby
planets. Here we report on our radial velocity observations of Ross 128
(Proxima Virginis, GJ447, HIP 57548), an M4 dwarf just 3.4 parsec away from our
Sun. This source hosts an exo-Earth with a projected mass $m \sin i = 1.35
M_\oplus$ and an orbital period of 9.9 days. Ross 128 b receives $\sim$1.38
times as much flux as Earth from the Sun and its equilibrium ranges in
temperature between 269 K for an Earth-like albedo and 213 K for a Venus-like
albedo. Recent studies place it close to the inner edge of the conventional
habitable zone. An 80-day long light curve from K2 campaign C01 demonstrates
that Ross~128~b does not transit. Together with the All Sky Automated Survey
(ASAS) photometry and spectroscopic activity indices, the K2 photometry shows
that Ross 128 rotates slowly and has weak magnetic activity. In a habitability
context, this makes survival of its atmosphere against erosion more likely.
Ross 128 b is the second closest known exo-Earth, after Proxima Centauri b (1.3
parsec), and the closest temperate planet known around a quiet star. The 15 mas
planet-star angular separation at maximum elongation will be resolved by ELTs
($>$ 3$\lambda/D$) in the optical bands of O$_2$.
",0,1,0,0,0,0
18994,Manifold Based Low-rank Regularization for Image Restoration and Semi-supervised Learning,"  Low-rank structures play important role in recent advances of many problems
in image science and data science. As a natural extension of low-rank
structures for data with nonlinear structures, the concept of the
low-dimensional manifold structure has been considered in many data processing
problems. Inspired by this concept, we consider a manifold based low-rank
regularization as a linear approximation of manifold dimension. This
regularization is less restricted than the global low-rank regularization, and
thus enjoy more flexibility to handle data with nonlinear structures. As
applications, we demonstrate the proposed regularization to classical inverse
problems in image sciences and data sciences including image inpainting, image
super-resolution, X-ray computer tomography (CT) image reconstruction and
semi-supervised learning. We conduct intensive numerical experiments in several
image restoration problems and a semi-supervised learning problem of
classifying handwritten digits using the MINST data. Our numerical tests
demonstrate the effectiveness of the proposed methods and illustrate that the
new regularization methods produce outstanding results by comparing with many
existing methods.
",1,0,1,0,0,0
18995,History-aware Autonomous Exploration in Confined Environments using MAVs,"  Many scenarios require a robot to be able to explore its 3D environment
online without human supervision. This is especially relevant for inspection
tasks and search and rescue missions. To solve this high-dimensional path
planning problem, sampling-based exploration algorithms have proven successful.
However, these do not necessarily scale well to larger environments or spaces
with narrow openings. This paper presents a 3D exploration planner based on the
principles of Next-Best Views (NBVs). In this approach, a Micro-Aerial Vehicle
(MAV) equipped with a limited field-of-view depth sensor randomly samples its
configuration space to find promising future viewpoints. In order to obtain
high sampling efficiency, our planner maintains and uses a history of visited
places, and locally optimizes the robot's orientation with respect to
unobserved space. We evaluate our method in several simulated scenarios, and
compare it against a state-of-the-art exploration algorithm. The experiments
show substantial improvements in exploration time ($2\times$ faster),
computation time, and path length, and advantages in handling difficult
situations such as escaping dead-ends (up to $20\times$ faster). Finally, we
validate the on-line capability of our algorithm on a computational constrained
real world MAV.
",1,0,0,0,0,0
18996,A factor-model approach for correlation scenarios and correlation stress-testing,"  In 2012, JPMorgan accumulated a USD~6.2 billion loss on a credit derivatives
portfolio, the so-called `London Whale', partly as a consequence of
de-correlations of non-perfectly correlated positions that were supposed to
hedge each other. Motivated by this case, we devise a factor model for
correlations that allows for scenario-based stress testing of correlations. We
derive a number of analytical results related to a portfolio of homogeneous
assets. Using the concept of Mahalanobis distance, we show how to identify
adverse scenarios of correlation risk. In addition, we demonstrate how
correlation and volatility stress tests can be combined. As an example, we
apply the factor-model approach to the ""London Whale"" portfolio and determine
the value-at-risk impact from correlation changes. Since our findings are
particularly relevant for large portfolios, where even small correlation
changes can have a large impact, a further application would be to stress test
portfolios of central counterparties, which are of systemically relevant size.
",0,0,0,0,0,1
18997,Topology data analysis of critical transitions in financial networks,"  We develop a topology data analysis-based method to detect early signs for
critical transitions in financial data. From the time-series of multiple stock
prices, we build time-dependent correlation networks, which exhibit topological
structures. We compute the persistent homology associated to these structures
in order to track the changes in topology when approaching a critical
transition. As a case study, we investigate a portfolio of stocks during a
period prior to the US financial crisis of 2007-2008, and show the presence of
early signs of the critical transition.
",0,1,1,0,0,0
18998,Construction of exact constants of motion and effective models for many-body localized systems,"  One of the defining features of many-body localization is the presence of
extensively many quasi-local conserved quantities. These constants of motion
constitute a corner-stone to an intuitive understanding of much of the
phenomenology of many-body localized systems arising from effective
Hamiltonians. They may be seen as local magnetization operators smeared out by
a quasi-local unitary. However, accurately identifying such constants of motion
remains a challenging problem. Current numerical constructions often capture
the conserved operators only approximately restricting a conclusive
understanding of many-body localization. In this work, we use methods from the
theory of quantum many-body systems out of equilibrium to establish a new
approach for finding a complete set of exact constants of motion which are in
addition guaranteed to represent Pauli-$z$ operators. By this we are able to
construct and investigate the proposed effective Hamiltonian using exact
diagonalization. Hence, our work provides an important tool expected to further
boost inquiries into the breakdown of transport due to quenched disorder.
",0,1,0,0,0,0
18999,Toric manifolds over cyclohedra,"  We study the action of the dihedral group on the (equivariant) cohomology of
the toric manifolds associated with cycle graphs.
",0,0,1,0,0,0
19000,Use of Genome Information-Based Potentials to Characterize Human Adaptation,"  As a living information and communications system, the genome encodes
patterns in single nucleotide polymorphisms (SNPs) reflecting human adaption
that optimizes population survival in differing environments. This paper
mathematically models environmentally induced adaptive forces that quantify
changes in the distribution of SNP frequencies between populations. We make
direct connections between biophysical methods (e.g. minimizing genomic free
energy) and concepts in population genetics. Our unbiased computer program
scanned a large set of SNPs in the major histocompatibility complex region, and
flagged an altitude dependency on a SNP associated with response to oxygen
deprivation. The statistical power of our double-blind approach is demonstrated
in the flagging of mathematical functional correlations of SNP
information-based potentials in multiple populations with specific
environmental parameters. Furthermore, our approach provides insights for new
discoveries on the biology of common variants. This paper demonstrates the
power of biophysical modeling of population diversity for better understanding
genome-environment interactions in biological phenomenon.
",0,0,0,0,1,0
19001,Communication-Efficient Algorithms for Decentralized and Stochastic Optimization,"  We present a new class of decentralized first-order methods for nonsmooth and
stochastic optimization problems defined over multiagent networks. Considering
that communication is a major bottleneck in decentralized optimization, our
main goal in this paper is to develop algorithmic frameworks which can
significantly reduce the number of inter-node communications. We first propose
a decentralized primal-dual method which can find an $\epsilon$-solution both
in terms of functional optimality gap and feasibility residual in
$O(1/\epsilon)$ inter-node communication rounds when the objective functions
are convex and the local primal subproblems are solved exactly. Our major
contribution is to present a new class of decentralized primal-dual type
algorithms, namely the decentralized communication sliding (DCS) methods, which
can skip the inter-node communications while agents solve the primal
subproblems iteratively through linearizations of their local objective
functions. By employing DCS, agents can still find an $\epsilon$-solution in
$O(1/\epsilon)$ (resp., $O(1/\sqrt{\epsilon})$) communication rounds for
general convex functions (resp., strongly convex functions), while maintaining
the $O(1/\epsilon^2)$ (resp., $O(1/\epsilon)$) bound on the total number of
intra-node subgradient evaluations. We also present a stochastic counterpart
for these algorithms, denoted by SDCS, for solving stochastic optimization
problems whose objective function cannot be evaluated exactly. In comparison
with existing results for decentralized nonsmooth and stochastic optimization,
we can reduce the total number of inter-node communication rounds by orders of
magnitude while still maintaining the optimal complexity bounds on intra-node
stochastic subgradient evaluations. The bounds on the subgradient evaluations
are actually comparable to those required for centralized nonsmooth and
stochastic optimization.
",1,0,1,0,0,0
19002,A Machine Learning Approach to Shipping Box Design,"  Having the right assortment of shipping boxes in the fulfillment warehouse to
pack and ship customer's online orders is an indispensable and integral part of
nowadays eCommerce business, as it will not only help maintain a profitable
business but also create great experiences for customers. However, it is an
extremely challenging operations task to strategically select the best
combination of tens of box sizes from thousands of feasible ones to be
responsible for hundreds of thousands of orders daily placed on millions of
inventory products. In this paper, we present a machine learning approach to
tackle the task by formulating the box design problem prescriptively as a
generalized version of weighted $k$-medoids clustering problem, where the
parameters are estimated through a variety of descriptive analytics. We test
this machine learning approach on fulfillment data collected from Walmart U.S.
eCommerce, and our approach is shown to be capable of improving the box
utilization rate by more than $10\%$.
",0,0,0,1,0,0
19003,Precise but Natural Specification for Robot Tasks,"  We present Flipper, a natural language interface for describing high-level
task specifications for robots that are compiled into robot actions. Flipper
starts with a formal core language for task planning that allows expressing
rich temporal specifications and uses a semantic parser to provide a natural
language interface. Flipper provides immediate visual feedback by executing an
automatically constructed plan of the task in a graphical user interface. This
allows the user to resolve potentially ambiguous interpretations. Flipper
extends itself via naturalization: its users can add definitions for
utterances, from which Flipper induces new rules and adds them to the core
language, gradually growing a more and more natural task specification
language. Flipper improves the naturalization by generalizing the definition
provided by users. Unlike other task-specification systems, Flipper enables
natural language interactions while maintaining the expressive power and formal
precision of a programming language. We show through an initial user study that
natural language interactions and generalization can considerably ease the
description of tasks. Moreover, over time, users employ more and more concepts
outside of the initial core language. Such extensions are available to the
Flipper community, and users can use concepts that others have defined.
",1,0,0,0,0,0
19004,Rigorous Analysis for Efficient Statistically Accurate Algorithms for Solving Fokker-Planck Equations in Large Dimensions,"  This article presents a rigorous analysis for efficient statistically
accurate algorithms for solving the Fokker-Planck equations associated with
high-dimensional nonlinear turbulent dynamical systems with conditional
Gaussian structures. Despite the conditional Gaussianity, these nonlinear
systems contain many strong non-Gaussian features such as intermittency and
fat-tailed probability density functions (PDFs). The algorithms involve a
hybrid strategy that requires only a small number of samples $L$ to capture
both the transient and the equilibrium non-Gaussian PDFs with high accuracy.
Here, a conditional Gaussian mixture in a high-dimensional subspace via an
extremely efficient parametric method is combined with a judicious Gaussian
kernel density estimation in the remaining low-dimensional subspace. Rigorous
analysis shows that the mean integrated squared error in the recovered PDFs in
the high-dimensional subspace is bounded by the inverse square root of the
determinant of the conditional covariance, where the conditional covariance is
completely determined by the underlying dynamics and is independent of $L$.
This is fundamentally different from a direct application of kernel methods to
solve the full PDF, where $L$ needs to increase exponentially with the
dimension of the system and the bandwidth shrinks. A detailed comparison
between different methods justifies that the efficient statistically accurate
algorithms are able to overcome the curse of dimensionality. It is also shown
with mathematical rigour that these algorithms are robust in long time provided
that the system is controllable and stochastically stable. Particularly,
dynamical systems with energy-conserving quadratic nonlinearity as in many
geophysical and engineering turbulence are proved to have these properties.
",0,0,1,1,0,0
19005,Rapid processing of 85Kr/Kr ratios using Atom Trap Trace Analysis,"  We report a methodology for measuring 85Kr/Kr isotopic abundances using Atom
Trap Trace Analysis (ATTA) that increases sample measurement throughput by over
an order of magnitude to 6 samples per 24 hours. The noble gas isotope 85Kr
(half-life = 10.7 yr) is a useful tracer for young groundwater in the age range
of 5-50 years. ATTA, an efficient and selective laser-based atom counting
method, has recently been applied to 85Kr/Kr isotopic abundance measurements,
requiring 5-10 microliters of krypton gas at STP extracted from 50-100 L of
water. Previously a single such measurement required 48 hours. Our new method
demonstrates that we can measure 85Kr/Kr ratios with 3-5% relative uncertainty
every 4 hours, on average, with the same sample requirements.
",0,1,0,0,0,0
19006,Adapting Everyday Manipulation Skills to Varied Scenarios,"  We address the problem of executing tool-using manipulation skills in
scenarios where the objects to be used may vary. We assume that point clouds of
the tool and target object can be obtained, but no interpretation or further
knowledge about these objects is provided. The system must interpret the point
clouds and decide how to use the tool to complete a manipulation task with a
target object; this means it must adjust motion trajectories appropriately to
complete the task. We tackle three everyday manipulations: scraping material
from a tool into a container, cutting, and scooping from a container. Our
solution encodes these manipulation skills in a generic way, with parameters
that can be filled in at run-time via queries to a robot perception module; the
perception module abstracts the functional parts for the tool and extracts key
parameters that are needed for the task. The approach is evaluated in
simulation and with selected examples on a PR2 robot.
",1,0,0,0,0,0
19007,Effect of Particle Number Conservation on the Berry Phase Resulting from Transport of a Bound Quasiparticle around a Superfluid Vortex,"  Motivated by understanding Majorana zero modes in topological superfluids in
particle-number conserving framework beyond the present framework, we study the
effect of particle number conservation on the Berry phase resulting from
transport of a bound quasiparticle around a superfluid vortex. We find that
particle-number non-conserving calculations based on Bogoliubov-de Gennes (BdG)
equations are unable to capture the correct physics when the quasiparticle is
within the penetration depth of the vortex core where the superfluid velocity
is non-zero. Particle number conservation is crucial for deriving the correct
Berry phase in this context, and the Berry phase takes non-universal values
depending on the system parameters and the external trap imposed to bind the
quasiparticle. Of particular relevance to Majorana physics are the findings
that superfluid condensate affects the part of the Berry phase not accounted
for in the standard BdG framework, and that the superfluid many-body ground
state of odd number of fermions involves superfluid condensate deformation due
to the presence of the bound quasiparticle - an effect which is beyond the
description of the BdG equations.
",0,1,0,0,0,0
19008,A new topological insulator - β-InTe strained in the layer plane,"  We have investigated the band structure of the bulk crystal and the (001)
surface of the \beta-InTe layered crystal subjected to biaxial stretching in
the layer plane. The calculation has been carried out using the full-potential
linearized augmented plane wave method (FP LAPW) implemented in WIEN2k. It has
been shown that at the strain \Deltaa/a=0.06, where a is the lattice parameter
in the layer plane, the band gap in the electronic spectrum collapses. With
further strain increase a band inversion occurs. The inclusion of the
spin-orbit interaction reopens the gap in the electronic spectrum of a bulk
crystal, and our calculations show that the spectrum of the surface states has
the form of a Dirac cone, typical for topological insulators.
",0,1,0,0,0,0
19009,Stock management (Gestão de estoques),"  There is a great need to stock materials for production, but storing
materials comes at a cost. Lack of organization in the inventory can result in
a very high cost for the final product, in addition to generating other
problems in the production chain. In this work we present mathematical and
statistical methods applicable to stock management. The stock analysis using
ABC curves serves to identify which are the priority items, the most expensive
and with the highest turnover (demand), and thus determine, through stock
control models, the purchase lot size and the periodicity that minimize the
total costs of storing these materials. Using the Economic Order Quantity (EOQ)
model and the (Q,R) model, the inventory costs of a company were minimized. The
comparison of the results provided by the models was performed.
",0,0,0,1,0,0
19010,From Principal Subspaces to Principal Components with Linear Autoencoders,"  The autoencoder is an effective unsupervised learning model which is widely
used in deep learning. It is well known that an autoencoder with a single
fully-connected hidden layer, a linear activation function and a squared error
cost function trains weights that span the same subspace as the one spanned by
the principal component loading vectors, but that they are not identical to the
loading vectors. In this paper, we show how to recover the loading vectors from
the autoencoder weights.
",0,0,0,1,0,0
19011,A critical nonlinear elliptic equation with non local regional diffusion,"  In this article we are interested in the nonlocal regional Schrödinger
equation with critical exponent \begin{eqnarray*} &\epsilon^{2\alpha}
(-\Delta)_{\rho}^{\alpha}u + u = \lambda u^q + u^{2_{\alpha}^{*}-1} \mbox{ in }
\mathbb{R}^{N}, \\ & u \in H^{\alpha}(\mathbb{R}^{N}), \end{eqnarray*} where
$\epsilon$ is a small positive parameter, $\alpha \in (0,1)$, $q\in
(1,2_{\alpha}^{*}-1)$, $2_{\alpha}^{*} = \frac{2N}{N-2\alpha}$ is the critical
Sobolev exponent, $\lambda >0$ is a parameter and $(-\Delta)_{\rho}^{\alpha}$
is a variational version of the regional laplacian, whose range of scope is a
ball with radius $\rho(x)>0$. We study the existence of a ground state and we
analyze the behavior of semi-classical solutions as $\varepsilon\to 0$.
",0,0,1,0,0,0
19012,"Don't Decay the Learning Rate, Increase the Batch Size","  It is common practice to decay the learning rate. Here we show one can
usually obtain the same learning curve on both training and test sets by
instead increasing the batch size during training. This procedure is successful
for stochastic gradient descent (SGD), SGD with momentum, Nesterov momentum,
and Adam. It reaches equivalent test accuracies after the same number of
training epochs, but with fewer parameter updates, leading to greater
parallelism and shorter training times. We can further reduce the number of
parameter updates by increasing the learning rate $\epsilon$ and scaling the
batch size $B \propto \epsilon$. Finally, one can increase the momentum
coefficient $m$ and scale $B \propto 1/(1-m)$, although this tends to slightly
reduce the test accuracy. Crucially, our techniques allow us to repurpose
existing training schedules for large batch training with no hyper-parameter
tuning. We train ResNet-50 on ImageNet to $76.1\%$ validation accuracy in under
30 minutes.
",1,0,0,1,0,0
19013,Collisions of Dark Matter Axion Stars with Astrophysical Sources,"  If QCD axions form a large fraction of the total mass of dark matter, then
axion stars could be very abundant in galaxies. As a result, collisions with
each other, and with other astrophysical bodies, can occur. We calculate the
rate and analyze the consequences of three classes of collisions, those
occurring between a dilute axion star and: another dilute axion star, an
ordinary star, or a neutron star. In all cases we attempt to quantify the most
important astrophysical uncertainties; we also pay particular attention to
scenarios in which collisions lead to collapse of otherwise stable axion stars,
and possible subsequent decay through number changing interactions. Collisions
between two axion stars can occur with a high total rate, but the low relative
velocity required for collapse to occur leads to a very low total rate of
collapses. On the other hand, collisions between an axion star and an ordinary
star have a large rate, $\Gamma_\odot \sim 3000$ collisions/year/galaxy, and
for sufficiently heavy axion stars, it is plausible that most or all such
collisions lead to collapse. We identify in this case a parameter space which
has a stable region and a region in which collision triggers collapse, which
depend on the axion number ($N$) in the axion star, and a ratio of mass to
radius cubed characterizing the ordinary star ($M_s/R_s^3$). Finally, we
revisit the calculation of collision rates between axion stars and neutron
stars, improving on previous estimates by taking cylindrical symmetry of the
neutron star distribution into account. Collapse and subsequent decay through
collision processes, if occurring with a significant rate, can affect dark
matter phenomenology and the axion star mass distribution.
",0,1,0,0,0,0
19014,Practical Algorithms for Best-K Identification in Multi-Armed Bandits,"  In the Best-$K$ identification problem (Best-$K$-Arm), we are given $N$
stochastic bandit arms with unknown reward distributions. Our goal is to
identify the $K$ arms with the largest means with high confidence, by drawing
samples from the arms adaptively. This problem is motivated by various
practical applications and has attracted considerable attention in the past
decade. In this paper, we propose new practical algorithms for the Best-$K$-Arm
problem, which have nearly optimal sample complexity bounds (matching the lower
bound up to logarithmic factors) and outperform the state-of-the-art algorithms
for the Best-$K$-Arm problem (even for $K=1$) in practice.
",1,0,0,1,0,0
19015,Hyperfield Grassmannians,"  In a recent paper Baker and Bowler introduced matroids over hyperfields,
offering a common generalization of matroids, oriented matroids, and linear
subspaces of based vector spaces. This paper introduces the notion of a
topological hyperfield and explores the generalization of Grassmannians and
realization spaces to this context, particularly in relating the (hyper)fields
R and C to hyperfields arising in matroid theory and in tropical geometry.
",0,0,1,0,0,0
19016,Statistical estimation in a randomly structured branching population,"  We consider a binary branching process structured by a stochastic trait that
evolves according to a diffusion process that triggers the branching events, in
the spirit of Kimmel's model of cell division with parasite infection. Based on
the observation of the trait at birth of the first n generations of the
process, we construct nonparametric estimator of the transition of the
associated bifurcating chain and study the parametric estimation of the
branching rate. In the limit, as n tends to infinity, we obtain asymptotic
efficiency in the parametric case and minimax optimality in the nonparametric
case.
",0,0,1,0,0,0
19017,Discovering Eastern European PCs by hacking them. Today,"  Computer science would not be the same without personal computers. In the
West the so called PC revolution started in the late '70s and has its roots in
hobbyists and do-it-yourself clubs. In the following years the diffusion of
home and personal computers has made the discipline closer to many people. A
bit later, to a lesser extent, yet in a similar way, the revolution took place
also in East European countries. Today, the scenario of personal computing has
completely changed, however the computers of the '80s are still objects of
fascination for a number of retrocomputing fans who enjoy using, programming
and hacking the old 8-bits. The paper highlights the continuity between
yesterday's hobbyists and today's retrocomputing enthusiasts, particularly
focusing on East European PCs. Besides the preservation of old hardware and
software, the community is engaged in the development of emulators and cross
compilers. Such tools can be used for historical investigation, for example to
trace the origins of the BASIC interpreters loaded in the ROMs of East European
PCs.
",1,0,0,0,0,0
19018,Neural Rating Regression with Abstractive Tips Generation for Recommendation,"  Recently, some E-commerce sites launch a new interaction box called Tips on
their mobile apps. Users can express their experience and feelings or provide
suggestions using short texts typically several words or one sentence. In
essence, writing some tips and giving a numerical rating are two facets of a
user's product assessment action, expressing the user experience and feelings.
Jointly modeling these two facets is helpful for designing a better
recommendation system. While some existing models integrate text information
such as item specifications or user reviews into user and item latent factors
for improving the rating prediction, no existing works consider tips for
improving recommendation quality. We propose a deep learning based framework
named NRT which can simultaneously predict precise ratings and generate
abstractive tips with good linguistic quality simulating user experience and
feelings. For abstractive tips generation, gated recurrent neural networks are
employed to ""translate"" user and item latent representations into a concise
sentence. Extensive experiments on benchmark datasets from different domains
show that NRT achieves significant improvements over the state-of-the-art
methods. Moreover, the generated tips can vividly predict the user experience
and feelings.
",1,0,0,0,0,0
19019,Calibration Uncertainty for Advanced LIGO's First and Second Observing Runs,"  Calibration of the Advanced LIGO detectors is the quantification of the
detectors' response to gravitational waves. Gravitational waves incident on the
detectors cause phase shifts in the interferometer laser light which are read
out as intensity fluctuations at the detector output. Understanding this
detector response to gravitational waves is crucial to producing accurate and
precise gravitational wave strain data. Estimates of binary black hole and
neutron star parameters and tests of general relativity require well-calibrated
data, as miscalibrations will lead to biased results. We describe the method of
producing calibration uncertainty estimates for both LIGO detectors in the
first and second observing runs.
",0,1,0,0,0,0
19020,Entanglement transitions induced by large deviations,"  The probability of large deviations of the smallest Schmidt eigenvalue for
random pure states of bipartite systems, denoted as $A$ and $B$, is computed
analytically using a Coulomb gas method. It is shown that this probability, for
large $N$, goes as $\exp[-\beta N^2\Phi(\zeta)]$, where the parameter $\beta$
is the Dyson index of the ensemble, $\zeta$ is the large deviation parameter
while the rate function $\Phi(\zeta)$ is calculated exactly. Corresponding
equilibrium Coulomb charge density is derived for its large deviations. Effects
of the large deviations of the extreme (largest and smallest) Schmidt
eigenvalues on the bipartite entanglement are studied using the von Neumann
entropy. Effect of these deviations is also studied on the entanglement between
subsystems $1$ and $2$, obtained by further partitioning the subsystem $A$,
using the properties of the density matrix's partial transpose
$\rho_{12}^\Gamma$. The density of states of $\rho_{12}^\Gamma$ is found to be
close to the Wigner's semicircle law with these large deviations. The
entanglement properties are captured very well by a simple random matrix model
for the partial transpose. The model predicts the entanglement transition
across a critical large deviation parameter $\zeta$. Log negativity is used to
quantify the entanglement between subsystems $1$ and $2$. Analytical formulas
for it are derived using the simple model. Numerical simulations are in
excellent agreement with the analytical results.
",0,0,1,1,0,0
19021,"Understanding System Characteristics of Online Erasure Coding on Scalable, Distributed and Large-Scale SSD Array Systems","  Large-scale systems with arrays of solid state disks (SSDs) have become
increasingly common in many computing segments. To make such systems resilient,
we can adopt erasure coding such as Reed-Solomon (RS) code as an alternative to
replication because erasure coding can offer a significantly lower storage cost
than replication. To understand the impact of using erasure coding on system
performance and other system aspects such as CPU utilization and network
traffic, we build a storage cluster consisting of approximately one hundred
processor cores with more than fifty high-performance SSDs, and evaluate the
cluster with a popular open-source distributed parallel file system, Ceph. Then
we analyze behaviors of systems adopting erasure coding from the following five
viewpoints, compared with those of systems using replication: (1) storage
system I/O performance; (2) computing and software overheads; (3) I/O
amplification; (4) network traffic among storage nodes; (5) the impact of
physical data layout on performance of RS-coded SSD arrays. For all these
analyses, we examine two representative RS configurations, which are used by
Google and Facebook file systems, and compare them with triple replication that
a typical parallel file system employs as a default fault tolerance mechanism.
Lastly, we collect 54 block-level traces from the cluster and make them
available for other researchers.
",1,0,0,0,0,0
19022,Three Questions on Special Homeomorphisms on Subgroups of $R$ and $R^\infty$,"  We provide justifications for two questions on special maps on subgroups of
the reals. We will show that the questions can be treated from different points
of view. We also discuss two versions of Anderson's Involution Conjecture.
",0,0,1,0,0,0
19023,Nearly Semiparametric Efficient Estimation of Quantile Regression,"  As a competitive alternative to least squares regression, quantile regression
is popular in analyzing heterogenous data. For quantile regression model
specified for one single quantile level $\tau$, major difficulties of
semiparametric efficient estimation are the unavailability of a parametric
efficient score and the conditional density estimation. In this paper, with the
help of the least favorable submodel technique, we first derive the
semiparametric efficient scores for linear quantile regression models that are
assumed for a single quantile level, multiple quantile levels and all the
quantile levels in $(0,1)$ respectively. Our main discovery is a one-step
(nearly) semiparametric efficient estimation for the regression coefficients of
the quantile regression models assumed for multiple quantile levels, which has
several advantages: it could be regarded as an optimal way to pool information
across multiple/other quantiles for efficiency gain; it is computationally
feasible and easy to implement, as the initial estimator is easily available;
due to the nature of quantile regression models under investigation, the
conditional density estimation is straightforward by plugging in an initial
estimator. The resulting estimator is proved to achieve the corresponding
semiparametric efficiency lower bound under regularity conditions. Numerical
studies including simulations and an example of birth weight of children
confirms that the proposed estimator leads to higher efficiency compared with
the Koenker-Bassett quantile regression estimator for all quantiles of
interest.
",0,0,0,1,0,0
19024,Relevance of backtracking paths in epidemic spreading on networks,"  The understanding of epidemics on networks has greatly benefited from the
recent application of message-passing approaches, which allow to derive exact
results for irreversible spreading (i.e. diseases with permanent acquired
immunity) in locally-tree like topologies. This success has suggested the
application of the same approach to reversible epidemics, for which an
individual can contract the epidemic and recover repeatedly. The underlying
assumption is that backtracking paths (i.e. an individual is reinfected by a
neighbor he/she previously infected) do not play a relevant role. In this paper
we show that this is not the case for reversible epidemics, since the neglect
of backtracking paths leads to a formula for the epidemic threshold that is
qualitatively incorrect in the large size limit. Moreover we define a modified
reversible dynamics which explicitly forbids direct backtracking events and
show that this modification completely upsets the phenomenology.
",1,0,0,0,0,0
19025,Hybrid graphene tunneling photoconductor with interface engineering towards fast photoresponse and high responsivity,"  Hybrid graphene photoconductor/phototransistor has achieved giant
photoresponsivity, but its response speed dramatically degrades as the expense
due to the long lifetime of trapped interfacial carriers. In this work, by
intercalating a large-area atomically thin MoS2 film into a hybrid graphene
photoconductor, we have developed a prototype tunneling photoconductor, which
exhibits a record-fast response (rising time ~17 ns) and a high responsivity
(~$3\times10^4$ A/W at 635 nm and 16.8 nW illumination) across the broad
spectral range. We demonstrate that the photo-excited carriers generated in
silicon are transferred into graphene through a tunneling process rather than
carrier drift. The atomically thin MoS2 film not only serves as tunneling layer
but also passivates surface states, which in combination delivers a superior
response speed (~3 order of magnitude improved than a device without MoS2
layer), while the responsivity remains high. This intriguing tunneling
photoconductor integrates both fast response and high responsivity and thus has
significant potential in practical applications of optoelectronic devices.
",0,1,0,0,0,0
19026,Embedded Real-Time Fall Detection Using Deep Learning For Elderly Care,"  This paper proposes a real-time embedded fall detection system using a
DVS(Dynamic Vision Sensor) that has never been used for traditional fall
detection, a dataset for fall detection using that, and a DVS-TN(DVS-Temporal
Network). The first contribution is building a DVS Falls Dataset, which made
our network to recognize a much greater variety of falls than the existing
datasets that existed before and solved privacy issues using the DVS. Secondly,
we introduce the DVS-TN : optimized deep learning network to detect falls using
DVS. Finally, we implemented a fall detection system which can run on
low-computing H/W with real-time, and tested on DVS Falls Dataset that takes
into account various falls situations. Our approach achieved 95.5% on the
F1-score and operates at 31.25 FPS on NVIDIA Jetson TX1 board.
",1,0,0,1,0,0
19027,A Distributed Algorithm for Solving Linear Algebraic Equations Over Random Networks,"  In this paper, we consider the problem of solving linear algebraic equations
of the form $Ax=b$ among multi agents which seek a solution by using local
information in presence of random communication topologies. The equation is
solved by $m$ agents where each agent only knows a subset of rows of the
partitioned matrix $[A,b]$. We formulate the problem such that this formulation
does not need the distribution of random interconnection graphs. Therefore,
this framework includes asynchronous updates or unreliable communication
protocols without B-connectivity assumption. We apply the random
Krasnoselskii-Mann iterative algorithm which converges almost surely and in
mean square to a solution of the problem for any matrices $A$ and $b$ and any
initial conditions of agents' states. We demonestrate that the limit point to
which the agents' states converge is determined by the unique solution of a
convex optimization problem regardless of the distribution of random
communication graphs. Eventually, we show by two numerical examples that the
rate of convergence of the algorithm cannot be guaranteed.
",1,0,0,0,0,0
19028,Doping-induced quantum cross-over in Er$_2$Ti$_{2-x}$Sn$_x$O$_7$,"  We present the results of the investigation of magnetic properties of the
Er$_2$Ti$_{2-x}$Sn$_x$O$_7$ series. For small doping values the ordering
temperature decreases linearly with $x$ while the moment configuration remains
the same as in the $x = 0$ parent compound. Around $x = 1.7$ doping level we
observe a change in the behavior, where the ordering temperature starts to
increase and new magnetic Bragg peaks appear. For the first time we present
evidence of a long-range order (LRO) in Er$_2$Sn$_2$O$_7$ ($x = 2.0$) below
$T_N = 130$ mK. It is revealed that the moment configuration corresponds to a
Palmer-Chalker type with a value of the magnetic moment significantly
renormalized compared to $x = 0$. We discuss our results in the framework of a
possible quantum phase transition occurring close to $x = 1.7$.
",0,1,0,0,0,0
19029,Stochastic Block Models with Multiple Continuous Attributes,"  The stochastic block model (SBM) is a probabilistic model for community
structure in networks. Typically, only the adjacency matrix is used to perform
SBM parameter inference. In this paper, we consider circumstances in which
nodes have an associated vector of continuous attributes that are also used to
learn the node-to-community assignments and corresponding SBM parameters. While
this assumption is not realistic for every application, our model assumes that
the attributes associated with the nodes in a network's community can be
described by a common multivariate Gaussian model. In this augmented,
attributed SBM, the objective is to simultaneously learn the SBM connectivity
probabilities with the multivariate Gaussian parameters describing each
community. While there are recent examples in the literature that combine
connectivity and attribute information to inform community detection, our model
is the first augmented stochastic block model to handle multiple continuous
attributes. This provides the flexibility in biological data to, for example,
augment connectivity information with continuous measurements from multiple
experimental modalities. Because the lack of labeled network data often makes
community detection results difficult to validate, we highlight the usefulness
of our model for two network prediction tasks: link prediction and
collaborative filtering. As a result of fitting this attributed stochastic
block model, one can predict the attribute vector or connectivity patterns for
a new node in the event of the complementary source of information
(connectivity or attributes, respectively). We also highlight two biological
examples where the attributed stochastic block model provides satisfactory
performance in the link prediction and collaborative filtering tasks.
",1,0,0,1,0,0
19030,Asymptotic Independence of Bivariate Order Statistics,"  It is well known that an extreme order statistic and a central order
statistic (os) as well as an intermediate os and a central os from a sample of
iid univariate random variables get asymptotically independent as the sample
size increases. We extend this result to bivariate random variables, where the
os are taken componentwise. An explicit representation of the conditional
distribution of bivariate os turns out to be a powerful tool.
",0,0,1,1,0,0
19031,Semi-Supervised Haptic Material Recognition for Robots using Generative Adversarial Networks,"  Material recognition enables robots to incorporate knowledge of material
properties into their interactions with everyday objects. For example, material
recognition opens up opportunities for clearer communication with a robot, such
as ""bring me the metal coffee mug"", and recognizing plastic versus metal is
crucial when using a microwave or oven. However, collecting labeled training
data with a robot is often more difficult than unlabeled data. We present a
semi-supervised learning approach for material recognition that uses generative
adversarial networks (GANs) with haptic features such as force, temperature,
and vibration. Our approach achieves state-of-the-art results and enables a
robot to estimate the material class of household objects with ~90% accuracy
when 92% of the training data are unlabeled. We explore how well this approach
can recognize the material of new objects and we discuss challenges facing
generalization. To motivate learning from unlabeled training data, we also
compare results against several common supervised learning classifiers. In
addition, we have released the dataset used for this work which consists of
time-series haptic measurements from a robot that conducted thousands of
interactions with 72 household objects.
",1,0,0,1,0,0
19032,Level set shape and topology optimization of finite strain bilateral contact problems,"  This paper presents a method for the optimization of multi-component
structures comprised of two and three materials considering large motion
sliding contact and separation along interfaces. The structural geometry is
defined by an explicit level set method, which allows for both shape and
topology changes. The mechanical model assumes finite strains, a nonlinear
elastic material behavior, and a quasi-static response. Identification of
overlapping surface position is handled by a coupled parametric representation
of contact surfaces. A stabilized Lagrange method and an active set strategy
are used to model frictionless contact and separation. The mechanical model is
discretized by the extended finite element method which maintains a clear
definition of geometry. Face-oriented ghost penalization and dynamic relaxation
are implemented to improve the stability of the physical response prediction. A
nonlinear programming scheme is used to solve the optimization problem, which
is regularized by introducing a perimeter penalty into the objective function.
Sensitivities are determined by the adjoint method. The main characteristics of
the proposed method are studied by numerical examples in two dimensions. The
numerical results demonstrate improved design performance when compared to
models optimized with a small strain assumption. Additionally, examples with
load path dependent objectives display non-intuitive designs.
",0,0,1,0,0,0
19033,Thermal distortions of non-Gaussian beams in Fabry-Perot cavities,"  Thermal effects are already important in currently operating interferometric
gravitational wave detectors. Planned upgrades of these detectors involve
increasing optical power to combat quantum shot noise. We consider the
ramifications of this increased power for one particular class of laser
beams--wide, flat-topped, mesa beams. In particular we model a single mesa beam
Fabry-Perot cavity having thermoelastically deformed mirrors. We calculate the
intensity profile of the fundamental cavity eigenmode in the presence of
thermal perturbations, and the associated changes in thermal noise. We also
outline an idealized method of correcting for such effects. At each stage we
contrast our results with those of a comparable Gaussian beam cavity. Although
we focus on mesa beams the techniques described are applicable to any
azimuthally symmetric system.
",0,1,0,0,0,0
19034,Rigidity and trace properties of divergence-measure vector fields,"  We show some rigidity properties of divergence-free vector fields defined on
half-spaces. As an application, we prove the existence of the classical trace
for a bounded, divergence-measure vector field $\xi$ defined on the Euclidean
plane, at almost every point of a locally oriented rectifiable set $S$, under
the assumption that its weak normal trace $[\xi\cdot \nu_S]$ attains a local
maximum for the norm of $\xi$ at the point.
",0,0,1,0,0,0
19035,A note on the Almansi property,"  The first goal of this note is to study the Almansi property on an
m-dimensional model in the sense of Greene and Wu and, more generally, in a
Riemannian geometric setting. In particular, we shall prove that the only model
on which the Almansi property is verified is the Euclidean space R^m. In the
second part of the paper we shall study Almansi's property and biharmonicity
for functions which depend on the distance from a given submanifold. Finally,
in the last section we provide an extension to the semi-Euclidean case R^{p,q}
which includes the proof of the classical Almansi property in R^m as a special
instance.
",0,0,1,0,0,0
19036,The Quantum Complexity of Computing Schatten $p$-norms,"  We consider the quantum complexity of computing Schatten $p$-norms and
related quantities, and find that the problem of estimating these quantities is
closely related to the one clean qubit model of computation. We show that the
problem of approximating $\text{Tr}\, (|A|^p)$ for a log-local $n$-qubit
Hamiltonian $A$ and $p=\text{poly}(n)$, up to a suitable level of accuracy, is
contained in DQC1; and that approximating this quantity up to a somewhat higher
level of accuracy is DQC1-hard. In some cases the level of accuracy achieved by
the quantum algorithm is substantially better than a natural classical
algorithm for the problem. The same problem can be solved for arbitrary sparse
matrices in BQP. One application of the algorithm is the approximate
computation of the energy of a graph.
",1,0,0,0,0,0
19037,Bounds for fidelity of semiclassical Lagrangian states in K{ä}hler quantization,"  We define mixed states associated with submanifolds with probability
densities in quantizable closed K{ä}hler manifolds. Then, we address the
problem of comparing two such states via their fidelity. Firstly, we estimate
the sub-fidelity and super-fidelity of two such states, giving lower and upper
bounds for their fidelity, when the underlying submanifolds are two Lagrangian
submanifolds intersecting transversally at a finite number of points, in the
semiclassical limit. Secondly, we investigate a family of examples on the
sphere, for which we manage to obtain a better upper bound for the fidelity. We
conclude by stating a conjecture regarding the fidelity in the general case.
",0,0,1,0,0,0
19038,Precision measurement of antiproton to proton ratio with the Alpha Magnetic Spectrometer on the International Space Station,"  A precision measurement by AMS of the antiproton-to-proton flux ratio in
primary cosmic rays in the absolute rigidity range from 1 to 450 GV is
presented based on $3.49\times10^5$ antiproton events and $2.42\times10^9$
proton events. Above $\sim60$ GV the antiproton to proton flux ratio is
consistent with being rigidity independent. A decreasing behaviour is expected
for this ratio considering the traditional models for the secondary antiproton
flux.
",0,1,0,0,0,0
19039,Stability criteria for the 2D $α$-Euler equations,"  We derive analogues of the classical Rayleigh, Fjortoft and Arnold stability
and instability theorems in the context of the 2D $\alpha$-Euler equations.
",0,1,0,0,0,0
19040,Superconducting spin valves controlled by spiral re-orientation in B20-family magnets,"  We propose a superconducting spin-triplet valve, which consists of a
superconductor and an itinerant magnetic material, with the magnet showing an
intrinsic non-collinear order characterized by a wave vector that may be
aligned in a few equivalent preferred directions under control of a weak
external magnetic field. Re-orienting the spiral direction allows one to
controllably modify long-range spin-triplet superconducting correlations,
leading to spin-valve switching behavior. Our results indicate that the
spin-valve effect may be noticeable. This bilayer may be used as a magnetic
memory element for cryogenic nanoelectronics. It has the following advantages
in comparison to superconducting spin valves proposed previously: (i) it
contains only one magnetic layer, which may be more easily fabricated and
controlled, (ii) its ground states are separated by a potential barrier, which
solves the ""half-select"" problem of the addressed switch of memory elements.
",0,1,0,0,0,0
19041,Experience Recommendation for Long Term Safe Learning-based Model Predictive Control in Changing Operating Conditions,"  Learning has propelled the cutting edge of performance in robotic control to
new heights, allowing robots to operate with high performance in conditions
that were previously unimaginable. The majority of the work, however, assumes
that the unknown parts are static or slowly changing. This limits them to
static or slowly changing environments. However, in the real world, a robot may
experience various unknown conditions. This paper presents a method to extend
an existing single mode GP-based safe learning controller to learn an
increasing number of non-linear models for the robot dynamics. We show that
this approach enables a robot to re-use past experience from a large number of
previously visited operating conditions, and to safely adapt when a new and
distinct operating condition is encountered. This allows the robot to achieve
safety and high performance in an large number of operating conditions that do
not have to be specified ahead of time. Our approach runs independently from
the controller, imposing no additional computation time on the control loop
regardless of the number of previous operating conditions considered. We
demonstrate the effectiveness of our approach in experiment on a 900\,kg ground
robot with both physical and artificial changes to its dynamics. All of our
experiments are conducted using vision for localization.
",1,0,0,0,0,0
19042,Calderón-type inequalities for affine frames,"  We prove sharp upper and lower bounds for generalized Calderón's sums
associated to frames on LCA groups generated by affine actions of cocompact
subgroup translations and general measurable families of automorphisms. The
proof makes use of techniques of analysis on metric spaces, and relies on a
counting estimate of lattice points inside metric balls. We will deduce as
special cases Calderón-type inequalities for families of expanding
automorphisms as well as for LCA-Gabor systems.
",0,0,1,0,0,0
19043,Magnetism in Semiconducting Molybdenum Dichalcogenides,"  Transition metal dichalcogenides (TMDs) are interesting for understanding
fundamental physics of two-dimensional materials (2D) as well as for many
emerging technologies, including spin electronics. Here, we report the
discovery of long-range magnetic order below TM = 40 K and 100 K in bulk
semiconducting TMDs 2H-MoTe2 and 2H-MoSe2, respectively, by means of muon
spin-rotation (muSR), scanning tunneling microscopy (STM), as well as density
functional theory (DFT) calculations. The muon spin rotation measurements show
the presence of a large and homogeneous internal magnetic fields at low
temperatures in both compounds indicative of long-range magnetic order. DFT
calculations show that this magnetism is promoted by the presence of defects in
the crystal. The STM measurements show that the vast majority of defects in
these materials are metal vacancies and chalcogen-metal antisites which are
randomly distributed in the lattice at the sub-percent level. DFT indicates
that the antisite defects are magnetic with a magnetic moment in the range of
0.9-2.8 mu_B. Further, we find that the magnetic order stabilized in 2H-MoTe2
and 2H-MoSe2 is highly sensitive to hydrostatic pressure. These observations
establish 2H-MoTe2 and 2H-MoSe2 as a new class of magnetic semiconductors and
opens a path to studying the interplay of 2D physics and magnetism in these
interesting semiconductors.
",0,1,0,0,0,0
19044,Characteristic Polynomial of Certain Hyperplane Arrangements through Graph Theory,"  We give a formula for computing the characteristic polynomial for certain
hyperplane arrangements in terms of the number of bipartite graphs of given
rank and cardinality.
",0,0,1,0,0,0
19045,Accelerated Evaluation of Automated Vehicles Using Piecewise Mixture Models,"  The process to certify highly Automated Vehicles has not yet been defined by
any country in the world. Currently, companies test Automated Vehicles on
public roads, which is time-consuming and inefficient. We proposed the
Accelerated Evaluation concept, which uses a modified statistics of the
surrounding vehicles and the Importance Sampling theory to reduce the
evaluation time by several orders of magnitude, while ensuring the evaluation
results are statistically accurate. In this paper, we further improve the
accelerated evaluation concept by using Piecewise Mixture Distribution models,
instead of Single Parametric Distribution models. We developed and applied this
idea to forward collision control system reacting to vehicles making cut-in
lane changes. The behavior of the cut-in vehicles was modeled based on more
than 403,581 lane changes collected by the University of Michigan Safety Pilot
Model Deployment Program. Simulation results confirm that the accuracy and
efficiency of the Piecewise Mixture Distribution method outperformed single
parametric distribution methods in accuracy and efficiency, and accelerated the
evaluation process by almost four orders of magnitude.
",1,0,0,0,0,0
19046,Phase shift's influence of two strong pulsed laser waves on effective interaction of electrons,"  The phase shift's influence of two strong pulsed laser waves on effective
interaction of electrons was studied. Considerable amplification of electrons
repulsion in the certain range of phase shifts and waves intensities is shown.
That leads to electrons scatter on greater distances than without an external
field. The value of the distance can be greater on 2-3 order of magnitude. Also
considerable influence of the phase shift of pulses of waves on the possibility
of effective attraction of electrons is shown.
",0,1,0,0,0,0
19047,Topological Sieving of Rings According to their Rigidity,"  We present a novel mechanism for resolving the mechanical rigidity of
nanoscopic circular polymers that flow in a complex environment. The emergence
of a regime of negative differential mobility induced by topological
interactions between the rings and the substrate is the key mechanism for
selective sieving of circular polymers with distinct flexibilities. A simple
model accurately describes the sieving process observed in molecular dynamics
simulations and yields experimentally verifiable analytical predictions, which
can be used as a reference guide for improving filtration procedures of
circular filaments. The topological sieving mechanism we propose ought to be
relevant also in probing the microscopic details of complex substrates.
",0,0,0,0,1,0
19048,A structural Markov property for decomposable graph laws that allows control of clique intersections,"  We present a new kind of structural Markov property for probabilistic laws on
decomposable graphs, which allows the explicit control of interactions between
cliques, so is capable of encoding some interesting structure. We prove the
equivalence of this property to an exponential family assumption, and discuss
identifiability, modelling, inferential and computational implications.
",0,0,0,1,0,0
19049,Fisher consistency for prior probability shift,"  We introduce Fisher consistency in the sense of unbiasedness as a desirable
property for estimators of class prior probabilities. Lack of Fisher
consistency could be used as a criterion to dismiss estimators that are
unlikely to deliver precise estimates in test datasets under prior probability
and more general dataset shift. The usefulness of this unbiasedness concept is
demonstrated with three examples of classifiers used for quantification:
Adjusted Classify & Count, EM-algorithm and CDE-Iterate. We find that Adjusted
Classify & Count and EM-algorithm are Fisher consistent. A counter-example
shows that CDE-Iterate is not Fisher consistent and, therefore, cannot be
trusted to deliver reliable estimates of class probabilities.
",1,0,0,1,0,0
19050,Quantifying the Effects of Enforcing Disentanglement on Variational Autoencoders,"  The notion of disentangled autoencoders was proposed as an extension to the
variational autoencoder by introducing a disentanglement parameter $\beta$,
controlling the learning pressure put on the possible underlying latent
representations. For certain values of $\beta$ this kind of autoencoders is
capable of encoding independent input generative factors in separate elements
of the code, leading to a more interpretable and predictable model behaviour.
In this paper we quantify the effects of the parameter $\beta$ on the model
performance and disentanglement. After training multiple models with the same
value of $\beta$, we establish the existence of consistent variance in one of
the disentanglement measures, proposed in literature. The negative consequences
of the disentanglement to the autoencoder's discriminative ability are also
asserted while varying the amount of examples available during training.
",1,0,0,1,0,0
19051,An omnibus test for the global null hypothesis,"  Global hypothesis tests are a useful tool in the context of, e.g, clinical
trials, genetic studies or meta analyses, when researchers are not interested
in testing individual hypotheses, but in testing whether none of the hypotheses
is false. There are several possibilities how to test the global null
hypothesis when the individual null hypotheses are independent. If it is
assumed that many of the individual null hypotheses are false, combinations
tests have been recommended to maximise power. If, however, it is assumed that
only one or a few null hypotheses are false, global tests based on individual
test statistics are more powerful (e.g., Bonferroni or Simes test). However,
usually there is no a-priori knowledge on the number of false individual null
hypotheses. We therefore propose an omnibus test based on the combination of
p-values. We show that this test yields an impressive overall performance. The
proposed method is implemented in the R-package omnibus.
",0,0,0,1,0,0
19052,Learning Without Mixing: Towards A Sharp Analysis of Linear System Identification,"  We prove that the ordinary least-squares (OLS) estimator attains nearly
minimax optimal performance for the identification of linear dynamical systems
from a single observed trajectory. Our upper bound relies on a generalization
of Mendelson's small-ball method to dependent data, eschewing the use of
standard mixing-time arguments. Our lower bounds reveal that these upper bounds
match up to logarithmic factors. In particular, we capture the correct
signal-to-noise behavior of the problem, showing that more unstable linear
systems are easier to estimate. This behavior is qualitatively different from
arguments which rely on mixing-time calculations that suggest that unstable
systems are more difficult to estimate. We generalize our technique to provide
bounds for a more general class of linear response time-series.
",0,0,0,1,0,0
19053,Positive and Unlabeled Learning through Negative Selection and Imbalance-aware Classification,"  Motivated by applications in protein function prediction, we consider a
challenging supervised classification setting in which positive labels are
scarce and there are no explicit negative labels. The learning algorithm must
thus select which unlabeled examples to use as negative training points,
possibly ending up with an unbalanced learning problem. We address these issues
by proposing an algorithm that combines active learning (for selecting negative
examples) with imbalance-aware learning (for mitigating the label imbalance).
In our experiments we observe that these two techniques operate
synergistically, outperforming state-of-the-art methods on standard protein
function prediction benchmarks.
",0,0,0,0,1,0
19054,Superconductivity in quantum wires: A symmetry analysis,"  We study properties of quantim wires with spin-orbit coupling and time
reversal symmetry breaking, in normal and superconducting states. Electronic
band structures are classified according to quasi-one-dimensional magnetic
point groups, or magnetic classes. The latter belong to one of three distinct
types, depending on the way the time reversal operation appears in the group
elements. The superconducting gap functions are constructed using antiunitary
operations and have different symmetry properties depending on the type of the
magnetic point group. We obtain the spectrum of the Andreev boundary modes near
the end of the wire in a model-independent way, using the semiclassical
approach with the boundary conditions described by a phenomenological
scattering matrix. Explicit expressions for the bulk topological invariants
controlling the number of the boundary zero modes are presented in the general
multiband case for two types of the magnetic point groups, corresponding to
DIII and BDI symmetry classes.
",0,1,0,0,0,0
19055,Exponentially convergent data assimilation algorithm for Navier-Stokes equations,"  The paper presents a new state estimation algorithm for a bilinear equation
representing the Fourier- Galerkin (FG) approximation of the Navier-Stokes (NS)
equations on a torus in R2. This state equation is subject to uncertain but
bounded noise in the input (Kolmogorov forcing) and initial conditions, and its
output is incomplete and contains bounded noise. The algorithm designs a
time-dependent gain such that the estimation error converges to zero
exponentially. The sufficient condition for the existence of the gain are
formulated in the form of algebraic Riccati equations. To demonstrate the
results we apply the proposed algorithm to the reconstruction a chaotic fluid
flow from incomplete and noisy data.
",0,1,0,0,0,0
19056,A Multi-Modal Approach to Infer Image Affect,"  The group affect or emotion in an image of people can be inferred by
extracting features about both the people in the picture and the overall makeup
of the scene. The state-of-the-art on this problem investigates a combination
of facial features, scene extraction and even audio tonality. This paper
combines three additional modalities, namely, human pose, text-based tagging
and CNN extracted features / predictions. To the best of our knowledge, this is
the first time all of the modalities were extracted using deep neural networks.
We evaluate the performance of our approach against baselines and identify
insights throughout this paper.
",0,0,0,1,0,0
19057,Joint Tilt Angle Adaptation and Beamforming in Multicell Multiuser Cellular Networks,"  3D beamforming is a promising approach for interference coordination in
cellular networks which brings significant improvements in comparison with
conventional 2D beamforming techniques. This paper investigates the problem of
joint beamforming design and tilt angle adaptation of the BS antenna array for
maximizing energy efficiency (EE) in downlink of multi-cell multi-user
coordinated cellular networks. An iterative algorithm based on fractional
programming approach is introduced to solve the resulting non-convex
optimization problem. In each iteration, users are clustered based on their
elevation angle. Then, optimization of the tilt angle is carried out through a
reduced complexity greedy search to find the best tilt angle for a given
placement of the users. Numerical results show that the proposed algorithm
achieves higher EE compared to the 2D beamforming techniques.
",1,0,1,0,0,0
19058,Testing Degree Corrections in Stochastic Block Models,"  We study sharp detection thresholds for degree corrections in Stochastic
Block Models in the context of a goodness of fit problem. When degree
corrections are relatively dense, a simple test based on the total number of
edges is asymptotically optimal. For sparse degree corrections in non-dense
graphs, simple degree based Higher Criticism Test (Mukherjee, Mukherjee, Sen
2016) is optimal with sharp constants. In contrast, for dense graphs, the
optimal procedure runs in two stages. It involves running a suitable community
recovery algorithm in stage 1, followed by a Higher Criticism Test based on a
linear combination of within and across (estimated) community degrees in stage
2. The necessity of the two step procedure is demonstrated by the failure of
the ordinary Maximum Degree Test in achieving sharp constants. As necessary
tools we also derive asymptotic distribution of the Maximum Degree in
Stochastic Block Models along with moderate deviation and local central limit
type asymptotics of positive linear combinations of independent Binomial random
variables.
",0,0,1,1,0,0
19059,A Survey on Mobile Edge Computing: The Communication Perspective,"  Driven by the visions of Internet of Things and 5G communications, recent
years have seen a paradigm shift in mobile computing, from the centralized
Mobile Cloud Computing towards Mobile Edge Computing (MEC). The main feature of
MEC is to push mobile computing, network control and storage to the network
edges (e.g., base stations and access points) so as to enable
computation-intensive and latency-critical applications at the resource-limited
mobile devices. MEC promises dramatic reduction in latency and mobile energy
consumption, tackling the key challenges for materializing 5G vision. The
promised gains of MEC have motivated extensive efforts in both academia and
industry on developing the technology. A main thrust of MEC research is to
seamlessly merge the two disciplines of wireless communications and mobile
computing, resulting in a wide-range of new designs ranging from techniques for
computation offloading to network architectures. This paper provides a
comprehensive survey of the state-of-the-art MEC research with a focus on joint
radio-and-computational resource management. We also present a research outlook
consisting of a set of promising directions for MEC research, including MEC
system deployment, cache-enabled MEC, mobility management for MEC, green MEC,
as well as privacy-aware MEC. Advancements in these directions will facilitate
the transformation of MEC from theory to practice. Finally, we introduce recent
standardization efforts on MEC as well as some typical MEC application
scenarios.
",1,0,1,0,0,0
19060,Deep Learning Approximation: Zero-Shot Neural Network Speedup,"  Neural networks offer high-accuracy solutions to a range of problems, but are
costly to run in production systems because of computational and memory
requirements during a forward pass. Given a trained network, we propose a
techique called Deep Learning Approximation to build a faster network in a tiny
fraction of the time required for training by only manipulating the network
structure and coefficients without requiring re-training or access to the
training data. Speedup is achieved by by applying a sequential series of
independent optimizations that reduce the floating-point operations (FLOPs)
required to perform a forward pass. First, lossless optimizations are applied,
followed by lossy approximations using singular value decomposition (SVD) and
low-rank matrix decomposition. The optimal approximation is chosen by weighing
the relative accuracy loss and FLOP reduction according to a single parameter
specified by the user. On PASCAL VOC 2007 with the YOLO network, we show an
end-to-end 2x speedup in a network forward pass with a 5% drop in mAP that can
be re-gained by finetuning.
",0,0,0,1,0,0
19061,Frequency truncated discrete-time system norm,"  Multirate digital signal processing and model reduction applications require
computation of the frequency truncated norm of a discrete-time system. This
paper explains how to compute the frequency truncated norm of a discrete-time
system. To this end, a much-generalized problem of integrating a transfer
function of a discrete-time system given in the descriptor form over an
interval of limited frequencies is also discussed along with its computation.
",1,0,0,0,0,0
19062,Formal Guarantees on the Robustness of a Classifier against Adversarial Manipulation,"  Recent work has shown that state-of-the-art classifiers are quite brittle, in
the sense that a small adversarial change of an originally with high confidence
correctly classified input leads to a wrong classification again with high
confidence. This raises concerns that such classifiers are vulnerable to
attacks and calls into question their usage in safety-critical systems. We show
in this paper for the first time formal guarantees on the robustness of a
classifier by giving instance-specific lower bounds on the norm of the input
manipulation required to change the classifier decision. Based on this analysis
we propose the Cross-Lipschitz regularization functional. We show that using
this form of regularization in kernel methods resp. neural networks improves
the robustness of the classifier without any loss in prediction performance.
",1,0,0,1,0,0
19063,Complete DFM Model for High-Performance Computing SoCs with Guard Ring and Dummy Fill Effect,"  For nanotechnology, the semiconductor device is scaled down dramatically with
additional strain engineering for device enhancement, the overall device
characteristic is no longer dominated by the device size but also circuit
layout. The higher order layout effects, such as well proximity effect (WPE),
oxide spacing effect (OSE) and poly spacing effect (PSE), play an important
role for the device performance, it is critical to understand Design for
Manufacturability (DFM) impacts with various layout topology toward the overall
circuit performance. Currently, the layout effects (WPE, OSE and PSE) are
validated through digital standard cell and analog differential pair test
structure. However, two analog layout structures: the guard ring and dummy fill
impact are not well studied yet, then, this paper describes the current mirror
test circuit to examine the guard ring and dummy fills DFM impacts using TSMC
28nm HPM process.
",1,0,0,0,0,0
19064,Investigating the potential of social network data for transport demand models,"  Location-based social network data offers the promise of collecting the data
from a large base of users over a longer span of time at negligible cost. While
several studies have applied social network data to activity and mobility
analysis, a comparison with travel diaries and general statistics has been
lacking. In this paper, we analysed geo-referenced Twitter activities from a
large number of users in Singapore and neighbouring countries. By combining
this data, population statistics and travel diaries and applying clustering
techniques, we addressed detection of activity locations, as well as spatial
separation and transitions between these locations. Kernel density estimation
performs best to detect activity locations due to the scattered nature of the
twitter data; more activity locations are detected per user than reported in
the travel survey. The descriptive analysis shows that determining home
locations is more difficult than detecting work locations for most planning
zones. Spatial separations between detected activity locations from Twitter
data - as reported in a travel survey and captured by public transport smart
card data - are mostly similarly distributed, but also show relevant
differences for very short and very long distances. This also holds for the
transitions between zones. Whether the differences between Twitter data and
other data sources stem from differences in the population sub-sample,
clustering methodology, or whether social networks are being used significantly
more at specific locations must be determined by further research. Despite
these shortcomings, location-based social network data offers a promising data
source for insights into activity locations and mobility patterns, especially
for regions where travel survey data is not readily available.
",1,1,0,0,0,0
19065,Generalized Internal Boundaries (GIB),"  Representing large-scale motions and topological changes in the finite volume
(FV) framework, while at the same time preserving the accuracy of the numerical
solution, is difficult. In this paper, we present a robust, highly efficient
method designed to achieve this capability. The proposed approach conceptually
shares many of the characteristics of the cut-cell interface tracking method,
but without the need for complex cell splitting/merging operations. The heart
of the new technique is to align existing mesh facets with the geometry to be
represented. We then modify the matrix contributions from these facets such
that they are represented in an identical fashion to traditional boundary
conditions. The collection of such faces is named a Generalised Internal
Boundary (GIB). In order to introduce motion into the system, we rely on the
classical ALE (Arbitrary Lagrangian-Eulerian) approach, but with the caveat
that the non-time-dependent motion of elements instantaneously crossing the
interface is handled separately from the time dependent component. The new
methodology is validated through comparison with: a) a body-fitted grid
simulation of an oscillating two dimensional cylinder and b) experimental
results of a butterfly valve.
",1,1,0,0,0,0
19066,Reducing Certification Granularity to Increase Adaptability of Avionics Software,"  A strong certification process is required to insure the safety of airplanes,
and more specifically the robustness of avionics applications. To implement
this process, the development of avionics software must follow long and costly
procedures. Most of these procedures have to be reexecuted each time the
software is modified. In this paper, we propose a framework to reduce the cost
and time impact of a software modification. With this new approach, the piece
of software likely to change is isolated from the rest of the application, so
it can be certified independently. This helps the system integrator to adapt an
avionics application to the specificities of the target airplane, without the
need for a new certification of the application.
",1,0,0,0,0,0
19067,Evolutionary multiplayer games on graphs with edge diversity,"  Evolutionary game dynamics in structured populations has been extensively
explored in past decades. However, most previous studies assume that payoffs of
individuals are fully determined by the strategic behaviors of interacting
parties and social ties between them only serve as the indicator of the
existence of interactions. This assumption neglects important information
carried by inter-personal social ties such as genetic similarity, geographic
proximity, and social closeness, which may crucially affect the outcome of
interactions. To model these situations, we present a framework of evolutionary
multiplayer games on graphs with edge diversity, where different types of edges
describe diverse social ties. Strategic behaviors together with social ties
determine the resulting payoffs of interactants. Under weak selection, we
provide a general formula to predict the success of one behavior over the
other. We apply this formula to various examples which cannot be dealt with
using previous models, including the division of labor and relationship- or
edge-dependent games. We find that labor division facilitates collective
cooperation by decomposing a many-player game into several games of smaller
sizes. The evolutionary process based on relationship-dependent games can be
approximated by interactions under a transformed and unified game. Our work
stresses the importance of social ties and provides effective methods to reduce
the calculating complexity in analyzing the evolution of realistic systems.
",0,0,0,0,1,0
19068,Robust Consensus for Multi-Agent Systems Communicating over Stochastic Uncertain Networks,"  In this paper, we study the robust consensus problem for a set of
discrete-time linear agents to coordinate over an uncertain communication
network, which is to achieve consensus against the transmission errors and
noises resulted from the information exchange between the agents. We model the
network by means of communication links subject to multiplicative stochastic
uncertainties, which are susceptible to describing packet dropout, random
delay, and fading phenomena. Different communication topologies, such as
undirected graphs and leader-follower graphs, are considered. We derive
sufficient conditions for robust consensus in the mean square sense. This
results unveil intrinsic constraints on consensus attainment imposed by the
network synchronizability, the unstable agent dynamics, and the channel
uncertainty variances. Consensus protocols are designed based on the state
information transmitted over the uncertain channels, by solving a modified
algebraic Riccati equation.
",1,0,0,0,0,0
19069,Fractional Topological Elasticity and Fracton Order,"  We analyze the ""higher rank"" gauge theories, that capture some of the
phenomenology of the Fracton order. It is shown that these theories loose gauge
invariance when arbitrarily weak and smooth curvature is introduced. We propose
a resolution to this problem by introducing a theory invariant under
area-preserving diffeomorphisms, which reduce to the ""higher rank"" gauge
transformations upon linearization around a flat background. The proposed
theory is \emph{geometric} in nature and is interpreted as a theory of
\emph{fractional topological elasticity}. This theory exhibits the Fracton
phenomenology. We explore the conservation laws, topological excitations,
linear response, various kinematical constraints, and canonical structure of
the theory. Finally, we emphasize that the very structure of Riemann-Cartan
geometry, which we use to formulate the theory, encodes the Fracton
phenomenology, suggesting that the Fracton order itself is \emph{geometric} in
nature.
",0,1,0,0,0,0
19070,Perturbing Eisenstein polynomials over local fields,"  Let $K$ be a local field whose residue field has characteristic $p$ and let
$L/K$ be a finite separable totally ramified extension. Let $\pi_L$ be a
uniformizer for $L$ and let $f(X)$ be the minimum polynomial for $\pi_L$ over
$K$. Suppose $\tilde{\pi}_L$ is another uniformizer for $L$ such that
$\tilde{\pi}_L\equiv\pi_L+r\pi_L^{\ell+1} \pmod{\pi_L^{\ell+2}}$ for some
$\ell\ge1$ and $r\in O_K$. Let $\tilde{f}(X)$ be the minimum polynomial for
$\tilde{\pi}_L$ over $K$. In this paper we give congruences for the
coefficients of $\tilde{f}(X)$ in terms of $r$ and the coefficients of $f(X)$.
These congruences improve and extend work of Krasner.
",0,0,1,0,0,0
19071,Galactic Orbits of Globular Clusters in the Region of the Galactic Bulge,"  Galactic orbits have been constructed over long time intervals for ten
globular clusters located near the Galactic center. A model with an axially
symmetric gravitational potential for the Galaxy was initially applied, after
which a non-axially symmetric potential corresponding to the central bar was
added. Variations in the trajectories of all these globular clusters in the XY
plane due to the influence of the bar were detected. These were greatest for
the cluster Terzan 4 in the meridional (RZ) plane. The globular clusters Terzan
1, Terzan 2, Terzan 4, Terzan 9, NGC 6522, and NGC 6558 always remained within
the Galactic bulge, no farther than 4 kpc from the Galactic center.
",0,1,0,0,0,0
19072,Streamlines for Motion Planning in Underwater Currents,"  Motion planning for underwater vehicles must consider the effect of ocean
currents. We present an efficient method to compute reachability and cost
between sample points in sampling-based motion planning that supports
long-range planning over hundreds of kilometres in complicated flows. The idea
is to search a reduced space of control inputs that consists of stream
functions whose level sets, or streamlines, optimally connect two given points.
Such stream functions are generated by superimposing a control input onto the
underlying current flow. A streamline represents the resulting path that a
vehicle would follow as it is carried along by the current given that control
input. We provide rigorous analysis that shows how our method avoids exhaustive
search of the control space, and demonstrate simulated examples in complicated
flows including a traversal along the east coast of Australia, using actual
current predictions, between Sydney and Brisbane.
",1,0,0,0,0,0
19073,"Eddington-Limited Accretion in z~2 WISE-selected Hot, Dust-Obscured Galaxies","  Hot, Dust-Obscured Galaxies, or ""Hot DOGs"", are a rare, dusty, hyperluminous
galaxy population discovered by the WISE mission. Predominantly at redshifts
2-3, they include the most luminous known galaxies in the universe. Their high
luminosities likely come from accretion onto highly obscured super massive
black holes (SMBHs). We have conducted a pilot survey to measure the SMBH
masses of five z~2 Hot DOGs via broad H_alpha emission lines, using
Keck/MOSFIRE and Gemini/FLAMINGOS-2. We detect broad H_alpha emission in all
five Hot DOGs. We find substantial corresponding SMBH masses for these Hot DOGs
(~ 10^{9} M_sun), and their derived Eddington ratios are close to unity. These
z~2 Hot DOGs are the most luminous AGNs at given BH masses, suggesting they are
accreting at the maximum rates for their BHs. A similar property is found for
known z~6 quasars. Our results are consistent with scenarios in which Hot DOGs
represent a transitional, high-accretion phase between obscured and unobscured
quasars. Hot DOGs may mark a special evolutionary stage before the red quasar
and optical quasar phases, and they may be present at other cosmic epochs.
",0,1,0,0,0,0
19074,Boltzmann Transport in Nanostructures as a Friction Effect,"  Surface scattering is the key limiting factor to thermal transport in
dielectric crystals as the length scales are reduced or when temperature is
lowered. To explain this phenomenon, it is commonly assumed that the mean free
paths of heat carriers are bound by the crystal size and that thermal
conductivity is reduced in a manner proportional to such mean free paths. We
show here that these conclusions rely on simplifying assumptions and
approximated transport models. Instead, starting from the linearized Boltzmann
transport equation in the relaxon basis, we show how the problem can be reduced
to a set of decoupled linear differential equations. Then, the heat flow can be
interpreted as a hydrodynamic phenomenon, with the relaxon gas being slowed
down in proximity of a surface by friction effects, similar to the flux of a
viscous fluid in a pipe. As an example, we study a ribbon and a trench of
monolayer molybdenum disulphide, describing the procedure to reconstruct the
temperature and thermal conductivity profile in the sample interior and showing
how to estimate the effect of nanostructuring. The approach is general and
could be extended to other transport carriers, such as electrons, or extended
to materials of higher dimensionality and to different geometries, such as thin
films.
",0,1,0,0,0,0
19075,Agile Software Engineering and Systems Engineering at SKA Scale,"  Systems Engineering (SE) is the set of processes and documentation required
for successfully realising large-scale engineering projects, but the classical
approach is not a good fit for software-intensive projects, especially when the
needs of the different stakeholders are not fully known from the beginning, and
requirement priorities might change. The SKA is the ultimate software-enabled
telescope, with enormous amounts of computing hardware and software required to
perform its data reduction. We give an overview of the system and software
engineering processes in the SKA1 development, and the tension between
classical and agile SE.
",1,1,0,0,0,0
19076,Prime geodesic theorem of Gallagher type,"  We reduce the exponent in the error term of the prime geodesic theorem for
compact Riemann surfaces from $\frac{3}{4}$ to $\frac{7}{10}$ outside a set of
finite logarithmic measure.
",0,0,1,0,0,0
19077,PriMaL: A Privacy-Preserving Machine Learning Method for Event Detection in Distributed Sensor Networks,"  This paper introduces PriMaL, a general PRIvacy-preserving MAchine-Learning
method for reducing the privacy cost of information transmitted through a
network. Distributed sensor networks are often used for automated
classification and detection of abnormal events in high-stakes situations, e.g.
fire in buildings, earthquakes, or crowd disasters. Such networks might
transmit privacy-sensitive information, e.g. GPS location of smartphones, which
might be disclosed if the network is compromised. Privacy concerns might slow
down the adoption of the technology, in particular in the scenario of social
sensing where participation is voluntary, thus solutions are needed which
improve privacy without compromising on the event detection accuracy. PriMaL is
implemented as a machine-learning layer that works on top of an existing event
detection algorithm. Experiments are run in a general simulation framework, for
several network topologies and parameter values. The privacy footprint of
state-of-the-art event detection algorithms is compared within the proposed
framework. Results show that PriMaL is able to reduce the privacy cost of a
distributed event detection algorithm below that of the corresponding
centralized algorithm, within the bounds of some assumptions about the
protocol. Moreover the performance of the distributed algorithm is not
statistically worse than that of the centralized algorithm.
",1,0,0,0,0,0
19078,Follow the Compressed Leader: Faster Online Learning of Eigenvectors and Faster MMWU,"  The online problem of computing the top eigenvector is fundamental to machine
learning. In both adversarial and stochastic settings, previous results (such
as matrix multiplicative weight update, follow the regularized leader, follow
the compressed leader, block power method) either achieve optimal regret but
run slow, or run fast at the expense of loosing a $\sqrt{d}$ factor in total
regret where $d$ is the matrix dimension.
We propose a $\textit{follow-the-compressed-leader (FTCL)}$ framework which
achieves optimal regret without sacrificing the running time. Our idea is to
""compress"" the matrix strategy to dimension 3 in the adversarial setting, or
dimension 1 in the stochastic setting. These respectively resolve two open
questions regarding the design of optimal and efficient algorithms for the
online eigenvector problem.
",1,0,1,1,0,0
19079,Solving satisfiability using inclusion-exclusion,"  Using Maple, we implement a SAT solver based on the principle of
inclusion-exclusion and the Bonferroni inequalities. Using randomly generated
input, we investigate the performance of our solver as a function of the number
of variables and number of clauses. We also test it against Maple's built-in
tautology procedure. Finally, we implement the Lovász local lemma with Maple
and discuss its applicability to SAT.
",1,0,0,0,0,0
19080,Isometric immersions into manifolds with metallic structures,"  We consider submanifolds into Riemannian manifold with metallic structures.
We obtain some new results for hypersurfaces in these spaces and we express the
fundamental theorem of submanifolds into products spaces in terms of metallic
structures. Moreover, we define new structures called complex metallic
structures. We show that these structures are linked with complex structures.
Then, we consider submanifolds into Riemannian manifold with such structures
with a focus on invariant submanifolds and hypersurfaces. We also express in
particular the fundamental theorem of submanifolds of complex space form in
terms of complex metallic structures.
",0,0,1,0,0,0
19081,Demonstration of an ac Josephson junction laser,"  Superconducting electronic devices have re-emerged as contenders for both
classical and quantum computing due to their fast operation speeds, low
dissipation and long coherence times. An ultimate demonstration of coherence is
lasing. We use one of the fundamental aspects of superconductivity, the ac
Josephson effect, to demonstrate a laser made from a Josephson junction
strongly coupled to a multi-mode superconducting cavity. A dc voltage bias to
the junction provides a source of microwave photons, while the circuit's
nonlinearity allows for efficient down-conversion of higher order Josephson
frequencies down to the cavity's fundamental mode. The simple fabrication and
operation allows for easy integration with a range of quantum devices, allowing
for efficient on-chip generation of coherent microwave photons at low
temperatures.
",0,1,0,0,0,0
19082,"Integrable systems, symmetries and quantization","  These notes correspond to a mini-course given at the Poisson 2016 conference
in Geneva. Starting from classical integrable systems in the sense of
Liouville, we explore the notion of non-degenerate singularity and expose
recent research in connection with semi-toric systems. The quantum and
semiclassical counterpart will also be presented, in the viewpoint of the
inverse question: from the quantum mechanical spectrum, can you recover the
classical system?
",0,1,1,0,0,0
19083,Detecting Friedel oscillations in ultracold Fermi gases,"  Investigating Friedel oscillations in ultracold gases would complement the
studies performed on solid state samples with scanning-tunneling microscopes.
In atomic quantum gases interactions and external potentials can be tuned
freely and the inherently slower dynamics allow to access non-equilibrium
dynamics following a potential or interaction quench. Here, we examine how
Friedel oscillations can be observed in current ultracold gas experiments under
realistic conditions. To this aim we numerically calculate the amplitude of the
Friedel oscillations which a potential barrier provokes in a 1D Fermi gas and
compare it to the expected atomic and photonic shot noise in a density
measurement. We find that to detect Friedel oscillations the signal from
several thousand one-dimensional systems has to be averaged. However, as up to
100 parallel one-dimensional systems can be prepared in a single run with
present experiments, averaging over about 100 images is sufficient.
",0,1,0,0,0,0
19084,Learning Proximal Operators: Using Denoising Networks for Regularizing Inverse Imaging Problems,"  While variational methods have been among the most powerful tools for solving
linear inverse problems in imaging, deep (convolutional) neural networks have
recently taken the lead in many challenging benchmarks. A remaining drawback of
deep learning approaches is their requirement for an expensive retraining
whenever the specific problem, the noise level, noise type, or desired measure
of fidelity changes. On the contrary, variational methods have a plug-and-play
nature as they usually consist of separate data fidelity and regularization
terms.
In this paper we study the possibility of replacing the proximal operator of
the regularization used in many convex energy minimization algorithms by a
denoising neural network. The latter therefore serves as an implicit natural
image prior, while the data term can still be chosen independently. Using a
fixed denoising neural network in exemplary problems of image deconvolution
with different blur kernels and image demosaicking, we obtain state-of-the-art
reconstruction results. These indicate the high generalizability of our
approach and a reduction of the need for problem-specific training.
Additionally, we discuss novel results on the analysis of possible optimization
algorithms to incorporate the network into, as well as the choices of algorithm
parameters and their relation to the noise level the neural network is trained
on.
",1,0,0,0,0,0
19085,Heated-Up Softmax Embedding,"  Metric learning aims at learning a distance which is consistent with the
semantic meaning of the samples. The problem is generally solved by learning an
embedding for each sample such that the embeddings of samples of the same
category are compact while the embeddings of samples of different categories
are spread-out in the feature space. We study the features extracted from the
second last layer of a deep neural network based classifier trained with the
cross entropy loss on top of the softmax layer. We show that training
classifiers with different temperature values of softmax function leads to
features with different levels of compactness. Leveraging these insights, we
propose a ""heating-up"" strategy to train a classifier with increasing
temperatures, leading the corresponding embeddings to achieve state-of-the-art
performance on a variety of metric learning benchmarks.
",0,0,0,1,0,0
19086,Fast Matrix Inversion and Determinant Computation for Polarimetric Synthetic Aperture Radar,"  This paper introduces a fast algorithm for simultaneous inversion and
determinant computation of small sized matrices in the context of fully
Polarimetric Synthetic Aperture Radar (PolSAR) image processing and analysis.
The proposed fast algorithm is based on the computation of the adjoint matrix
and the symmetry of the input matrix. The algorithm is implemented in a general
purpose graphical processing unit (GPGPU) and compared to the usual approach
based on Cholesky factorization. The assessment with simulated observations and
data from an actual PolSAR sensor show a speedup factor of about two when
compared to the usual Cholesky factorization. Moreover, the expressions
provided here can be implemented in any platform.
",1,0,0,1,0,0
19087,The imprint of neutrinos on clustering in redshift-space,"  (abridged) We investigate the signatures left by the cosmic neutrino
background on the clustering of matter, CDM+baryons and halos in redshift-space
using a set of more than 1000 N-body and hydrodynamical simulations with
massless and massive neutrinos. We find that the effect neutrinos induce on the
clustering of CDM+baryons in redshift-space on small scales is almost entirely
due to the change in $\sigma_8$. Neutrinos imprint a characteristic signature
in the quadrupole of the matter (CDM+baryons+neutrinos) field on small scales,
that can be used to disentangle the effect of $\sigma_8$ and $M_\nu$. We show
that the effect of neutrinos on the clustering of halos is very different, on
all scales, to the one induced by $\sigma_8$. We find that the effects of
neutrinos of the growth rate of CDM+baryons ranges from $\sim0.3\%$ to $2\%$ on
scales $k\in[0.01, 0.5]~h{\rm Mpc}^{-1}$ for neutrinos with masses $M_\nu
\leqslant 0.15$ eV. We compute the bias between the momentum of halos and the
momentum of CDM+baryon and find it to be 1 on large scales for all models with
massless and massive neutrinos considered. This point towards a velocity bias
between halos and total matter on large scales that it is important to account
for in order to extract unbiased neutrino information from velocity/momentum
surveys such as kSZ observations. We show that baryonic effects can affect the
clustering of matter and CDM+baryons in redshift-space by up to a few percent
down to $k=0.5~h{\rm Mpc}^{-1}$. We find that hydrodynamics and astrophysical
processes, as implemented in our simulations, only distort the relative effect
that neutrinos induce on the anisotropic clustering of matter, CDM+baryons and
halos in redshift-space by less than $1\%$. Thus, the effect of neutrinos in
the fully non-linear regime can be written as a transfer function with very
weak dependence on astrophysics.
",0,1,0,0,0,0
19088,Quantum Privacy-Preserving Data Analytics,"  Data analytics (such as association rule mining and decision tree mining) can
discover useful statistical knowledge from a big data set. But protecting the
privacy of the data provider and the data user in the process of analytics is a
serious issue. Usually, the privacy of both parties cannot be fully protected
simultaneously by a classical algorithm. In this paper, we present a quantum
protocol for data mining that can much better protect privacy than the known
classical algorithms: (1) if both the data provider and the data user are
honest, the data user can know nothing about the database except the
statistical results, and the data provider can get nearly no information about
the results mined by the data user; (2) if the data user is dishonest and tries
to disclose private information of the other, she/he will be detected with a
high probability; (3) if the data provider tries to disclose the privacy of the
data user, she/he cannot get any useful information since the data user hides
his privacy among noises.
",1,0,0,0,0,0
19089,Magnetic Properties of Transition-Metal Adsorbed ot-Phosphorus Monolayer: A First-principles and Monte Carlo Study,"  Using the first-principles and Monte Carlo methods, here we systematically
study magnetic properties of monolayer octagonal-tetragonal phosphorus with 3d
transition-metal (TM) adatoms. Different from the puckered hexagonal black
phosphorus monolayer (phosphorene or $\alpha$-P), the octagonal-tetragonal
phase of 2D phosphorus (named as ot-P or $\epsilon$-P in this article) is
buckled with octagon-tetragon structure. Our calculations show that all TMs,
except the closed-shell Zn atom, are able to strongly bind onto monolayer
$ot$-P with significant binding energies. Local magnetic moments (up to 6
$\mu$B) on adatoms of Sc, Ti, V, Cr, Mn, Fe and Co originate from the exchange
and crystal-field splitting of TM 3d orbitals. The magnetic coupling between
localized magnetic states of adatoms is dependent on adatomic distances and
directions. Lastly, the uniformly magnetic order is investigated to screening
two-dimensional dilute ferromagnets with high Curie temperature for
applications of spintronics. It is found that ot-P with V atoms homogeneously
adsorbed at the centre of octagons with a concentration of 5% has the most
stable ferromagnetic ground state. Its Curie temperature is estimated to be 173
K using the Monte Carlo method.
",0,1,0,0,0,0
19090,A bootstrap for the number of $\mathbb{F}_{q^r}$-rational points on a curve over $\mathbb{F}_q$,"  In this note we present a fast algorithm that finds for any $r$ the number
$N_r$ of $\mathbb{F}_{q^r}$ rational points on a smooth absolutely irreducible
curve $C$ defined over $\mathbb{F}_{q}$ assuming that we know $N_1,\cdots,N_g$,
where $g$ is the genus of $C$. The proof of its validity is given in detail and
its working are illustrated with several examples. In an Appendix we list the
Python function in which we have implemented the algorithm together with other
routines used in the examples.
",0,0,1,0,0,0
19091,Doping anatase TiO2 with group V-b and VI-b transition metal atoms: a hybrid functional first-principles study,"  We investigate the role of transition metal atoms of group V-b (V, Nb, and
Ta) and VI-b (Cr, Mo, and W) as n- or p-type dopants in anatase TiO2 using
thermodynamic principles and density functional theory with the
Heyd-Scuseria-Ernzerhof HSE06 hybrid functional. The HSE06 functional provides
a realistic value for the band gap, which ensures a correct classification of
dopants as shallow or deep donors or acceptors. Defect formation energies and
thermodynamic transition levels are calculated taking into account the
constraints imposed by the stability of TiO2 and the solubility limit of the
impurities. Nb, Ta, W and Mo are identified as shallow donors. Although W
provides two electrons, Nb and Ta show a considerably lower formation energy,
in particular under O-poor conditions. Mo donates in principle one electron,
but under specific conditions can turn into a double donor. V impurities are
deep donors and Cr shows up as an amphoteric defect, thereby acting as an
electron trapping center in n-type TiO2 especially under O-rich conditions. A
comparison with the available experimental data yields excellent agreement.
",0,1,0,0,0,0
19092,Patch-planting spin-glass solution for benchmarking,"  We introduce an algorithm to generate (not solve) spin-glass instances with
planted solutions of arbitrary size and structure. First, a set of small
problem patches with open boundaries is solved either exactly or with a
heuristic, and then the individual patches are stitched together to create a
large problem with a known planted solution. Because in these problems
frustration is typically smaller than in random problems, we first assess the
typical computational complexity of the individual patches using population
annealing Monte Carlo, and introduce an approach that allows one to fine-tune
the typical computational complexity of the patch-planted system. The scaling
of the typical computational complexity of these planted instances with various
numbers of patches and patch sizes is investigated and compared to random
instances.
",0,1,0,0,0,0
19093,Evidence for the formation of comet 67P/Churyumov-Gerasimenko through gravitational collapse of a bound clump of pebbles,"  The processes that led to the formation of the planetary bodies in the Solar
System are still not fully understood. Using the results obtained with the
comprehensive suite of instruments on-board ESA's Rosetta mission, we present
evidence that comet 67P/Churyumov-Gerasimenko likely formed through the gentle
gravitational collapse of a bound clump of mm-sized dust aggregates
(""pebbles""), intermixed with microscopic ice particles. This formation scenario
leads to a cometary make-up that is simultaneously compatible with the global
porosity, homogeneity, tensile strength, thermal inertia, vertical temperature
profiles, sizes and porosities of emitted dust, and the steep increase in
water-vapour production rate with decreasing heliocentric distance, measured by
the instruments on-board the Rosetta spacecraft and the Philae lander. Our
findings suggest that the pebbles observed to be abundant in protoplanetary
discs around young stars provide the building material for comets and other
minor bodies.
",0,1,0,0,0,0
19094,Detection of virial shocks in stacked Fermi-LAT clusters,"  Galaxy clusters are thought to grow by accreting mass through large-scale,
strong, yet elusive, virial shocks. Such a shock is expected to accelerate
relativistic electrons, thus generating a spectrally-flat leptonic virial-ring.
However, until now, only the nearby Coma cluster has shown evidence for a
$\gamma$-ray virial ring. We stack Fermi-LAT data for the 112 most massive,
high latitude, extended clusters, enhancing the ring sensitivity by rescaling
clusters to their virial radii and utilizing the expected flat energy spectrum.
In addition to a central unresolved, hard signal (detected at the $\sim
5.8\sigma$ confidence level), probably dominated by AGN, we identify (at the
$5.8\sigma$ confidence level) a bright, spectrally-flat $\gamma$-ray ring at
the expected virial shock position. The ring signal implies that the shock
deposits $\sim 0.6\%$ (with an interpretation uncertainty factor $\sim2$) of
the thermal energy in relativistic electrons over a Hubble time. This result,
consistent with the Coma signal, validates and calibrates the virial shock
model, and indicates that the cumulative emission from such shocks
significantly contributes to the diffuse extragalactic $\gamma$-ray and
low-frequency radio backgrounds.
",0,1,0,0,0,0
19095,Closed-form formulae of hyperbolic metamaterial made by stacked hole-array layers working at terahertz or microwave radiation,"  A metamaterial made by stacked hole-array layers known as a fishnet
metamaterial behaves as a hyperbolic metamaterial at wavelength much longer
than hole-array period. However, the analytical formulae of effective
parameters of a fishnet metamaterial have not been reported hindering the
design of deep-subwavelength imaging devices using this structure. We report
the new closed-form formulae of effective parameters comprising anisotropic
dispersion relation of a fishnet metamaterial working at terahertz or microwave
frequency. These effective parameters of a fishnet metamaterial are consistent
with those obtained by quasi-full solutions using known effective parameters of
a hole-array layer working at frequency below its spoof plasma frequency with
the superlattice period much smaller than the hole-array period. We also
theoretically demonstrate the deep-subwavelength focusing at {\lambda}/83 using
the composite structure of a slit-array layer and a fishnet metamaterial. It is
found that the focused intensity inside a fishnet metamaterial is several times
larger than that without the fishnet metamaterial, but the transmitted
intensity is still restricted by large-wavevector difference in air and a
fishnet metamaterial. Our effective parameters may aid the next-generation
deep-subwavelength imaging devices working at terahertz or microwave radiation.
",0,1,0,0,0,0
19096,RMPflow: A Computational Graph for Automatic Motion Policy Generation,"  We develop a novel policy synthesis algorithm, RMPflow, based on
geometrically consistent transformations of Riemannian Motion Policies (RMPs).
RMPs are a class of reactive motion policies designed to parameterize
non-Euclidean behaviors as dynamical systems in intrinsically nonlinear task
spaces. Given a set of RMPs designed for individual tasks, RMPflow can
consistently combine these local policies to generate an expressive global
policy, while simultaneously exploiting sparse structure for computational
efficiency. We study the geometric properties of RMPflow and provide sufficient
conditions for stability. Finally, we experimentally demonstrate that
accounting for the geometry of task policies can simplify classically difficult
problems, such as planning through clutter on high-DOF manipulation systems.
",1,0,0,0,0,0
19097,Long-range correlations and fractal dynamics in C. elegans: changes with aging and stress,"  Reduced motor control is one of the most frequent features associated with
aging and disease. Nonlinear and fractal analyses have proved to be useful in
investigating human physiological alterations with age and disease. Similar
findings have not been established for any of the model organisms typically
studied by biologists, though. If the physiology of a simpler model organism
displays the same characteristics, this fact would open a new research window
on the control mechanisms that organisms use to regulate physiological
processes during aging and stress. Here, we use a recently introduced animal
tracking technology to simultaneously follow tens of Caenorhabdits elegans for
several hours and use tools from fractal physiology to quantitatively evaluate
the effects of aging and temperature stress on nematode motility. Similarly to
human physiological signals, scaling analysis reveals long-range correlations
in numerous motility variables, fractal properties in behavioral shifts, and
fluctuation dynamics over a wide range of timescales. These properties change
as a result of a superposition of age and stress-related adaptive mechanisms
that regulate motility.
",0,1,0,1,0,0
19098,A Computational Approach to Extinction Events in Chemical Reaction Networks with Discrete State Spaces,"  Recent work of M.D. Johnston et al. has produced sufficient conditions on the
structure of a chemical reaction network which guarantee that the corresponding
discrete state space system exhibits an extinction event. The conditions
consist of a series of systems of equalities and inequalities on the edges of a
modified reaction network called a domination-expanded reaction network. In
this paper, we present a computational implementation of these conditions
written in Python and apply the program on examples drawn from the biochemical
literature, including a model of polyamine metabolism in mammals and a model of
the pentose phosphate pathway in Trypanosoma brucei. We also run the program on
458 models from the European Bioinformatics Institute's BioModels Database and
report our results.
",0,0,1,0,0,0
19099,Face-to-BMI: Using Computer Vision to Infer Body Mass Index on Social Media,"  A person's weight status can have profound implications on their life,
ranging from mental health, to longevity, to financial income. At the societal
level, ""fat shaming"" and other forms of ""sizeism"" are a growing concern, while
increasing obesity rates are linked to ever raising healthcare costs. For these
reasons, researchers from a variety of backgrounds are interested in studying
obesity from all angles. To obtain data, traditionally, a person would have to
accurately self-report their body-mass index (BMI) or would have to see a
doctor to have it measured. In this paper, we show how computer vision can be
used to infer a person's BMI from social media images. We hope that our tool,
which we release, helps to advance the study of social aspects related to body
weight.
",1,0,0,0,0,0
19100,Recursive simplex stars,"  This paper proposes a new method which builds a simplex based approximation
of a $d-1$-dimensional manifold $M$ separating a $d$-dimensional compact set
into two parts, and an efficient algorithm classifying points according to this
approximation. In a first variant, the approximation is made of simplices that
are defined in the cubes of a regular grid covering the compact set, from
boundary points that approximate the intersection between $M$ and the edges of
the cubes. All the simplices defined in a cube share the barycentre of the
boundary points located in the cube and include simplices similarly defined in
cube facets, and so on recursively. In a second variant, the Kuhn triangulation
is used to break the cubes into simplices and the approximation is defined in
these simplices from the boundary points computed on their edges, with the same
principle. Both the approximation in cubes and in simplices define a separating
surface on the whole grid and classifying a point on one side or the other of
this surface requires only a small number (at most $d$) of simple tests. Under
some conditions on the definition of the boundary points and on the reach of
$M$, for both variants the Hausdorff distance between $M$ and its approximation
decreases like $\mathcal{O}(d n_G^{-2})$, where $n_G$ is the number of points
on each axis of the grid. The approximation in cubes requires computing less
boundary points than the approximation in simplices but the latter is always a
manifold and is more accurate for a given value of $n_G$. The paper reports
tests of the method when varying $n_G$ and the dimensionality of the space (up
to 9).
",1,0,0,0,0,0
19101,An Analytic Formula for Numbers of Restricted Partitions from Conformal Field Theory,"  We study the correlators of irregular vertex operators in two-dimensional
conformal field theory (CFT) in order to propose an exact analytic formula for
calculating numbers of partitions, that is:
1) for given $N,k$, finding the total number $\lambda(N|k)$ of length $k$
partitions of $N$: $N=n_1+...+n_k;0<n_1\leq{n_2}...\leq{n_k}$.
2) finding the total number $\lambda(N)=\sum_{k=1}^N\lambda(N|k)$ of
partitions of a natural number $N$
We propose an exact analytic expression for $\lambda(N|k)$ by relating
two-point short-distance correlation functions of irregular vertex operators in
$c=1$ conformal field theory ( the form of the operators is established in this
paper): with the first correlator counting the partitions in the upper
half-plane and the second one obtained from the first correlator by conformal
transformations of the form $f(z)=h(z)e^{-{i\over{z}}}$ where $h(z)$ is regular
and non-vanishing at $z=0$. The final formula for $\lambda(N|k)$ is given in
terms of regularized ($\epsilon$-ordered) finite series in the generalized
higher-derivative Schwarzians and incomplete Bell polynomials of the above
conformal transformation at $z=i\epsilon$ ($\epsilon\rightarrow{0}$)
",0,0,1,0,0,0
19102,Hyperopic Cops and Robbers,"  We introduce a new variant of the game of Cops and Robbers played on graphs,
where the robber is invisible unless outside the neighbor set of a cop. The
hyperopic cop number is the corresponding analogue of the cop number, and we
investigate bounds and other properties of this parameter. We characterize the
cop-win graphs for this variant, along with graphs with the largest possible
hyperopic cop number. We analyze the cases of graphs with diameter 2 or at
least 3, focusing on when the hyperopic cop number is at most one greater than
the cop number. We show that for planar graphs, as with the usual cop number,
the hyperopic cop number is at most 3. The hyperopic cop number is considered
for countable graphs, and it is shown that for connected chains of graphs, the
hyperopic cop density can be any real number in $[0,1/2].$
",1,0,0,0,0,0
19103,New pinching estimates for Inverse curvature flows in space forms,"  We prove new pinching estimate for the inverse curvature flow of strictly
convex hypersurfaces in the space form $N$ of constant sectional curvature
$K_N$ with speed given by $F^{-\alpha}$, where $\alpha\in (0,1]$ for $K_N=0,-1$
and $\alpha=1$ for $K_N=1$, $F$ is a smooth, symmetric homogeneous of degree
one function which is inverse concave and has dual $F_*$ approaching zero on
the boundary of the positive cone $\Gamma_+$. We show that the ratio of the
largest principal curvature to the smallest principal curvature of the flow
hypersurface is controlled by its initial value. This can be used to prove the
smooth convergence of the flow.
",0,0,1,0,0,0
19104,"automan: a simple, Python-based, automation framework for numerical computing","  We present an easy-to-use, Python-based framework that allows a researcher to
automate their computational simulations. In particular the framework
facilitates assembling several long-running computations and producing various
plots from the data produced by these computations. The framework makes it
possible to reproduce every figure made for a publication with a single
command. It also allows one to distribute the computations across a network of
computers. The framework has been used to write research papers in numerical
computing. This paper discusses the design of the framework, and the benefits
of using it. The ideas presented are general and should help researchers
organize their computations for better reproducibility.
",1,0,0,0,0,0
19105,Extensive characterization of a high Reynolds number decelerating boundary layer using advanced optical metrology,"  An experiment conducted in the framework of the EUHIT project and designed to
characterize large scale structures in an adverse pressure gradient boundary
layer flow is presented. Up to 16 sCMOS cameras were used in order to perform
large scale turbulent boundary layer PIV measurements with a large field of
view and appropriate spatial resolution. To access the span-wise / wall-normal
signature of the structures as well, stereoscopic PIV measurements in
span-wise/wall-normal planes were performed at specific stream-wise locations.
To complement these large field of view measurements, long-range micro-PIV,
time resolved near wall velocity profiles and film-based measurements were
performed in order to determine the wall-shear stress and its fluctuations at
some specific locations along the model.
",0,1,0,0,0,0
19106,Q-analogues of the Fibo-Stirling numbers,"  Let $F_n$ denote the $n^{th}$ Fibonacci number relative to the initial
conditions $F_0=0$ and $F_1=1$. Bach, Paudyal, and Remmel introduced Fibonacci
analogues of the Stirling numbers called Fibo-Stirling numbers of the first and
second kind. These numbers serve as the connection coefficients between the
Fibo-falling factorial basis $\{(x)_{\downarrow_{F,n}}:n \geq 0\}$ and the
Fibo-rising factorial basis $\{(x)_{\uparrow_{F,n}}:n \geq 0\}$ which are
defined by $(x)_{\downarrow_{F,0}} = (x)_{\uparrow_{F,0}} = 1$ and for $k \geq
1$, $(x)_{\downarrow_{F,k}} = x(x-F_1) \cdots (x-F_{k-1})$ and
$(x)_{\uparrow_{F,k}} = x(x+F_1) \cdots (x+F_{k-1})$. We gave a general rook
theory model which allowed us to give combinatorial interpretations of the
Fibo-Stirling numbers of the first and second kind.
There are two natural $q$-analogues of the falling and rising Fibo-factorial
basis. That is, let $[x]_q = \frac{q^x-1}{q-1}$. Then we let
$[x]_{\downarrow_{q,F,0}} = \overline{[x]}_{\downarrow_{q,F,0}} =
[x]_{\uparrow_{q,F,0}} = \overline{[x]}_{\uparrow_{q,F,0}}=1$ and, for $k > 0$,
we let $[x]_{\downarrow_{q,F,k}} = [x]_q [x-F_1]_q \cdots [x-F_{k-1}]_q$,
$\overline{[x]}_{\downarrow_{q,F,k}}= [x]_q ([x]_q-[F_1]_q) \cdots
([x]_q-[F_{k-1}]_q)$, $[x]_{\uparrow_{q,F,k}}= [x]_q [x+F_1]_q \cdots
[x+F_{k-1}]_q$, and $\overline{[x]}_{\uparrow_{q,F,k}}= [x]_q ([x]_q+[F_1]_q)
\cdots ([x]_q+[F_{k-1}]_q)$.
In this paper, we show we can modify the rook theory model of Bach, Paudyal,
and Remmel to give combinatorial interpretations for the two different types
$q$-analogues of the Fibo-Stirling numbers which arise as the connection
coefficients between the two different $q$-analogues of the Fibonacci falling
and rising factorial bases. \end{abstract}
",0,0,1,0,0,0
19107,Hybrid Dirac Semimetal in CaAgBi Materials Family,"  Based on their formation mechanisms, Dirac points in three-dimensional
systems can be classified as accidental or essential. The former can be further
distinguished into type-I and type-II, depending on whether the Dirac cone
spectrum is completely tipped over along certain direction. Here, we predict
the coexistence of all three kinds of Dirac points in the low-energy band
structure of CaAgBi-family materials with a stuffed Wurtzite structure. Two
pairs of accidental Dirac points reside on the rotational axis, with one pair
being type-I and the other pair type-II; while another essential Dirac point is
pinned at the high symmetry point on the Brillouin zone boundary. Due to broken
inversion symmetry, the band degeneracy around accidental Dirac points is
completely lifted except along the rotational axis, which may enable the
splitting of chiral carriers at a ballistic p-n junction with a double negative
refraction effect. We clarify their symmetry protections, and find both the
Dirac-cone and Fermi arc topological surface states.
",0,1,0,0,0,0
19108,On the martingale property in the rough Bergomi model,"  We consider a class of fractional stochastic volatility models (including the
so-called rough Bergomi model), where the volatility is a superlinear function
of a fractional Gaussian process. We show that the stock price is a true
martingale if and only if the correlation $\rho$ between the driving Brownian
motions of the stock and the volatility is nonpositive. We also show that for
each $\rho<0$ and $m> \frac{1}{1-\rho^2}$, the $m$-th moment of the stock
price is infinite at each positive time.
",0,0,0,0,0,1
19109,Weighted Surface Algebras,"  A finite-dimensional algebra $A$ over an algebraically closed field $K$ is
called periodic if it is periodic under the action of the syzygy operator in
the category of $A-A-$ bimodules. The periodic algebras are self-injective and
occur naturally in the study of tame blocks of group algebras, actions of
finite groups on spheres, hypersurface singularities of finite Cohen-Macaulay
type, and Jacobian algebras of quivers with potentials. Recently, the tame
periodic algebras of polynomial growth have been classified and it is natural
to attempt to classify all tame periodic algebras. We introduce the weighted
surface algebras of triangulated surfaces with arbitrarily oriented triangles
and describe their basic properties. In particular, we prove that all these
algebras, except the singular tetrahedral algebras, are symmetric tame periodic
algebras of period $4$. Moreover, we describe the socle deformations of the
weighted surface algebras and prove that all these algebras are symmetric tame
periodic algebras of period $4$. The main results of the paper form an
important step towards a classification of all periodic symmetric tame algebras
of non-polynomial growth, and lead to a complete description of all algebras of
generalized quaternion type. Further, the orbit closures of the weighted
surface algebras (and their socle deformations) in the affine varieties of
associative $K$-algebra structures contain wide classes of tame symmetric
algebras related to algebras of dihedral and semidihedral types, which occur in
the study of blocks of group algebras with dihedral and semidihedral defect
groups.
",0,0,1,0,0,0
19110,A Statistical Learning Approach to Modal Regression,"  This paper studies the nonparametric modal regression problem systematically
from a statistical learning view. Originally motivated by pursuing a
theoretical understanding of the maximum correntropy criterion based regression
(MCCR), our study reveals that MCCR with a tending-to-zero scale parameter is
essentially modal regression. We show that nonparametric modal regression
problem can be approached via the classical empirical risk minimization. Some
efforts are then made to develop a framework for analyzing and implementing
modal regression. For instance, the modal regression function is described, the
modal regression risk is defined explicitly and its \textit{Bayes} rule is
characterized; for the sake of computational tractability, the surrogate modal
regression risk, which is termed as the generalization risk in our study, is
introduced. On the theoretical side, the excess modal regression risk, the
excess generalization risk, the function estimation error, and the relations
among the above three quantities are studied rigorously. It turns out that
under mild conditions, function estimation consistency and convergence may be
pursued in modal regression as in vanilla regression protocols, such as mean
regression, median regression, and quantile regression. However, it outperforms
these regression models in terms of robustness as shown in our study from a
re-descending M-estimation view. This coincides with and in return explains the
merits of MCCR on robustness. On the practical side, the implementation issues
of modal regression including the computational algorithm and the tuning
parameters selection are discussed. Numerical assessments on modal regression
are also conducted to verify our findings empirically.
",0,0,1,1,0,0
19111,Teaching a Machine to Read Maps with Deep Reinforcement Learning,"  The ability to use a 2D map to navigate a complex 3D environment is quite
remarkable, and even difficult for many humans. Localization and navigation is
also an important problem in domains such as robotics, and has recently become
a focus of the deep reinforcement learning community. In this paper we teach a
reinforcement learning agent to read a map in order to find the shortest way
out of a random maze it has never seen before. Our system combines several
state-of-the-art methods such as A3C and incorporates novel elements such as a
recurrent localization cell. Our agent learns to localize itself based on 3D
first person images and an approximate orientation angle. The agent generalizes
well to bigger mazes, showing that it learned useful localization and
navigation capabilities.
",1,0,0,1,0,0
19112,The fan beam model for the pulse evolution of PSR J0737-3039B,"  Average radio pulse profile of a pulsar B in a double pulsar system PSR
J0737-3039A/B exhibits an interesting behaviour. During the observation period
between 2003 and 2009, the profile evolves from a single-peaked to a
double-peaked form, following disappearance in 2008 indicating that the
geodetic precession of the pulsar is a possible origin of such behaviour. The
known pulsar beam models can be used to determine the geometry of PSR
J0737-3039B in the context of the precession. We study how the fan-beam
geometry performs in explaining the observed variations of the radio profile
morphology. It is shown that the fan beam can successfully reproduce the
observed evolution of the pulse width, and should be considered as a serious
alternative for the conal-like models.
",0,1,0,0,0,0
19113,Resilience of Core-Periphery Networks in the Case of Rich-Club,"  Core-periphery networks are structures that present a set of central and
densely connected nodes, namely the core, and a set of non-central and sparsely
connected nodes, namely the periphery. The rich-club refers to a set in which
the highest degree nodes show a high density of connections. Thus, a network
that displays a rich-club can be interpreted as a core-periphery network in
which the core is made up by a number of hubs. In this paper, we test the
resilience of networks showing a progressively denser rich-club and we observe
how this structure is able to affect the network measures in terms of both
cohesion and efficiency in information flow. Additionally, we consider the case
in which, instead of making the core denser, we add links to the periphery.
These two procedures of core and periphery thickening delineate a decision
process in the placement of new links and allow us to conduct a scenario
analysis that can be helpful in the comprehension and supervision of complex
networks under the resilience perspective. The advantages of the two
procedures, as well as their implications, are discussed in relation to both
network effciency and node heterogeneity.
",1,0,0,0,0,0
19114,Reservoir of Diverse Adaptive Learners and Stacking Fast Hoeffding Drift Detection Methods for Evolving Data Streams,"  The last decade has seen a surge of interest in adaptive learning algorithms
for data stream classification, with applications ranging from predicting ozone
level peaks, learning stock market indicators, to detecting computer security
violations. In addition, a number of methods have been developed to detect
concept drifts in these streams. Consider a scenario where we have a number of
classifiers with diverse learning styles and different drift detectors.
Intuitively, the current 'best' (classifier, detector) pair is application
dependent and may change as a result of the stream evolution. Our research
builds on this observation. We introduce the $\mbox{Tornado}$ framework that
implements a reservoir of diverse classifiers, together with a variety of drift
detection algorithms. In our framework, all (classifier, detector) pairs
proceed, in parallel, to construct models against the evolving data streams. At
any point in time, we select the pair which currently yields the best
performance. We further incorporate two novel stacking-based drift detection
methods, namely the $\mbox{FHDDMS}$ and $\mbox{FHDDMS}_{add}$ approaches. The
experimental evaluation confirms that the current 'best' (classifier, detector)
pair is not only heavily dependent on the characteristics of the stream, but
also that this selection evolves as the stream flows. Further, our
$\mbox{FHDDMS}$ variants detect concept drifts accurately in a timely fashion
while outperforming the state-of-the-art.
",1,0,0,1,0,0
19115,Extremes in Random Graphs Models of Complex Networks,"  Regarding the analysis of Web communication, social and complex networks the
fast finding of most influential nodes in a network graph constitutes an
important research problem. We use two indices of the influence of those nodes,
namely, PageRank and a Max-linear model. We consider the PageRank %both as
%Galton-Watson branching process and as an autoregressive process with a random
number of random coefficients that depend on ranks of incoming nodes and their
out-degrees and assume that the coefficients are independent and distributed
with regularly varying tail and with the same tail index. Then it is proved
that the tail index and the extremal index are the same for both PageRank and
the Max-linear model and the values of these indices are found. The
achievements are based on the study of random sequences of a random length and
the comparison of the distribution of their maxima and linear combinations.
",0,0,1,1,0,0
19116,Estimates of the Reconstruction Error in Partially Redressed Warped Frames Expansions,"  In recent work, redressed warped frames have been introduced for the analysis
and synthesis of audio signals with non-uniform frequency and time resolutions.
In these frames, the allocation of frequency bands or time intervals of the
elements of the representation can be uniquely described by means of a warping
map. Inverse warping applied after time-frequency sampling provides the key to
reduce or eliminate dispersion of the warped frame elements in the conjugate
variable, making it possible, e.g., to construct frequency warped frames with
synchronous time alignment through frequency. The redressing procedure is
however exact only when the analysis and synthesis windows have compact support
in the domain where warping is applied. This implies that frequency warped
frames cannot have compact support in the time domain. This property is
undesirable when online computation is required. Approximations in which the
time support is finite are however possible, which lead to small reconstruction
errors. In this paper we study the approximation error for compactly supported
frequency warped analysis-synthesis elements, providing a few examples and case
studies.
",1,0,0,0,0,0
19117,On the Optimality of Secret Key Agreement via Omniscience,"  For the multiterminal secret key agreement problem under a private source
model, it is known that the maximum key rate, i.e., the secrecy capacity, can
be achieved through communication for omniscience, but the omniscience strategy
can be strictly suboptimal in terms of minimizing the public discussion rate.
While a single-letter characterization is not known for the minimum discussion
rate needed for achieving the secrecy capacity, we derive single-letter lower
and upper bounds that yield some simple conditions for omniscience to be
discussion-rate optimal. These conditions turn out to be enough to deduce the
optimality of omniscience for a large class of sources including the
hypergraphical sources. Through conjectures and examples, we explore other
source models to which our methods do not easily extend.
",1,0,0,0,0,0
19118,Learning Motion Predictors for Smart Wheelchair using Autoregressive Sparse Gaussian Process,"  Constructing a smart wheelchair on a commercially available powered
wheelchair (PWC) platform avoids a host of seating, mechanical design and
reliability issues but requires methods of predicting and controlling the
motion of a device never intended for robotics. Analog joystick inputs are
subject to black-box transformations which may produce intuitive and adaptable
motion control for human operators, but complicate robotic control approaches;
furthermore, installation of standard axle mounted odometers on a commercial
PWC is difficult. In this work, we present an integrated hardware and software
system for predicting the motion of a commercial PWC platform that does not
require any physical or electronic modification of the chair beyond plugging
into an industry standard auxiliary input port. This system uses an RGB-D
camera and an Arduino interface board to capture motion data, including visual
odometry and joystick signals, via ROS communication. Future motion is
predicted using an autoregressive sparse Gaussian process model. We evaluate
the proposed system on real-world short-term path prediction experiments.
Experimental results demonstrate the system's efficacy when compared to a
baseline neural network model.
",1,0,0,0,0,0
19119,Kernelized Hashcode Representations for Relation Extraction,"  Kernel methods have produced state-of-the-art results for a number of NLP
tasks such as relation extraction, but suffer from poor scalability due to the
high cost of computing kernel similarities between natural language structures.
A recently proposed technique, kernelized locality-sensitive hashing (KLSH),
can significantly reduce the computational cost, but is only applicable to
classifiers operating on kNN graphs. Here we propose to use random subspaces of
KLSH codes for efficiently constructing an explicit representation of NLP
structures suitable for general classification methods. Further, we propose an
approach for optimizing the KLSH model for classification problems by
maximizing an approximation of mutual information between the KLSH codes
(feature vectors) and the class labels. We evaluate the proposed approach on
biomedical relation extraction datasets, and observe significant and robust
improvements in accuracy w.r.t. state-of-the-art classifiers, along with
drastic (orders-of-magnitude) speedup compared to conventional kernel methods.
",1,0,0,1,0,0
19120,Can Adversarial Networks Hallucinate Occluded People With a Plausible Aspect?,"  When you see a person in a crowd, occluded by other persons, you miss visual
information that can be used to recognize, re-identify or simply classify him
or her. You can imagine its appearance given your experience, nothing more.
Similarly, AI solutions can try to hallucinate missing information with
specific deep learning architectures, suitably trained with people with and
without occlusions. The goal of this work is to generate a complete image of a
person, given an occluded version in input, that should be a) without occlusion
b) similar at pixel level to a completely visible people shape c) capable to
conserve similar visual attributes (e.g. male/female) of the original one. For
the purpose, we propose a new approach by integrating the state-of-the-art of
neural network architectures, namely U-nets and GANs, as well as discriminative
attribute classification nets, with an architecture specifically designed to
de-occlude people shapes. The network is trained to optimize a Loss function
which could take into account the aforementioned objectives. As well we propose
two datasets for testing our solution: the first one, occluded RAP, created
automatically by occluding real shapes of the RAP dataset (which collects also
attributes of the people aspect); the second is a large synthetic dataset, AiC,
generated in computer graphics with data extracted from the GTA video game,
that contains 3D data of occluded objects by construction. Results are
impressive and outperform any other previous proposal. This result could be an
initial step to many further researches to recognize people and their behavior
in an open crowded world.
",1,0,0,0,0,0
19121,"John, the semi-conductor : a tool for comprovisation","  This article presents ""John"", an open-source software designed to help
collective free improvisation. It provides generated screen-scores running on
distributed, reactive web-browsers. The musicians can then concurrently edit
the scores in their own browser. John is used by ONE, a septet playing
improvised electro-acoustic music with digital musical instruments (DMI). One
of the original features of John is that its design takes care of leaving the
musician's attention as free as possible. Firstly, a quick review of the
context of screen-based scores will help situate this research in the history
of contemporary music notation. Then I will trace back how improvisation
sessions led to John's particular ""notational perspective"". A brief description
of the software will precede a discussion about the various aspects guiding its
design.
",1,0,0,0,0,0
19122,Eigenvalue and Eigenfunction for the $PT$-symmetric Potential $V = - (ix)^N$,"  The real energy spectrum from the $PT$-symmetric Hamiltonian $H = p^2 -
(ix)^N$ with $x\in\mathbb{C}$ was examined within one pair of Stokes wedges in
1998 by Bender and Boettcher. For this Hamiltonian we discuss the following
three questions. First, since their paper used a Runge-Kutta method to
integrate along a path at the center of the Stokes wedges to calculate
eigenvalues $E$ with high accuracy, we wonder if the same eigenvalues can be
obtained if integrate along some other paths in different shapes. Second, what
the corresponding eigenfunctions look like? Should the eigenfunctions be
independent from the shapes of path or not? Third, since for large $N$ the
Hamiltonian contains many pairs of Stokes wedges symmetric with respect to the
imaginary axis of $x$, thus multiple families of real energy spectrum can be
obtained. What do they look like? Any relation among them?
",0,0,1,0,0,0
19123,Random walks on activity-driven networks with attractiveness,"  Virtually all real-world networks are dynamical entities. In social networks,
the propensity of nodes to engage in social interactions (activity) and their
chances to be selected by active nodes (attractiveness) are heterogeneously
distributed. Here, we present a time-varying network model where each node and
the dynamical formation of ties are characterised by these two features. We
study how these properties affect random walk processes unfolding on the
network when the time scales describing the process and the network evolution
are comparable. We derive analytical solutions for the stationary state and the
mean first passage time of the process and we study cases informed by empirical
observations of social networks. Our work shows that previously disregarded
properties of real social systems such heterogeneous distributions of activity
and attractiveness as well as the correlations between them, substantially
affect the dynamical process unfolding on the network.
",1,1,0,0,0,0
19124,Predicting B Cell Receptor Substitution Profiles Using Public Repertoire Data,"  B cells develop high affinity receptors during the course of affinity
maturation, a cyclic process of mutation and selection. At the end of affinity
maturation, a number of cells sharing the same ancestor (i.e. in the same
""clonal family"") are released from the germinal center, their amino acid
frequency profile reflects the allowed and disallowed substitutions at each
position. These clonal-family-specific frequency profiles, called ""substitution
profiles"", are useful for studying the course of affinity maturation as well as
for antibody engineering purposes. However, most often only a single sequence
is recovered from each clonal family in a sequencing experiment, making it
impossible to construct a clonal-family-specific substitution profile. Given
the public release of many high-quality large B cell receptor datasets, one may
ask whether it is possible to use such data in a prediction model for
clonal-family-specific substitution profiles. In this paper, we present the
method ""Substitution Profiles Using Related Families"" (SPURF), a penalized
tensor regression framework that integrates information from a rich assemblage
of datasets to predict the clonal-family-specific substitution profile for any
single input sequence. Using this framework, we show that substitution profiles
from similar clonal families can be leveraged together with simulated
substitution profiles and germline gene sequence information to improve
prediction. We fit this model on a large public dataset and validate the
robustness of our approach on an external dataset. Furthermore, we provide a
command-line tool in an open-source software package
(this https URL) implementing these ideas and providing easy
prediction using our pre-fit models.
",0,0,0,0,1,0
19125,Energy Efficient Mobile Edge Computing in Dense Cellular Networks,"  Merging Mobile Edge Computing (MEC), which is an emerging paradigm to meet
the increasing computation demands from mobile devices, with the dense
deployment of Base Stations (BSs), is foreseen as a key step towards the next
generation mobile networks. However, new challenges arise for designing energy
efficient networks since radio access resources and computing resources of BSs
have to be jointly managed, and yet they are complexly coupled with traffic in
both spatial and temporal domains. In this paper, we address the challenge of
incorporating MEC into dense cellular networks, and propose an efficient online
algorithm, called ENGINE (ENErgy constrained offloadINg and slEeping) which
makes joint computation offloading and BS sleeping decisions in order to
maximize the quality of service while keeping the energy consumption low. Our
algorithm leverages Lyapunov optimization technique, works online and achieves
a close-to-optimal performance without using future information. Our simulation
results show that our algorithm can effectively reduce energy consumption
without sacrificing the user quality of service.
",1,0,0,0,0,0
19126,Delone dynamical systems and spectral convergence,"  In the realm of Delone sets in locally compact, second countable, Hausdorff
groups, we develop a dynamical systems approach in order to study the
continuity behavior of measured quantities arising from point sets. A special
focus is both on the autocorrelation, as well as on the density of states for
random bounded operators. It is shown that for uniquely ergodic limit systems,
the latter measures behave continuously with respect to the Chabauty-Fell
convergence of hulls. In the special situation of Euclidean spaces, our results
complement recent developments in describing spectra as topological limits: we
show that the measured quantities under consideration can be approximated via
periodic analogs.
",0,0,1,0,0,0
19127,On biconservative surfaces in Euclidean spaces,"  In this paper, we study biconservative surfaces with parallel normalized mean
curvature vector in $\mathbb{E}^4$. We obtain complete local classification in
$\mathbb{E}^4$ for a biconservative PNMCV surface. We also give an example to
show the existence of PNMCV biconservative surfaces in $\mathbb{E}^4$.
",0,0,1,0,0,0
19128,Automatic implementation of material laws: Jacobian calculation in a finite element code with TAPENADE,"  In an effort to increase the versatility of finite element codes, we explore
the possibility of automatically creating the Jacobian matrix necessary for the
gradient-based solution of nonlinear systems of equations. Particularly, we aim
to assess the feasibility of employing the automatic differentiation tool
TAPENADE for this purpose on a large Fortran codebase that is the result of
many years of continuous development. As a starting point we will describe the
special structure of finite element codes and the implications that this code
design carries for an efficient calculation of the Jacobian matrix. We will
also propose a first approach towards improving the efficiency of such a
method. Finally, we will present a functioning method for the automatic
implementation of the Jacobian calculation in a finite element software, but
will also point out important shortcomings that will have to be addressed in
the future.
",1,1,0,0,0,0
19129,Asymptotic analysis for Hamilton-Jacobi equations with large drift term,"  We investigate the asymptotic behavior of solutions of Hamilton-Jacobi
equations with large drift term in an open subset of two-dimensional Euclidean
space. When the drift is given by $\varepsilon^{-1} (H_{x_2}, -H_{x_1})$ of a
Hamiltonian $H$, with $\varepsilon > 0$, we establish the convergence, as
$\varepsilon \to 0+$, of solutions of the Hamilton-Jacobi equations and
identify the limit of the solutions as the solution of systems of ordinary
differential equations on a graph. This result generalizes the previous one
obtained by the author to the case where the Hamiltonian $H$ admits a
degenerate critical point and, as a consequence, the graph may have segments
more than four at a node.
",0,0,1,0,0,0
19130,An Upper Bound of the Minimal Dispersion via Delta Covers,"  For a point set of $n$ elements in the $d$-dimensional unit cube and a class
of test sets we are interested in the largest volume of a test set which does
not contain any point. For all natural numbers $n$, $d$ and under the
assumption of a $delta$-cover with cardinality $\vert \Gamma_\delta \vert$ we
prove that there is a point set, such that the largest volume of such a test
set without any point is bounded by $\frac{\log \vert \Gamma_\delta \vert}{n} +
\delta$. For axis-parallel boxes on the unit cube this leads to a volume of at
most $\frac{4d}{n}\log(\frac{9n}{d})$ and on the torus to $\frac{4d}{n}\log
(2n)$.
",1,0,1,0,0,0
19131,Three-dimensional band structure of LaSb and CeSb:Absence of band inversion,"  We have performed angle-resolved photoemission spectroscopy (ARPES) of LaSb
and CeSb, a candidate of topological insulator. Using soft-x-ray photons, we
have accurately determined the three-dimensional bulk band structure and
revealed that the band inversion at the Brillouin-zone corner - a prerequisite
for realizing topological-insulator phase - is absent in both LaSb and CeSb.
Moreover, unlike the ARPES data obtained with soft-x-ray photons, those with
vacuum ultraviolet (VUV) photons were found to suffer significant $k_z$
broadening. These results suggest that LaSb and CeSb are topologically trivial
semimetals, and unusual Dirac-cone-like states observed with VUV photons are
not of the topological origin.
",0,1,0,0,0,0
19132,Stellar-to-halo mass relation of cluster galaxies,"  In the hierarchical formation model, galaxy clusters grow by accretion of
smaller groups or isolated galaxies. During the infall into the centre of a
cluster, the properties of accreted galaxies change. In particular, both
observations and numerical simulations suggest that its dark matter halo is
stripped by the tidal forces of the host.
We use galaxy-galaxy weak lensing to measure the average mass of dark matter
haloes of satellite galaxies as a function of projected distance to the centre
of the host, for different stellar mass bins. Assuming that the stellar
component of the galaxy is less disrupted by tidal stripping, stellar mass can
be used as a proxy of the infall mass. We study the stellar to halo mass
relation of satellites as a function of the cluster-centric distance to measure
tidal stripping.
We use the shear catalogues of the DES science verification archive, the
CFHTLenS and the CFHT Stripe 82 (CS82) surveys, and we select satellites from
the redMaPPer catalogue of clusters. For galaxies located in the outskirts of
clusters, we find a stellar to halo mass relation in good agreement with the
theoretical expectations from \citet{moster2013} for central galaxies. In the
centre of the cluster, we find that this relation is shifted to smaller halo
mass for a given stellar mass. We interpret this finding as further evidence
for tidal stripping of dark matter haloes in high density environments.
",0,1,0,0,0,0
19133,Undecidability and Finite Automata,"  Using a novel rewriting problem, we show that several natural decision
problems about finite automata are undecidable (i.e., recursively unsolvable).
In contrast, we also prove three related problems are decidable. We apply one
result to prove the undecidability of a related problem about k-automatic sets
of rational numbers.
",1,0,0,0,0,0
19134,A globally stable attractor that is locally unstable everywhere,"  We construct two examples of invariant manifolds that despite being locally
unstable at every point in the transverse direction are globally stable. Using
numerical simulations we show that these invariant manifolds temporarily repel
nearby trajectories but act as global attractors. We formulate an explanation
for such global stability in terms of the `rate of rotation' of the stable and
unstable eigenvectors spanning the normal subspace associated with each point
of the invariant manifold. We discuss the role of this rate of rotation on the
transitions between the stable and unstable regimes.
",0,1,0,0,0,0
19135,On Centers and Central Lines of Triangles in the Elliptic Plane,"  We determine barycentric coordinates of triangle centers in the elliptic
plane. The main focus is put on centers that lie on lines whose euclidean limit
(triangle excess --> 0) is the Euler line or the Brocard line. We also
investigate curves which can serve in elliptic geometry as substitutes for the
euclidean nine-point-circle, the first Lemoine circle or the apollonian
circles.
",0,0,1,0,0,0
19136,Stopping Active Learning based on Predicted Change of F Measure for Text Classification,"  During active learning, an effective stopping method allows users to limit
the number of annotations, which is cost effective. In this paper, a new
stopping method called Predicted Change of F Measure will be introduced that
attempts to provide the users an estimate of how much performance of the model
is changing at each iteration. This stopping method can be applied with any
base learner. This method is useful for reducing the data annotation bottleneck
encountered when building text classification systems.
",1,0,0,1,0,0
19137,Statistical properties of random clique networks,"  In this paper, a random clique network model to mimic the large clustering
coefficient and the modular structure that exist in many real complex networks,
such as social networks, artificial networks, and protein interaction networks,
is introduced by combining the random selection rule of the Erdös and Rényi
(ER) model and the concept of cliques. We find that random clique networks
having a small average degree differ from the ER network in that they have a
large clustering coefficient and a power law clustering spectrum, while
networks having a high average degree have similar properties as the ER model.
In addition, we find that the relation between the clustering coefficient and
the average degree shows a non-monotonic behavior and that the degree
distributions can be fit by multiple Poisson curves; we explain the origin of
such novel behaviors and degree distributions.
",1,1,0,0,0,0
19138,"Quo Vadis, Action Recognition? A New Model and the Kinetics Dataset","  The paucity of videos in current action classification datasets (UCF-101 and
HMDB-51) has made it difficult to identify good video architectures, as most
methods obtain similar performance on existing small-scale benchmarks. This
paper re-evaluates state-of-the-art architectures in light of the new Kinetics
Human Action Video dataset. Kinetics has two orders of magnitude more data,
with 400 human action classes and over 400 clips per class, and is collected
from realistic, challenging YouTube videos. We provide an analysis on how
current architectures fare on the task of action classification on this dataset
and how much performance improves on the smaller benchmark datasets after
pre-training on Kinetics.
We also introduce a new Two-Stream Inflated 3D ConvNet (I3D) that is based on
2D ConvNet inflation: filters and pooling kernels of very deep image
classification ConvNets are expanded into 3D, making it possible to learn
seamless spatio-temporal feature extractors from video while leveraging
successful ImageNet architecture designs and even their parameters. We show
that, after pre-training on Kinetics, I3D models considerably improve upon the
state-of-the-art in action classification, reaching 80.9% on HMDB-51 and 98.0%
on UCF-101.
",1,0,0,0,0,0
19139,Selfish Cops and Active Robber: Multi-Player Pursuit Evasion on Graphs,"  We introduce and study the game of ""Selfish Cops and Active Robber"" (SCAR)
which can be seen as an multiplayer variant of the ""classic"" two-player Cops
and Robbers (CR) game. In classic CR all cops are controlled by a single
player, who has no preference over which cop captures the robber. In SCAR, on
the other hand, each of N-1 cops is controlled by a separate player, and a
single robber is controlled by the N-th player; and the capturing cop player
receives a higher reward than the non-capturing ones. Consequently, SCAR is an
N-player pursuit game on graphs, in which each cop player has an increased
motive to be the one who captures the robber. The focus of our study is the
existence and properties of SCAR Nash Equilibria (NE). In particular, we prove
that SCAR always has one NE in deterministic positional strategies and (for N
greater than two) another in deterministic nonpositional strategies.
Furthermore, we study conditions which, at equilibrium, guarantee either
capture or escape of the robber and show that (because of the antagonism
between the ""selfish"" cop players) the robber may, in certain SCAR
configurations, be captured later than he would be in classic CR, or even not
captured at all. Finally we define the selfish cop number of a graph and study
its connection to the classic cop number.
",1,0,0,0,0,0
19140,Singly-Thermostated Ergodicity in Gibbs' Canonical Ensemble and the 2016 Ian Snook Prize Award,"  The 2016 Snook Prize has been awarded to Diego Tapias, Alessandro Bravetti,
and David Sanders for their paper -- Ergodicity of One-Dimensional Systems
Coupled to the Logistic Thermostat. They introduced a relatively stiff
hyperbolic tangent thermostat force and successfully tested its ability to
reproduce Gibbs' canonical distribution for the harmonic oscillator, the
quartic oscillator, and the Mexican Hat potentials. Their work constitutes an
effective response to the 2016 Ian Snook Prize Award goal -- Finding ergodic
algorithms for Gibbs' canonical ensemble using a single thermostat variable. We
confirm their work here and highlight an interesting feature of the Mexican Hat
problem when it is solved with an adaptive integrator.
",0,1,0,0,0,0
19141,Robust Estimation via Robust Gradient Estimation,"  We provide a new computationally-efficient class of estimators for risk
minimization. We show that these estimators are robust for general statistical
models: in the classical Huber epsilon-contamination model and in heavy-tailed
settings. Our workhorse is a novel robust variant of gradient descent, and we
provide conditions under which our gradient descent variant provides accurate
estimators in a general convex risk minimization problem. We provide specific
consequences of our theory for linear regression, logistic regression and for
estimation of the canonical parameters in an exponential family. These results
provide some of the first computationally tractable and provably robust
estimators for these canonical statistical models. Finally, we study the
empirical performance of our proposed methods on synthetic and real datasets,
and find that our methods convincingly outperform a variety of baselines.
",0,0,0,1,0,0
19142,SMT Solving for Vesicle Traffic Systems in Cells,"  In biology, there are several questions that translate to combinatorial
search. For example, vesicle traffic systems that move cargo within eukaryotic
cells have been proposed to exhibit several graph properties such as three
connectivity. These properties are consequences of underlying biophysical
constraints. A natural question for biologists is: what are the possible
networks for various combinations of those properties? In this paper, we
present novel SMT based encodings of the properties over vesicle traffic
systems and a tool that searches for the networks that satisfies the properties
using SMT solvers. In our experiments, we show that our tool can search for
networks of sizes that are considered to be relevant by biologists.
",1,0,0,0,0,0
19143,Endemicity and prevalence of multipartite viruses under heterogeneous between-host transmission,"  Multipartite viruses replicate through a puzzling evolutionary strategy.
Their genome is segmented into two or more parts, and encapsidated in separate
particles that appear to propagate independently. Completing the replication
cycle, however, requires the full genome, so that a persistent infection of a
host requires the concurrent presence of several particles. This represents an
apparent evolutionary drawback of multipartitism, while its advantages remain
unclear. A transition from monopartite to multipartite viral forms has been
described in vitro under conditions of high multiplicity of infection,
suggesting that cooperation between defective mutants is a plausible
evolutionary pathway towards multipartitism. However, it is unknown how the
putative advantages that multipartitism might enjoy affect its epidemiology, or
if an explicit advantage is needed to explain its ecological persistence. To
disentangle which mechanisms might contribute to the rise and fixation of
multipartitism, we here investigate the interaction between viral spreading
dynamics and host population structure. We set up a compartmental model of the
spread of a virus in its different forms and explore its epidemiology using
both analytical and numerical techniques. We uncover that the impact of host
contact structure on spreading dynamics entails a rich phenomenology of
ecological relationships that includes cooperation, competition, and
commensality. Furthermore, we find out that multipartitism might rise to
fixation even in the absence of explicit microscopic advantages. Multipartitism
allows the virus to colonize environments that could not be invaded by the
monopartite form, while homogeneous contacts between hosts facilitate its
spread. We conjecture that there might have been an increase in the diversity
and prevalence of multipartite viral forms concomitantly with the expansion of
agricultural practices.
",0,0,0,0,1,0
19144,Correlating Cell Shape and Cellular Stress in Motile Confluent Tissues,"  Collective cell migration is a highly regulated process involved in wound
healing, cancer metastasis and morphogenesis. Mechanical interactions among
cells provide an important regulatory mechanism to coordinate such collective
motion. Using a Self-Propelled Voronoi (SPV) model that links cell mechanics to
cell shape and cell motility, we formulate a generalized mechanical inference
method to obtain the spatio-temporal distribution of cellular stresses from
measured traction forces in motile tissues and show that such traction-based
stresses match those calculated from instantaneous cell shapes. We additionally
use stress information to characterize the rheological properties of the
tissue. We identify a motility-induced swim stress that adds to the interaction
stress to determine the global contractility or extensibility of epithelia. We
further show that the temporal correlation of the interaction shear stress
determines an effective viscosity of the tissue that diverges at the
liquid-solid transition, suggesting the possibility of extracting rheological
information directly from traction data.
",0,1,0,0,0,0
19145,Eigenvalue Decay Implies Polynomial-Time Learnability for Neural Networks,"  We consider the problem of learning function classes computed by neural
networks with various activations (e.g. ReLU or Sigmoid), a task believed to be
computationally intractable in the worst-case. A major open problem is to
understand the minimal assumptions under which these classes admit provably
efficient algorithms. In this work we show that a natural distributional
assumption corresponding to {\em eigenvalue decay} of the Gram matrix yields
polynomial-time algorithms in the non-realizable setting for expressive classes
of networks (e.g. feed-forward networks of ReLUs). We make no assumptions on
the structure of the network or the labels. Given sufficiently-strong
polynomial eigenvalue decay, we obtain {\em fully}-polynomial time algorithms
in {\em all} the relevant parameters with respect to square-loss. Milder decay
assumptions also lead to improved algorithms. This is the first purely
distributional assumption that leads to polynomial-time algorithms for networks
of ReLUs, even with one hidden layer. Further, unlike prior distributional
assumptions (e.g., the marginal distribution is Gaussian), eigenvalue decay has
been observed in practice on common data sets.
",1,0,0,0,0,0
19146,A New Steganographic Technique Matching the Secret Message and Cover image Binary Value,"  Steganography involves hiding a secret message or image inside another cover
image. Changes are made in the cover image without affecting visual quality of
the image. In contrast to cryptography, Steganography provides complete secrecy
of the communication. Security of very sensitive data can be enhanced by
combining cryptography and steganography. A new technique that uses the concept
of Steganography to obtain the position values from an image is suggested. This
paper proposes a new method where no change is made to the cover image, only
the pixel position LSB (Least Significant Bit) values that match with the
secret message bit values are noted in a separate position file. At the sending
end the position file along with the cover image is sent. At the receiving end
the position file is opened only with a secret key. The bit positions are taken
from the position file and the LSB values from the positions are combined to
get ASCII values and then form characters of the secret message
",1,0,0,0,0,0
19147,On topological fluid mechanics of non-ideal systems and virtual frozen-in dynamics,"  Euler and Navier-Stokes have variant systems with dynamical invariance of
helicity and thus (weak) topological equivalence, allowing a strong `frozen-in'
(to, or, dually, `Lie-carried' by the \textit{virtual} velocity $V$)
formulation of the vorticity with a flavor of `inverse Helmholtz theorem'. We
remark on the non-ideal (statistical) topological fluid mechanics (TFM) for (1)
the Constantin-Iyer formulation of Navier-Stokes, (2) our own extension of the
Gallavotti-Cohen type dynamical ensembles of modified Navier-Stokes with
energy-helicity constraints and (3) the Galerkin truncated Euler, as the
typical case variants with dynamical time reversibility and helicity
invariance. Ideal TFM is thus bridged with non-ideal flows. An example virtual
(Lie-)carrier of the vorticity in a Galerkin-truncated Euler system is
calculated to demonstrate the issue of determining $V$.
",0,1,0,0,0,0
19148,Topological AdS/CFT,"  We define a holographic dual to the Donaldson-Witten topological twist of
$\mathcal{N}=2$ gauge theories on a Riemannian four-manifold. This is described
by a class of asymptotically locally hyperbolic solutions to $\mathcal{N}=4$
gauged supergravity in five dimensions, with the four-manifold as conformal
boundary. Under AdS/CFT, minus the logarithm of the partition function of the
gauge theory is identified with the holographically renormalized supergravity
action. We show that the latter is independent of the metric on the boundary
four-manifold, as required for a topological theory. Supersymmetric solutions
in the bulk satisfy first order differential equations for a twisted $Sp(1)$
structure, which extends the quaternionic Kahler structure that exists on any
Riemannian four-manifold boundary. We comment on applications and extensions,
including generalizations to other topological twists.
",0,0,1,0,0,0
19149,Using highly uniform and smooth Selenium colloids as low-loss magnetodielectric building blocks of optical metafluids,"  We systematically analyzed magnetodielectric resonances of Se colloids for
the first time to exploit the possibility for use as building blocks of
all-dielectric optical metafluids. By taking synergistic advantages of Se
colloids, including (i) high-refractive-index at optical frequencies, (ii)
unprecedented structural uniformity, and (iii) versatile access to copious
quantities, the Kerker-type directional light scattering resulting from
efficient coupling between strong electric and magnetic resonances were
observed directly from Se colloidal suspension. Thus, the use of Se colloid as
a generic magnetodielectric building block highlights an opportunity for the
fluidic low-loss optical antenna, which can be processed via spin-coating and
painting.
",0,1,0,0,0,0
19150,A Vector Field Method for Radiating Black Hole Spacetimes,"  We develop a commuting vector field method for a general class of radiating
spacetimes. The metrics considered are certain long range perturbations of
Minkowski space including those constructed from global stability problems in
general relativity. Our method provides sharp peeling estimates for solutions
to both linear and nonlinear (null form) scalar fields.
",0,0,1,0,0,0
19151,Experimental Constraint on an Exotic Spin- and Velocity-Dependent Interaction in the Sub-meV Range of Axion Mass with a Spin-Exchange Relaxation-Free Magnetometer,"  We conducted a search for an exotic spin- and velocity-dependent interaction
for polarized electrons with an experimental approach based on a
high-sensitivity spin-exchange relaxation-free (SERF) magnetometer, which
serves as both a source of polarized electrons and a magnetic-field sensor. The
experiment aims to sensitively detect magnetic-fieldlike effects from the
exotic interaction between the polarized electrons in a SERF vapor cell and
unpolarized nucleons of a closely located solid-state mass. We report
experimental results on the interaction with 82 h of data averaging, which sets
an experimental limit on the coupling strength around $10^{-19}$ for the axion
mass $m_a \lesssim 10^{-3}$ eV, within the important axion window.
",0,1,0,0,0,0
19152,Generalization Error in Deep Learning,"  Deep learning models have lately shown great performance in various fields
such as computer vision, speech recognition, speech translation, and natural
language processing. However, alongside their state-of-the-art performance, it
is still generally unclear what is the source of their generalization ability.
Thus, an important question is what makes deep neural networks able to
generalize well from the training set to new data. In this article, we provide
an overview of the existing theory and bounds for the characterization of the
generalization error of deep neural networks, combining both classical and more
recent theoretical and empirical results.
",0,0,0,1,0,0
19153,Block Compressive Sensing of Image and Video with Nonlocal Lagrangian Multiplier and Patch-based Sparse Representation,"  Although block compressive sensing (BCS) makes it tractable to sense
large-sized images and video, its recovery performance has yet to be
significantly improved because its recovered images or video usually suffer
from blurred edges, loss of details, and high-frequency oscillatory artifacts,
especially at a low subrate. This paper addresses these problems by designing a
modified total variation technique that employs multi-block gradient
processing, a denoised Lagrangian multiplier, and patch-based sparse
representation. In the case of video, the proposed recovery method is able to
exploit both spatial and temporal similarities. Simulation results confirm the
improved performance of the proposed method for compressive sensing of images
and video in terms of both objective and subjective qualities.
",1,0,0,0,0,0
19154,A multi-phase-field method for surface tension-induced elasticity,"  A consistent treatment of the coupling of surface energy and elasticity
within the multi-phase- field framework is presented. The model accurately
reproduces stress distribution in a number of analytically tractable, yet
non-trivial, cases including different types of spherical heterogeneities and a
thin plate suspending in a gas environment. It is then used to study the stress
distribution inside elastic bodies with non-spherical geometries, such as a
solid ellipsoid and a sintered structure. In these latter cases, it is shown
that the interplay between deformation and spatially variable surface curvature
leads to heterogeneous stress distribution across the specimen.
",0,1,0,0,0,0
19155,On the restriction theorem for paraboloid in $\mathbb R^4$,"  We prove that recent breaking by Zahl of the $\frac32$ barrier in Wolff's
estimate on the Kakeya maximal operator in $\mathbb R^4$ leads to improving the
$\frac{14}{5}$ threshold for the restriction problem for the paraboloid in
$\mathbb R^4$. One of the ingredients is a new trilinear estimate. The proofs
are deliberately presented in a nontechnical and concise format, so as to make
the arguments more readable and focus attention on the key tools.
",0,0,1,0,0,0
19156,"Existence of Stein Kernels under a Spectral Gap, and Discrepancy Bound","  We establish existence of Stein kernels for probability measures on
$\mathbb{R}^d$ satisfying a Poincaré inequality, and obtain bounds on the
Stein discrepancy of such measures. Applications to quantitative central limit
theorems are discussed, including a new CLT in Wasserstein distance $W_2$ with
optimal rate and dependence on the dimension. As a byproduct, we obtain a
stability version of an estimate of the Poincaré constant of probability
measures under a second moment constraint. The results extend more generally to
the setting of converse weighted Poincaré inequalities. The proof is based on
simple arguments of calculus of variations.
Further, we establish two general properties enjoyed by the Stein
discrepancy, holding whenever a Stein kernel exists: Stein discrepancy is
strictly decreasing along the CLT, and it controls the skewness of a random
vector.
",1,0,1,0,0,0
19157,High-order asynchrony-tolerant finite difference schemes for partial differential equations,"  Synchronizations of processing elements (PEs) in massively parallel
simulations, which arise due to communication or load imbalances between PEs,
significantly affect the scalability of scientific applications. We have
recently proposed a method based on finite-difference schemes to solve partial
differential equations in an asynchronous fashion -- synchronization between
PEs is relaxed at a mathematical level. While standard schemes can maintain
their stability in the presence of asynchrony, their accuracy is drastically
affected. In this work, we present a general methodology to derive
asynchrony-tolerant (AT) finite difference schemes of arbitrary order of
accuracy, which can maintain their accuracy when synchronizations are relaxed.
We show that there are several choices available in selecting a stencil to
derive these schemes and discuss their effect on numerical and computational
performance. We provide a simple classification of schemes based on the stencil
and derive schemes that are representative of different classes. Their
numerical error is rigorously analyzed within a statistical framework to obtain
the overall accuracy of the solution. Results from numerical experiments are
used to validate the performance of the schemes.
",0,1,0,0,0,0
19158,Hazard Analysis and Risk Assessment for an Automated Unmanned Protective Vehicle,"  For future application of automated vehicles in public traffic, ensuring
functional safety is essential. In this context, a hazard analysis and risk
assessment is an important input for designing functionally vehicle automation
systems. In this contribution, we present a detailed hazard analysis and risk
assessment (HARA) according to the ISO 26262 standard for a specific Level 4
application, namely an unmanned protective vehicle operated without human
supervision for motorway hard shoulder roadworks.
",1,0,0,0,0,0
19159,Detailed experimental and numerical analysis of a cylindrical cup deep drawing: pros and cons of using solid-shell elements,"  The Swift test was originally proposed as a formability test to reproduce the
conditions observed in deep drawing operations. This test consists on forming a
cylindrical cup from a circular blank, using a flat bottom cylindrical punch
and has been extensively studied using both analytical and numerical methods.
This test can also be combined with the Demeri test, which consists in cutting
a ring from the wall of a cylindrical cup, in order to open it afterwards to
measure the springback. This combination allows their use as benchmark test, in
order to improve the knowledge concerning the numerical simulation models,
through the comparison between experimental and numerical results. The focus of
this study is the experimental and numerical analyses of the Swift cup test,
followed by the Demeri test, performed with an AA5754-O alloy at room
temperature. In this context, a detailed analysis of the punch force evolution,
the thickness evolution along the cup wall, the earing profile, the strain
paths and their evolution and the ring opening is performed. The numerical
simulation is performed using the finite element code ABAQUS, with solid and
solid-shell elements, in order to compare the computational efficiency of these
type of elements. The results show that the solid-shell element is more
cost-effective than the solid, presenting global accurate predictions, excepted
for the thinning zones. Both the von Mises and the Hill48 yield criteria
predict the strain distributions in the final cup quite accurately. However,
improved knowledge concerning the stress states is still required, because the
Hill48 criterion showed difficulties in the correct prediction of the
springback, whatever the type of finite element adopted.
",0,1,0,0,0,0
19160,Hubble Frontier Fields: systematic errors in strong lensing models of galaxy clusters - Implications for cosmography,"  Strong gravitational lensing by galaxy clusters is a fundamental tool to
study dark matter and constrain the geometry of the Universe. Recently, the
Hubble Space Telescope Frontier Fields programme has allowed a significant
improvement of mass and magnification measurements but lensing models still
have a residual root mean square between 0.2 arcsec and few arcsec- onds, not
yet completely understood. Systematic errors have to be better understood and
treated in order to use strong lensing clusters as reliable cosmological
probes. We have analysed two simulated Hubble-Frontier-Fields-like clusters
from the Hubble Frontier Fields Comparison Challenge, Ares and Hera. We use
several estimators (relative bias on magnification, den- sity profiles,
ellipticity and orientation) to quantify the goodness of our reconstructions by
comparing our multiple models, optimized with the parametric software LENSTOOL
, with the input models. We have quantified the impact of systematic errors
arising, first, from the choice of different density profiles and
configurations and, secondly, from the availability of con- straints
(spectroscopic or photometric redshifts, redshift ranges of the background
sources) in the parametric modelling of strong lensing galaxy clusters and
therefore on the retrieval of cosmological parameters. We find that
substructures in the outskirts have a significant im- pact on the position of
the multiple images, yielding tighter cosmological contours. The need for
wide-field imaging around massive clusters is thus reinforced. We show that
competitive cosmological constraints can be obtained also with complex
multimodal clusters and that photometric redshifts improve the constraints on
cosmological parameters when considering a narrow range of (spectroscopic)
redshifts for the sources.
",0,1,0,0,0,0
19161,A Note on the Spectral Transfer Morphisms for Affine Hecke Algebras,"  E. Opdam introduced the tool of spectral transfer morphism (STM) of affine
Hecke algebras to study the formal degrees of unipotent discrete series
representations. He established a uniqueness property of STM for the affine
Hecke algebras associated of unipotent discrete series representations. Based
on this result, Opdam gave an explanation for Lusztig's arithmetic/geometric
correspondence (in Lusztig's classification of unipotent representations of
$p$-adic adjoint simple groups) in terms of harmonic analysis, and partitioned
the unipotent discrete series representations into $L$-packets based on the
Lusztig-Langlands parameters. The present paper provides some omitted details
for the argument of the uniqueness property of STM. In the last section, we
prove that three finite morphisms of algebraic tori are spectral transfer
morphisms, and hence complete the proof of the uniqueness property.
",0,0,1,0,0,0
19162,Batch-Expansion Training: An Efficient Optimization Framework,"  We propose Batch-Expansion Training (BET), a framework for running a batch
optimizer on a gradually expanding dataset. As opposed to stochastic
approaches, batches do not need to be resampled i.i.d. at every iteration, thus
making BET more resource efficient in a distributed setting, and when
disk-access is constrained. Moreover, BET can be easily paired with most batch
optimizers, does not require any parameter-tuning, and compares favorably to
existing stochastic and batch methods. We show that when the batch size grows
exponentially with the number of outer iterations, BET achieves optimal
$O(1/\epsilon)$ data-access convergence rate for strongly convex objectives.
Experiments in parallel and distributed settings show that BET performs better
than standard batch and stochastic approaches.
",1,0,0,0,0,0
19163,Coordinate Descent with Bandit Sampling,"  Coordinate descent methods usually minimize a cost function by updating a
random decision variable (corresponding to one coordinate) at a time. Ideally,
we would update the decision variable that yields the largest decrease in the
cost function. However, finding this coordinate would require checking all of
them, which would effectively negate the improvement in computational
tractability that coordinate descent is intended to afford. To address this, we
propose a new adaptive method for selecting a coordinate. First, we find a
lower bound on the amount the cost function decreases when a coordinate is
updated. We then use a multi-armed bandit algorithm to learn which coordinates
result in the largest lower bound by interleaving this learning with
conventional coordinate descent updates except that the coordinate is selected
proportionately to the expected decrease. We show that our approach improves
the convergence of coordinate descent methods both theoretically and
experimentally.
",1,0,0,1,0,0
19164,Closing the gap for pseudo-polynomial strip packing,"  The set of 2-dimensional packing problems builds an important class of
optimization problems and Strip Packing together with 2-dimensional Bin Packing
and 2-dimensional Knapsack is one of the most famous of these problems. Given a
set of rectangular axis parallel items and a strip with bounded width and
infinite height the objective is to find a packing of the items into the strip
which minimizes the packing height. We speak of pseudo-polynomial Strip Packing
if we consider algorithms with pseudo-polynomial running time with respect to
the width of the strip.
It is known that there is no pseudo-polynomial algorithm for Strip Packing
with a ratio better than $5/4$ unless $\mathrm{P} = \mathrm{NP}$. The best
algorithm so far has a ratio of $(4/3 + \varepsilon)$. In this paper, we close
this gap between inapproximability result and best known algorithm by
presenting an algorithm with approximation ratio $(5/4 + \varepsilon)$ and thus
categorize the problem accurately. The algorithm uses a structural result which
states that each optimal solution can be transformed such that it has one of a
polynomial number of different forms. The strength of this structural result is
that it applies to other problem settings as well for example to Strip Packing
with rotations (90 degrees) and Contiguous Moldable Task Scheduling. This fact
enabled us to present algorithms with approximation ratio $(5/4 + \varepsilon)$
for these problems as well.
",1,0,0,0,0,0
19165,"Notes on the replica symmetric solution of the classical and quantum SK model, including the matrix of second derivatives and the spin glass susceptibility","  A review of the replica symmetric solution of the classical and quantum,
infinite-range, Sherrington-Kirkpatrick spin glass is presented.
",0,1,0,0,0,0
19166,Self-supervised Knowledge Distillation Using Singular Value Decomposition,"  To solve deep neural network (DNN)'s huge training dataset and its high
computation issue, so-called teacher-student (T-S) DNN which transfers the
knowledge of T-DNN to S-DNN has been proposed. However, the existing T-S-DNN
has limited range of use, and the knowledge of T-DNN is insufficiently
transferred to S-DNN. To improve the quality of the transferred knowledge from
T-DNN, we propose a new knowledge distillation using singular value
decomposition (SVD). In addition, we define a knowledge transfer as a
self-supervised task and suggest a way to continuously receive information from
T-DNN. Simulation results show that a S-DNN with a computational cost of 1/5 of
the T-DNN can be up to 1.1\% better than the T-DNN in terms of classification
accuracy. Also assuming the same computational cost, our S-DNN outperforms the
S-DNN driven by the state-of-the-art distillation with a performance advantage
of 1.79\%. code is available on this https URL\_SVD.
",0,0,0,1,0,0
19167,Can justice be fair when it is blind? How social network structures can promote or prevent the evolution of despotism,"  Hierarchy is an efficient way for a group to organize, but often goes along
with inequality that benefits leaders. To control despotic behaviour, followers
can assess leaders decisions by aggregating their own and their neighbours
experience, and in response challenge despotic leaders. But in hierarchical
social networks, this interactional justice can be limited by (i) the high
influence of a small clique who are treated better, and (ii) the low
connectedness of followers. Here we study how the connectedness of a social
network affects the co-evolution of despotism in leaders and tolerance to
despotism in followers. We simulate the evolution of a population of agents,
where the influence of an agent is its number of social links. Whether a leader
remains in power is controlled by the overall satisfaction of group members, as
determined by their joint assessment of the leaders behaviour. We demonstrate
that centralization of a social network around a highly influential clique
greatly increases the level of despotism. This is because the clique is more
satisfied, and their higher influence spreads their positive opinion of the
leader throughout the network. Finally, our results suggest that increasing the
connectedness of followers limits despotism while maintaining hierarchy.
",0,0,0,0,1,0
19168,GSAE: an autoencoder with embedded gene-set nodes for genomics functional characterization,"  Bioinformatics tools have been developed to interpret gene expression data at
the gene set level, and these gene set based analyses improve the biologists'
capability to discover functional relevance of their experiment design. While
elucidating gene set individually, inter gene sets association is rarely taken
into consideration. Deep learning, an emerging machine learning technique in
computational biology, can be used to generate an unbiased combination of gene
set, and to determine the biological relevance and analysis consistency of
these combining gene sets by leveraging large genomic data sets. In this study,
we proposed a gene superset autoencoder (GSAE), a multi-layer autoencoder model
with the incorporation of a priori defined gene sets that retain the crucial
biological features in the latent layer. We introduced the concept of the gene
superset, an unbiased combination of gene sets with weights trained by the
autoencoder, where each node in the latent layer is a superset. Trained with
genomic data from TCGA and evaluated with their accompanying clinical
parameters, we showed gene supersets' ability of discriminating tumor subtypes
and their prognostic capability. We further demonstrated the biological
relevance of the top component gene sets in the significant supersets. Using
autoencoder model and gene superset at its latent layer, we demonstrated that
gene supersets retain sufficient biological information with respect to tumor
subtypes and clinical prognostic significance. Superset also provides high
reproducibility on survival analysis and accurate prediction for cancer
subtypes.
",0,0,0,1,1,0
19169,Natasha: Faster Non-Convex Stochastic Optimization Via Strongly Non-Convex Parameter,"  Given a nonconvex function that is an average of $n$ smooth functions, we
design stochastic first-order methods to find its approximate stationary
points. The convergence of our new methods depends on the smallest (negative)
eigenvalue $-\sigma$ of the Hessian, a parameter that describes how nonconvex
the function is.
Our methods outperform known results for a range of parameter $\sigma$, and
can be used to find approximate local minima. Our result implies an interesting
dichotomy: there exists a threshold $\sigma_0$ so that the currently fastest
methods for $\sigma>\sigma_0$ and for $\sigma<\sigma_0$ have different
behaviors: the former scales with $n^{2/3}$ and the latter scales with
$n^{3/4}$.
",1,0,1,1,0,0
19170,Higher-Order Bounded Model Checking,"  We present a Bounded Model Checking technique for higher-order programs. The
vehicle of our study is a higher-order calculus with general references. Our
technique is a symbolic state syntactical translation based on SMT solvers,
adapted to a setting where the values passed and stored during computation can
be functions of arbitrary order. We prove that our algorithm is sound, and
devise an optimisation based on points-to analysis to improve scalability. We
moreover provide a prototype implementation of the algorithm with experimental
results showcasing its performance.
",1,0,0,0,0,0
19171,Smoothing for the fractional Schrodinger equation on the torus and the real line,"  In this paper we study the cubic fractional nonlinear Schrodinger equation
(NLS) on the torus and on the real line. Combining the normal form and the
restricted norm methods we prove that the nonlinear part of the solution is
smoother than the initial data. Our method applies to both focusing and
defocusing nonlinearities. In the case of full dispersion (NLS) and on the
torus, the gain is a full derivative, while on the real line we get a
derivative smoothing with an $\epsilon$ loss. Our result lowers the regularity
requirement of a recent theorem of Kappeler et al. on the periodic defocusing
cubic NLS, and extends it to the focusing case and to the real line. We also
obtain estimates on the higher order Sobolev norms of the global smooth
solutions in the defocusing case.
",0,0,1,0,0,0
19172,Deep Learning Based Cryptographic Primitive Classification,"  Cryptovirological augmentations present an immediate, incomparable threat.
Over the last decade, the substantial proliferation of crypto-ransomware has
had widespread consequences for consumers and organisations alike. Established
preventive measures perform well, however, the problem has not ceased. Reverse
engineering potentially malicious software is a cumbersome task due to platform
eccentricities and obfuscated transmutation mechanisms, hence requiring
smarter, more efficient detection strategies. The following manuscript presents
a novel approach for the classification of cryptographic primitives in compiled
binary executables using deep learning. The model blueprint, a DCNN, is
fittingly configured to learn from variable-length control flow diagnostics
output from a dynamic trace. To rival the size and variability of contemporary
data compendiums, hence feeding the model cognition, a methodology for the
procedural generation of synthetic cryptographic binaries is defined, utilising
core primitives from OpenSSL with multivariate obfuscation, to draw a vastly
scalable distribution. The library, CryptoKnight, rendered an algorithmic pool
of AES, RC4, Blowfish, MD5 and RSA to synthesis combinable variants which are
automatically fed in its core model. Converging at 91% accuracy, CryptoKnight
is successfully able to classify the sample algorithms with minimal loss.
",1,0,0,0,0,0